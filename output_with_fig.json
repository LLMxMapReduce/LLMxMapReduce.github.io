{"title": "Recent Advances in Direct Speech-to-text Translation", "cost_time": "00:12:56", "block_cycle_count": 1, "block_avg_score": [[8.641428571428571, 8.6935]], "self_refine_score": [[[9.4, 9.2, 8.2], [8.6, 8.42, 7.8], [9.75, 9.75, 8.6]]], "skeleton_batch_size": 2, "digest_batch_size": 3, "conv_layer": 1, "receptive_field": 3, "top_k": 6, "result_num": 10, "best_of": 3, "refine_count": 3, "cite_ratio": 1.0, "outline": "# Recent Advances in Direct Speech-to-text Translation\n\n## 1. Introduction\n Extract the definition, motivations, and significance of direct speech-to-text translation. Identify the advantages of direct approaches over traditional cascaded systems (ASR + MT), such as reduced latency, streamlined pipelines, and mitigation of error propagation. Note the application scenarios and the scope of the survey. Cite the source paper(s) from which the information is extracted.\nSynthesize a compelling introduction that defines direct speech-to-text translation and highlights its importance in the field. Emphasize the shift from cascaded to end-to-end approaches and articulate the potential benefits of direct ST, setting the stage for the survey by outlining its scope and objectives ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. Compare and contrast the motivations and advantages of direct ST as presented in different papers by using the extracted information from Digest Construction. Highlight similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in the motivations for direct ST. Summarize the current state of knowledge in this sub-field. Identify the key challenges and limitations in the introduction section. Analyze the underlying reasons and root causes for these challenges, going beyond superficial descriptions to identify fundamental issues. Consider interdisciplinary perspectives from fields like ASR, MT, NLP, and Computer Vision when analyzing the challenges and suggesting future research directions. Based on the analysis of current techniques and their limitations, identify potential knowledge gaps and suggest specific directions for future research in the introduction to direct speech-to-text translation.\n\n## 2. Background: Evolution of Speech Translation Paradigms\nBuilding upon the introduction, extract information from survey papers and introductory sections to trace the historical evolution of speech translation paradigms. Detail the characteristics and limitations of traditional cascaded models, focusing on ASR and MT components, error propagation, and separate training objectives. Contrast these with the theoretical and practical advantages of end-to-end models, emphasizing joint training and direct mapping from speech to text. Cite the source paper(s) from which the information is extracted.\nBuilding upon the introduction, this chapter will delve into the historical evolution of speech translation paradigms. Using the extracted information from Digest Construction, organize the historical progression of speech translation, contrasting cascaded and end-to-end approaches chronologically. Analyze the inherent limitations of cascaded models, such as error accumulation and complex pipelines. Articulate how end-to-end models emerged as a solution to these limitations, highlighting their simplified architecture and potential for improved performance through joint optimization. This section should provide a clear understanding of the evolution leading to the current focus on end-to-end direct ST ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. Systematically compare and contrast the characteristics and limitations of cascaded and end-to-end models as described in different sources by highlighting similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in the evolution of speech translation paradigms. Summarize the current state of knowledge in this sub-field. Identify the key challenges and limitations in the background section. Analyze the underlying reasons and root causes for these challenges, going beyond superficial descriptions to identify fundamental issues. Consider interdisciplinary perspectives from fields like ASR, MT, NLP, and Computer Vision when analyzing the challenges and suggesting future research directions. Based on the analysis of current techniques and their limitations, identify potential knowledge gaps and suggest specific directions for future research in the evolution of speech translation paradigms.\n\n## 3. Techniques for Direct Speech-to-text Translation: Emphasizing Pre-training Strategies and Encoder Design Challenges\nFollowing the historical context, broadly categorize and extract information on various techniques employed to improve direct speech-to-text translation systems, with a focus on pre-training strategies and encoder design. This includes pre-training strategies, data augmentation methods, and architectural innovations. For each technique, note the core methodology, datasets used, experimental results, and reported improvements. For pre-training strategies, extract the specific transfer learning paradigms (homogeneous, heterogeneous) applied. Cite the source paper(s) from which the information is extracted.\nFollowing the historical context, this chapter will explore various techniques to improve direct speech-to-text translation systems, with a focus on pre-training strategies and encoder design challenges. Using the extracted information from Digest Construction, organize the diverse techniques into logical subcategories like encoder design challenges, pre-training, data augmentation, and architectural innovations. Analyze the effectiveness of each category of techniques based on the synthesis of reported results across different papers. Identify common themes, contrasting approaches, and the overall impact of these techniques on advancing direct ST performance. For pre-training strategies, analyze their effectiveness through the lens of transfer learning principles, categorizing and comparing pre-training strategies based on transfer learning types, such as feature-based transfer and parameter-based transfer ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation', 'stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders', 'a_comprehensive_survey_on_transfer_learning']. Systematically compare and contrast the effectiveness of different techniques by highlighting similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in techniques for direct ST. Summarize the current state of knowledge in this sub-field. Identify the key challenges and limitations of techniques for direct speech-to-text translation. Analyze the underlying reasons and root causes for these challenges, going beyond superficial descriptions to identify fundamental issues. Consider interdisciplinary perspectives from fields like ASR, MT, NLP, and Computer Vision when analyzing the challenges and suggesting future research directions. Based on the analysis of current techniques and their limitations, identify potential knowledge gaps and suggest specific directions for future research in techniques for direct speech-to-text translation.\n\n### 3.1 Challenges in ST Encoder Design\nExtract details about the inherent difficulties in designing effective ST encoders, such as 'Modeling deficiency' and 'Representation inconsistency', as discussed in relevant papers. Cite the source paper(s) from which the information is extracted.\nUsing the extracted information from Digest Construction, analyze the identified challenges in ST encoder design, such as 'Modeling deficiency' and 'Representation inconsistency'. Discuss how these challenges motivate the development of pre-training strategies and architectural innovations that are explored in the subsequent subsections ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']. Systematically compare and contrast different perspectives on encoder design challenges by highlighting similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in encoder design challenges. Summarize the current state of knowledge in this sub-field. Identify the key challenges and limitations of ST encoder design. Analyze the underlying reasons and root causes for these challenges, going beyond superficial descriptions to identify fundamental issues. Consider interdisciplinary perspectives from fields like ASR, MT, NLP, and Computer Vision when analyzing the challenges and suggesting future research directions. Based on the analysis of current techniques and their limitations, identify potential knowledge gaps and suggest specific directions for future research in ST encoder design.\n\n### 3.2 Pre-training Strategies\nPre-training strategies are crucial techniques to address the challenges in direct ST. This section will explore various pre-training methods. Focus on extracting details about different pre-training strategies applied to direct ST models. Gather information on pre-training tasks (e.g., ASR, MT, self-supervised), types of pre-training data (e.g., transcribed speech, text data, unlabeled speech), model architectures, and transfer learning methodologies. Note the reported performance gains in ST from various pre-training approaches, especially in low-resource scenarios. Extract the specific transfer learning paradigms (homogeneous, heterogeneous) applied in each pre-training method. Extract details about techniques that reuse subnets, ensure consistency in semantic representation and sequence length, and improve the transfer of knowledge from pre-training to the ST task. Extract specific details about the SATE and FATE methods, including their motivations for addressing modeling deficiency and representation inconsistency, detailed architectures, the role of adaptor modules and multi-teacher knowledge distillation (for SATE), the FAT-MLM pre-training objective (for FATE), FAT-ST model architecture (for FATE), and experimental results for both. Cite the source paper(s) from which the information is extracted.\nPre-training strategies are crucial techniques to address the challenges in direct ST. This section will explore various pre-training methods. Using the extracted information from Digest Construction, categorize and compare different pre-training strategies, such as adversarial pre-training with ASR and MT data, self-supervised pre-training, curriculum pre-training, multimodal pre-training and transfer learning paradigms. Analyze their effectiveness in improving direct ST performance, particularly in low-resource settings. Discuss the transferability of knowledge from related tasks and modalities, and evaluate the advantages and disadvantages of each pre-training approach in terms of data requirements, computational cost, and performance gains. Analyze the effectiveness of these techniques through the lens of transfer learning principles, categorizing and comparing pre-training strategies based on transfer learning types, such as feature-based transfer and parameter-based transfer ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data', 'pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation', 'analyzing_asr_pretraining_for_low_resource_speech_to_text_translation', 'self_supervised_representations_improve_end_to_end_speech_translation', 'investigating_self_supervised_pre_training_for_end_to_end_speech_translation', 'bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation', 'curriculum_pre_training_for_end_to_end_speech_translation', 'a_comprehensive_survey_on_transfer_learning', 'bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation', 'stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders', 'fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. Systematically compare and contrast the findings and methodologies of different papers discussed in this section by highlighting the similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in pre-training strategies. Summarize the current state of knowledge in this sub-field. Identify the key challenges and limitations of pre-training strategies in direct ST. Analyze the underlying reasons and root causes for these challenges, going beyond superficial descriptions to identify fundamental issues. Consider interdisciplinary perspectives from fields like ASR, MT, NLP, and Computer Vision when analyzing the challenges and suggesting future research directions. Based on the analysis of current techniques and their limitations, identify potential knowledge gaps and suggest specific directions for future research in pre-training strategies. Transfer learning paradigms are implicitly covered within the discussions of different pre-training techniques in this section.\n\n#### 3.2.1 Adversarial Pre-training with ASR and MT Data\nSpecifically extract details on pre-training methods that leverage ASR and MT datasets using adversarial regularization. Investigate how encoder and decoder components are pre-trained using these modalities, including techniques to bridge the modality gap between speech and text. Note the use of adversarial regularization, adaptor modules, and knowledge distillation in these pre-training strategies. Cite the source paper(s) from which the information is extracted.\nBased on the extracted pre-training tasks and data from Digest Construction, analyze and synthesize information on adversarial pre-training with ASR and MT data. Compare strategies for pre-training encoders with ASR data and decoders with MT data. Discuss the modality gap challenge and how methods like adversarial regularizers and adaptor modules address it. Evaluate the individual and combined effectiveness of ASR and MT pre-training on direct ST performance ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data', 'stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']. Systematically compare and contrast the adversarial pre-training methods using ASR and MT data by highlighting similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in adversarial pre-training. Summarize the current state of knowledge in this sub-field. Identify the key challenges and limitations of adversarial pre-training with ASR and MT data. Analyze the underlying reasons and root causes for these challenges, going beyond superficial descriptions to identify fundamental issues. Consider interdisciplinary perspectives from fields like ASR, MT, NLP, and Computer Vision when analyzing the challenges and suggesting future research directions. Based on the analysis of current techniques and their limitations, identify potential knowledge gaps and suggest specific directions for future research in adversarial pre-training.\n\n#### 3.2.2 Self-Supervised Pre-training\nExtract details about self-supervised pre-training techniques like Masked Acoustic Modeling (MAM) and wav2vec. Describe the pre-training tasks, model architectures, use of unlabeled speech data, and reported performance gains in ST. Note the advantages of self-supervised methods in low-resource scenarios and for leveraging untranscribed speech. Cite the source paper(s) from which the information is extracted.\nBased on the extracted pre-training tasks and data from Digest Construction, analyze self-supervised pre-training methods as a means to leverage untranscribed speech data and improve performance, especially in low-resource scenarios. Compare different self-supervised approaches like MAM and wav2vec, focusing on their pre-training objectives and effectiveness. Discuss their impact on reducing reliance on labeled data and enhancing the robustness of ST models ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation', 'wav2vec_unsupervised_pre_training_for_speech_recognition', 'self_supervised_representations_improve_end_to_end_speech_translation', 'investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. Systematically compare and contrast different self-supervised pre-training techniques like MAM and wav2vec by highlighting similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in self-supervised pre-training. Summarize the current state of knowledge in this sub-field. Identify the key challenges and limitations of self-supervised pre-training. Analyze the underlying reasons and root causes for these challenges, going beyond superficial descriptions to identify fundamental issues. Consider interdisciplinary perspectives from fields like ASR, MT, NLP, and Computer Vision when analyzing the challenges and suggesting future research directions. Based on the analysis of current techniques and their limitations, identify potential knowledge gaps and suggest specific directions for future research in self-supervised pre-training.\n\n#### 3.2.3 Curriculum Pre-training\nExtract information about curriculum pre-training approaches, focusing on the staged learning process, different courses (e.g., transcription, understanding, cross-lingual mapping), and the incorporation of linguistic knowledge. Note the reported performance improvements and comparisons to traditional ASR pre-training. Cite the source paper(s) from which the information is extracted.\nBased on the extracted pre-training tasks and data from Digest Construction, evaluate the effectiveness of curriculum pre-training in injecting linguistic knowledge into ST models. Analyze the benefits of staged learning and compare it with simpler pre-training methods. Discuss whether curriculum pre-training is particularly beneficial for complex tasks like ST and analyze its impact on performance compared to other pre-training techniques ['curriculum_pre_training_for_end_to_end_speech_translation']. Systematically compare and contrast curriculum pre-training with other pre-training methods by highlighting similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in curriculum pre-training. Summarize the current state of knowledge in this sub-field. Identify the key challenges and limitations of curriculum pre-training. Analyze the underlying reasons and root causes for these challenges, going beyond superficial descriptions to identify fundamental issues. Consider interdisciplinary perspectives from fields like ASR, MT, NLP, and Computer Vision when analyzing the challenges and suggesting future research directions. Based on the analysis of current techniques and their limitations, identify potential knowledge gaps and suggest specific directions for future research in curriculum pre-training.\n\n#### 3.2.4 Multimodal Pre-training\nExtract details about multimodal pre-training approaches that leverage both speech and text modalities during pre-training. Focus on methods that aim to learn unified representations and bridge the modality gap more effectively by utilizing both speech and text modalities. Cite the source paper(s) from which the information is extracted.\nBased on the extracted pre-training tasks and data from Digest Construction, analyze multimodal pre-training approaches and their effectiveness in learning unified representations and bridging the modality gap. Evaluate how utilizing both speech and text modalities during pre-training impacts the performance of direct ST models. Discuss the advantages and challenges of multimodal pre-training compared to unimodal pre-training approaches ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. Systematically compare and contrast multimodal pre-training with unimodal pre-training approaches by highlighting similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in multimodal pre-training. Summarize the current state of knowledge in this sub-field. Identify the key challenges and limitations of multimodal pre-training. Analyze the underlying reasons and root causes for these challenges, going beyond superficial descriptions to identify fundamental issues. Consider interdisciplinary perspectives from fields like ASR, MT, NLP, and Computer Vision when analyzing the challenges and suggesting future research directions. Based on the analysis of current techniques and their limitations, identify potential knowledge gaps and suggest specific directions for future research in multimodal pre-training.\n\n### 3.3 Data Augmentation Techniques\nIdentify and extract details about data augmentation techniques used in direct ST. Note the types of augmentations applied (e.g., noise injection, speed perturbation, SpecAugment, pseudo-data generation), and how these techniques are used to address data scarcity and improve model robustness. Extract information on the GigaST corpus as a large-scale pseudo-ST corpus. Cite the source paper(s) from which the information is extracted.\nUsing the extracted information from Digest Construction, summarize common data augmentation techniques used in direct ST, including pseudo-data generation (e.g., leveraging GigaST corpus) and noise injection. Analyze their effectiveness in improving performance, particularly in low-resource scenarios. Discuss the use of pseudo-data augmentation and evaluate the methodology and impact of corpora like GigaST. Analyze the complementarity of data augmentation with other techniques like pre-training ['gigast_a_10000_hour_pseudo_speech_translation_corpus', 'analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. Systematically compare and contrast different data augmentation techniques by highlighting similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in data augmentation. Summarize the current state of knowledge in this sub-field. Identify the key challenges and limitations of data augmentation techniques. Analyze the underlying reasons and root causes for these challenges, going beyond superficial descriptions to identify fundamental issues. Consider interdisciplinary perspectives from fields like ASR, MT, NLP, and Computer Vision when analyzing the challenges and suggesting future research directions. Based on the analysis of current techniques and their limitations, identify potential knowledge gaps and suggest specific directions for future research in data augmentation.\n\n### 3.4 Architectural Innovations and Key Components\nFocus on papers proposing novel architectures for end-to-end ST, as well as innovative modules and mechanisms. Extract details about encoder-decoder structures, attention mechanisms, and specific modules designed to address challenges like the modality gap. Note innovations like decoupled encoders, shrink mechanisms, neural acoustic feature modeling, and adaptor modules. Cite the source paper(s) from which the information is extracted.\nUsing the extracted information from Digest Construction, categorize and compare architectural innovations and key components proposed for direct ST. Analyze the effectiveness of approaches like decoupled encoders, mechanisms for bridging the modality gap, and adaptor modules for cross-modal representation bridging. Discuss the potential of neural acoustic feature modeling and the development of efficient and compact models. Identify common architectural trends and potential areas for future improvements ['bridging_the_modality_gap_for_speech_to_text_translation', 'revisiting_end_to_end_speech_to_text_translation_from_scratch', 'end_to_end_automatic_speech_translation_of_audiobooks', 'stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']. Systematically compare and contrast different architectural innovations and key components by highlighting similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in architectural innovations. Summarize the current state of knowledge in this sub-field. Identify the key challenges and limitations of current architectural innovations. Analyze the underlying reasons and root causes for these challenges, going beyond superficial descriptions to identify fundamental issues. Consider interdisciplinary perspectives from fields like ASR, MT, NLP, and Computer Vision when analyzing the challenges and suggesting future research directions. Based on the analysis of current techniques and their limitations, identify potential knowledge gaps and suggest specific directions for future research in architectural innovations and key components.\n\n#### 3.4.1 Addressing the Modality Gap\nExtract details about specific architectures designed to address the modality gap between speech and text, such as the STAST model, and different adaptor modules and mechanisms used to bridge the modality gap between acoustic and textual representations. Note the use of decoupled encoders, shrink mechanisms, and cross-modal adaptation methods, and specific adaptor modules. Cite the source paper(s) from which the information is extracted.\nUsing the extracted information from Digest Construction, analyze specific architectural approaches like STAST designed to address the modality gap and the role and effectiveness of adaptor modules in cross-modal information transfer and representation alignment. Evaluate the effectiveness of their components in bridging this gap and improving translation quality. Discuss the significance of these innovations for future architectural design in direct ST ['bridging_the_modality_gap_for_speech_to_text_translation', 'stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']. Systematically compare and contrast different architectural approaches and adaptor modules for addressing the modality gap by highlighting similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in addressing the modality gap. Summarize the current state of knowledge in this sub-field. Identify the key challenges and limitations of current approaches to address the modality gap. Analyze the underlying reasons and root causes for these challenges, going beyond superficial descriptions to identify fundamental issues. Consider interdisciplinary perspectives from fields like ASR, MT, NLP, and Computer Vision when analyzing the challenges and suggesting future research directions. Based on the analysis of current techniques and their limitations, identify potential knowledge gaps and suggest specific directions for future research in addressing the modality gap.\n\n#### 3.4.2 Efficient and Compact Models\nExtract information about approaches to develop efficient and compact direct ST models, noting techniques like model compression and specific architectural choices for efficiency. Cite the source paper(s) from which the information is extracted.\nUsing the extracted information from Digest Construction, analyze the techniques used to create efficient and compact direct ST models. Discuss the importance of model efficiency for real-world applications and compare different efficiency-focused techniques and their impact on model performance ['end_to_end_automatic_speech_translation_of_audiobooks']. Systematically compare and contrast different techniques for creating efficient and compact models by highlighting similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in efficient and compact models. Summarize the current state of knowledge in this sub-field. Identify the key challenges and limitations of efficient and compact models. Analyze the underlying reasons and root causes for these challenges, going beyond superficial descriptions to identify fundamental issues. Consider interdisciplinary perspectives from fields like ASR, MT, NLP, and Computer Vision when analyzing the challenges and suggesting future research directions. Based on the analysis of current techniques and their limitations, identify potential knowledge gaps and suggest specific directions for future research in efficient and compact models.\n\n## 4. Datasets and Evaluation Metrics for Direct Speech-to-text Translation\nBuilding upon the previous chapter discussing techniques, this chapter will focus on datasets and evaluation metrics. Identify and categorize datasets used for training and evaluating direct ST models. Extract details about dataset characteristics (size, language pairs, domain), creation methodologies, and their role in advancing research. Focus on corpora like MuST-C and GigaST, and note the datasets used in evaluation campaigns like IWSLT. Also, extract information about evaluation metrics commonly used for direct ST. Cite the source paper(s) from which the information is extracted.\nBuilding upon the previous chapter, analyze the landscape of datasets and evaluation metrics in direct ST research by using the extracted information from Digest Construction. Discuss the characteristics and contributions of key datasets like MuST-C and GigaST. Evaluate the role of evaluation campaigns like IWSLT in driving progress and standardizing evaluation. Discuss the challenges and limitations of current datasets and evaluation metrics and suggest potential directions for improvement ['must_c_a_multilingual_speech_translation_corpus', 'gigast_a_10000_hour_pseudo_speech_translation_corpus', 'findings_of_the_iwslt_2022_evaluation_campaign', 'librispeech_an_asr_corpus_based_on_public_domain_audio_books']. Systematically compare and contrast different datasets and benchmarks by highlighting similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in datasets and evaluation metrics. Summarize the current state of knowledge in this sub-field. Based on the analysis of current datasets and benchmarks and their limitations, identify potential knowledge gaps and suggest specific directions for future research in datasets and evaluation metrics for direct speech-to-text translation.\n\n### 4.1 Multilingual Speech Translation Corpora\nFocus on multilingual corpora like MuST-C. Extract details about language coverage, size, creation methodology (e.g., TED Talks, alignment process), and quality verification. Cite the source paper(s) from which the information is extracted.\nConsidering the datasets identified in Digest Construction, analyze the benefits of multilingual corpora for direct ST research. Discuss how these corpora facilitate multilingual model development and cross-lingual transfer learning. Evaluate the quality and scalability of corpus creation methodologies, using MuST-C as a case study ['must_c_a_multilingual_speech_translation_corpus']. Systematically compare and contrast different multilingual speech translation corpora by highlighting similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in multilingual corpora. Summarize the current state of knowledge in this sub-field. Based on the analysis of current multilingual corpora and their limitations, identify potential knowledge gaps and suggest specific directions for future research in multilingual speech translation corpora.\n\n### 4.2 Large-scale Pseudo Speech Translation Corpora\nFocus on pseudo ST corpora like GigaST. Extract details about their creation process, source ASR corpora, translation methods (MT), size, and quality verification. Cite the source paper(s) from which the information is extracted.\nConsidering the datasets identified in Digest Construction, analyze the use of pseudo-data for ST and evaluate the methodology and effectiveness of large-scale pseudo-ST corpora like GigaST in improving model performance. Discuss the potential and limitations of this approach ['gigast_a_10000_hour_pseudo_speech_translation_corpus']. Systematically compare and contrast different large-scale pseudo speech translation corpora by highlighting similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in pseudo speech translation corpora. Summarize the current state of knowledge in this sub-field. Based on the analysis of current pseudo speech translation corpora and their limitations, identify potential knowledge gaps and suggest specific directions for future research in pseudo speech translation corpora.\n\n### 4.3 Benchmarking and Evaluation Campaigns (IWSLT)\nExtract information about benchmarking efforts, particularly the IWSLT evaluation campaign. Note the shared tasks related to speech translation, datasets, evaluation metrics, and key findings from recent campaigns. Cite the source paper(s) from which the information is extracted.\nConsidering the evaluation metrics identified in Digest Construction, analyze the role of benchmarking campaigns like IWSLT in driving progress in direct ST. Summarize the tasks, trends, and challenges highlighted by these campaigns. Discuss their impact on standardizing evaluation and fostering innovation ['findings_of_the_iwslt_2022_evaluation_campaign']. Systematically compare and contrast different benchmarking and evaluation campaigns by highlighting similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in benchmarking and evaluation campaigns. Summarize the current state of knowledge in this sub-field. Based on the analysis of current benchmarking and evaluation campaigns and their limitations, identify potential knowledge gaps and suggest specific directions for future research in benchmarking and evaluation campaigns.\n\n## 5. Toolkits and Frameworks for Direct Speech-to-text Translation\nMoving from datasets to implementation, this chapter will identify toolkits and frameworks. Identify and describe available toolkits and frameworks that support direct ST development. Focus on their functionalities, ease of use, modularity, supported tasks, and availability of pre-trained models. Extract details about toolkits like ESPnet-ST. Cite the source paper(s) from which the information is extracted.\nMoving from datasets to implementation and using the extracted information from Digest Construction, compare and contrast different toolkits and frameworks for direct ST. Analyze the features and advantages of integrated toolkits like ESPnet-ST in streamlining development and promoting reproducibility. Discuss the importance of toolkits in democratizing access to ST technology and advancing research ['espnet_st_all_in_one_speech_translation_toolkit']. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in toolkits and frameworks. Summarize the current state of knowledge in this sub-field. Based on the analysis of current toolkits and frameworks and their limitations, identify potential knowledge gaps and suggest specific directions for future research in toolkits and frameworks for direct speech-to-text translation.\n\n### 5.1 Open-Source Toolkits vs. Commercial Toolkits for ST\nExtract information about open-source and commercial toolkits for speech translation, focusing on their availability, cost, customizability, community support, and specific features. Cite the source paper(s) from which the information is extracted.\nBased on the extracted features from Digest Construction, compare and contrast open-source and commercial toolkits for ST. Analyze the advantages and disadvantages of each type for different research and development scenarios. Synthesize the collective findings to identify overarching patterns and trends in the types of toolkits available. Summarize the current state of knowledge in this sub-field. Identify the key challenges and limitations of open-source and commercial toolkits for ST. Analyze the underlying reasons and root causes for these challenges, going beyond superficial descriptions to identify fundamental issues. Consider interdisciplinary perspectives from fields like ASR, MT, NLP, and Computer Vision when analyzing the challenges and suggesting future research directions. Based on the analysis of current toolkits and their limitations, identify potential knowledge gaps and suggest specific directions for future research in open-source and commercial toolkits for ST.\n\n### 5.2 Feature Comparison of Popular ST Toolkits (e.g., ESPnet-ST, Fairseq-ST)\nExtract detailed feature lists for popular ST toolkits such as ESPnet-ST and Fairseq-ST, including supported models, pre-training capabilities, data handling, training pipelines, and evaluation metrics. Cite the source paper(s) from which the information is extracted.\nBased on the extracted feature lists from Digest Construction, compare and contrast the features of popular ST toolkits like ESPnet-ST and Fairseq-ST. Analyze their strengths and weaknesses based on feature comparisons and discuss which toolkits are better suited for specific research tasks or development needs. Synthesize the collective findings to identify overarching patterns and trends in popular ST toolkits. Summarize the current state of knowledge in this sub-field. Identify the key challenges and limitations of popular ST toolkits. Analyze the underlying reasons and root causes for these challenges, going beyond superficial descriptions to identify fundamental issues. Consider interdisciplinary perspectives from fields like ASR, MT, NLP, and Computer Vision when analyzing the challenges and suggesting future research directions. Based on the analysis of current toolkits and their limitations, identify potential knowledge gaps and suggest specific directions for future research in popular ST toolkits.\n\n### 5.3 Toolkits for Specific Tasks in Direct ST (e.g., Low-Resource ST Toolkits)\nIdentify and extract information about toolkits specifically designed or optimized for particular tasks in direct ST, such as low-resource ST, multilingual ST, or real-time ST. Cite the source paper(s) from which the information is extracted.\nBased on the extracted information from Digest Construction, analyze toolkits designed for specific tasks in direct ST, such as low-resource scenarios. Discuss how these specialized toolkits address the unique challenges of their target tasks and evaluate their effectiveness. Synthesize the collective findings to identify overarching patterns and trends in specialized ST toolkits. Summarize the current state of knowledge in this sub-field. Identify the key challenges and limitations of toolkits for specific tasks in direct ST. Analyze the underlying reasons and root causes for these challenges, going beyond superficial descriptions to identify fundamental issues. Consider interdisciplinary perspectives from fields like ASR, MT, NLP, and Computer Vision when analyzing the challenges and suggesting future research directions. Based on the analysis of current toolkits and their limitations, identify potential knowledge gaps and suggest specific directions for future research in toolkits for specific tasks in direct ST.\n\n## 6. Challenges and Future Directions in Direct Speech-to-text Translation\nConcluding the survey, this chapter will discuss challenges and future directions. Identify the limitations and unresolved issues in direct ST research. Based on the surveyed papers and evaluation campaign findings, extract broader open challenges and specific potential future research directions suggested in the papers. Cite the source paper(s) from which the information is extracted.\nConcluding the survey, this chapter will synthesize the identified challenges and propose future research directions. Using the identified limitations from Digest Construction, synthesize the identified challenges and limitations into a coherent discussion of the current bottlenecks in direct ST research. Propose future research directions from a holistic perspective, aiming for innovative and impactful solutions. Ensure future directions are specific and actionable, outlining concrete steps or methodologies. Consider adopting successful methodologies from the history of speech processing, machine translation, and other relevant disciplines, and explore analogies or inspirations from other fields. Future directions should include improving low-resource translation (analyzing why pre-trained models are still insufficient), handling noisy speech, enhancing robustness, exploring novel architectures and pre-training strategies (e.g., more effective cross-lingual transfer techniques, advanced pseudo-data generation methods, different discriminator architectures for adversarial pre-training), and addressing specific task challenges highlighted in evaluation campaigns. Future directions should also include addressing sequence length inconsistency in adaptor modules and exploring further improvements to cross-modal representation alignment ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are', 'findings_of_the_iwslt_2022_evaluation_campaign', 'effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data', 'gigast_a_10000_hour_pseudo_speech_translation_corpus', 'analyzing_asr_pretraining_for_low_resource_speech_to_text_translation', 'stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders', 'fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. Systematically compare and contrast the challenges and future directions from different surveyed papers by highlighting similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in challenges and future directions. Summarize the current state of knowledge in this sub-field regarding challenges and future directions. Identify the key challenges and limitations in the current research of direct speech-to-text translation. Analyze the underlying reasons and root causes for these challenges, going beyond superficial descriptions to identify fundamental issues. Consider interdisciplinary perspectives from fields like ASR, MT, NLP, and Computer Vision when analyzing the challenges and suggesting future research directions. Based on the analysis of current techniques and their limitations, identify potential knowledge gaps and suggest specific directions for future research in direct speech-to-text translation.\n\n### 6.1 6.x Ethical and Societal Implications\nExtract information related to the ethical and societal implications of direct speech-to-text translation technology discussed in the surveyed papers. Identify potential biases, fairness issues, accessibility concerns, and the broader societal impact of this technology. Cite the source paper(s) from which the information is extracted.\nAnalyze the ethical and societal implications of direct speech-to-text translation based on the extracted information from Digest Construction. Discuss potential biases in datasets or models, fairness considerations across different demographic groups, and accessibility impacts for diverse users.  Evaluate the broader societal consequences of widespread direct ST technology, both positive and negative. Based on the analysis, suggest future research directions that address these ethical and societal concerns, ensuring responsible development and deployment of direct speech-to-text translation.\n\n## 7. Conclusion\nFinally, this survey will conclude by summarizing key advances and looking forward. Extract the key takeaways and summarizing statements from the surveyed papers regarding recent advances in direct ST. Identify the main contributions and the overall outlook for the field. Cite the source paper(s) from which the information is extracted.\nFinally, summarize the key advances in direct speech-to-text translation highlighted in the survey, reiterating the significance of these advancements by using the key takeaways from Digest Construction. Provide a forward-looking perspective on the future of direct ST, based on the synthesized information, identified future directions, and the ongoing research efforts in the field ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. Systematically compare and contrast the conclusions from different surveyed papers by highlighting similarities, differences, and potential contradictions among them. Synthesize the collective findings from the digested papers to identify overarching patterns and trends in the conclusions. Summarize the current state of knowledge in the conclusion section. Identify the key limitations in the conclusions of current research of direct speech-to-text translation. Analyze the underlying reasons and root causes for these limitations, going beyond superficial descriptions to identify fundamental issues. Consider interdisciplinary perspectives from fields like ASR, MT, NLP, and Computer Vision when analyzing the challenges and suggesting future research directions. Based on the analysis of current conclusions and their limitations, identify potential knowledge gaps and suggest specific directions for future research in the conclusions of direct speech-to-text translation survey.", "outline_suggestion": "1. Refocus Section 3 to emphasize pre-training, encoder challenges and integrate transfer learning perspective:\n- Modify the title of Section 3 from \"Techniques for Direct Speech-to-text Translation\" to \"Techniques for Direct Speech-to-text Translation: Emphasizing Pre-training Strategies and Encoder Design Challenges\" to better highlight the significance of these aspects in current research and align with the insights from digested papers ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation', 'stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders'].\n- Add a new subsection \"3.0 Challenges in ST Encoder Design\" before \"3.1 Pre-training Strategies\" to explicitly address the inherent difficulties in designing effective ST encoders, such as 'Modeling deficiency' and 'Representation inconsistency' as pointed out by ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']. The Digest Construction instruction for this subsection should focus on extracting details about these challenges from relevant papers. The Digest Analysis should then analyze how these challenges motivate the development of pre-training and architectural innovations in subsequent subsections.\n- Refine Digest Construction and Digest Analysis instructions for all subsections under Section 3 to explicitly incorporate a transfer learning perspective based on the principles discussed in ['a_comprehensive_survey_on_transfer_learning']. For instance, in \"3.1 Pre-training Strategies\", Digest Construction should include extracting the specific transfer learning paradigms (homogeneous, heterogeneous) applied in each pre-training method. Digest Analysis should then analyze the effectiveness of these techniques through the lens of transfer learning principles, categorizing and comparing pre-training strategies based on transfer learning types, such as feature-based transfer and parameter-based transfer.\n\n2. Expand Pre-training Strategies section to include multimodal pre-training and transfer learning paradigms:\n- Add a new sub-subsection \"3.1.4 Multimodal Pre-training\" to Section 3.1 \"Pre-training Strategies\" to specifically discuss and analyze pre-training approaches that leverage multiple modalities, such as speech and text, during pre-training. This addition is motivated by methods like FAT-MLM which aim to learn unified representations and bridge the modality gap more effectively by utilizing both speech and text modalities as described in ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n- Rename sub-subsection \"3.1.1 Pre-training with ASR and MT Data\" to \"3.1.1 Adversarial Pre-training with ASR and MT Data\" to specifically highlight the adversarial regularization technique used to align ASR encoder and MT decoder representations, a key method discussed in papers like ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. This renaming will provide a more accurate and focused description of the subsection's content.\n- Expand Section 3.1 \"Pre-training Strategies\" by adding a subsection \"3.1.7 Transfer Learning Paradigms in ST Pre-training\". This new subsection should provide Digest Construction instructions to extract information on how different transfer learning paradigms (instance-based, feature-based, parameter-based) are applied in pre-training direct ST models, drawing upon the comprehensive overview of transfer learning paradigms in ['a_comprehensive_survey_on_transfer_learning']. Digest Analysis should then summarize and compare the effectiveness of these paradigms specifically within the context of speech-to-text translation.\n\n3. Broaden Architectural Innovations section and enhance analysis of specific pre-training methods:\n- Broaden the scope of Section 3.3 from \"Architectural Innovations\" to \"Architectural Innovations and Key Components\" to encompass not only novel architectures but also innovative modules and mechanisms that are crucial for addressing specific challenges in direct ST, such as adaptor modules and attention mechanisms. This expansion acknowledges the importance of component-level innovations, as exemplified by adaptor modules in ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders'] for bridging representation inconsistency.\n- Add a new sub-section \"3.3.3 Adaptor Modules for Cross-Modal Representation Bridging\" under \"3.3 Architectural Innovations and Key Components\" to specifically discuss and compare different adaptor modules and mechanisms used to bridge the modality gap between acoustic and textual representations in direct ST architectures. This addition will allow for a focused analysis on the role and effectiveness of adaptor modules in cross-modal information transfer and representation alignment, drawing insights from papers like ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders', 'bridging_the_modality_gap_for_speech_to_text_translation'].\n- For \"3.1.5 Stacked Acoustic-and-Textual Encoding (SATE)\" and \"3.1.6 Fused Acoustic and Text Encoding (FATE)\", refine Digest Construction and Analysis guidance to ensure a more in-depth evaluation. For SATE, Digest Construction should extract motivations, detailed architecture, the role of Multi-Teacher Knowledge Distillation, and experimental results. Digest Analysis should thoroughly evaluate SATE's contribution to addressing modeling deficiency and representation inconsistency ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']. Similarly, for FATE, Digest Construction should focus on unified representation learning approach, the FAT-MLM pre-training objective, FAT-ST model architecture, and performance improvements. Digest Analysis should evaluate FATE's advancements in translation quality and the effectiveness of its fused encoding approach ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n\n4. Enhance Data Augmentation section and refine Future Directions for actionable research areas:\n- Expand Section 3.2 \"Data Augmentation Techniques\" to include a broader range of augmentation methods relevant to direct ST, such as pseudo-data generation (e.g., leveraging GigaST corpus ['gigast_a_10000_hour_pseudo_speech_translation_corpus']) and noise injection, in addition to speed perturbation and SpecAugment. Digest Construction should be broadened to extract information about various data augmentation techniques used in direct ST, while Digest Analysis should compare and analyze the effectiveness of these different methods and their complementarity with pre-training techniques.\n- Modify the description of Section 6 to \"Broader Open Challenges and Specific Future Directions Highlighted in the Paper\" to encourage a more comprehensive discussion of open challenges and to specifically incorporate future research directions suggested in the surveyed papers. The Digest Analysis for Section 6 should be structured to synthesize identified limitations and propose actionable future research directions. For example, it should encourage analysis of why pre-trained models are still insufficient in low-resource settings as discussed in ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation'] and suggest future directions like exploring more effective cross-lingual transfer techniques or advanced pseudo-data generation methods. Future directions should also include investigating different discriminator architectures for adversarial pre-training, addressing sequence length inconsistency in adaptor modules, and exploring further improvements to cross-modal representation alignment as suggested in ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data', 'stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders', 'fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].", "outline_eval_score": 9.375, "outline_eval_detail": "Rationale:\nThe outline demonstrates a strong logical structure and comprehensive coverage of the topic \"Recent Advances in Direct Speech-to-text Translation\".\n\n**Information Entropy of Structure:**\n\n1. **Logicality and Generality within Chapters (Score: 9/10):**\n    - **Strength:**  Each chapter follows a clear hierarchical structure. For example, Chapter 3 \"Techniques for Direct Speech-to-text Translation\" is logically broken down into \"Challenges in ST Encoder Design\", \"Pre-training Strategies\", \"Data Augmentation Techniques\", and \"Architectural Innovations\", each further divided into specific subsections. This hierarchical approach facilitates in-depth analysis and avoids superficiality. The \"Digest Construction\" and \"Digest Analysis\" format within each section ensures a consistent analytical framework, promoting comprehensiveness.\n    - **Strength:** Subsections are arranged in a logical thematic progression within each chapter. For instance, in Chapter 3.2 \"Pre-training Strategies\", the subsections move from general adversarial pre-training to specific methods like self-supervised, curriculum, and multimodal pre-training, and finally to bridging pre-training and fine-tuning, creating a coherent flow.\n    - **Shortage:** While generally strong, Chapter 5 \"Toolkits and Frameworks\" only contains one sub-chapter \"End-to-End Speech Translation Toolkits\". This section could potentially be expanded to include subsections discussing different types of toolkits or specific features of existing toolkits beyond just \"end-to-end\" aspect, to enhance within-chapter logicality and depth. However, this is a minor issue as the content itself may justify a single sub-chapter.\n\n2. **Redundancy and Complementarity between Chapters (Score: 9.5/10):**\n    - **Strength:** The chapters explore distinct aspects of direct speech-to-text translation, moving from foundational concepts (Introduction, Background) to technical details (Techniques, Datasets, Toolkits), and finally to future perspectives (Challenges, Conclusion). This division ensures minimal overlap and a clear progression of themes.\n    - **Strength:** The outline structure is clear and concise. There is no excessive redundancy apparent between chapters. Each chapter addresses a specific facet of the survey, contributing to the overall comprehensive picture. For example, Chapter 2 sets the historical context, while Chapter 3 delves into the technical methodologies, demonstrating complementarity rather than redundancy.\n    - **Strength:** All chapters adhere to the sub-chapter limit. The number of sub-chapters in Chapters 3 and 4 is appropriate given the breadth of techniques and datasets to be covered, avoiding both insufficient and excessive granularity.\n\n3. **Overall Theme Coverage and Logicality (Score: 8.5/10):**\n    - **Strength:** The outline constructs a comprehensive framework to introduce \"Recent Advances in Direct Speech-to-text Translation\". It covers essential aspects from the evolution of the field to current techniques, datasets, and future directions. The inclusion of \"Digest Construction\" and \"Digest Analysis\" throughout the outline suggests an intention to provide both summary and in-depth analysis.\n    - **Strength:** The logical flow of the entire article is strong. The chapters transition smoothly from introduction to background, techniques, resources, challenges, and conclusion, creating a clear narrative for the reader.\n    - **Shortage:** While the outline addresses challenges and future directions in Chapter 6, it lacks explicit consideration of broader ethical impacts of direct speech-to-text translation technology.  While technical challenges are thoroughly covered, incorporating a section or mentioning in Chapter 6 potential ethical concerns (e.g., privacy, bias amplification) would enhance the overall theme coverage and forward-looking perspective.  Although not explicitly requested in the prompt, considering ethical implications strengthens academic surveys, especially in rapidly evolving AI fields.\n\n**Information Entropy of Chapter Descriptions:**\n\n1. **Single-Article Extraction: Digest Construction (Score: 10/10):**\n    - **Strength:** The \"Digest Construction\" sections are meticulously designed to encompass essential elements for summarizing and analyzing papers relevant to each chapter's theme. For instance, in Chapter 3.2 \"Pre-training Strategies\", the \"Digest Construction\" explicitly directs the extraction of \"pre-training tasks, types of pre-training data, model architectures, and transfer learning methodologies,\" ensuring comprehensive information gathering.\n    - **Strength:** The extracted information is directly applicable and valuable for the \"Digest Analysis\" sections.  The \"Digest Construction\" in each section clearly sets the stage for the analytical tasks in the subsequent \"Digest Analysis\", ensuring a coherent and purposeful approach to literature review.\n    - **Strength:** The outline provides clear and actionable steps for handling papers. The \"Digest Construction\" and \"Digest Analysis\" instructions are structured as clear directives, guiding the researcher on how to process and synthesize information from the provided papers effectively.\n\n2. **Analysis of Relationships among Cited Articles: Digest Analysis (Score: 9.5/10):**\n    - **Strength:** The \"Digest Analysis\" framework promotes a \"What-Why-How\" approach, encouraging a deeper understanding of the literature. For example, in Chapter 3.2.1 \"Adversarial Pre-training with ASR and MT Data\", the \"Digest Analysis\" prompts for comparison of strategies and discussion of the \"modality gap challenge\", pushing beyond superficial summaries to explore the underlying motivations and methodologies.\n    - **Strength:** The \"Digest Analysis\" instructions explicitly call for comprehensive analysis of strengths and weaknesses, and for supporting analysis with technical details. For example, in Chapter 3.2.6 \"Stacked Acoustic-and-Textual Encoding (SATE)\", the \"Digest Analysis\" requires a detailed analysis of SATE's contribution to addressing \"modeling deficiency and representation inconsistency\", promoting rigorous evaluation.\n    - **Strength:** The \"Digest Analysis\" sections encourage forward-looking perspectives by asking to identify challenges, propose solutions, and consider innovative approaches. Chapter 6 entirely focuses on synthesizing challenges and proposing future research directions, demonstrating a commitment to moving beyond a mere summary of the current state to a forward-looking analysis.\n    - **Shortage:** While the \"Digest Analysis\" prompts are excellent, they could be slightly enhanced by explicitly encouraging the consideration of methodologies and solutions adopted from other related fields (e.g., Computer Vision, NLP in general). While the current prompts encourage \"broader perspective\", explicitly mentioning cross-disciplinary thinking might further stimulate innovative analysis and solution proposals. However, this is a minor suggestion, as the current framework is already very comprehensive.\n\nFinal Score:\n<SCORE>9.375</SCORE>", "content": "#  Recent Advances in Direct Speech-to-text Translation\n\n## 1. Introduction\nDirect speech-to-text translation (ST), also referred to as end-to-end speech translation, is defined as the task of converting speech from a source language directly into text in a target language, bypassing the intermediate step of generating a source language transcription [13,20].\n<figure-link title='Comparison of Cascaded and Direct Speech-to-text Translation Approaches' type='mermaid' content='graph LR\\n    A[Cascaded Speech Translation] --> B(ASR);\\n    B --> C(MT);\\n    C --> D[Target Text];\\n    E[Direct Speech Translation] --> F(Direct ST Model);\\n    F --> D;\\n    style A fill:#f9f,stroke:#333,stroke-width:2px\\n    style E fill:#ccf,stroke:#333,stroke-width:2px\\n    classDef highlight fill:#f9f,stroke:#333,stroke-width:2px'></figure-link>\n This approach represents a significant shift from traditional cascaded systems, which typically involve automatic speech recognition (ASR) followed by machine translation (MT) [1,10]. The importance of direct ST stems from its potential to streamline the translation pipeline and enhance efficiency in various applications, including international conferences, foreign-language video subtitling, emergency calls, online courses, and cross-border services [6,9,16].\n\nThe motivations for adopting direct ST are consistently highlighted across the literature. A primary advantage is the reduction in latency, achieved by eliminating sequential processing inherent in cascaded models [5,8].\n<figure-link title='Motivations for Adopting Direct Speech-to-text Translation' type='mermaid' content='mindmap\\n  root((Motivations for Direct ST))\\n    central((Efficiency))\\n      Latency Reduction\\n      Computational Efficiency\\n    central((Accuracy))\\n      Error Propagation Mitigation\\n    central((Resource Scarcity))\\n      Low-Resource Languages\\n      Endangered Languages\\n      Languages without Written Forms\\n    central((Simplicity))\\n      Simplified Translation Process'></figure-link>\n  Furthermore, direct ST aims to mitigate the error propagation that plagues cascaded systems, where errors from the ASR component can negatively impact the subsequent MT stage [5,6,18]. This is particularly crucial as ASR systems are not perfect and their inaccuracies can be compounded in the translation process [21].  Several digests emphasize the computational efficiency of end-to-end models at inference time, presenting them as more compact and efficient alternatives to complex pipelines [5,6,22].  Direct ST is also particularly appealing for low-resource languages, endangered languages, or languages without written forms, where obtaining transcriptions for ASR training is challenging, while translation into higher-resource languages might be more feasible [4,11,15].  In contrast to the motivations centered on efficiency and accuracy, some papers also highlight the theoretical appeal of direct ST in simplifying the overall spoken language translation process [21]. Despite these shared motivations, a subtle difference emerges in the emphasis: while some papers focus on broad applicability across various scenarios, others specifically underscore the benefits for resource-constrained settings.\n\nCurrently, research in direct ST is actively exploring techniques to improve model performance, particularly focusing on pre-training strategies to overcome data scarcity and enhance translation quality [4,15,16,17].  Toolkits and evaluation campaigns are also crucial components of the current landscape, facilitating research and benchmarking progress in both end-to-end and cascaded approaches [3,8].\n\nHowever, the development of robust direct ST systems faces significant challenges. The most prominent limitation, consistently identified across the digests, is the scarcity of large-scale parallel speech-to-text translation corpora [2,7,18,19,20,22]. This data bottleneck is further exacerbated when compared to the availability of data for related tasks like ASR and MT, hindering the training of effective end-to-end models from scratch [7,20]. While cascaded systems can leverage abundant task-specific data for ASR and MT components independently, direct ST models are constrained by the limited availability of paired speech and translated text data [7].  Furthermore, some end-to-end ST solutions rely on source language transcriptions for pre-training or multi-task learning, which becomes problematic in low-resource scenarios where transcribed speech data is limited [6].  This reliance can inadvertently reintroduce complexities similar to cascaded approaches, undermining the intended simplicity of direct ST.  A fundamental challenge arises from the inherent complexity of directly mapping raw audio signals to text in another language, requiring models to learn both speech recognition and machine translation simultaneously from potentially noisy and less structured input compared to text-to-text translation.\n\nAddressing these challenges requires exploring several interdisciplinary directions. Drawing insights from ASR, techniques like self-supervised learning and unsupervised pre-training on large unlabeled speech datasets can be leveraged to improve acoustic representation learning, particularly for low-resource languages [11,18].  From the MT field, advancements in data augmentation, transfer learning, and multilingual modeling can be adapted to enhance direct ST models and mitigate data scarcity issues [2,22].  Further research is needed to investigate novel pre-training strategies that effectively utilize readily available ASR and MT data to bootstrap direct ST models, bridging the gap between end-to-end and cascaded approaches [4,19].  Exploring methods to better integrate acoustic and textual information within end-to-end architectures, potentially inspired by multimodal learning from NLP and computer vision, also presents a promising avenue for future research [22].  This survey aims to delve into these recent advancements in direct speech-to-text translation, focusing on the techniques and methodologies employed to address these challenges and pave the way for more robust and efficient spoken language translation systems.\n## 2. Background: Evolution of Speech Translation Paradigms\nThe landscape of speech translation has undergone significant evolution, transitioning from traditional cascaded systems to more integrated end-to-end approaches. Historically, speech translation systems were built upon a cascaded architecture, decomposing the task into two sequential stages: Automatic Speech Recognition (ASR) and Machine Translation (MT) [1,2,3,4,5,6,7,8,9,11,13,16,17,18,19,21,22].\n<figure-link title='Evolution of Speech Translation Paradigms' type='mermaid' content='graph LR\\n    A[Traditional Cascaded Systems] --> B{ASR Stage};\\n    B --> C{MT Stage};\\n    C --> D[Target Text];\\n    E[Emergence of End-to-End Systems] --> F{Single Neural Network};\\n    F --> D;\\n    style A fill:#f9f,stroke:#333,stroke-width:2px\\n    style E fill:#ccf,stroke:#333,stroke-width:2px'></figure-link>\n In this paradigm, the ASR component first transcribes the source speech into text, which is then fed into the MT component for translation into the target language [4,5].\n\nWhile cascaded systems have been effective and benefit from independently trained, high-performing ASR and MT models using abundant task-specific data [2,7], they inherently suffer from several limitations.\n<figure-link title='Comparison of Cascaded and End-to-End Speech Translation Paradigms' type='markdown' content='| Feature             | Cascaded Systems                       | End-to-End Systems                     |\\n|----------------------|----------------------------------------|-----------------------------------------|\\n| **Architecture**    | Sequential ASR + MT                   | Single Neural Network                   |\\n| **Data Usage**      | Task-specific data for ASR & MT        | Parallel speech-translation data         |\\n| **Error Propagation** | High                                   | Low                                     |\\n| **Training**        | Separate ASR & MT training             | Joint training for ST task              |\\n| **Latency**         | High                                   | Low                                     |\\n| **Optimization**    | Optimized for ASR then MT             | Optimized directly for ST               |\\n| **Data Efficiency**   | High for ASR & MT components          | Low, suffers from data scarcity         |\\n| **Complexity**      | More complex pipeline                  | Simplified architecture                 |'></figure-link>\n A primary drawback is error propagation, where inaccuracies introduced by the ASR component are passed on and potentially amplified by the MT component, ultimately degrading the overall translation quality [1,2,4,5,6,7,8,9,13,17,18,19,21,22]. Furthermore, the separate training of ASR and MT components leads to training objectives that are not jointly optimized for the speech translation task itself [5,6,7,9,21,22]. Cascaded systems also introduce increased latency due to the sequential processing of speech through distinct modules [2,6,8,17,19,21,22].\n\nIn response to these limitations, end-to-end speech translation (E2E-ST) models have emerged as a compelling alternative paradigm [2,3,4,5,6,8,9,11,13,16,17,18,21,22]. End-to-end models directly map speech from the source language to text in the target language using a single neural network, eliminating the explicit intermediate text transcription step [2,4,5,6,8,9,11,16,17,18,21,22]. This approach offers several theoretical and practical advantages. Firstly, by jointly training all model components, end-to-end models can mitigate error propagation inherent in cascaded systems and optimize directly for the speech translation task [2,5,6,7,8,9,13,16,17,18,19,21,22]. Secondly, the simplified architecture of end-to-end models leads to reduced latency compared to the complex pipelines of cascaded systems [2,6,8,17,21,22]. Finally, end-to-end models have the potential to better preserve acoustic information from the source speech, as they bypass the information bottleneck introduced by explicit text transcription in cascaded models [18].\n\nDespite the theoretical advantages, the practical implementation of end-to-end speech translation faces significant challenges, primarily due to the scarcity of large-scale speech-to-text translation corpora [2,7,13,17,19].  This contrasts with the data-rich environments of ASR and MT, where large amounts of task-specific data are available for training cascaded components [4,7].  The limited availability of parallel speech-translation data makes training robust end-to-end ST models challenging, often leading to underperformance compared to cascaded models, especially when data is scarce [13,19].\n\nThe evolution from cascaded to end-to-end models represents a significant paradigm shift in speech translation research, driven by the desire to overcome the inherent limitations of cascaded systems and create more efficient, accurate, and low-latency translation systems [3,5,22].  While end-to-end models hold great promise, addressing the data scarcity challenge remains a critical research direction to fully realize their potential and surpass the performance of traditional cascaded approaches. Future research should focus on effective techniques to train end-to-end models with limited speech translation data, potentially exploring advanced pre-training strategies, data augmentation methods, and multi-task learning approaches, drawing insights from related fields like ASR, MT, and self-supervised learning to bridge this data gap and further advance the field of direct speech-to-text translation.\n## 3. Techniques for Direct Speech-to-text Translation: Emphasizing Pre-training Strategies and Encoder Design Challenges\nThis section explores the advancements in direct speech-to-text translation (ST) systems, with a particular emphasis on pre-training methodologies and encoder design innovations that have emerged to address the inherent complexities of this task.\n<figure-link title='Key Techniques in Direct Speech-to-text Translation' type='mermaid' content='mindmap\\n  root((Direct ST Techniques))\\n    central((Encoder Design Challenges))\\n      Modeling Deficiency\\n      Representation Inconsistency\\n      Modality Gap\\n      Training Complexities\\n    central((Pre-training Strategies))\\n      Adversarial Pre-training\\n      Self-Supervised Pre-training\\n      Curriculum Pre-training\\n      Multimodal Pre-training\\n    central((Data Augmentation Techniques))\\n      Audio Manipulation\\n      Pseudo-data Generation\\n    central((Architectural Innovations))\\n      Addressing Modality Gap\\n      Efficient and Compact Models'></figure-link>\n Departing from the historical context, this section delves into the contemporary techniques that enhance direct ST performance by systematically organizing these approaches into key categories: encoder design challenges, pre-training strategies, data augmentation techniques, and architectural innovations. This structured analysis aims to provide a comprehensive overview of the current landscape, highlighting the effectiveness of each category and identifying overarching trends in the field.\n\nThe initial focus is on **Challenges in ST Encoder Design**, recognizing the encoder's pivotal role in simultaneously processing speech, understanding source language semantics, and mapping these into a cross-lingual space [17,19]. This subsection elucidates the multifaceted challenges that impede effective encoder design, broadly categorized as modeling deficiencies stemming from ASR pre-training limitations, representation inconsistencies between encoder and decoder spaces, the inherent modality gap between speech and text, and complexities in training end-to-end ST models [6]. Understanding these challenges is crucial for appreciating the subsequent techniques designed to mitigate them.\n\nFollowing the problem definition, the section transitions to **Pre-training Strategies**, a cornerstone in enhancing direct ST, particularly in data-scarce scenarios [4,17,18]. This part systematically examines various pre-training paradigms, including adversarial pre-training leveraging ASR and MT data [2], self-supervised pre-training techniques like Masked Acoustic Modeling (MAM) and wav2vec that exploit unlabeled speech data [6,14], curriculum pre-training for progressive linguistic knowledge injection [17], and multimodal pre-training that synergistically utilizes speech and text modalities [22]. Through the lens of transfer learning principles [23], the effectiveness of these strategies is analyzed, categorizing them based on transfer learning types and comparing their respective strengths and limitations in improving ST performance.\n\nBeyond pre-training, **Data Augmentation Techniques** are explored as essential tools to combat data scarcity and improve model generalization [8,13]. This section details audio manipulation methods like speed perturbation and SpecAugment [8,15,19] and pseudo-data generation strategies, exemplified by the large-scale GigaST corpus [20]. The analysis extends to the complementarity of data augmentation with pre-training, highlighting their synergistic effect in enhancing ST robustness and performance [15].\n\nFinally, the section culminates in **Architectural Innovations and Key Components**, which are critical for realizing efficient and effective direct ST systems. This part investigates novel architectures designed to address the modality gap, such as Stacked Acoustic and Textual Encoding (SATE) and Speech-Text Asynchronous Stacked Transformer (STAST) [9,19], and architectures aimed at achieving efficient and compact models suitable for deployment in resource-constrained environments [5,22]. The discussion encompasses the evolution towards end-to-end models and the adoption of Transformer-based architectures, alongside innovations like Neural Acoustic Feature Modeling (NAFM) [21].\n\nSynthesizing the insights from these sub-sections, this section aims to provide a holistic understanding of the techniques driving advancements in direct ST. It identifies the persistent challenges, such as the modality gap and data scarcity, and highlights the ongoing efforts to overcome these through innovative pre-training, data augmentation, and architectural designs. Future research directions are suggested, emphasizing the need for interdisciplinary approaches drawing from ASR, MT, NLP, and computer vision to further refine these techniques and pave the way for more robust, efficient, and accurate direct speech-to-text translation systems.\n### 3.1 Challenges in ST Encoder Design\nThe end-to-end speech translation (E2E-ST) encoder is tasked with a multifaceted challenge, requiring it to simultaneously transcribe speech, understand source language semantics, and map these understandings into a cross-lingual semantic space for translation [17,19]. This inherent complexity renders E2E-ST significantly more demanding than either machine translation (MT) or automatic speech recognition (ASR) individually [6].  Several key challenges impede the design of effective ST encoders, which can be broadly categorized into modeling deficiencies, representation inconsistencies, modality gaps, and training complexities.\n<figure-link title='Challenges in ST Encoder Design' type='markdown' content='| Challenge                  | Description                                                                 | Impact on ST Encoder                                                      |\\n|---------------------------|-----------------------------------------------------------------------------|----------------------------------------------------------------------------|\\n| **Modeling Deficiency**    | ASR pre-training focuses on acoustic-phonetic alignment, not translation needs | Limited capacity for high-level linguistic understanding for translation |\\n| **Representation Inconsistency** | Mismatch between MT decoder (text-like input) and ASR encoder (acoustic output) | Hinders knowledge transfer and translation quality                             |\\n| **Modality Gap**           | Differences between speech and text features (length, semantics, variability) | Overloads encoder, hinders effective representation learning                 |\\n| **Training Complexities**  | Data scarcity, global reordering in ST, subnet waste in pre-training         | Data efficiency issues, increased model complexity, underutilized pre-training |'></figure-link>\n\n\nA primary challenge lies in **modeling deficiency**, stemming from the limitations of leveraging ASR pre-trained models directly for ST encoders [17,19]. Traditional ASR pre-training predominantly focuses on acoustic-phonetic alignment, enabling the encoder to capture local dependencies within the acoustic sequence for accurate transcription [17,19]. However, this approach often falls short of capturing the global context and long-distance dependencies crucial for translation, which are typically addressed by MT encoders designed to process textual input [19]. Consequently, directly employing ASR encoders in ST systems can lead to sub-optimal performance due to their limited capacity for higher-level linguistic understanding and semantic abstraction necessary for translation [17].\n\nFurthermore, **representation inconsistency** emerges as a significant hurdle when integrating pre-trained models. ST decoders are frequently initialized with MT decoders, which are trained to expect encoder representations that are semantically rich and text-like [19]. In contrast, ST encoders initialized with ASR models tend to produce representations that are more acoustically oriented, resembling ASR encoder outputs [19]. This fundamental mismatch in the expected representation space between the encoder and decoder can hinder effective knowledge transfer and impede the overall translation quality [19]. Adding to this, the role of the encoder itself undergoes a transformation from pre-training to fine-tuning; initially focused purely on acoustic modeling, it transitions to extracting both acoustic and semantic/linguistic features during ST fine-tuning, potentially increasing learning difficulty if not addressed appropriately [16].\n\nThe inherent **modality gap** between speech and text introduces further complexities [6,9]. Speech feature sequences are typically much longer than their corresponding text counterparts, complicating the learning of effective alignments between modalities [6,9]. Moreover, speech features, often hand-crafted, inherently lack the rich semantic information embedded in text embeddings, requiring the ST encoder to bridge this semantic divide [9]. Speech is also intrinsically more variable and susceptible to noise compared to clean, structured text data, demanding robust encoder designs capable of handling acoustic variability across speakers and channels [4,9]. These modality-specific challenges collectively overload the ST encoder, requiring it to concurrently learn acoustic and semantic information from inherently different input types, potentially hindering effective representation learning [9].\n\nBeyond these core issues, **training end-to-end ST models** presents its own set of difficulties. The scarcity of parallel speech-translation data, compared to the abundance of ASR and MT corpora, leads to data efficiency challenges for direct ST models [13]. Furthermore, unlike monotonic alignments in ASR, ST necessitates learning global reordering between speech and translation, adding to the model's complexity [6]. Traditional pre-training strategies also often suffer from 'subnet waste', where potentially valuable pre-trained subnets like MT encoders are discarded, and 'non-pre-trained attention modules' which are trained from scratch for ST, failing to leverage pre-training benefits [16].\n\nIn summary, the design of ST encoders is fraught with challenges stemming from modeling limitations inherited from ASR systems, inconsistencies in representation spaces, inherent modality differences between speech and text, and complexities in training direct ST models. Addressing these challenges is crucial for advancing the field of end-to-end speech translation, motivating the exploration of innovative pre-training methodologies and architectural designs that can effectively bridge the gaps and harness the full potential of multi-modal learning. Future research should focus on developing encoder architectures that can simultaneously capture acoustic and semantic information from speech, generate consistent representations compatible with MT decoders, and effectively leverage pre-training techniques to overcome data scarcity and modality gaps. Interdisciplinary approaches, drawing insights from ASR, MT, NLP, and even computer vision (for handling noisy and variable input data), will be essential in tackling these multifaceted challenges and paving the way for more robust and effective ST encoder designs.\n### 3.2 Pre-training Strategies\nPre-training strategies are essential for mitigating data scarcity and enhancing the performance of direct speech-to-text translation (ST) models, particularly in low-resource scenarios.\n<figure-link title='Pre-training Strategies for Direct Speech-to-text Translation' type='mermaid' content='graph LR\\n    A[Pre-training Strategies] --> B(Adversarial Pre-training);\\n    A --> C(Self-Supervised Pre-training);\\n    A --> D(Curriculum Pre-training);\\n    A --> E(Multimodal Pre-training);\\n    style A fill:#f9f,stroke:#333,stroke-width:2px'></figure-link>\n This section provides an overview of prominent pre-training methodologies explored in recent research, categorized into adversarial pre-training, self-supervised pre-training, curriculum pre-training, and multimodal pre-training. These strategies leverage transfer learning principles to imbue ST models with knowledge acquired from related tasks and diverse data sources, thereby improving translation quality and efficiency.\n\nAdversarial pre-training, as detailed in the first subsection, offers a method to bridge the modality gap between speech and text by leveraging readily available Automatic Speech Recognition (ASR) and Machine Translation (MT) data. By employing adversarial learning, encoders are trained to produce modality-agnostic representations, facilitating the transfer of knowledge from text-based MT data to speech-based ST tasks [2]. This approach addresses the challenge of aligning different modalities by encouraging the encoder to learn features that are indistinguishable across speech and text.\n\nThe subsequent subsection delves into self-supervised pre-training, which capitalizes on vast amounts of unlabeled speech data to learn effective speech representations. Techniques like Masked Acoustic Modeling (MAM) and wav2vec are highlighted for their ability to learn from untranscribed audio, broadening the scope of usable pre-training data. These methods, rooted in contrastive predictive coding and masked signal reconstruction, learn robust speech features that demonstrably improve ST performance, especially in low-resource and cross-lingual transfer settings [11,18]. The success of models like wav2vec and HuBERT, evidenced by their adoption in the IWSLT 2022 evaluation campaign [3,20], underscores the effectiveness of self-supervised learning in capturing valuable speech information without relying on transcriptions.\n\nCurriculum pre-training, presented in the third subsection, introduces a structured, stage-wise approach to inject linguistic knowledge into ST models. This strategy progresses through courses of increasing complexity, starting with basic transcription skills using ASR tasks, advancing to semantic understanding via Frame-based Masked Language Modeling (FMLM), and culminating in cross-lingual mapping with Frame-based Bilingual Lexicon Translation (FBLT) [17]. By systematically building linguistic capabilities, curriculum pre-training aims to enhance the encoder's ability to capture sentence meaning and cross-lingual word correspondences, ultimately improving translation quality.\n\nFinally, multimodal pre-training explores the synergistic use of both speech and text modalities to learn unified representations. Methods like FAT-MLM leverage paired speech and text data to train models to reconstruct masked portions of both modalities, fostering a shared representation space where acoustic and textual features are aligned [22]. This approach directly addresses the modality gap inherent in ST by encouraging the model to learn representations that are informed by and reconstructible from both acoustic and textual information, potentially leading to more robust and modality-invariant language understanding.\n\nCollectively, these pre-training strategies represent diverse yet complementary approaches to enhancing direct ST. While adversarial pre-training focuses on modality alignment, self-supervised methods leverage unlabeled speech data for robust feature learning, curriculum pre-training systematically injects linguistic knowledge, and multimodal pre-training exploits the synergy between speech and text.  The effectiveness of each strategy is evaluated in terms of performance gains, data requirements, and computational cost, highlighting the trade-offs and optimal use cases for each method. Future research directions across these pre-training paradigms include exploring more efficient and targeted pre-training tasks, mitigating data dependencies, and further investigating the theoretical underpinnings of transfer learning in the context of direct speech-to-text translation.\n#### 3.2.1 Adversarial Pre-training with ASR and MT Data\nAdversarial pre-training emerges as a promising technique to leverage Automatic Speech Recognition (ASR) and Machine Translation (MT) data for enhancing direct Speech-to-text Translation (ST) systems. The foundational concept of adversarial learning, where a generator and a discriminator engage in a minimax game, is recognized for its potential in training domain-invariant encoders and bridging modality gaps in transfer learning scenarios, although its direct application to ST is not explicitly detailed in general surveys on transfer learning [23]. Specifically for direct ST, adversarial pre-training can be strategically employed to address the modality disparity between speech and text, thereby improving the utilization of readily available ASR and MT datasets.\n\nOne notable approach focuses on pre-training the decoder of a direct ST model using MT data, a strategy detailed in [2]. This method tackles the challenge of aligning encoder representations derived from acoustic (ASR) and textual (NMT) modalities by introducing an adversarial regularizer. The core idea involves the simultaneous training of ASR and NMT models that share an encoder architecture. An adversarial loss term is integrated into the training objective to encourage the encoder to produce modality-agnostic representations. This is achieved through a discriminator network, a three-layer feedforward network with 1024 hidden units and Leaky-ReLU activation functions, designed to distinguish between encoder representations originating from ASR or NMT inputs. The adversarial loss, denoted as $L_{DISC}$, penalizes the encoder for generating representations that allow the discriminator to accurately identify the input modality. Consequently, minimizing $L_{DISC}$ compels the encoder to learn representations that are indistinguishable across modalities, effectively bridging the speech-text modality gap. The overall training objective for both ASR and NMT tasks is then formulated as a combination of the standard Cross-Entropy loss ($L_{CE}$) and the adversarial regularization loss: $L = L_{CE} + \\alpha L_{DISC}$, where $\\alpha$ is a hyperparameter, empirically set to 5, to balance the contribution of both loss terms [2]. This approach aims to enhance the compatibility of a pre-trained NMT decoder with an ASR encoder within a direct ST framework, by ensuring that the decoder is conditioned on encoder representations that are less modality-specific.\n\nIn summary, adversarial pre-training with ASR and MT data represents a significant direction in direct ST research. It leverages the principles of adversarial learning to mitigate the modality gap, particularly when transferring knowledge from text-based MT data to speech-based ST tasks. The method of employing adversarial regularizers to enforce modality invariance in encoder representations, as exemplified by the simultaneous ASR and NMT training approach, demonstrates a concrete strategy for effectively utilizing MT data to pre-train ST decoders [2]. While the initial exploration of adversarial techniques in ST pre-training is promising, further research is needed to comprehensively evaluate its effectiveness, explore variations in adversarial architectures and training methodologies, and address potential challenges such as the optimization of hyperparameters like $\\alpha$ and the generalizability of modality-invariant representations across diverse datasets and languages. Future work should also investigate the combined effectiveness of pre-training both encoders and decoders using adversarial methods with ASR and MT data, respectively, to fully realize the potential of this pre-training paradigm for direct ST.\n#### 3.2.2 Self-Supervised Pre-training\nSelf-supervised pre-training has emerged as a pivotal technique in direct speech-to-text translation (ST), primarily aimed at leveraging the vast amounts of untranscribed speech data to enhance model performance, particularly in data-scarce scenarios [3,18]. This approach aligns with the broader principles of transfer learning, where models pre-trained on auxiliary tasks are fine-tuned for the target ST task, effectively transferring learned representations [23]. Within self-supervised pre-training, Masked Acoustic Modeling (MAM) and wav2vec stand out as prominent methodologies, differing in their pre-training objectives and mechanisms.\n<figure-link title='Comparison of Self-Supervised Pre-training Methods: MAM and wav2vec' type='markdown' content='| Method                     | Masked Acoustic Modeling (MAM)                                     | wav2vec                                                                |\\n|-----------------------------|----------------------------------------------------------------------|------------------------------------------------------------------------|\\n| **Pre-training Objective** | Reconstruct masked portions of speech signal                         | Predict future audio samples based on context                           |\\n| **Data Used**              | Unlabeled speech data                                                | Unlabeled speech data                                                  |\\n| **Mechanism**              | Masking and reconstruction using deconvolutional function             | Contrastive Predictive Coding (CPC)                                    |\\n| **Key Feature**            | Modality-specific, operates exclusively on speech modality              | Contrastive loss function to distinguish true future from negative samples |'></figure-link>\n\n\nMasked Acoustic Modeling (MAM) is a technique focused on reconstructing masked portions of the speech signal, using the surrounding context as a cue [6]. Unlike some self-supervised methods, MAM does not necessitate transcriptions or forced alignments, rendering it applicable to untranscribed and even non-speech audio data, broadening the scope of usable pre-training data [6]. MAM pre-training can be conducted on diverse acoustic datasets, including source language speech, multilingual speech, or arbitrary acoustic data [6]. The pre-training task in MAM involves applying random masks, either as single frames or spans of consecutive frames (Span Masking), to the speech audio input, and training the model to recover the original speech signal by minimizing the mean squared error between the reconstructed and original signals using a deconvolutional reconstruction function [6]. Conceptually related to FAT-MLM, MAM is modality-specific, operating exclusively on the speech modality, in contrast to multimodal approaches [22].\n\nWav2vec, another influential self-supervised pre-training method, adopts a contrastive predictive coding (CPC) approach, aiming to predict future audio samples based on the given context [11,14].  It employs an encoder network to convert raw audio input into latent representations ($\\mathbf{z}_{i}$) and a context network to generate contextualized representations ($\\mathbf{c}_{i}$) [11,14]. The pre-training objective is formalized as minimizing a contrastive loss function, which can be expressed as:\n$$\n\\mathcal{L}_{k}=-\\sum_{i=1}^{T-k}\\Big(\\log\\sigma(\\mathbf{z}_{i+k}^{\\top}h_{k}(\\mathbf{c}_{i}))+\\lambda\\mathbb{E}_{\\mathbf{\\widetilde{z}}\\sim p_{n}}\\Big\\Big)\n$$\nwhere the model is trained to distinguish the true future sample $\\mathbf{z}_{i+k}$ from negative samples $\\mathbf{\\widetilde{z}}$ [14]. The total loss $\\mathcal{L}=\\sum_{k=1}^{K}\\mathcal{L}_{k}$ is summed over different prediction steps $K$ [14]. Variants of wav2vec include vq-wav2vec, which incorporates a quantization module to produce discrete tokens, and the use of BERT features on top of vq-wav2vec representations [18].\n\nBoth MAM and wav2vec, despite their differing pre-training objectives—reconstruction versus future prediction—share the commonality of leveraging unlabeled speech data to learn effective speech representations. In practice, models pre-trained with wav2vec, and its variants like HuBERT, have been widely adopted in ST, as evidenced by submissions to the IWSLT 2022 evaluation campaign that utilized these models as speech encoders [3,20]. Empirical evaluations indicate that features derived from self-supervised pre-training, such as wav2vec, lead to improved ST performance compared to traditional features like log-mel filterbanks, particularly in low-resource settings and cross-lingual transfer scenarios [11,18]. For instance, studies have shown BLEU score improvements of 4.28 to 6.37 on French-English ST tasks using wav2vec features, without relying on ASR pre-training [18]. Furthermore, models initialized with wav2vec representations demonstrate faster convergence and enhanced performance, especially when training data is limited [11].\n\nWhile self-supervised pre-training has shown significant promise, challenges and limitations remain. One key challenge lies in optimizing pre-training tasks and architectures specifically for the downstream ST task, as most current methods are adapted from ASR or general speech representation learning. The optimal pre-training objectives, masking strategies, and model architectures for maximizing ST performance are still under investigation. Furthermore, the computational cost associated with pre-training large models on massive unlabeled datasets can be substantial. Future research directions could explore more efficient pre-training methodologies, investigate novel self-supervised tasks tailored for ST that capture translation-relevant information, and explore the integration of multimodal self-supervision to leverage both audio and textual modalities more effectively. Investigating the theoretical underpinnings of why and how self-supervised representations benefit ST could also provide valuable insights for developing more targeted and effective pre-training strategies.\n#### 3.2.3 Curriculum Pre-training\nCurriculum pre-training has emerged as a strategy to enhance direct speech-to-text (ST) models by progressively injecting linguistic knowledge. Wang et al. (2020b) introduced a curriculum pre-training approach designed to enable encoders to grasp sentence meaning and establish cross-lingual word mappings [9].\n<figure-link title='Curriculum Pre-training Stages for Direct Speech-to-text Translation' type='mermaid' content='graph LR\\n    A[Curriculum Pre-training] --> B(Transcription (Elementary Course) - ASR Task);\\n    B --> C(Understanding (Advanced Course - FMLM) - Semantic Understanding);\\n    C --> D(Cross-lingual Mapping (Advanced Course - FBLT) - Cross-lingual Words);\\n    style A fill:#f9f,stroke:#333,stroke-width:2px'></figure-link>\n This method emphasizes a staged learning process, structured into courses of increasing complexity to guide the model's learning trajectory [17].\n\nThe curriculum pre-training method is organized into three distinct courses, each targeting a specific linguistic ability [17]. The initial course, **Transcription (Elementary Course)**, focuses on equipping the encoder with basic transcription skills. This is achieved through an auxiliary automatic speech recognition (ASR) task, which aligns acoustic features with phonemes or words. The training objective is formalized as minimizing a combined loss function, $\\mathcal{L}_{ASR}=\\alpha\\mathcal{L}_{CTC}+(1-\\alpha)\\mathcal{L}_{CE}$, encompassing both Connectionist Temporal Classification (CTC) and cross-entropy (CE) losses [17]. Progressing to the **Understanding (Advanced Course - FMLM)** stage, the curriculum introduces a Frame-based Masked Language Model (FMLM) task. This course aims to cultivate the encoder's ability to understand sentence meaning by randomly masking word-level speech segments and tasking the encoder to predict these masked words. The learning objective for this stage is defined by the KL-Divergence loss, $\\mathcal{L}_{F M L M}=-\\sum_{y_{j}^{s}\\in\\tilde{\\pmb{y}}^{s}}\\sum q(y_{j}^{s})\\mathrm{log}\\frac{p(y_{j}^{s}|\\tilde{\\pmb{x}})}{q(y_{j}^{s})}$, encouraging the model to learn contextual representations [17]. Finally, the **Cross-lingual Mapping (Advanced Course - FBLT)** course implements a Frame-based Bilingual Lexicon Translation (FBLT) task to explicitly teach cross-lingual word correspondences. In this phase, the model is trained to predict target language words that correspond to segments of source language speech. Similar to FMLM, FBLT employs a KL-Divergence loss function, $\\mathcal{L}_{F B L T}=-\\sum_{\\tilde{y}_{i}^{t}}\\sum q(\\tilde{y}_{i}^{t})\\mathrm{log}\\frac{p(\\tilde{y}_{i}^{t}|\\tilde{\\pmb{x}})}{q(y_{i}^{t})}$, to facilitate the learning of cross-lingual alignments [17].  The implementation of this curriculum is hierarchical, dedicating the initial eight encoder layers to ASR and FMLM tasks, and the subsequent four layers to the FBLT task [17]. This staged learning approach is designed to progressively imbue the encoder with essential capabilities, starting from basic transcription to semantic understanding and finally cross-lingual mapping, ultimately aiming to enhance ST performance [17].\n\nWhile curriculum pre-training offers a structured approach to inject linguistic knowledge, it is not without potential drawbacks. One critique highlights the reliance on force-alignment, which is often necessary to align speech segments with textual units for tasks like FMLM and FBLT [9]. This reliance introduces a potential dependency on an external ASR model, and any inaccuracies in the force-alignment process can propagate as alignment errors, potentially hindering the effectiveness of subsequent pre-training stages [9].  Compared to simpler pre-training methods that might directly leverage readily available paired or unpaired speech and text data, curriculum pre-training introduces additional complexity in terms of task design and data preparation.\n\nIn summary, curriculum pre-training represents a sophisticated strategy for enhancing ST models through staged learning of transcription, understanding, and cross-lingual mapping. The current state of knowledge suggests that such structured approaches can be beneficial for complex tasks like ST by progressively building necessary linguistic representations within the model. However, challenges remain, particularly concerning the reliance on accurate force-alignment and the potential for error propagation. Future research directions should explore methods to mitigate the dependence on force-alignment, perhaps by investigating alternative curriculum designs that are less reliant on precise alignments or by incorporating techniques to improve the robustness of alignment processes. Furthermore, exploring the optimal granularity and sequencing of curriculum stages, and investigating the transferability of knowledge learned through curriculum pre-training to various ST architectures and language pairs, are important avenues for future investigation.  Drawing insights from related fields like ASR and MT, future work could also explore incorporating techniques such as self-training or unsupervised learning within a curriculum framework to further enhance the effectiveness and robustness of pre-training for direct speech-to-text translation.\n#### 3.2.4 Multimodal Pre-training\nMultimodal pre-training has emerged as a promising approach to enhance direct speech translation (ST) models by leveraging the complementary information from both speech and text modalities, aiming to learn unified representations and effectively bridge the inherent modality gap [22]. This strategy contrasts with unimodal pre-training, which relies solely on either speech or text data. By utilizing both modalities during pre-training, multimodal approaches seek to capture richer and more comprehensive linguistic and acoustic features, potentially leading to improved performance in direct ST tasks.\n\nOne notable example of multimodal pre-training is FAT-MLM, which explicitly fuses acoustic and text embeddings and employs a shared Transformer encoder to process these modalities jointly [22]. The core objective of FAT-MLM is to learn unified representations through a multimodal masked language modeling (MLM) objective. This objective encompasses reconstructing masked speech segments ($\\ell_{\\mathbf{s}}(D_{\\mathbf{s},\\mathbf{x}})$) and text tokens ($\\ell_{\\mathbf{x}}(D_{\\mathbf{s},\\mathbf{x}})$) from the fused multimodal input for monolingual pre-training. For translation-oriented pre-training, it further includes a translation reconstruction loss ($\\ell_{\\mathbf{y}}(D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})$) to align representations across languages [22]. The loss functions for monolingual and translation FAT-MLM are defined respectively as:\n$$\n\\ell_{\\mathrm{FAT-MLM}}(D_{\\mathbf{s},\\mathbf{x}})=\\ell_{\\mathbf{s}}(D_{\\mathbf{s},\\mathbf{x}})+\\ell_{\\mathbf{x}}(D_{\\mathbf{s},\\mathbf{x}})\n$$\nand\n$$\n\\ell_{\\mathrm{FAT-MLM}}(D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})=\\ell_{\\mathbf{s}}(D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})+\\ell_{\\mathbf{x}}(D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})+\\ell_{\\mathbf{y}}(D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})\n$$\nThis formulation underscores the approach's commitment to learning representations that are simultaneously informed by and reconstructible from both acoustic and textual information.\n\nCompared to unimodal pre-training, multimodal pre-training offers the distinct advantage of leveraging the inherent correlations and complementarities between speech and text. Unimodal pre-training, while effective within its modality, may fall short in capturing cross-modal dependencies crucial for tasks like ST, which inherently involves mapping from one modality to another. Multimodal pre-training, by design, encourages the model to learn a shared representation space where speech and text features are aligned, potentially leading to a more robust and modality-invariant understanding of language.\n\nHowever, multimodal pre-training also presents unique challenges. One significant challenge is the increased complexity in data requirements. Effective multimodal pre-training may necessitate large-scale paired speech and text data, which can be more expensive and difficult to acquire compared to unimodal data. Furthermore, the fusion of acoustic and textual representations and the design of pre-training objectives that effectively leverage both modalities require careful consideration. Potential issues such as modality imbalance, where one modality dominates the learning process, or negative transfer, where modalities interfere with each other, need to be addressed.\n\nThe current state of knowledge indicates that multimodal pre-training is a promising direction for advancing direct ST. Initial studies, such as those introducing FAT-MLM, demonstrate the potential of this approach to learn unified representations and bridge the modality gap [22]. However, further research is needed to fully explore the potential and address the challenges of multimodal pre-training. Future research directions could focus on investigating more sophisticated fusion techniques, exploring novel pre-training tasks that better exploit cross-modal interactions, and developing strategies to mitigate potential issues like data scarcity and modality imbalance. From an interdisciplinary perspective, insights from ASR, MT, NLP, and even computer vision on multimodal representation learning can be valuable in guiding the development of more effective multimodal pre-training techniques for direct ST.\n### 3.3 Data Augmentation Techniques\nData augmentation is a crucial strategy in direct speech translation (ST) to combat data scarcity and enhance model generalization, particularly in low-resource scenarios [8,13].\n<figure-link title='Data Augmentation Techniques for Direct Speech-to-text Translation' type='markdown' content='| Technique Category    | Technique                  | Description                                                               | Example/Tool                                    | Benefit                                      |\\n|-----------------------|----------------------------|---------------------------------------------------------------------------|-------------------------------------------------|----------------------------------------------|\\n| **Audio Manipulation**  | Speed Perturbation         | Altering audio speed (e.g., 0.9x, 1.1x)                                 | Kaldi's 3-way speed perturbation, ESPnet-ST     | Increased data volume, improved robustness     |\\n|                       | SpecAugment                | Time and frequency masking on log mel-filterbank features                  | LibriSpeech-Deep (LD) policy                   | Enhanced robustness and generalization       |\\n|                       | Noise Injection            | Adding Gaussian noise, dropping frames                                   | -                                               | Improved robustness and generalization       |\\n| **Pseudo-data Generation** | Pseudo-data Generation   | Creating synthetic ST data using existing resources (e.g., MT)         | GigaST (MT from GigaSpeech transcripts)        | Large-scale data augmentation, data scarcity mitigation |'></figure-link>\n Several techniques have been explored, broadly categorized into audio manipulation and pseudo-data generation.\n\nAmong audio manipulation techniques, **speed perturbation** is widely adopted due to its simplicity and effectiveness [8,15,17]. This method involves altering the audio speed by factors such as 0.9 and 1.1, effectively increasing the data volume by creating variations of the original speech signals [15]. For instance, Kaldi's 3-way speed perturbation is used to generate sped-up and sped-down versions of audio data [15]. Experiments have demonstrated that speed perturbation can significantly improve BLEU scores, even in low-resource settings, and offers complementary benefits when combined with pre-training techniques, boosting BLEU from 10.3 to 13.3 for a 20-hour system [15].  Furthermore, ESPnet-ST toolkit integrates speed perturbation with factors 0.9, 1.0, and 1.1, effectively tripling the training data size and stabilizing end-to-end ST training [8].\n\n**SpecAugment**, another prominent audio augmentation technique, applies time and frequency masking to the log mel-filterbank features of speech inputs [6,8,19].  Originally developed for Automatic Speech Recognition (ASR), SpecAugment has proven effective in enhancing the robustness and generalization of end-to-end ST models [8]. Studies show that SpecAugment contributes to noticeable BLEU improvements, with one study reporting a 1.9 BLEU point gain over a cascaded baseline on MuST-C En-De dataset [19].  Different policies and parameters of SpecAugment are explored, including the LibriSpeech-Deep (LD) policy and adjustments to frequency masking based on embedding size to act as a form of dropout [18]. In addition to speed perturbation and SpecAugment, other noise injection methods are employed, such as adding Gaussian noise to MFCC features and dropping frames, further improving model robustness and generalization [4].\n\nBeyond audio-level augmentations, **pseudo-data generation** represents a distinct approach to data augmentation, leveraging existing resources to create synthetic ST data. A significant contribution in this direction is the introduction of **GigaST**, a large-scale pseudo speech-to-text translation corpus [20]. GigaST is constructed by translating the transcripts of the GigaSpeech ASR corpus using a strong Machine Translation (MT) system, resulting in a corpus up to 25 times larger than open-source datasets like MuST-C [20]. While the training set translations are machine-generated, the test set translations are manually annotated, ensuring evaluation quality [20]. GigaST serves as a substantial pseudo ST corpus, effectively augmenting the training data size and potentially mitigating data scarcity issues, especially in low-resource scenarios. The effectiveness of such pseudo-data augmentation hinges on the quality of the MT system used for translation and the domain similarity between the pseudo-data and the target task.\n\nData augmentation techniques are not only beneficial independently but also exhibit **complementarity with pre-training**. Research indicates that data augmentation, particularly speed perturbation, yields additional performance gains when combined with pre-training strategies [15]. This synergy suggests that data augmentation and pre-training address different aspects of model improvement, with augmentation enhancing robustness and generalization on the training data, while pre-training provides better initialization and feature representations.\n\nIn comparison, speed perturbation and SpecAugment operate at different levels, with speed perturbation manipulating the raw audio signal and SpecAugment operating on the feature representations. Speed perturbation is computationally inexpensive and easy to implement, making it a widely accessible technique. SpecAugment, while requiring feature extraction, offers more sophisticated control over the input data by selectively masking time and frequency components. Pseudo-data generation, like GigaST, provides a different scale of augmentation by dramatically increasing the dataset size, but its effectiveness is contingent on the quality and relevance of the generated data. Techniques like backtranslation and knowledge distillation, while also considered forms of data augmentation, often involve text-based manipulations or leveraging external models, adding further dimensions to the augmentation landscape in ST [3,13].\n\nThe current state of knowledge highlights data augmentation as an indispensable component in direct ST, especially for low-resource scenarios. While techniques like speed perturbation and SpecAugment are well-established and widely used, pseudo-data generation with corpora like GigaST represents a promising direction for scaling up training data. However, challenges and limitations remain. Pseudo-data quality and domain mismatch are critical concerns for pseudo-data augmentation. Over-application of augmentation, especially noise injection, can potentially degrade performance if it excessively distorts the data distribution. Furthermore, systematic investigations into optimal augmentation strategies for different ST scenarios and languages are still needed. Future research directions should focus on exploring adaptive augmentation techniques that dynamically adjust augmentation parameters based on data characteristics or training progress. Investigating the combination of different augmentation methods and exploring novel techniques inspired by other domains like computer vision and NLP could also be fruitful.  Moreover, understanding the impact of data augmentation on model interpretability and robustness against adversarial attacks warrants further exploration, potentially drawing insights from related fields like ASR and MT. Addressing these challenges and knowledge gaps will be crucial for advancing the effectiveness and applicability of data augmentation in direct speech-to-text translation.\n### 3.4 Architectural Innovations and Key Components\nThis section delves into the architectural innovations and key components that underpin recent progress in direct speech-to-text translation (ST).\n<figure-link title='Architectural Innovations in Direct Speech-to-text Translation' type='mermaid' content='mindmap\\n  root((Architectural Innovations))\\n    central((Addressing Modality Gap))\\n      Adaptor Modules (SATE)\\n      Decoupled Encoders (STAST)\\n      Unified Representation (FAT-ST, FAT-MLM)\\n      Adversarial Pre-training\\n      Consistency Mechanisms (TCEN)\\n    central((Efficient and Compact Models))\\n      End-to-End Architectures\\n      Convolutional and LSTM Layers\\n      Conformer Architecture\\n      Optimized Encoding (FAT-ST, SATE)'></figure-link>\n  Direct ST, in contrast to cascaded approaches, aims to translate speech directly into text, bypassing the intermediate step of automatic speech recognition (ASR). This paradigm shift necessitates novel architectural designs capable of effectively processing speech input and generating translations in the target language.  The exploration of these architectures is crucial for understanding the current landscape and future directions of the field. We will categorize and compare various architectural innovations, focusing on their effectiveness in addressing inherent challenges in direct ST, such as bridging the modality gap between speech and text and developing efficient and compact models suitable for real-world applications.\n\nThe following sub-sections will systematically dissect these architectural advancements.  \"Addressing the Modality Gap\" will explore techniques designed to align speech and text representations, a fundamental challenge given the distinct nature of these modalities. This includes methods like decoupled encoders and adaptor modules that facilitate cross-modal representation bridging. \"Efficient and Compact Models\" will then focus on architectural strategies aimed at reducing model size and computational complexity, crucial for deploying ST systems in resource-constrained environments.  Throughout this section, we will analyze the effectiveness of different approaches, identify common trends, and pinpoint potential areas for future innovation, drawing upon insights from various studies in the field [5,9,19,21].\n\nA central theme in direct ST architecture is the evolution towards end-to-end models, which learn the translation process directly from speech to text, streamlining the traditional cascaded approach and enhancing efficiency [5].  Transformer-based architectures have become increasingly dominant, offering robust performance in handling sequence-to-sequence tasks and facilitating the integration of attention mechanisms, crucial for capturing long-range dependencies in both speech and text [2,8,17].  Beyond the encoder-decoder framework, innovations like Neural Acoustic Feature Modeling (NAFM) challenge conventional feature extraction methods by learning ST-oriented acoustic features directly from raw waveforms, potentially leading to more optimized representations for translation [21].  Furthermore, models like Stacked Acoustic and Textual Encoding (SATE) and Speech-Text Asynchronous Stacked Transformer (STAST) exemplify efforts to explicitly address the modality gap through adaptor modules and decoupled encoder architectures, respectively, aiming to bridge the representational differences between speech and text and leverage knowledge from text-based machine translation [9,19].\n\nHowever, despite these advancements, significant challenges remain.  The inherent modality gap continues to be a major hurdle, as capturing the full semantic and contextual richness of speech, including prosodic and paralinguistic cues, and effectively mapping it to textual representations is a complex task. Current methods may not fully exploit these speech-specific nuances, potentially leading to information loss.  Moreover, the pursuit of efficient and compact models often presents a trade-off with translation accuracy, requiring careful balancing of model complexity and performance.  Future research should focus on developing more sophisticated cross-modal representation learning techniques that can effectively capture and preserve the rich information in speech while aligning it with textual semantics.  Exploring novel architectures that inherently prioritize efficiency without sacrificing accuracy, and investigating advanced model compression and efficient training methodologies are also critical directions for advancing the field of direct speech-to-text translation.  Interdisciplinary approaches, drawing insights from ASR, MT, NLP, and even computer vision, could offer valuable perspectives in overcoming these challenges and paving the way for more robust and practical direct ST systems.\n#### 3.4.1 Addressing the Modality Gap\nThe modality gap, stemming from the inherent differences between speech and text representations, poses a significant challenge in direct speech-to-text translation. To mitigate this issue, several architectural innovations have been proposed, primarily focusing on aligning cross-modal representations and facilitating effective knowledge transfer from text-based machine translation (MT) to speech translation (ST) systems.\n<figure-link title='Architectures Designed to Address the Modality Gap' type='markdown' content='| Architecture                               | Approach to Modality Gap                                                                 | Key Feature                                                                       |\\n|--------------------------------------------|-----------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|\\n| **SATE (Stacked Acoustic & Textual Encoding)** | Adaptor module to transform acoustic representations into textual space                    | Fuses acoustic encoder output with soft token representation from CTC outputs        |\\n| **STAST (Speech-Text Asynchronous Stacked Transformer)** | Decoupled encoder architecture, shrink mechanism, cross-modal adaptation                             | Enhances semantic knowledge transfer from text MT models to speech translation task |\\n| **FAT-ST & FAT-MLM**                         | Multimodal pre-training, joint training on acoustic and textual data                         | Shared encoder to foster coherent cross-modal representation                       |\\n| **Adversarial Pre-training**                | Encourages encoder to produce modality-invariant representations                            | Minimizes discrepancies between speech and text features                             |\\n| **TCEN**                                      | Semantic and length consistency mechanisms                                               | Weight sharing between CTC layer and word embeddings, noisy MT data for length robustness |'></figure-link>\n Among these, the Stacked Acoustic and Textual Encoding (SATE) architecture and the Speech-Text Asynchronous Stacked Transformer (STAST) model stand out for their explicit design to bridge this modality gap [9,19].\n\nThe SATE architecture, particularly through its adaptor module, directly addresses the modality gap by transforming acoustic representations into a textual space compatible with MT encoders [19]. This adaptor module operates by generating a fused representation $\\hat{h^{s}}$ from the acoustic encoder output $h^{s}$ and the soft token representation $h_{\\mathrm{soft}}^{s}$ derived from Connectionist Temporal Classification (CTC) outputs. The fusion is mathematically formulated as:\n$$\nA(h^{s},P(\\pi|h^{s})) = \\lambda\\cdot h_{\\mathrm{map}}^{s} + (1-\\lambda)\\cdot h_{\\mathrm{soft}}^{s}\n$$\nwhere $h_{\\mathrm{map}}^{s}$ represents the mapped acoustic representation. This approach aims to combine the semantic richness of soft tokens with the acoustic nuances preserved in the mapped acoustic features, thus creating an input for the textual encoder that effectively bridges the modality divide [19].\n\nSimilarly, the STAST model employs a decoupled encoder architecture alongside a shrink mechanism and cross-modal adaptation strategies to align speech and text representations [9]. These components are designed to enhance semantic knowledge transfer from text MT models to the speech translation task, effectively leveraging the strengths of text-domain pre-training for speech translation. In contrast to adaptor-based methods, approaches like FAT-ST and FAT-MLM, along with the adversarial pre-training technique, aim to learn a unified representation space for both speech and text modalities [2,22]. FAT-ST and FAT-MLM achieve this through multimodal pre-training and joint training on acoustic and textual data, utilizing a shared encoder to foster a coherent cross-modal representation [22]. Adversarial pre-training, on the other hand, explicitly encourages the encoder to produce modality-invariant representations, thereby minimizing the discrepancies between speech and text features [2]. The TCEN architecture further addresses the modality gap through semantic and length consistency mechanisms [16]. Semantic consistency is enforced via weight sharing between the CTC layer and word embeddings, aligning the speech encoder's output space with the text embedding space. Length consistency is achieved by incorporating noisy MT data, enhancing the text encoder's robustness to sequence length variations between word embeddings and speech encoder outputs [16].\n\nCollectively, these approaches highlight a trend towards explicit alignment of speech and text representations as a crucial strategy for overcoming the modality gap. While adaptor modules in SATE focus on transforming acoustic features to a textual domain, other methods explore unified representation learning or enforce consistency constraints. Despite these advancements, challenges persist. A fundamental limitation lies in the inherent complexity of capturing the full semantic and contextual information from speech, which is often richer in prosodic and paralinguistic cues compared to text. Current methods may not fully exploit these speech-specific nuances, potentially leading to information loss during modality transformation or alignment. Furthermore, the effectiveness of these approaches is often data-dependent, and their generalization to low-resource languages or noisy speech environments remains to be thoroughly investigated. Future research should explore more sophisticated techniques for cross-modal representation learning that can effectively capture and preserve the rich information in speech while aligning it with textual semantics. Investigating the integration of insights from automatic speech recognition (ASR), natural language processing (NLP), and even computer vision, particularly in handling non-verbal cues, could pave the way for more robust and efficient solutions to bridge the modality gap in direct speech-to-text translation.\n#### 3.4.2 Efficient and Compact Models\nThe pursuit of efficient and compact models is crucial for the practical deployment of direct speech translation (ST) systems, particularly in resource-constrained environments and real-time applications. Model efficiency directly impacts accessibility and broadens the applicability of ST technology [3].  Compared to traditional cascaded systems, direct ST models inherently offer efficiency advantages by eliminating intermediate steps like explicit automatic speech recognition (ASR), leading to reduced computational overhead and latency.\n\nSeveral techniques are employed to further enhance the efficiency and compactness of direct ST models.\n<figure-link title='Techniques for Efficient and Compact Direct ST Models' type='markdown' content='| Technique                     | Description                                                                    | Benefit                                                                  | Example Architectures/Methods           |\\n|------------------------------|--------------------------------------------------------------------------------|--------------------------------------------------------------------------|------------------------------------------|\\n| **End-to-End Architectures**   | Learn translation directly from speech to text                                   | Reduced parameters, lower latency compared to cascaded models             | -                                        |\\n| **Convolutional & LSTM Layers** | Use of convolutional and LSTM layers in model architecture                       | Create compact model architectures                                       | -                                        |\\n| **Conformer Architecture**      | Conformer architecture with CTC loss                                           | Improved efficiency by avoiding ASR pre-training, simplified model structure | -                                        |\\n| **Optimized Model Designs**    | Fused Acoustic and Text Encoding (FAT-ST), Stacked Acoustic and Textual Encoding (SATE) | Smaller model size, faster decoding time                                   | FAT-ST, SATE                               |'></figure-link>\n  A primary approach is the development of end-to-end architectures, which learn the translation directly from speech to text, streamlining the process and reducing the number of parameters compared to cascaded models [5]. For instance, studies have demonstrated that end-to-end models can achieve comparable performance to cascaded systems with significantly fewer parameters.  Specifically, an end-to-end model on LibriSpeech was reported to have 9.4 million parameters, in contrast to a cascaded model requiring 6.3 + 15.9 million parameters [5].  The architectural choices within these end-to-end models also play a vital role.  The use of convolutional and Long Short-Term Memory (LSTM) layers has been explored to create compact model architectures [5], while the Conformer architecture, coupled with Connectionist Temporal Classification (CTC) loss, has been shown to improve efficiency by avoiding ASR encoder pre-training and simplifying the model structure [3].\n\nBeyond architectural choices, efficiency is also gained through optimized model designs like the Fused Acoustic and Text Encoding for Speech Translation (FAT-ST) and Stacked Acoustic and Textual Encoding (SATE) models.  FAT-ST not only achieves state-of-the-art performance but also maintains a smaller model size and faster decoding time, with a base model size of 39.34M parameters, significantly less than cascaded models (83.79M) and other end-to-end models like Le et al. (2020) (51.20M) [22].  It also demonstrates a decoding speed almost $2\\times$ faster than cascaded systems [22].  Similarly, the SATE model achieves a speedup of up to 1.69x compared to cascaded ST models, highlighting the efficiency gains possible through integrated encoding strategies [19].\n\nThe current state of research indicates a strong trend towards developing efficient and compact direct ST models, primarily driven by the practical needs of real-world applications. End-to-end architectures, optimized model components like Conformer and LSTM, and innovative encoding techniques are key strategies in this pursuit. These methods collectively contribute to reducing model size, accelerating inference speed, and lowering computational demands, while striving to maintain or even improve translation quality.\n\nDespite the progress, challenges remain in achieving optimal efficiency without compromising translation accuracy. Further research is needed to explore the trade-offs between model compactness, computational efficiency, and translation performance. Investigating more advanced model compression techniques, such as pruning and quantization, specifically tailored for direct ST models could be beneficial.  Furthermore, exploring novel architectures that inherently prioritize efficiency, potentially drawing inspiration from advancements in efficient NLP and computer vision models, could lead to breakthroughs.  Future research directions should also focus on developing more efficient training methodologies, such as knowledge distillation and transfer learning, to train compact models effectively and rapidly. Addressing these challenges will be crucial for realizing the full potential of direct ST technology and enabling its widespread adoption across diverse applications and platforms.\n## 4. Datasets and Evaluation Metrics for Direct Speech-to-text Translation\nThis section provides an overview of the datasets and evaluation metrics that are fundamental to the advancement of direct speech-to-text translation (ST) research.\n<figure-link title='Key Components for Direct Speech-to-text Translation Advancement: Datasets and Evaluation' type='mermaid' content='mindmap\\n  root((Datasets & Evaluation Metrics))\\n    central((Multilingual Speech Translation Corpora))\\n      MuST-C\\n      CoVoST\\n    central((Large-scale Pseudo Speech Translation Corpora))\\n      GigaST\\n      Augmented LibriSpeech\\n    central((Benchmarking and Evaluation Campaigns))\\n      IWSLT'></figure-link>\n Building upon the necessity of robust datasets for training data-intensive neural models in direct ST, as highlighted by [7], this section delves into the characteristics, contributions, and limitations of key datasets and benchmarking initiatives. It will systematically explore the current landscape, focusing on multilingual corpora, large-scale pseudo-speech translation datasets, and the role of evaluation campaigns in standardizing progress. The subsequent sub-sections will dissect these areas in detail, starting with an examination of **Multilingual Speech Translation Corpora**, which are crucial for developing models capable of handling multiple languages and facilitating cross-lingual learning. Datasets like MuST-C, derived from TED Talks, and CoVoST, sourced from crowd-sourced speech, exemplify the efforts in this domain, each with unique methodologies and language coverage [7,18].\n\nFollowing the discussion on multilingual corpora, the section will address the emergence of **Large-scale Pseudo Speech Translation Corpora** as a response to the scarcity of parallel speech translation data.  Corpora like GigaST, generated by translating transcriptions of large speech datasets, and Augmented LibriSpeech, created through automatic alignment of audiobooks and e-books, represent innovative approaches to scaling up training data, albeit with different trade-offs in data quality [5,20]. The section will compare and contrast these methodologies, assessing the impact of pseudo-data on model training and generalization.\n\nFinally, the section will analyze the role of **Benchmarking and Evaluation Campaigns**, specifically focusing on the International Workshop on Spoken Language Translation (IWSLT). IWSLT campaigns are pivotal for driving progress by providing standardized datasets, evaluation protocols, and shared tasks that allow for objective comparison of different ST systems [3]. These campaigns, utilizing datasets like MuST-C and incorporating metrics like BLEU and human evaluations, not only track advancements but also highlight persistent challenges and guide future research directions. By examining these three facets – multilingual corpora, pseudo-data, and benchmarking campaigns – this section aims to provide a comprehensive understanding of the datasets and evaluation metrics that underpin current research in direct speech-to-text translation, setting the stage for identifying knowledge gaps and suggesting avenues for future exploration and improvement in this rapidly evolving field.\n### 4.1 Multilingual Speech Translation Corpora\nMultilingual speech translation (ST) corpora are pivotal for advancing direct ST research, primarily by enabling the development of multilingual models and facilitating cross-lingual transfer learning. These corpora provide the necessary data for models to learn mappings between multiple languages, thereby improving the efficiency and effectiveness of training processes, especially for low-resource languages. Among the prominent multilingual ST corpora, MuST-C stands out as a widely adopted benchmark, designed to alleviate the scarcity of resources for training end-to-end speech translation models [7].\n<figure-link title='Comparison of Multilingual Speech Translation Corpora' type='markdown' content='| Corpus                   | Source Data   | Languages           | Size (approx.) | Key Features                               |\\n|--------------------------|---------------|--------------------|----------------|--------------------------------------------|\\n| **MuST-C**               | TED Talks     | En-De, Es, Fr, It, ... | 452 hours total| Widely adopted benchmark, scalable creation |\\n| **Augmented LibriSpeech** | LibriSpeech ASR & e-books | En-Fr             | 236 hours      | Large-scale for En-Fr, cost-effective      |\\n| **ST-TED En-De**         | TED Talks     | En-De             | 272 hours      | TED Talks based, spontaneous speech         |\\n| **CoVoST**               | Crowd-sourced | X-En (11 source)   | -              | X-to-English, diverse topics, crowd-sourced |'></figure-link>\n\n\nMuST-C, derived from TED Talks, offers English-to-X translations for eight target languages: German, Spanish, French, Italian, Dutch, Portuguese, Romanian, and Russian [7]. Each language pair in MuST-C boasts a substantial size, containing at least 385 hours of audio recordings, sentence-level aligned with manual transcriptions and translations, with some reports indicating approximately 452 hours and an average of 252K utterances per language [7,21]. The creation methodology of MuST-C is noteworthy for its scalability and systematic approach. It involves downloading TED Talks videos and HTML files, leveraging Gargantua for aligning transcriptions and translations, and employing Gentle forced-aligner for audio-text alignment [7]. Quality control is ensured through filtering out noisy segments and talks based on word alignment rates [7]. MuST-C's significance is underscored by its frequent use in evaluations, including the IWSLT 2022 Offline Speech Translation task, where it was extended to language directions like English-Chinese and English-Japanese in its V2 version [3]. Its adoption across various studies [2,6,8,9,11,18,19,20,22] highlights its role as a primary resource in the field.\n\nIn contrast to MuST-C's multilingual breadth, the Augmented LibriSpeech corpus offers a large-scale resource specifically for English-to-French ST [5]. With 236 hours of speech, it is created by aligning the LibriSpeech ASR corpus with French e-books, representing a cost-effective approach to generating parallel speech-text data [5,17]. Another corpus, ST-TED En-De, also derived from TED Talks, provides 272 hours of English speech with aligned German translations [16]. While TED Talks based corpora like MuST-C and ST-TED offer spontaneous speech, which can be more challenging due to noise and disfluencies [9,22], Augmented LibriSpeech, based on audiobooks, may present a cleaner but potentially less diverse acoustic environment.  Furthermore, CoVoST dataset expands multilingual ST research by focusing on X-to-English translation, covering 11 source languages with English as the target, and utilizing crowd-sourced speech data across diverse topics [18].\n\nThe collective analysis of these corpora reveals a trend towards leveraging diverse sources and methodologies for creating multilingual ST datasets. TED Talks have emerged as a popular source for multilingual corpora due to their availability and diverse content, as evidenced by MuST-C and ST-TED.  For scalability and cost-effectiveness, aligning existing resources like ASR corpora with translated text, as seen in Augmented LibriSpeech, presents a viable strategy.  While MuST-C has significantly propelled multilingual ST research, its English-centric approach highlights a potential gap in resources for non-English source languages, which CoVoST partially addresses.  Future research should focus on expanding language coverage in multilingual ST corpora, particularly for low-resource language pairs and diverse language directions. Enhancing data quality and exploring corpora from varied domains beyond TED Talks and audiobooks will also be crucial for building more robust and generalizable direct ST systems.  The development of methodologies for creating and curating high-quality, scalable multilingual corpora remains a critical direction for advancing the field of direct speech-to-text translation.\n### 4.2 Large-scale Pseudo Speech Translation Corpora\nThe scarcity of large-scale parallel speech translation (ST) corpora has motivated the exploration of pseudo-data generation techniques to enhance model performance. One prominent approach involves creating large-scale pseudo-ST corpora by leveraging machine translation (MT) models to translate readily available speech transcripts.\n<figure-link title='Comparison of Large-scale Pseudo Speech Translation Corpora' type='markdown' content='| Corpus                   | Generation Method                                  | Size (approx.) | Key Features                                       |\\n|--------------------------|----------------------------------------------------|----------------|----------------------------------------------------|\\n| **GigaST**               | MT translation of GigaSpeech transcripts              | 10,000 hours   | Very large scale, high-quality MT translations     |\\n| **Augmented LibriSpeech (as pseudo)** | Automatic alignment of audiobooks and e-books | 236 hours      | Large scale, automatic alignment, potential noise |'></figure-link>\n This strategy is exemplified by corpora like GigaST, which addresses the data limitations in direct ST by generating translations for the transcriptions of the large-scale GigaSpeech corpus [20]. GigaST utilizes a strong Transformer-based MT model, trained on extensive datasets like WMT2021 and OPUS, to produce translations for the GigaSpeech training set. Human evaluations of GigaST's MT-generated translations, particularly for English to German (En-De), indicate a high quality, approaching human translation standards. The resulting En-De and En-Zh training sets of GigaST significantly surpass the size of manually created corpora like MuST-C, offering up to 10,000 hours of pseudo ST data. This substantial scale positions GigaST as a valuable resource for training robust direct ST models and investigating the effectiveness of large-scale pseudo-data in this domain [20].\n\nAnother notable instance of pseudo-ST corpus creation, albeit less explicitly defined as such, is the Augmented LibriSpeech corpus [5]. This corpus leverages automatically aligned e-books as translations for audiobooks, resulting in a large-scale dataset of 236 hours. While the test set undergoes manual verification, the training and development sets rely on automatic alignment, potentially introducing noise or inaccuracies.  The use of automatically aligned e-books represents a form of pseudo-data generation, offering a substantial volume of training data, even if the translation quality may not perfectly mirror human-level quality [5].  Furthermore, the concept of leveraging MT for data augmentation in ST is implicitly acknowledged in other works. For example, the augmentation of the Libri-trans corpus using Google Translate to generate translation references, as mentioned in [8], reflects a similar motivation to expand training data through automated translation, though it is not explicitly framed as creating a large-scale pseudo-ST corpus like GigaST.\n\nComparing GigaST and Augmented LibriSpeech reveals distinct methodologies and potential trade-offs. GigaST prioritizes translation quality by employing a high-performing MT model and focusing on transcript translations, while Augmented LibriSpeech emphasizes scalability through automatic alignment, potentially at the cost of translation accuracy due to the automated process and inherent challenges in aligning spoken and written text. Both corpora, however, underscore the potential of pseudo-data to create large ST datasets. The creation of GigaST highlights the effectiveness of leveraging advanced MT models to generate high-quality pseudo-translations, suggesting that the quality of pseudo-data is crucial for its utility in ST. Conversely, Augmented LibriSpeech, while potentially containing noisy alignments, demonstrates that even automatically derived pseudo-data can yield a substantial dataset for training.\n\nThe current state of knowledge indicates a growing interest in and utilization of pseudo-ST corpora, particularly large-scale ones, to overcome data scarcity in direct ST. These corpora, generated through MT or automatic alignment, offer a pathway to train more powerful ST models. However, limitations remain, primarily concerning the quality and potential noise introduced by automated translation or alignment processes.  Future research directions should focus on mitigating the limitations of pseudo-data. Investigating methods to enhance the quality of MT-generated pseudo-translations, perhaps through techniques like back-translation or incorporating human-in-the-loop refinement, is crucial.  Furthermore, exploring strategies to reduce noise and improve alignment accuracy in automatically aligned corpora could enhance their effectiveness.  Finally, exploring alternative methods for pseudo-data generation, beyond MT and automatic alignment, may also be a fruitful avenue for future research in the field of direct speech-to-text translation.\n### 4.3 Benchmarking and Evaluation Campaigns (IWSLT)\nBenchmarking campaigns, exemplified by the International Workshop on Spoken Language Translation (IWSLT), play a pivotal role in propelling advancements within the field of direct speech-to-text translation.\n<figure-link title='Role of IWSLT Benchmarking Campaigns in Direct Speech-to-text Translation' type='markdown' content='| Aspect of IWSLT Campaign | Description                                                                | Impact on Direct ST Field                                                    |\\n|--------------------------|----------------------------------------------------------------------------|------------------------------------------------------------------------------|\\n| **Standardized Datasets** | Provides datasets like MuST-C, IWSLT En-De for evaluation                   | Enables objective comparison of different ST systems                        |\\n| **Evaluation Protocols** | Defines metrics (BLEU, chrF, latency, human evaluation) and procedures      | Standardizes evaluation methodologies, fosters innovation                   |\\n| **Shared Tasks**         | Features diverse tasks (offline, simultaneous, low-resource, etc.)             | Drives research in various aspects of ST, tackles real-world challenges     |\\n| **Community Building**   | Brings together researchers, facilitates knowledge sharing and collaboration | Accelerates progress, identifies promising directions and persistent challenges |'></figure-link>\n  IWSLT serves as a major benchmarking effort, providing standardized datasets and evaluation protocols that are crucial for comparing and contrasting different speech translation systems [3,11,16,17]. These campaigns have significantly contributed to standardizing evaluation methodologies and fostering innovation by clearly defining tasks and metrics, thereby enabling researchers to objectively assess progress and identify promising directions.\n\nThe IWSLT evaluation campaigns encompass a diverse range of tasks, reflecting the evolving landscape of speech translation. The 2022 campaign, for instance, featured shared tasks spanning simultaneous, offline, speech-to-speech, low-resource, multilingual, dialect, formality control, and isometric speech translation [3]. This expansion of tasks over the years, from traditional offline speech translation to more challenging and nuanced areas like simultaneous and low-resource scenarios, illustrates a trend towards tackling the multifaceted nature of real-world speech translation applications.  Datasets employed in these campaigns are carefully selected or newly created to address the specific challenges of each task. For example, TED talks have been a recurring source for offline ST tasks, while campaigns have also introduced novel datasets tailored for low-resource and dialectal speech translation [3]. Evaluation metrics within IWSLT are equally comprehensive, incorporating BLEU and chrF for translation quality, latency metrics for simultaneous translation, and human evaluations to capture more subjective aspects of system performance [3].\n\nFindings from IWSLT campaigns highlight both progress and persistent challenges in direct ST.  While consistent progress has been observed in offline speech translation, areas such as low-resource and dialect speech translation continue to present significant hurdles [3].  Furthermore, the inclusion of speech-to-speech and isometric ST tasks in recent campaigns signals an initial exploration into more complex and user-centric speech translation modalities [3].\n\nBeyond IWSLT, other evaluation campaigns, such as TC-STAR, have historically contributed to the field [10].  However, IWSLT has emerged as a consistently prominent and influential benchmarking effort in recent years.  Comparisons between datasets used in different contexts also reveal important insights.  For example, MuST-C, another multilingual speech translation corpus, has been shown to outperform the IWSLT18 English-German corpus in ASR, MT, and SLT tasks, suggesting potential variations in dataset quality and alignment methodologies even when derived from similar sources like TED talks [7].  This highlights the critical role of dataset quality in benchmarking and the need for careful consideration of corpus construction when evaluating ST systems.\n\nThe datasets and benchmarks provided by IWSLT are not only valuable for direct ST evaluation but also extend their utility to related tasks.  As demonstrated by research utilizing the ESPnet-ST toolkit, IWSLT datasets like IWSLT 2016 En-De serve as benchmarks for evaluating machine translation components within cascaded ST systems, further underscoring the broad impact of these campaigns on the wider speech and language processing community [8].\n\nIn summary, benchmarking campaigns like IWSLT have been instrumental in standardizing evaluation practices, fostering innovation, and charting the progress of direct speech-to-text translation.  They have provided valuable datasets, defined key tasks, and employed comprehensive evaluation metrics that have enabled the community to track advancements and identify remaining challenges.  Despite the significant contributions of IWSLT, potential knowledge gaps and future research directions remain.  These include a continued need for improved benchmarks and datasets specifically designed for low-resource and dialectal speech translation, as well as a deeper investigation into the impact of dataset quality on system performance. Future benchmarking efforts could also focus on developing more robust evaluation metrics that better capture the nuances of translation quality and user experience across diverse speech translation modalities.\n## 5. Toolkits and Frameworks for Direct Speech-to-text Translation\nThis section provides an overview of the toolkits and frameworks that are pivotal in the advancement of direct speech-to-text translation (ST). It synthesizes insights from recent research to compare open-source and commercial options, highlighting the features of prominent toolkits such as ESPnet-ST and Fairseq-ST, and addresses the necessity for specialized toolkits tailored for specific ST tasks. The discussion underscores the role of these technological resources in fostering reproducibility, democratizing access to ST technology, and propelling future research directions.\n\nThe landscape of ST toolkits is largely divided into open-source and commercial options.\n<figure-link title='Open-Source vs. Commercial Toolkits for Speech-to-text Translation' type='markdown' content='| Feature             | Open-Source Toolkits (e.g., ESPnet-ST) | Commercial Toolkits                                 |\\n|----------------------|---------------------------------------|-----------------------------------------------------|\\n| **Availability**    | Publicly available, often on GitHub    | Proprietary, usually licensed                          |\\n| **Customizability**   | High, source code accessible         | Limited, often \"black-box\"                           |\\n| **Transparency**    | High, algorithms and code visible       | Low, algorithms and implementation details hidden    |\\n| **Community Support** | Community-driven, collaborative       | Dedicated customer support, documentation           |\\n| **Ease of Use**       | Can be complex, steeper learning curve | User-friendly interfaces, streamlined workflows      |\\n| **Cost**             | Often free of charge                  | Typically require payment, licensing fees              |\\n| **Target User**     | Researchers, developers for customization | Industry, rapid deployment, ease of use prioritized |'></figure-link>\n Open-source toolkits, exemplified by ESPnet-ST [8], offer researchers unparalleled flexibility and transparency, enabling deep customization and community-driven development. In contrast, while commercial toolkits are not explicitly detailed in the digests, they are inferred to prioritize user-friendliness and rapid deployment, potentially at the expense of transparency and customizability. This distinction dictates toolkit selection based on research or application contexts, with open-source toolkits favoring academic exploration and commercial options suiting industrial applications where ease of use is paramount. However, both categories face limitations: open-source toolkits can suffer from fragmented documentation and support, while commercial alternatives may lack transparency and impose vendor lock-in. Addressing these limitations necessitates future efforts to enhance the usability of open-source toolkits and increase the transparency of commercial offerings, possibly through hybrid models that integrate the strengths of both paradigms.\n\nFeature comparisons of popular toolkits reveal ESPnet-ST as a comprehensive, 'all-in-one' solution, integrating functionalities for ASR, MT, and TTS within a unified environment [8]. This integration streamlines workflows, especially for tasks requiring multi-modal processing, and is supported by readily available recipes and pre-trained models, lowering the barrier to entry for researchers. While Fairseq is also noted as a significant framework used in ST research [3,14,18], in-depth feature-level comparisons with ESPnet-ST remain limited in the current digests, highlighting a gap in detailed comparative evaluations across different toolkit architectures and functionalities.  The findings from the IWSLT 2022 Evaluation Campaign further illustrate the practical application of these toolkits, with ESPnet and Fairseq being utilized by multiple teams for tasks ranging from dialect ST to end-to-end ST, and other toolkits like SpeechBrain, Marian-NMT, and SIMULEvAL serving specific purposes within the evaluation framework [3].\n\nBeyond general-purpose toolkits, the field is recognizing the growing need for specialized toolkits optimized for specific direct ST tasks, such as low-resource and multilingual translation. While toolkits explicitly designed for these scenarios are not yet prevalent, general-purpose toolkits like ESPnet-ST demonstrate potential for adaptation to multilingual contexts [8].  However, the inherent limitations of general-purpose toolkits in addressing the unique challenges of low-resource or real-time ST scenarios highlight a critical area for development. Future research should prioritize the creation of specialized toolkits, incorporating techniques from low-resource ASR and MT for data scarcity, and efficient model architectures for latency constraints in real-time applications.\n\nIn summary, the current state of knowledge emphasizes the critical role of toolkits in direct ST research, with ESPnet-ST emerging as a versatile and comprehensive open-source option.  A key trend is the increasing recognition of the need for both detailed comparative studies of existing toolkits and the development of specialized toolkits tailored to specific task demands. Addressing the identified limitations and knowledge gaps, future research directions should focus on enhancing toolkit usability, promoting transparency, and fostering the development of task-optimized solutions to advance the field of direct speech-to-text translation.\n### 5.1 Open-Source Toolkits vs. Commercial Toolkits for ST\nThe landscape of speech-to-text (ST) translation toolkits is broadly divided into open-source and commercial offerings, each presenting distinct advantages and disadvantages for researchers and developers. Open-source toolkits, exemplified by ESPnet-ST, are characterized by their public availability, often hosted on platforms like GitHub, and their community-driven development models [8]. This inherent openness fosters customizability and adaptability, allowing researchers to delve into the toolkit's architecture, modify components, and tailor functionalities to meet specific research needs. The emphasis on community support within open-source projects like ESPnet-ST further enhances their appeal, providing a collaborative environment for troubleshooting, sharing knowledge, and collectively advancing the toolkit's capabilities [8].\n\nWhile commercial toolkits are not explicitly discussed in the provided digests, we can infer their characteristics by contrasting them with the highlighted features of open-source options. Commercial toolkits typically offer user-friendly interfaces and streamlined workflows, prioritizing ease of use and rapid deployment for practical applications. They often come with dedicated customer support and comprehensive documentation, reducing the learning curve and facilitating integration into existing systems. However, this user-friendliness can come at the cost of flexibility and transparency. Commercial toolkits are often proprietary, limiting the extent to which users can understand and modify the underlying algorithms or customize specific modules. This \"black-box\" nature can be a disadvantage in research settings where algorithmic transparency and fine-grained control are crucial for experimentation and in-depth analysis.\n\nThe choice between open-source and commercial toolkits hinges on the specific research and development scenario. Open-source toolkits are particularly advantageous in academic research and exploratory projects where adaptability, transparency, and cost-effectiveness are paramount. The ability to inspect and modify the source code enables researchers to deeply understand the models and algorithms, facilitating innovation and the development of novel approaches. Conversely, commercial toolkits may be preferred in industrial settings or projects with tight deadlines where ease of use, robust support, and rapid prototyping are prioritized over customization and in-depth algorithmic understanding.\n\nDespite the advancements in both categories, challenges and limitations persist. Open-source toolkits, while offering flexibility, can sometimes suffer from fragmented documentation, varying levels of user support, and a steeper learning curve for users unfamiliar with the underlying codebase. Integrating different modules or adapting them for specific tasks might require significant technical expertise and effort. On the other hand, commercial toolkits, while simplifying deployment, can be expensive, lack transparency, and potentially limit the researcher's ability to explore and modify the core ST models.  The proprietary nature of these toolkits can also create vendor lock-in and hinder the reproducibility of research findings, as the exact implementation details may not be publicly accessible.\n\nAnalyzing the root causes of these limitations reveals fundamental issues related to resource allocation and development priorities in both open-source and commercial domains. Open-source toolkits often rely on volunteer contributions and community efforts, which can lead to uneven development progress and challenges in maintaining comprehensive documentation and user support. Drawing parallels from ASR and MT, the complexity of ST, which integrates both speech processing and machine translation, necessitates substantial interdisciplinary expertise and resources. Commercial toolkits, driven by market demands and profit motives, may prioritize features that cater to broader industry needs, potentially overlooking niche research requirements or neglecting to fully disclose algorithmic details for competitive reasons, echoing similar trends observed in NLP and Computer Vision software.\n\nFuture research directions should focus on mitigating these limitations and bridging the gap between open-source flexibility and commercial usability. For open-source toolkits, enhancing documentation, developing user-friendly interfaces, and establishing standardized module integration frameworks could significantly improve accessibility and ease of use. Inspired by successful open-source projects in related fields, fostering more structured community contributions and dedicated funding for core development could address resource limitations. For commercial toolkits, promoting greater transparency through open APIs and more flexible licensing models could enhance their utility in research settings without compromising proprietary interests. Furthermore, investigating hybrid approaches that combine the strengths of both open-source and commercial paradigms, such as commercially supported open-source distributions or open-source plugins for commercial platforms, could represent a promising avenue for future development in ST toolkits.\n### 5.2 Feature Comparison of Popular ST Toolkits (e.g., ESPnet-ST, Fairseq-ST)\nPopular Speech-to-Text (ST) toolkits, notably ESPnet-ST and Fairseq-ST, are indispensable resources for the advancement of direct ST research. ESPnet has been recognized as a common baseline for comparative studies in the field [9], while ESPnet-ST distinguishes itself as a comprehensive, multi-task platform within the ESPnet ecosystem [8]. A comparative feature analysis, documented in [8], evaluates ESPnet-ST alongside other frameworks including Fairseq, Lingvo, NeMo, RETURNN, and SLT.KIT. This analysis, detailed in Table 1 of [8], primarily contrasts toolkits based on their support for various tasks—encompassing ASR, LM, E2E-ST, Cascade-ST, MT, and TTS—and the accessibility of practical resources like example recipes and pre-trained models.\n\nESPnet-ST's principal strength, as evidenced by [8], is its 'all-in-one' design philosophy. By natively integrating functionalities for ASR, MT, TTS, and ST, ESPnet-ST offers a unified development environment that significantly streamlines research workflows involving multiple speech and language modalities. This integration is particularly advantageous for studies exploring joint modeling or transfer learning across tasks, making ESPnet-ST well-suited for comprehensive research projects that require a broad and integrated approach to speech and language processing.  The provision of extensive, readily available recipes and pre-trained models further reduces the barrier to entry and accelerates experimentation, making ESPnet-ST highly accessible to both novice and experienced researchers. In contrast, while Fairseq is included in the comparative analysis of [8], the digest information does not provide granular feature details for a direct, in-depth comparison with ESPnet-ST. From the available digest, Fairseq's specific strengths and weaknesses relative to ESPnet-ST in terms of features remain undifferentiated, and further investigation is needed to ascertain Fairseq's suitability for specific research needs compared to ESPnet-ST based on feature-level details.\n\nHowever, a limitation identified in [8] is the high-level nature of the feature comparison. While the comparison effectively illustrates the scope of task support and resource availability, it lacks detailed scrutiny of the underlying toolkit architectures, algorithmic implementations, and specific configurations. This broad-stroke comparison, while useful for initial toolkit selection based on task coverage, may not sufficiently inform decisions requiring fine-grained understanding of toolkit capabilities for specialized research objectives. For instance, researchers focusing on tasks demanding specific architectural features or optimization strategies, such as low-resource ST or real-time translation, would benefit from comparative data on model efficiency, adaptability, and latency, which are not explicitly addressed in the provided digests.\n\nCurrently, the understanding of popular ST toolkits, based on these digests, positions ESPnet-ST as a comprehensive and user-friendly option, especially for projects requiring broad task integration, rapid prototyping, and leveraging pre-trained models across multiple speech and language processing tasks.  Fairseq’s capabilities and feature set, in direct comparison to ESPnet-ST, require further investigation beyond the high-level comparison provided in [8]. A key challenge in the field is the absence of detailed, feature-centric comparative studies that delve into architectural nuances, training paradigms, and performance benchmarks across diverse ST scenarios. Future research should prioritize in-depth comparative evaluations, examining aspects like model modularity, customization flexibility, and computational efficiency across different toolkits. Such analyses should also incorporate benchmarks on diverse datasets and languages to provide a more nuanced understanding of toolkit suitability for various ST research directions, including but not limited to low-resource scenarios, real-time applications, and domain adaptation. This will empower researchers to make more informed toolkit selections and guide the evolution of ST toolkit development towards addressing current limitations and knowledge gaps, ultimately fostering more targeted and efficient advancements in direct speech-to-text translation.\n### 5.3 Toolkits for Specific Tasks in Direct ST (e.g., Low-Resource ST Toolkits)\nThe landscape of direct speech-to-text translation (ST) is increasingly recognizing the necessity for specialized toolkits tailored to distinct task demands, particularly in challenging scenarios like low-resource and multilingual translation. While dedicated toolkits explicitly designed and optimized for specific tasks such as low-resource or real-time ST remain relatively underexplored, general-purpose toolkits, such as ESPnet-ST, demonstrate latent capabilities applicable to certain specialized domains [8].  Although not explicitly conceived as a \"multilingual ST toolkit,\" ESPnet-ST's performance across diverse language pairs within the Must-C corpus suggests its inherent aptitude for multilingual ST. This implies that adaptable, general-purpose toolkits can be effectively leveraged or modified to address multilingual translation needs.\n\nHowever, the current state of research reveals a discernible gap in the explicit creation and rigorous evaluation of toolkits specifically engineered for demanding tasks in direct ST. A primary limitation lies in the potential inadequacy of general-purpose toolkits to effectively tackle the unique obstacles presented by specific scenarios. Low-resource ST, for example, poses substantial challenges due to the scarcity of training data, which can severely impede the efficacy of ST models. Similarly, real-time ST imposes stringent latency constraints that might not be adequately satisfied by toolkits primarily optimized for general translation tasks. The fundamental reasons behind these limitations are rooted in the design priorities of existing toolkits, which often emphasize overall performance across common scenarios rather than specialized optimization for specific edge cases. This generalized design philosophy may not inherently incorporate specialized techniques crucial for effectively managing low-resource data or minimizing processing delays for real-time applications.\n\nFuture research should prioritize the development of specialized toolkits explicitly designed to overcome the unique challenges of specific tasks in direct ST. For low-resource ST, future directions should explore integrating techniques from low-resource automatic speech recognition (ASR) and machine translation (MT), such as cross-lingual transfer learning, unsupervised pre-training, and data augmentation strategies like synthetic data generation, directly into ST toolkit architectures. For real-time ST, investigations should concentrate on efficient model architectures like streaming Transformer variants and optimized decoding algorithms such as monotonic attention mechanisms to minimize latency without compromising translation accuracy. Furthermore, exploring methods to enhance multilingual capabilities beyond the inherent multilingual capacity of general toolkits, potentially through language-specific adapters, mixture-of-experts models, or adaptive fine-tuning techniques, is essential. Addressing these knowledge gaps will necessitate interdisciplinary approaches, drawing upon insights from ASR, MT, natural language processing (NLP), and potentially computer vision for multimodal ST, to create more robust and task-optimized direct ST toolkits.\n## 6. Challenges and Future Directions in Direct Speech-to-text Translation\nThe landscape of direct speech-to-text translation is marked by significant progress, yet several challenges persist that hinder its full potential.\n<figure-link title='Key Challenges and Future Directions in Direct Speech-to-text Translation' type='mermaid' content='mindmap\\n  root((Challenges & Future Directions))\\n    central((Challenges))\\n      Performance Gap (vs. Cascaded)\\n      Data Scarcity\\n      Modality Gap\\n      Robustness to Noisy Speech\\n    central((Future Directions))\\n      Improved Pre-training Methodologies\\n      Architectural Innovations (Encoder Design)\\n      Noise Robust Training Techniques\\n      Deeper Understanding of Transfer Learning\\n      Tighter Integration of Recognition & Translation\\n      Exploration of Speech-to-Speech Translation'></figure-link>\n Current research consistently points to a performance gap between end-to-end (E2E) models and cascaded systems, where the latter, particularly those leveraging powerful Machine Translation (MT) components, often achieve superior translation quality [5,8,20]. Bridging this gap remains a central objective, necessitating exploration into enhanced E2E architectures and optimization strategies.  A fundamental bottleneck lies in data scarcity, as the development of robust E2E models is heavily reliant on large, high-quality speech translation corpora, which are considerably less abundant than resources for related tasks like Automatic Speech Recognition (ASR) and MT [7,22]. This data limitation is especially pronounced in low-resource scenarios, where the effectiveness of pre-trained models becomes crucial yet still insufficient to overcome the data deficit [4,15].\n\nFuture research directions are multifaceted and aim to address these core challenges.  Improving pre-training methodologies is paramount.  This includes exploring more effective cross-lingual transfer techniques, advanced pseudo-data generation methods like GigaST [20], and innovative pre-training tasks that can better leverage available ASR and MT data [2,22]. Self-supervised learning approaches, such as wav2vec and Masked Acoustic Modeling (MAM), offer promising avenues to reduce reliance on transcribed data and exploit unlabeled speech for pre-training, particularly beneficial in low-resource settings [6,14]. Further investigation into multilingual pre-training configurations and the optimal selection of pre-training data based on metrics like ASR Word Error Rate (WER) is also warranted [15].\n\nBeyond data and pre-training, architectural innovations are crucial.  Addressing the 'modeling deficiency' and 'representation inconsistency' inherent in directly mapping speech to text requires novel encoder designs, potentially drawing inspiration from stacked acoustic and textual encoding strategies like SATE [19].  Specifically, resolving sequence length inconsistencies within adaptor modules and enhancing cross-modal representation alignment are important technical directions [19]. Moreover, improving robustness to noisy speech and audio quality variations is essential for real-world applicability.  Exploration of noise-robust training techniques and methods to handle the inherent variability in audio data, even within curated corpora like LibriSpeech [12], are necessary.\n\nThe evaluation campaigns, such as IWSLT, highlight task-specific challenges and future directions, particularly in simultaneous speech translation where improving latency metrics and supporting speech output are key areas [3].  Furthermore, a deeper understanding of transfer learning in the context of direct ST is needed, focusing on measuring transferability, mitigating negative transfer, and enhancing model interpretability [23].  Finally, exploring methods to more tightly integrate speech recognition and translation processes, moving beyond cascaded approaches, remains a fundamental direction, potentially inspired by early works on coupling recognition and translation [1].  Future models should also consider incorporating prosodic information more effectively and exploring joint inference models that output both transcripts and translations to cater to diverse application needs [13].\n### 6.1 6.x Ethical and Societal Implications\nBased on the current input, there are no individual paper digests provided for analysis. Consequently, it is not possible to synthesize content from digests to analyze the ethical and societal implications of direct speech-to-text translation as requested in the subsection description. To fulfill this task, please provide the relevant paper digests. With the digests, I can analyze potential biases, fairness, accessibility, and broader societal consequences, and then suggest future research directions as outlined in the subsection description.\n## 7. Conclusion\nThe landscape of direct speech-to-text translation has undergone significant transformations, primarily driven by innovations in pre-training methodologies and the creation of large-scale multilingual datasets. A pivotal advancement lies in leveraging pre-training techniques, particularly those utilizing high-resource Automatic Speech Recognition (ASR) data to enhance the performance of direct Speech Translation (ST) systems, especially in low-resource scenarios [4]. This approach is underscored by the effectiveness of transferring encoder parameters, which are instrumental in learning robust acoustic representations. Multimodal pre-training, exemplified by models like FAT-MLM and FAT-ST, further enriches the representation learning process by fusing acoustic and textual data, leading to state-of-the-art performance and efficient model sizes [22].  Furthermore, research has explored bridging the inherent gap between pre-training and fine-tuning phases through methods like TCEN, which strategically reuses subnets and pre-trains attention mechanisms to maintain consistency and improve performance [16].  The integration of pre-trained ASR and Machine Translation (MT) models, as seen in the SATE method, represents another significant stride, achieving performance levels comparable to cascaded systems, particularly when abundant ASR and MT data are available [19]. Adversarial regularization has also proven effective in aligning latent representations across speech and text modalities, thereby boosting the efficacy of MT decoder pre-training for direct ST [2]. Self-supervised learning paradigms, such as CPC and Masked Acoustic Modeling (MAM), offer compelling alternatives, especially in data-scarce environments, by enabling the learning of superior speech representations and leveraging untranscribed audio data [6,11].\n\nWhile pre-training dominates the current research landscape, a contrasting perspective emerges from studies demonstrating the viability of training end-to-end ST models from scratch. Optimized training methodologies, incorporating techniques like Parameterized Distance Penalty (PDP) and CTC regularization, have shown that from-scratch models can achieve performance levels that rival or even surpass pre-training-based models in certain contexts [21]. This suggests that the perceived performance gap between these two approaches may have been overstated, and that further optimization of from-scratch training warrants continued exploration.\n\nThe availability of large, high-quality multilingual corpora, such as MuST-C, has been instrumental in fueling the progress of data-intensive neural ST models [7].  The introduction of pseudo-ST corpora like GigaST further underscores the benefits of large-scale training data in achieving state-of-the-art results [20].  Moreover, the development of comprehensive toolkits like ESPnet-ST plays a crucial role in democratizing access to advanced ST technologies and accelerating research by providing unified frameworks and pre-trained models [8]. The IWSLT 2022 Evaluation Campaign serves as a testament to the continuous advancements in direct ST, showcasing progress in offline translation while highlighting ongoing challenges in low-resource and dialectal speech translation, and venturing into novel areas like speech-to-speech and isometric translation [3].\n\nDespite these remarkable advancements, several limitations and knowledge gaps persist.  Data scarcity remains a significant impediment, particularly for low-resource language pairs and dialectal variations, hindering the generalization and robustness of direct ST models. Although pre-training techniques mitigate data dependency to some extent, their effectiveness is intrinsically linked to the quality and relevance of the pre-training data, especially in ASR pre-training where WER becomes a crucial factor [15]. The inherent modality gap between speech and text representations continues to pose a challenge, necessitating sophisticated mechanisms like encoder decoupling and cross-modal adaptation to bridge this divide effectively [9].  Furthermore, while end-to-end models demonstrate promising results, they do not consistently outperform cascaded ASR+MT systems across all scenarios, suggesting that cascaded models may still hold advantages in certain aspects, potentially in terms of interpretability and modularity [5].\n\nLooking ahead, future research directions should prioritize addressing the identified limitations and knowledge gaps.  Continued exploration of self-supervised learning methods, including masked acoustic modeling and contrastive predictive coding, holds immense potential for reducing reliance on transcribed data and leveraging the vast amounts of unlabeled speech data available.  Developing more effective techniques for cross-lingual transfer and multilingual modeling is crucial for advancing low-resource speech translation.  Further investigation into the optimization of from-scratch training methodologies and a more nuanced comparison with pre-training-based approaches are warranted to fully understand the trade-offs and optimal scenarios for each paradigm.  Addressing the modality gap through innovative architectural designs and representation learning techniques remains a critical area of focus.  Inspired by the principles of tightly coupled ASR and SMT systems in statistical approaches [1,10], exploring tighter integration of acoustic and linguistic information processing within end-to-end models could yield substantial improvements.  Finally, the nascent explorations into speech-to-speech translation and isometric translation signal exciting new frontiers for future research, potentially leading to more natural and seamless spoken language translation systems.  The ongoing development of comprehensive evaluation benchmarks and open-source toolkits will be indispensable in fostering collaboration, reproducibility, and accelerated progress in the dynamic field of direct speech-to-text translation.", "ref_str": "## References\n[1] Speech translation: coupling of recognition and translation \n\n[2] Effectively pretraining a speech translation decoder with Machine Translation data https://aclanthology.org/2020.emnlp-main.644/\n\n[3] Findings of the IWSLT 2022 Evaluation Campaign https://aclanthology.org/2022.iwslt-1.10/\n\n[4] Pre-training on high-resource speech recognition improves low-resource  speech-to-text translation http://arxiv.org/abs/1809.01431v2\n\n[5] End-to-End Automatic Speech Translation of Audiobooks http://arxiv.org/abs/1802.04200v1\n\n[6] MAM: Masked Acoustic Modeling for End-to-End Speech-to-Text Translation http://arxiv.org/abs/2010.11445\n\n[7] MuST-C: a Multilingual Speech Translation Corpus https://aclanthology.org/N19-1202/\n\n[8] ESPnet-ST: All-in-One Speech Translation Toolkit https://aclanthology.org/2020.acl-demos.34/\n\n[9] Bridging the Modality Gap for Speech-to-Text Translation http://arxiv.org/abs/2010.14920\n\n[10] Statistical Phrase-Based Speech Translation https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ec418938314fb29156a9a0bc6db49038a19abb37\n\n[11] Investigating Self-Supervised Pre-Training for End-to-End Speech Translation https://hal.archives-ouvertes.fr/hal-02962186/file/Paper_Template_for_INTERSPEECH_2019-3.pdf\n\n[12] Librispeech: An ASR corpus based on public domain audio books https://doi.org/10.1109/ICASSP.2015.7178964\n\n[13] Speech Translation and the End-to-End Promise: Taking Stock of Where We Are http://arxiv.org/abs/2004.06358v1\n\n[14] wav2vec: Unsupervised Pre-Training for Speech Recognition https://arxiv.org/pdf/1904.05862\n\n[15] Analyzing ASR pretraining for low-resource speech-to-text translation http://arxiv.org/abs/1910.10762v2\n\n[16] Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End Speech Translation https://ojs.aaai.org/index.php/AAAI/article/download/6452/6308\n\n[17] Curriculum Pre-training for End-to-End Speech Translation http://arxiv.org/abs/2004.10093v1\n\n[18] Self-Supervised Representations Improve End-to-End Speech Translation http://arxiv.org/abs/2006.12124v2\n\n[19] Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained  Models into Speech Translation Encoders http://arxiv.org/abs/2105.05752v2\n\n[20] GigaST: A 10,000-hour Pseudo Speech Translation Corpus https://arxiv.org/pdf/2204.03939\n\n[21] Revisiting End-to-End Speech-to-Text Translation From Scratch http://arxiv.org/abs/2206.04571v1\n\n[22] Fused Acoustic and Text Encoding for Multimodal Bilingual Pretraining  and Speech Translation http://arxiv.org/abs/2102.05766v2\n\n[23] A Comprehensive Survey on Transfer Learning http://arxiv.org/abs/1911.02685v3\n\n", "digests": [{"bibkey": "stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders, effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data, fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation", "content": "#  Recent Advances in Direct Speech-to-text Translation\n## 1. Introduction\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nDirect speech-to-text translation (ST) aims to translate audio signals directly into text in the target language, bypassing the intermediate step of transcription. Direct approaches offer advantages over traditional cascaded systems (ASR + MT), including reduced latency, streamlined pipelines, and mitigation of error propagation from ASR to MT. However, the scarcity of available resources, particularly speech translation data, remains a significant challenge for direct ST. ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']\n--------------------\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nEnd-to-end Speech Translation (E2E ST) has gained popularity due to its ability to simplify the translation pipeline and reduce latency compared to cascaded systems. However, the scarcity of speech-to-translation paired data poses a challenge for training robust E2E ST models. This paper explores the challenges of pre-training in ST and investigates how pre-trained Automatic Speech Recognition (ASR) and Machine Translation (MT) models can be effectively integrated to enhance ST performance, aiming to bridge the gap between end-to-end and cascaded approaches. ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nDirect speech-to-text translation offers advantages over traditional cascaded systems by reducing latency, streamlining pipelines, and mitigating error propagation ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. End-to-end speech translation is desired for its efficiency and reduced complexity compared to pipeline approaches ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. However, the quality of end-to-end speech translation models is often limited by the scarcity of large-scale parallel speech translation data ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The paper addresses this limitation by proposing a method to leverage abundant data from speech recognition and text machine translation to improve speech translation quality ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n## 2. Background: Evolution of Speech Translation Paradigms\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nTraditional speech translation systems often rely on a cascaded approach, combining Automatic Speech Recognition (ASR) to transcribe speech into text, followed by Machine Translation (MT) to translate the transcribed text. While cascaded models can benefit from well-trained ASR and MT components, they suffer from error propagation between components and increased inference latency. End-to-end models for direct speech-to-text translation have emerged as a promising alternative, offering advantages such as lower latency and joint training that directly maps speech to text, potentially mitigating error propagation. However, end-to-end models face challenges due to the limited availability of speech translation data. ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']\n--------------------\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nTraditional speech translation systems are often cascaded, involving sequential ASR and MT components, which can lead to error propagation and high latency. End-to-end methods have emerged as a promising alternative, offering advantages like joint training and direct mapping from speech to text. However, end-to-end ST models struggle with data scarcity, making pre-training techniques essential to leverage ASR and MT data for improved performance.  ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nTraditional cascaded speech translation models consist of separate Automatic Speech Recognition (ASR) and Machine Translation (MT) components, which can suffer from error propagation and have separate training objectives ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. End-to-end models aim to overcome these limitations by directly mapping speech to text in a single model, allowing for joint training and potentially better performance ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The evolution towards end-to-end models is motivated by the desire to create more efficient, lower latency, and less error-prone speech translation systems ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n## 3. Techniques for Direct Speech-to-text Translation: Emphasizing Pre-training Strategies and Encoder Design Challenges\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nTechniques to improve direct speech-to-text translation systems often focus on pre-training strategies to overcome data scarcity. Pre-training the encoder using ASR models has shown to be effective. However, leveraging pre-trained MT decoders has been less successful, suggesting a gap between the representations learned by ASR encoders and the input expected by MT decoders. ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']\n--------------------\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThis paper focuses on pre-training strategies and encoder design for direct speech-to-text translation, particularly addressing the challenges of 'Modeling deficiency' and 'Representation inconsistency' when adapting pre-trained ASR and MT models for ST encoders. The paper introduces a novel approach called Stacked Acoustic-and-Textual Encoding (SATE) to effectively integrate pre-trained models and improve ST performance. ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nThe paper introduces a novel pre-training strategy called Fused Acoustic and Text Masked Language Model (FAT-MLM) to improve direct speech-to-text translation ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. This technique focuses on learning a unified representation for both acoustic and text inputs by leveraging various types of corpora, including parallel data for speech recognition and machine translation, as well as pure speech and text data ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The proposed method aims to address the challenge of data scarcity in speech translation by utilizing more abundant speech and text resources through a multimodal pre-training approach ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The experimental results show that fine-tuning a speech translation model (FAT-ST) from FAT-MLM substantially improves translation quality ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n### 3.1 Challenges in ST Encoder Design\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThe paper identifies two main challenges in ST encoder design when leveraging pre-trained ASR and MT models: 'Modeling deficiency' and 'Representation inconsistency'. 'Modeling deficiency' arises because ASR encoders tend to focus on local dependencies in the acoustic sequence, lacking the global context representation needed for translation, whereas MT encoders are designed to capture long-distance dependencies in language. 'Representation inconsistency' occurs because ST decoders are often initialized with MT decoders, expecting MT-like encoders, while ST encoders initialized with ASR models behave more like ASR encoders, leading to a mismatch. ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']\n### 3.2 Pre-training Strategies\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nPre-training strategies are crucial for enhancing the performance of direct ST models, especially in low-resource scenarios. Pre-training the encoder with ASR data is a common strategy. This paper investigates pre-training strategies, specifically focusing on using Machine Translation (MT) data to pre-train the decoder component of a Speech Translation (ST) system. It addresses the challenge of representation mismatch between ASR encoders and MT decoders by proposing an adversarial pre-training method. ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']\n--------------------\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThis paper introduces the Stacked Acoustic-and-Textual Encoding (SATE) method as a pre-training strategy. SATE addresses the identified challenges by cascading an ASR encoder and an MT encoder. The architecture consists of an acoustic encoder, an adaptor module, and a textual encoder. The acoustic encoder is pre-trained as an ASR model and is responsible for processing the acoustic input and is trained with CTC loss. The textual encoder is pre-trained as an MT encoder and is designed to generate a global representation suitable for translation. An adaptor module is introduced to bridge the gap between the acoustic and textual encoders, ensuring informative and adaptive representation transfer. Multi-teacher knowledge distillation (MTKD) is also employed to preserve pre-training knowledge during fine-tuning.  ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nThe paper proposes the Fused Acoustic and Text Masked Language Model (FAT-MLM) as a pre-training strategy to learn a unified representation for acoustic and text inputs ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. FAT-MLM extends the masked language model concept to multimodal corpora, including speech recognition and speech translation data ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The pre-training task involves reconstructing masked portions of both speech and text inputs based on their context within a shared encoder ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The transfer learning paradigm applied is heterogeneous, as it transfers knowledge from pre-training on combined acoustic and text data to the speech translation task ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The FAT-MLM method addresses the 'representation inconsistency' by learning a unified representation space for speech and text through joint pre-training ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n#### 3.2.1 Adversarial Pre-training with ASR and MT Data\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nThis paper proposes an adversarial pre-training method to effectively utilize MT data for pre-training the decoder in direct ST. The core idea is to use an adversarial regularizer to align the encoder representations from ASR and NMT tasks, even though they are from different modalities (speech and text). This approach involves simultaneously training ASR and NMT models with a shared encoder architecture and an adversarial loss term. The adversarial loss is designed to make the encoder representations indistinguishable to a discriminator network trained to identify the modality (ASR or NMT) of the encoder input. By minimizing this adversarial loss, the encoder is incentivized to learn representations that are modality-invariant, bridging the gap between speech and text modalities. The discriminator consists of a three-layer feedforward network with 1024 hidden units and Leaky-ReLU activation. The final training objective for both ASR and NMT tasks includes both the Cross-Entropy loss ($L_{CE}$) and the adversarial regularization loss ($L_{DISC}$), formulated as $L = L_{CE} + \\alpha L_{DISC}$, where $\\alpha$ is a hyperparameter set to 5 in the experiments to balance the two loss terms.  This method aims to make a pre-trained NMT decoder more compatible with an ASR encoder for speech translation. ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']\n#### 3.2.2 Self-Supervised Pre-training\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nThe Masked Acoustic Modeling (MAM) technique is mentioned as a self-supervised pre-training method for speech encoders, which can utilize speech data without transcriptions ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. MAM replaces spans of speech spectrograms with mask tokens and trains the model to reconstruct the masked spectrograms ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The paper compares FAT-MLM with MAM, showing that FAT-MLM achieves better performance in speech translation pre-training ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n#### 3.2.3 Curriculum Pre-training\n#### 3.2.4 Multimodal Pre-training\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nFAT-MLM is a multimodal pre-training approach that leverages both speech and text modalities during pre-training to learn unified representations ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. It fuses acoustic embeddings and text embeddings and uses a shared Transformer encoder to process both modalities jointly ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The pre-training objective, FAT-MLM loss, includes both speech reconstruction loss $\\ell_{\\mathbf{s}}(D_{\\mathbf{s},\\mathbf{x}})$ and text reconstruction loss $\\ell_{\\mathbf{x}}(D_{\\mathbf{s},\\mathbf{x}})$ for monolingual FAT-MLM, and additionally translation reconstruction loss $\\ell_{\\mathbf{y}}(D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})$ for translation FAT-MLM ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The final loss for monolingual FAT-MLM is:\n$$\n\\ell_{\\mathrm{FAT-MLM}}(D_{\\mathbf{s},\\mathbf{x}})=\\ell_{\\mathbf{s}}(D_{\\mathbf{s},\\mathbf{x}})+\\ell_{\\mathbf{x}}(D_{\\mathbf{s},\\mathbf{x}})\n$$\nand for translation FAT-MLM is:\n$$\n\\ell_{\\mathrm{FAT-MLM}}(D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})=\\ell_{\\mathbf{s}}(D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})+\\ell_{\\mathbf{x}}(D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})+\\ell_{\\mathbf{y}}(D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})\n$$\nThis multimodal pre-training is designed to bridge the modality gap by learning a common representation space ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n### 3.3 Data Augmentation Techniques\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nData augmentation is mentioned as a potential technique to further improve translation quality, but it is not explored in this paper. Prior studies have examined data augmentation techniques in speech translation. ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']\n--------------------\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThe paper utilizes SpecAugment as a data augmentation technique applied to input speech features to improve generalization and robustness of the ST model. SpecAugment contributed to a 1.9 BLEU point improvement over the cascaded baseline on the MuST-C En-De dataset. ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']\n### 3.4 Architectural Innovations and Key Components\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nThe paper utilizes the Transformer architecture and S-Transformer for the AST, ASR, and MT models. The S-Transformer uses CNN layers and 2D self-attention for encoding audio features. The architecture is based on prior work by Di Gangi et al. (2019b).  The core innovation is not in architectural changes to the encoder-decoder structure itself, but rather in the adversarial pre-training method applied to this architecture. ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']\n--------------------\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThe paper proposes the Stacked Acoustic-and-Textual Encoding (SATE) architecture. SATE stacks an acoustic encoder and a textual encoder, separated by an adaptor module. The acoustic encoder is designed to process acoustic features and is trained with CTC loss, mimicking an ASR encoder. The textual encoder, stacked on top, is designed to generate a global representation for translation, similar to an MT encoder. The adaptor module plays a crucial role in bridging the gap between the acoustic and textual representations. It produces a new representation from the acoustic encoding output that is suitable for the textual encoder, based on two principles: adaptive and informative. The adaptor combines a soft contextual representation derived from CTC output and a mapped acoustic representation using a single-layer neural network.  ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nThe paper proposes the Fused Acoustic and Text Speech Translation (FAT-ST) model, which is an end-to-end speech translation model architecture based on the FAT-MLM pre-training strategy ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. FAT-ST's encoder shares the same architecture as the FAT-MLM encoder, allowing it to encode both acoustic and text features ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The model can be optimized using a combination of speech translation loss $\\ell_{\\mathrm{ST}}$, machine translation loss $\\ell_{\\mathrm{MT}}$, and FAT-MLM loss $\\ell_{\\mathrm{FAT-MLM}}$ to leverage different types of data ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The loss function of FAT-ST is:\n$$\n{\\ell_{\\mathrm{FAT-ST}}(D_{\\mathbf{s},\\mathbf{y}}\\cup D_{\\mathbf{s},\\mathbf{x}}\\cup D_{\\mathbf{x},\\mathbf{y}})=\\ell_{\\mathrm{ST}}(D_{\\mathbf{s},\\mathbf{y}})+\\ell_{\\mathrm{MT}}(D_{\\mathbf{x},\\mathbf{y}})+{\\ell_{\\mathrm{FAT-MLM}}(D_{\\mathbf{s},\\mathbf{x}})}}\n$$\nThe FAT-ST architecture innovatively integrates acoustic and text encoding within a single encoder-decoder framework for speech translation ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n#### 3.4.1 Addressing the Modality Gap\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nThe proposed adversarial pre-training method is explicitly designed to address the modality gap between speech and text representations by making the encoder representations modality-invariant. ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']\n--------------------\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThe SATE architecture with the adaptor module is specifically designed to address the modality gap between speech and text. The adaptor module generates both a soft token representation $h_{\\mathrm{soft}}^{s}$ from CTC output and a mapped acoustic representation $h_{\\mathrm{map}}^{s}$ from the acoustic encoder output $h^{s}$. The final adaptor output $\\hat{h^{s}}$ is a fusion of these two representations:\n$$\nA(h^{s},P(\\pi|h^{s})) = \\lambda\\cdot h_{\\mathrm{map}}^{s} + (1-\\lambda)\\cdot h_{\\mathrm{soft}}^{s}\n$$\nThis combined representation aims to provide the textual encoder with an input that is both semantically meaningful (through soft tokens) and retains acoustic information relevant for translation (through mapped acoustic features), effectively bridging the modality gap. ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nThe FAT-ST model and FAT-MLM pre-training are specifically designed to address the modality gap between speech and text by learning a unified representation space ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. By jointly training on both acoustic and textual data and using a shared encoder, the model aims to create a more coherent and aligned representation across modalities, facilitating better speech translation performance ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n#### 3.4.2 Efficient and Compact Models\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nThe paper shows that the proposed FAT-ST model achieves state-of-the-art performance while maintaining a smaller model size and faster decoding time compared to cascaded systems and some other end-to-end models ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The FAT-ST base model has 39.34M parameters, which is less than cascaded models (83.79M) and some other end-to-end models like Le et al. (2020) (51.20M) ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n## 4. Datasets and Evaluation Metrics for Direct Speech-to-text Translation\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nThe study uses the MuST-C corpus for English-German speech translation and the Libri-Trans corpus for English-French. For ASR pre-training, the LibriSpeech corpus is used. For MT pre-training, TED and Opensubtitle2018 corpora are used for En-De, and WMT14 En-Fr corpus is used for En-Fr. BLEU score is used for evaluating translation quality (AST and MT), and Word Error Rate (WER) is used for ASR evaluation. ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']\n--------------------\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThe paper uses LibriSpeech English-French (En-Fr) and MuST-C English-German (En-De) corpora for experiments. LibriSpeech En-Fr is used as a low-resource dataset (100 hours), and MuST-C En-De as a relatively high-resource dataset (400 hours). For the unrestricted setting, additional data is used: 960 hours LibriSpeech ASR corpus and 10M/18M sentence pairs from WMT14 En-Fr/Opensubtitle 2018 En-De MT datasets. Evaluation metric used is case-sensitive SacreBLEU. ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nThe paper utilizes several datasets for training and evaluating the proposed FAT-MLM and FAT-ST models, categorized by modality and task ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. These include:\n- **MuST-C (Ds,x,y):** A speech translation dataset used for bilingual experiments, with sizes ranging from 408 to 504 hours of speech across different language pairs (En-De, En-Es, En-Nl) ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n- **Librispeech (Ds,x):** A speech recognition dataset with 960 hours of English speech, used to leverage speech and transcription data for pre-training and multi-task learning ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n- **Europarl MT (Dx,y):** A machine translation dataset with 1.9M to 2.0M sentence pairs for each language pair, used to incorporate text translation data ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n- **Libri-light Speech (Ds):** A speech-only dataset with 3,748 hours of English speech, used as unlabeled speech data for pre-training ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n- **Europarl/Wiki Text (Dx/Dy):** Monolingual text data from Europarl and Wiki Text (for Dutch), used as additional text data for pre-training ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\nThe models are evaluated using BLEU score on the MuST-C dev and test sets for speech translation quality ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n### 4.1 Multilingual Speech Translation Corpora\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nThe MuST-C corpus, used for En-De experiments, consists of 408 hours of speech data aligned with 234K translated sentences, derived from TED talks. ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']\n--------------------\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThe paper utilizes MuST-C En-De, a multilingual speech translation corpus derived from TED Talks, containing 400 hours of English speech translated to German with 230K utterances. ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nMuST-C is used as the primary multilingual speech translation corpus in the experiments ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. It is based on TED Talks and contains spontaneous speeches, making it a challenging dataset for speech translation ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The dataset includes language pairs such as English to German, English to Spanish, and English to Dutch, with sizes around 400-500 hours of speech per language pair ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n### 4.2 Large-scale Pseudo Speech Translation Corpora\n### 4.3 Benchmarking and Evaluation Campaigns (IWSLT)\n## 5. Toolkits and Frameworks for Direct Speech-to-text Translation\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThe experiments are implemented based on the ESPnet toolkit. ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nThe paper mentions using ESPnet-ST as a baseline framework for their Transformer-based end-to-end speech translation system ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The implementation details are similar to ESPnet-ST, suggesting the use of ESPnet or similar toolkits for direct ST development ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n### 5.1 Open-Source Toolkits vs. Commercial Toolkits for ST\n### 5.2 Feature Comparison of Popular ST Toolkits (e.g., ESPnet-ST, Fairseq-ST)\n### 5.3 Toolkits for Specific Tasks in Direct ST (e.g., Low-Resource ST Toolkits)\n## 6. Challenges and Future Directions in Direct Speech-to-text Translation\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nThe paper identifies that while the proposed adversarial pre-training method improves performance, the cascaded model with access to external ASR and MT data still achieves better translation quality. Future directions include exploring data augmentation techniques and other methods to further improve the performance of end-to-end ST models to close the gap with cascaded systems.  Further research could investigate the effectiveness of this adversarial pre-training approach with different network architectures, datasets, and language pairs. ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']\n--------------------\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThe paper highlights the challenge of pre-training in ST due to the differences between ASR and MT encoders, specifically 'modeling deficiency' and 'representation inconsistency'. Future directions include investigating the sequence length inconsistency issue in the adaptor module of SATE. ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nThe paper addresses the challenge of data scarcity in direct speech translation by proposing a pre-training method that leverages additional speech recognition and machine translation data ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The limitations of current end-to-end ST models include their performance gap compared to cascaded systems, especially on spontaneous speech ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. Future research directions could focus on further improving the unified representation learning for speech and text, exploring more effective pre-training tasks and architectures, and scaling up models and training data to close the performance gap with cascaded systems and achieve robust speech translation in various scenarios ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n### 6.1 6.x Ethical and Societal Implications\n## 7. Conclusion\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nThis paper concludes that adversarial regularization can effectively align latent representations of speech and text modalities, making MT decoder pre-training more effective for direct speech translation. The proposed method improves performance by approximately 1.5 BLEU points compared to conventional pre-training methods on En-De and En-Fr language pairs. The main contribution is demonstrating the effectiveness of adversarial loss in bridging the modality gap and improving the transfer of knowledge from MT data to ST decoder pre-training. ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']\n--------------------\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThe paper concludes that the Stacked Acoustic-and-Textual Encoding (SATE) method effectively addresses the challenges of pre-training in end-to-end speech translation by integrating pre-trained ASR and MT models. SATE achieves state-of-the-art performance on LibriSpeech En-Fr and MuST-C En-De datasets, even surpassing cascaded ST systems when large-scale ASR and MT data are available. The proposed adaptor module and multi-teacher knowledge distillation further enhance the performance by bridging the modality gap and preserving pre-training knowledge. The SATE approach demonstrates the potential of end-to-end ST to match or exceed the performance of cascaded systems while offering reduced latency and a simplified pipeline. ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nThe paper concludes that the proposed Fused Acoustic and Text Masked Language Model (FAT-MLM) effectively learns a unified representation for text and speech, and the Fused Acoustic and Text Speech Translation (FAT-ST) model, pre-trained with FAT-MLM, significantly improves end-to-end speech translation quality ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The FAT-ST model achieves state-of-the-art performance on the MuST-C dataset, reaching comparable accuracy to cascaded systems while being more efficient and having a smaller model size ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The main contribution is the introduction of FAT-MLM and FAT-ST, which enable leveraging diverse speech and text data for pre-training and improve the performance of direct speech translation models ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The overall outlook for the field is positive, with multimodal pre-training showing promising results in addressing data scarcity and improving the quality of direct speech translation systems ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']."}, {"bibkey": "gigast_a_10000_hour_pseudo_speech_translation_corpus, must_c_a_multilingual_speech_translation_corpus, findings_of_the_iwslt_2022_evaluation_campaign", "content": "#  Recent Advances in Direct Speech-to-text Translation\n## 1. Introduction\nPaper bibkey: [must_c_a_multilingual_speech_translation_corpus]\nDigest: \nDirect speech-to-text translation (ST) is gaining attention with the rise of neural end-to-end approaches in natural language processing, mirroring the success in automatic speech recognition (ASR) and machine translation (MT) driven by increased computing power and data availability ['must_c_a_multilingual_speech_translation_corpus']. End-to-end models for ST offer advantages over cascaded ASR+MT systems, yet their mainstream adoption is hindered by the scarcity of sizeable training corpora ['must_c_a_multilingual_speech_translation_corpus']. This paper introduces MuST-C, a multilingual ST corpus designed to address this data gap and facilitate end-to-end SLT system training ['must_c_a_multilingual_speech_translation_corpus'].\n--------------------\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nThe International Conference on Spoken Language Translation (IWSLT) is a premier annual scientific conference dedicated to all aspects of spoken language translation ['findings_of_the_iwslt_2022_evaluation_campaign'].  The 2022 IWSLT Evaluation Campaign featured eight shared tasks addressing scientific challenges in spoken language translation, including simultaneous and offline speech translation, which can be considered as direct speech-to-text translation tasks ['findings_of_the_iwslt_2022_evaluation_campaign']. Direct speech-to-text translation is motivated by the need for efficient and low-latency spoken language translation systems, offering advantages over cascaded systems by streamlining the pipeline and potentially reducing error propagation ['findings_of_the_iwslt_2022_evaluation_campaign'].\n## 2. Background: Evolution of Speech Translation Paradigms\nPaper bibkey: [must_c_a_multilingual_speech_translation_corpus]\nDigest: \nTraditional cascaded ASR+MT systems, while capable of leveraging task-specific data for ASR and MT, face challenges in spoken language translation (SLT) due to error propagation between stages and separate training objectives ['must_c_a_multilingual_speech_translation_corpus']. In contrast, end-to-end SLT models theoretically offer advantages through joint training and direct mapping from speech to text, but their practical application is limited by the lack of large-scale, publicly available training corpora, unlike ASR and MT which benefit from abundant data resources ['must_c_a_multilingual_speech_translation_corpus']. The scarcity of SLT corpora is a major obstacle to the progress and adoption of end-to-end paradigms in SLT, in comparison to the steady progress observed in ASR and MT ['must_c_a_multilingual_speech_translation_corpus'].\n--------------------\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nIn recent years, offline speech translation has seen a rapid evolution, with direct end-to-end models advancing and reducing the performance gap compared to traditional cascaded approaches (ASR + MT) ['findings_of_the_iwslt_2022_evaluation_campaign']. The 2022 IWSLT campaign aimed to compare these two paradigms, reflecting the ongoing research and development in both cascaded and end-to-end approaches for speech translation ['findings_of_the_iwslt_2022_evaluation_campaign']. The campaign acknowledges the progress in end-to-end models, referencing findings that suggest the performance gap between cascaded and end-to-end approaches has substantially closed ['findings_of_the_iwslt_2022_evaluation_campaign'].\n## 3. Techniques for Direct Speech-to-text Translation: Emphasizing Pre-training Strategies and Encoder Design Challenges\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThe paper evaluates the performance of Speech-Transformer and SSL-Transformer models for direct speech-to-text translation using the newly created GigaST dataset. They explored different model sizes (S-Transf-S, S-Transf-M, S-Transf-L) for Speech-Transformer, and incorporated pre-trained speech encoders (wav2vec2-base, wav2vec2-large, hubert-base, hubert-large) for SSL-Transformer models to improve translation performance. The experiments show that increasing the size of the training dataset with GigaST improves BLEU scores across various test sets for all models. SSL-Transformer models generally outperform Speech-Transformer models, indicating the effectiveness of pre-trained speech encoders in direct ST.  The study also found that fine-tuning pre-trained speech encoders is crucial for ST tasks, as freezing them leads to significantly worse performance, contrasting with ASR tasks where freezing can be effective. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n--------------------\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nThe IWSLT 2022 Evaluation Campaign showcased various techniques employed by participating teams in direct speech-to-text translation, particularly in the offline and simultaneous speech translation tasks ['findings_of_the_iwslt_2022_evaluation_campaign']. These techniques include the use of pre-trained models, data augmentation, and different neural network architectures, with both cascaded and end-to-end approaches being explored ['findings_of_the_iwslt_2022_evaluation_campaign'].\n### 3.1 Challenges in ST Encoder Design\n### 3.2 Pre-training Strategies\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThe paper investigates the effectiveness of incorporating self-supervised learning (SSL) based pre-trained speech encoders (Wav2vec2 and HuBERT) into direct ST models. They compare SSL-Transformer models, which replace Fbank features in Speech-Transformer with representations from these pre-trained encoders, against standard Speech-Transformer models. The SSL modules are not frozen during training, and the entire model is fine-tuned on ST data. The results demonstrate that SSL-Transformer models generally achieve better performance than Speech-Transformer models, especially when training data is limited.  This suggests that pre-trained speech encoders, leveraging self-supervision on large amounts of unlabeled speech, provide more effective speech representations for direct ST, thus acting as a form of pre-training strategy. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n--------------------\nPaper bibkey: [must_c_a_multilingual_speech_translation_corpus]\nDigest: \nThe paper mentions that current neural SLT approaches circumvent the problem of data scarcity by relying on techniques like encoder/decoder pre-training, which assumes the availability of ASR and MT data ['must_c_a_multilingual_speech_translation_corpus']. This highlights the importance of pre-training as a strategy when direct ST data is limited, and the reliance on related tasks like ASR and MT to improve ST performance, especially in data-scarce scenarios ['must_c_a_multilingual_speech_translation_corpus'].\n--------------------\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nPre-training strategies were widely adopted by participants in the IWSLT 2022 campaign to enhance direct speech-to-text translation systems ['findings_of_the_iwslt_2022_evaluation_campaign'].  Many teams integrated pre-trained models, including audio encoders like wav2vec 2.0 and HuBERT, and text models like mBART, into their submissions for the offline speech translation task ['findings_of_the_iwslt_2022_evaluation_campaign']. In the simultaneous speech translation task, CUNI-KIT utilized a pre-trained wav2vec 2.0 encoder and a pre-trained mBART decoder in their end-to-end multilingual model ['findings_of_the_iwslt_2022_evaluation_campaign']. UPC also used wav2vec 2.0 and HuBERT as speech encoders and mBART50 fine-tuned on multilingual MT as a text decoder ['findings_of_the_iwslt_2022_evaluation_campaign']. The success of systems using pre-trained models in the campaign suggests their importance as knowledge sources for speech translation ['findings_of_the_iwslt_2022_evaluation_campaign'].\n#### 3.2.1 Adversarial Pre-training with ASR and MT Data\n#### 3.2.2 Self-Supervised Pre-training\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThe paper utilizes pre-trained speech encoders Wav2vec2 and HuBERT, which are based on self-supervised learning techniques, to enhance the performance of direct ST models. These encoders are pre-trained on large amounts of unlabeled speech data using tasks like Masked Acoustic Modeling, enabling them to learn robust and generalizable speech representations. In the SSL-Transformer models evaluated in the paper, representations from w2v2-base, w2v2-large, hubert-base, and hubert-large are used as input features instead of Fbank features. The experimental results show that SSL-Transformer models, leveraging these self-supervised pre-trained encoders, consistently outperform Speech-Transformer models, especially in low-data scenarios. For instance, using hubert-Large trained on 250 hours of speech data achieves comparable or even better performance than S-Transf S trained on 10,000 hours of data in En-Zh translation. This highlights the effectiveness of self-supervised pre-training for improving direct ST, particularly in data-constrained settings. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n--------------------\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nSelf-supervised pre-training models like wav2vec 2.0 and HuBERT were prominently used by several teams in the IWSLT 2022 campaign ['findings_of_the_iwslt_2022_evaluation_campaign']. These models served as pre-trained audio encoders in end-to-end speech translation systems, demonstrating the effectiveness of leveraging self-supervised learning for feature extraction from speech data ['findings_of_the_iwslt_2022_evaluation_campaign'].\n#### 3.2.3 Curriculum Pre-training\n#### 3.2.4 Multimodal Pre-training\n### 3.3 Data Augmentation Techniques\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThe paper introduces GigaST, a large-scale pseudo speech-to-text translation corpus, as a form of data augmentation for direct ST. GigaST is created by translating the transcript of the GigaSpeech ASR corpus into German and Chinese using machine translation models. The training set translations are generated by a strong MT system, while the test set translations are manually annotated. The creation of GigaST effectively augments the available ST training data by up to 25 times compared to existing open-source datasets like MuST-C. Experiments demonstrate that training ST models with the addition of GigaST leads to significant improvements in BLEU scores on MuST-C En-De and GigaST En-Zh benchmarks, achieving new state-of-the-art results on MuST-C En-De test set. This highlights the effectiveness of using large-scale pseudo ST corpora, generated through MT, as a data augmentation technique to enhance the performance of direct ST models, particularly in data-hungry end-to-end approaches. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n--------------------\nPaper bibkey: [must_c_a_multilingual_speech_translation_corpus]\nDigest: \nThe paper mentions that current neural SLT approaches use synthesized speech data or machine-translated target text data as data augmentation methods to circumvent the problem of data scarcity ['must_c_a_multilingual_speech_translation_corpus']. However, it points out that these methods rely on training material derived from sub-optimal automatic data creation/augmentation procedures ['must_c_a_multilingual_speech_translation_corpus'].\n--------------------\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nData augmentation methods were utilized by some participants to improve the robustness and performance of their speech translation systems ['findings_of_the_iwslt_2022_evaluation_campaign']. For example, XIAOMI employed tagged backtranslation, knowledge distillation, and iterative backtranslation for data augmentation in their text-to-text simultaneous translation system ['findings_of_the_iwslt_2022_evaluation_campaign']. AISP-SJTU also used knowledge distillation and tagged backtranslation, along with marking data with lowercased and non-punctuated input, for their simultaneous speech translation system ['findings_of_the_iwslt_2022_evaluation_campaign']. UPC highlighted data augmentation as a key aspect of their submission to the offline speech translation task ['findings_of_the_iwslt_2022_evaluation_campaign'].\n### 3.4 Architectural Innovations and Key Components\nPaper bibkey: [must_c_a_multilingual_speech_translation_corpus]\nDigest: \nFor ASR and SLT experiments, the paper uses an attentional encoder-decoder model with specific components. The encoder consists of two fully-connected layers, two 2D strided convolutional layers, and three stacked LSTMs. The decoder uses a two-layered deep transition LSTM with a general soft attention network ['must_c_a_multilingual_speech_translation_corpus'].\n--------------------\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nVarious architectural choices and innovations were explored by participants in the IWSLT 2022 campaign. Conformer architecture was used by FBK in their end-to-end simultaneous and offline speech translation systems, aiming to reduce computational costs and avoid ASR encoder pre-training ['findings_of_the_iwslt_2022_evaluation_campaign']. HW-TSC contrasted cascaded and end-to-end methods and used Conformer and Transformer models for both ASR and MT components in their cascaded systems ['findings_of_the_iwslt_2022_evaluation_campaign']. USTC-NELSLIP also combined Transformer and Conformer architectures in both cascaded and end-to-end systems ['findings_of_the_iwslt_2022_evaluation_campaign']. MLLP-VRAIN adopted a cascaded approach with DNN-HMM ASR and Transformer-based MT models for simultaneous translation ['findings_of_the_iwslt_2022_evaluation_campaign']. CUNI-KIT proposed a method to convert an offline model to a simultaneous model by chunking input and detecting stable hypotheses, using a pretrained wav2vec 2.0 encoder and mBART decoder ['findings_of_the_iwslt_2022_evaluation_campaign'].\n#### 3.4.1 Addressing the Modality Gap\n#### 3.4.2 Efficient and Compact Models\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nFBK focused on reducing computation requirements in their end-to-end speech translation system, aiming to make the task more accessible to academic participants, which can be seen as an effort towards more efficient models ['findings_of_the_iwslt_2022_evaluation_campaign'].\n## 4. Datasets and Evaluation Metrics for Direct Speech-to-text Translation\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThis paper introduces GigaST, a large-scale pseudo speech-to-text translation corpus, designed to address the data scarcity issue in direct ST. GigaST is presented as a new dataset for training and evaluating ST systems, offering significantly larger scale compared to existing datasets. The paper details the creation process of GigaST, including the use of GigaSpeech ASR corpus as the source speech data and machine translation for generating target language translations. It provides statistics on the size and composition of GigaST in English-Chinese (En-Zh) and English-German (En-De) language pairs, across different subsets (S, M, L, XL, Test).  The paper uses BLEU score as the primary evaluation metric for ST performance and evaluates models on GigaST test sets, MuST-C tst-COMMON for En-De, and an in-house test set for En-Zh. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n--------------------\nPaper bibkey: [must_c_a_multilingual_speech_translation_corpus]\nDigest: \nThe paper focuses on the creation of MuST-C, a large multilingual corpus for direct ST from English to 8 languages (German, Spanish, French, Italian, Dutch, Portuguese, Romanian, and Russian) ['must_c_a_multilingual_speech_translation_corpus']. The corpus addresses the scarcity of training data for end-to-end SLT models, which is a major bottleneck in the field ['must_c_a_multilingual_speech_translation_corpus']. The paper uses Word Error Rate (WER) for ASR evaluation and BLEU score for MT and SLT evaluation ['must_c_a_multilingual_speech_translation_corpus'].\n--------------------\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nThe IWSLT evaluation campaign itself serves as a benchmark for direct speech-to-text translation, utilizing specific datasets and evaluation metrics for each task ['findings_of_the_iwslt_2022_evaluation_campaign']. Datasets used in the offline speech translation task included MuST-C V2 (English-German, English-Chinese, English-Japanese), MuST-C V1, CoVoST, and others, primarily TED talks, for training and development ['findings_of_the_iwslt_2022_evaluation_campaign']. For evaluation, new test sets were created from TED talks for each language direction ['findings_of_the_iwslt_2022_evaluation_campaign']. Evaluation metrics for translation quality included BLEU and chrF, and for simultaneous translation, latency metrics like Average Proportion (AP), Average Lagging (AL), and Differentiable Average Lagging (DAL) were used, along with human evaluation in some tasks ['findings_of_the_iwslt_2022_evaluation_campaign'].\n### 4.1 Multilingual Speech Translation Corpora\nPaper bibkey: [must_c_a_multilingual_speech_translation_corpus]\nDigest: \nMuST-C is a multilingual speech translation corpus with approximately 400 hours of English speech per target language, translated into 8 languages, sourced from TED Talks ['must_c_a_multilingual_speech_translation_corpus']. Each language section of MuST-C is larger than any existing publicly available SLT resource at the time of publication ['must_c_a_multilingual_speech_translation_corpus']. The corpus is created using a methodology involving downloading TED talks, aligning manual transcriptions and translations at the sentence level using Gargantua, and then aligning the English side with audio using Gentle forced aligner ['must_c_a_multilingual_speech_translation_corpus']. Quality control steps include filtering out talks and sentences with high unrecognized word proportions based on forced alignment results ['must_c_a_multilingual_speech_translation_corpus'].\n--------------------\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nMuST-C V2 corpus was used for the offline speech translation task, providing data for English-German, English-Chinese, and English-Japanese language pairs ['findings_of_the_iwslt_2022_evaluation_campaign']. These corpora, derived from TED talks, include training, development, and test sets, structured similarly across the language directions ['findings_of_the_iwslt_2022_evaluation_campaign'].\n### 4.2 Large-scale Pseudo Speech Translation Corpora\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThe paper focuses on the creation of GigaST, a large-scale pseudo ST corpus. GigaST is generated by translating the transcripts of the GigaSpeech ASR corpus into German and Chinese. For the training set, high-quality MT models were used for translation, while for the test sets, human translators provided translations to ensure quality and avoid alignment errors present in datasets like MuST-C. The paper details the MT model training process, including the use of WMT2021 and OPUS datasets, data filtering, pre-processing, and techniques like iterative sequence-level knowledge distillation and back translation.  The quality of the MT-generated training data is verified through both automated metrics (BLEU) and human evaluation, showing comparable performance to online MT systems and reasonable semantic consistency with source texts. The test sets are human-translated and verified, aiming for high quality. GigaST is significantly larger than existing ST datasets, with the XL subset reaching approximately 10,000 hours of speech, making it a valuable resource for training data-intensive direct ST models. The statistics of GigaST, including the number of segments, hours of speech, and tokens for different subsets and language pairs, are provided in Table 3 of the paper. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n### 4.3 Benchmarking and Evaluation Campaigns (IWSLT)\nPaper bibkey: [must_c_a_multilingual_speech_translation_corpus]\nDigest: \nThe paper compares MuST-C with the English-German IWSLT18 corpus in terms of corpus quality and system performance ['must_c_a_multilingual_speech_translation_corpus']. Experiments show that models trained on MuST-C data achieve better ASR, MT, and SLT performance compared to those trained on IWSLT18, indicating higher corpus quality ['must_c_a_multilingual_speech_translation_corpus']. The IWSLT18 corpus is mentioned as a TED-derived English-German corpus, similar to MuST-C in source data but different in creation methodology, which uses time information from SRT files for segment extraction and alignment ['must_c_a_multilingual_speech_translation_corpus'].\n--------------------\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nThe IWSLT evaluation campaign is a significant benchmarking effort in the field of spoken language translation ['findings_of_the_iwslt_2022_evaluation_campaign']. The 2022 campaign featured shared tasks in simultaneous, offline, speech-to-speech, low-resource, multilingual, dialect, formality control, and isometric speech translation ['findings_of_the_iwslt_2022_evaluation_campaign'].  It utilized specific datasets for each task, applied automatic metrics like BLEU and latency measures, and incorporated human evaluation to assess translation and speech quality, providing valuable insights into the state-of-the-art and progress in the field ['findings_of_the_iwslt_2022_evaluation_campaign'].\n## 5. Toolkits and Frameworks for Direct Speech-to-text Translation\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThe paper mentions using NeurST toolkit for implementing and evaluating the ST models. They also release the training scripts on NeurST to facilitate replication of their systems. This suggests that NeurST is a toolkit that supports direct ST development and is used in their experiments. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n--------------------\nPaper bibkey: [must_c_a_multilingual_speech_translation_corpus]\nDigest: \nThe ASR and SLT models in the paper are re-implemented in the fairseq toolkit ['must_c_a_multilingual_speech_translation_corpus']. Log Mel 40-dimensional filter-bank features are extracted from the aligned audio using the XNMT tool ['must_c_a_multilingual_speech_translation_corpus']. The MT experiments use the open source version of ModernMT, which is based on the Transformer architecture ['must_c_a_multilingual_speech_translation_corpus'].\n--------------------\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nESPnet was used by CMU and ON-TRAC teams for their submissions in the dialect speech translation task and low-resource speech translation task, respectively, indicating its utility for direct ST development ['findings_of_the_iwslt_2022_evaluation_campaign']. Fairseq was utilized by JHU and ON-TRAC for the MT component in their cascaded systems for the dialect speech translation task and low-resource speech translation task, respectively, showcasing its role in speech translation frameworks ['findings_of_the_iwslt_2022_evaluation_campaign']. SIMULEvAL toolkit was employed for evaluating simultaneous speech translation systems in terms of latency and quality ['findings_of_the_iwslt_2022_evaluation_campaign']. Appraise was used for conducting human evaluations in the offline, simultaneous, and isometric speech translation tasks, highlighting its importance in assessing translation quality from a human perspective ['findings_of_the_iwslt_2022_evaluation_campaign'].\n### 5.1 Open-Source Toolkits vs. Commercial Toolkits for ST\n### 5.2 Feature Comparison of Popular ST Toolkits (e.g., ESPnet-ST, Fairseq-ST)\n### 5.3 Toolkits for Specific Tasks in Direct ST (e.g., Low-Resource ST Toolkits)\n## 6. Challenges and Future Directions in Direct Speech-to-text Translation\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThe paper implicitly addresses the challenge of data scarcity in direct ST by introducing the large-scale GigaST dataset. Their experimental results highlight that increasing training data size significantly improves ST performance, suggesting that data scarcity is a major limitation in the field.  Furthermore, in the discussion of cascade vs. end-to-end systems, the paper notes that while end-to-end models trained on the same data perform better than cascade systems, cascade models can achieve higher BLEU scores when boosted by larger amounts of text training data available for MT. This implies that further improvement in end-to-end ST might require techniques to leverage large text corpora more effectively, possibly through pre-training of decoders or multi-task learning. The paper also points out the performance gap between end-to-end and cascade models, even with powerful MT models, suggesting room for improvement in end-to-end ST, particularly in areas like decoder pre-training and multi-task training, which they propose as future work. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n--------------------\nPaper bibkey: [must_c_a_multilingual_speech_translation_corpus]\nDigest: \nThe paper suggests that future work will extend MuST-C by increasing the coverage of existing target languages and adding new languages, leveraging the scalable corpus creation procedure and the expanding TED Talks data ['must_c_a_multilingual_speech_translation_corpus']. The ongoing expansion of TED talks provides a continuous source for creating and expanding SLT corpora like MuST-C ['must_c_a_multilingual_speech_translation_corpus'].\n--------------------\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nThe IWSLT 2022 campaign identified several challenges and future directions in direct speech-to-text translation research ['findings_of_the_iwslt_2022_evaluation_campaign']. For simultaneous speech translation, challenges include adapting latency metrics for long unsegmented input and extending systems to support speech output ['findings_of_the_iwslt_2022_evaluation_campaign']. For offline speech translation, further investigation into the comparison between cascade and end-to-end technologies is needed, along with exploring the usefulness of pre-trained models ['findings_of_the_iwslt_2022_evaluation_campaign']. Low-resource and dialect speech translation remain highly challenging areas requiring more research ['findings_of_the_iwslt_2022_evaluation_campaign']. The introduction of speech-to-speech translation as a task opens new avenues for future research, including simultaneous S2ST and deeper human evaluation of speech output quality ['findings_of_the_iwslt_2022_evaluation_campaign'].\n### 6.1 6.x Ethical and Societal Implications\n## 7. Conclusion\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThe paper concludes that GigaST, the newly introduced large-scale pseudo ST corpus, is a valuable resource for training and evaluating ST systems.  It summarizes the key contribution as the creation and release of GigaST, demonstrating its effectiveness in improving ST performance when used to train Speech-Transformer and SSL-Transformer models. The paper highlights that models trained with GigaST achieve new state-of-the-art results on the MuST-C En-De benchmark. The release of the dataset, human-translated test sets, and training scripts is intended to facilitate further research in speech translation and open new avenues in the field. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n--------------------\nPaper bibkey: [must_c_a_multilingual_speech_translation_corpus]\nDigest: \nMuST-C is introduced as a large, publicly available multilingual speech translation corpus to address the resource scarcity for training data-hungry neural SLT models ['must_c_a_multilingual_speech_translation_corpus']. It is the largest publicly available corpus of its kind at the time of publication, containing at least 385 hours of speech per language in 8 languages ['must_c_a_multilingual_speech_translation_corpus']. The corpus and its creation methodology are expected to facilitate research in end-to-end SLT by providing a high-quality, large-scale resource ['must_c_a_multilingual_speech_translation_corpus'].\n--------------------\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nThe IWSLT 2022 Evaluation Campaign showcased key advances and the current state-of-the-art in direct speech-to-text translation and related tasks ['findings_of_the_iwslt_2022_evaluation_campaign']. The campaign highlighted the progress in offline speech translation, the emergence of speech-to-speech translation, and the continued challenges in low-resource and dialect speech translation ['findings_of_the_iwslt_2022_evaluation_campaign']. A significant takeaway is the increasing importance and effectiveness of pre-trained models in improving translation quality, particularly in offline settings ['findings_of_the_iwslt_2022_evaluation_campaign']. The campaign also underscored the need for further research into various aspects of speech translation, including architectural choices, evaluation methodologies, and addressing the unique challenges of different speech translation scenarios ['findings_of_the_iwslt_2022_evaluation_campaign']."}, {"bibkey": "stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders, a_comprehensive_survey_on_transfer_learning, fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation", "content": "#  Recent Advances in Direct Speech-to-text Translation\n## 1. Introduction\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nDirect speech-to-text translation (ST) has gained popularity due to its ability to simplify translation pipelines and reduce latency compared to traditional cascaded systems. However, the scarcity of speech-to-translation data poses a significant challenge. Pre-trained Automatic Speech Recognition (ASR) and Machine Translation (MT) models are often used to enhance ST systems, but end-to-end ST systems with pre-trained models still struggle to outperform cascaded ST models when large-scale ASR and MT data are available ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']. This paper explores the reasons for these challenges and proposes a novel approach to effectively integrate pre-trained models into end-to-end ST encoders, aiming to bridge the performance gap with cascaded systems ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders'].\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nEnd-to-end speech translation (ST) is advantageous over traditional pipeline paradigms due to its low latency, reduced error propagation, and fewer parameters ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. However, the translation quality of end-to-end ST models is limited by the scarcity of large-scale parallel speech translation data, despite the abundance of data for speech recognition and text machine translation ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n## 2. Background: Evolution of Speech Translation Paradigms\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nTraditional cascaded speech translation systems, which sequentially run ASR and MT models, suffer from error propagation and high latency. End-to-end ST models offer theoretical advantages by jointly training and directly mapping speech to text. However, the limited availability of annotated speech-to-translation data hinders the training of robust end-to-end ST models. Data augmentation and multi-task learning have been explored to mitigate data scarcity. Pre-training different components of the ST system using ASR and MT data has emerged as a promising approach, but integrating pre-trained models effectively in ST remains challenging, often underperforming compared to cascaded models, motivating the need for improved pre-training strategies in end-to-end ST ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders'].\n## 3. Techniques for Direct Speech-to-text Translation: Emphasizing Pre-training Strategies and Encoder Design Challenges\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThis paper focuses on pre-training strategies and encoder design challenges in direct ST. It identifies key challenges in ST encoder design and proposes a novel pre-training method called Stacked Acoustic-and-Textual Encoding (SATE) to address these challenges and effectively leverage pre-trained ASR and MT models ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders'].\n--------------------\nPaper bibkey: [a_comprehensive_survey_on_transfer_learning]\nDigest: \nThis survey broadly categorizes transfer learning approaches into data-based and model-based methods, offering a structured way to understand techniques applicable to direct speech-to-text translation ['a_comprehensive_survey_on_transfer_learning']. Data-based approaches adjust and transform data, aiming to reduce distribution differences, while model-based approaches focus on model-level regularizations to transfer knowledge from source models to target models ['a_comprehensive_survey_on_transfer_learning']. These perspectives can be used to categorize techniques used in direct ST, although the survey itself does not directly discuss ST techniques.\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nTo address the limitations of existing pre-training models which only handle single modality (text or speech), the paper proposes Fused Acoustic and Text Masked Language Model (FAT-MLM), a multimodal pretraining model which encodes acoustic and text inputs into a unified representation ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. Based on FAT-MLM, the paper further proposes Fused Acoustic and Text Speech Translation model (FAT-ST) in a sequence-to-sequence framework, which can be trained from speech translation, speech recognition and text machine translation data in a single encoder-decoder model ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n### 3.1 Challenges in ST Encoder Design\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThe paper highlights two main challenges in designing effective ST encoders: 'Modeling deficiency' and 'Representation inconsistency' ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']. 'Modeling deficiency' refers to the fact that ASR encoders, when used as ST encoders, tend to focus on local dependencies in acoustic sequences and lack the ability to capture long-distance dependencies crucial for translation, unlike MT encoders which are designed for capturing global context in text. 'Representation inconsistency' arises because while ST decoders are often initialized with MT decoders expecting MT-like encoder representations, the ST encoders, often initialized as ASR encoders, behave more like ASR encoders, leading to a mismatch in representation spaces between the encoder and decoder ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders'].\n### 3.2 Pre-training Strategies\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThe paper introduces Stacked Acoustic-and-Textual Encoding (SATE) as a pre-training strategy to address the challenges in direct ST ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']. SATE consists of two cascaded encoders: an acoustic encoder and a textual encoder. The acoustic encoder processes the input speech and is pre-trained as an ASR encoder, focusing on acoustic feature processing. The textual encoder is stacked on top of the acoustic encoder and is pre-trained as an MT encoder, aiming to capture global context for translation ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']. An adaptor module is introduced between the acoustic and textual encoders to bridge the representation gap and ensure informative and adaptive transfer of acoustic encoding output to the textual encoder ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']. To further enhance pre-training knowledge preservation during fine-tuning, a multi-teacher knowledge distillation (MTKD) method is employed, using pre-trained ASR and MT models as teachers to guide the training of the SATE model ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders'].\n--------------------\nPaper bibkey: [a_comprehensive_survey_on_transfer_learning]\nDigest: \nPre-training strategies in transfer learning are crucial for improving model performance by leveraging knowledge from related domains ['a_comprehensive_survey_on_transfer_learning']. The survey discusses various model-based transfer learning strategies that could be adapted as pre-training methods for direct ST. These strategies aim to control model parameters or enforce model-level regularizations to facilitate knowledge transfer ['a_comprehensive_survey_on_transfer_learning']. Homogeneous transfer learning, which deals with domains having the same feature space, is a primary focus, and its techniques are particularly relevant when adapting models for ST tasks with similar input features (e.g., spectrograms) ['a_comprehensive_survey_on_transfer_learning'].\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nThe paper introduces Fused Acoustic and Text Masked Language Model (FAT-MLM) to learn a unified acoustic and text representation ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. FAT-MLM extends the masked language model to multimodal corpora containing both acoustic and text data, such as speech recognition and speech translation data ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. There are two versions of FAT-MLM: Monolingual FAT-MLM and Translation FAT-MLM ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. Monolingual FAT-MLM takes speech and transcription tuples as input, and Translation FAT-MLM takes speech, transcription and translation triplets as input ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The pre-training task is to reconstruct the masked speech signal and masked tokens in transcription and translation ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The model architecture consists of Convolution layers and Transformer encoders for acoustic embeddings, and Transformer encoder for text embeddings, and another Transformer encoder to fuse acoustic and text embeddings ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The transfer learning paradigm is heterogeneous transfer learning, transferring knowledge from FAT-MLM pre-trained model to FAT-ST speech translation model ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n#### 3.2.1 Adversarial Pre-training with ASR and MT Data\nPaper bibkey: [a_comprehensive_survey_on_transfer_learning]\nDigest: \nThe survey discusses adversarial deep learning as a technique where a generator and a discriminator play a minimax game ['a_comprehensive_survey_on_transfer_learning']. This concept of adversarial learning can be relevant to pre-training ST models by using ASR or MT data to adversarially train encoders to be domain-invariant or to bridge modality gaps, although the paper does not provide specific examples in ST ['a_comprehensive_survey_on_transfer_learning'].\n#### 3.2.2 Self-Supervised Pre-training\nPaper bibkey: [a_comprehensive_survey_on_transfer_learning]\nDigest: \nSelf-supervised pre-training is mentioned as a technique that can be incorporated into transfer learning ['a_comprehensive_survey_on_transfer_learning']. The survey introduces Stacked Denoising Autoencoder (SDA) and Marginalized Stacked Linear Denoising Autoencoder (mSLDA) as examples of deep learning models that use autoencoders for feature encoding ['a_comprehensive_survey_on_transfer_learning']. These methods, which learn representations from unlabeled data, align with the principles of self-supervised pre-training and could be explored for ST using unlabeled speech data, though not explicitly detailed in the paper ['a_comprehensive_survey_on_transfer_learning'].\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nMasked Acoustic Modeling (MAM) is mentioned as a self-supervised pre-training technique for speech encoder, which is related to FAT-MLM in terms of masked input and reconstruction objective, but MAM only operates on speech modality ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. FAT-MLM can be seen as extending self-supervised pre-training to multimodal scenario ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n#### 3.2.3 Curriculum Pre-training\n#### 3.2.4 Multimodal Pre-training\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nFAT-MLM is a multimodal pre-training approach that leverages both speech and text modalities during pre-training to learn a unified representation for both acoustic and text input ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. It can utilize various types of corpora including parallel data for speech recognition and machine translation, and even pure speech and text data ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n### 3.3 Data Augmentation Techniques\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThe paper utilizes SpecAugment, a data augmentation technique applied to input speech features, to improve the generalization and robustness of the SATE model ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']. Experiments show that SpecAugment contributes to a significant BLEU score improvement ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders'].\n--------------------\nPaper bibkey: [a_comprehensive_survey_on_transfer_learning]\nDigest: \nThe survey discusses data-based transfer learning strategies, including instance weighting and feature transformation, which are related to data augmentation in the context of improving model robustness and addressing data scarcity ['a_comprehensive_survey_on_transfer_learning']. Instance weighting, where source-domain instances are weighted, and feature transformation, which creates new feature representations, can be seen as analogous to augmentation techniques that modify or expand the training data to enhance model generalization, although specific augmentation techniques for ST are not mentioned in this survey ['a_comprehensive_survey_on_transfer_learning'].\n### 3.4 Architectural Innovations and Key Components\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThe Stacked Acoustic-and-Textual Encoding (SATE) architecture itself is an architectural innovation, featuring a cascade of ASR and MT encoders with an adaptor module in between ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']. The adaptor module is a key component, designed to address the modality gap by transforming the output of the acoustic encoder into a representation suitable for the textual encoder. It combines a soft contextual representation derived from CTC output and a mapped acoustic representation to ensure both adaptivity and informativeness ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders'].\n--------------------\nPaper bibkey: [a_comprehensive_survey_on_transfer_learning]\nDigest: \nThe survey mentions deep learning techniques and model control strategies as part of model-based transfer learning ['a_comprehensive_survey_on_transfer_learning']. Deep learning models, with their capacity for complex feature extraction and adaptation, and model control strategies, which directly regularize models, can be considered in the context of architectural innovations for direct ST. However, the paper does not detail specific architectural innovations for ST ['a_comprehensive_survey_on_transfer_learning'].\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nThe paper proposes Fused Acoustic and Text Speech Translation model (FAT-ST), which adapts the architecture of monolingual FAT-MLM to a sequence-to-sequence speech translation model ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. FAT-ST's encoder shares identical architecture with monolingual FAT-MLM, which can encode either acoustic or text features ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. FAT-ST model can be optimized by speech translation loss, machine translation loss and FAT-MLM loss, enabling it to learn from speech translation, machine translation and speech recognition data simultaneously ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n#### 3.4.1 Addressing the Modality Gap\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThe SATE architecture, particularly with its adaptor module, is explicitly designed to address the modality gap between speech and text representations. The adaptor module aims to bridge this gap by transforming the acoustic representation from the ASR encoder into a textual representation that is compatible with the MT encoder, facilitating effective integration of pre-trained ASR and MT models in the end-to-end ST system ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders'].\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nFAT-MLM and FAT-ST are designed to bridge the modality gap between speech and text by learning a unified representation for both modalities through multimodal pre-training and joint training with different types of data ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n#### 3.4.2 Efficient and Compact Models\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThe paper demonstrates that the SATE model achieves a speedup of up to 1.69x compared to cascaded ST models, highlighting its efficiency in inference. While not explicitly focused on model compactness, the architecture offers a balance between performance and efficiency ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders'].\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nThe paper shows that FAT-ST achieves better performance than cascaded system with a smaller model size and faster decoding time ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The decoding speed of FAT-ST is almost $2\\times$ faster than the Cascade system ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n## 4. Datasets and Evaluation Metrics for Direct Speech-to-text Translation\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThe paper uses LibriSpeech English-French (En-Fr) and MuST-C English-German (En-De) corpora for experiments, representing low-resource and high-resource ST tasks, respectively ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']. Experiments are conducted under both restricted (ST data only) and unrestricted (additional ASR and MT data allowed) settings ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']. Performance is evaluated using case-sensitive SacreBLEU score ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders'].\n--------------------\nPaper bibkey: [a_comprehensive_survey_on_transfer_learning]\nDigest: \nThe paper includes an experimental section where datasets like Amazon Reviews, Reuters-21578, and Office-31 are used to evaluate transfer learning models ['a_comprehensive_survey_on_transfer_learning'].  These datasets, while for different tasks (sentiment classification, text classification, object recognition), highlight the importance of dataset selection in evaluating transfer learning techniques. The evaluation metric used in the experiments is accuracy, calculated as the proportion of correctly classified instances in the test data ['a_comprehensive_survey_on_transfer_learning']. This demonstrates the use of quantitative metrics for assessing the effectiveness of transfer learning approaches, though not specifically in the context of speech-to-text translation ['a_comprehensive_survey_on_transfer_learning'].\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nThe paper uses MuST-C (En$\\rightarrow$De, En$\\rightarrow$Es, En$\\rightarrow$Nl) dataset for speech translation experiments, Librispeech for speech recognition data, Europarl V7 for machine translation and monolingual text data, Libri-Light (medium version) for speech only data, and Wiki Text for Nl monolingual text data ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. BLEU score is used as the evaluation metric for translation quality ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n### 4.1 Multilingual Speech Translation Corpora\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nMuST-C En-De corpus, a multilingual speech translation corpus extracted from TED Talks, is used in the experiments. It contains approximately 400 hours of English speech translated to German, with around 230K utterances, and is considered a high-resource dataset in the paper ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders'].\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nMuST-C dataset is used for multilingual speech translation experiments, which is collected based on spontaneous speeches (TED) ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. The statistics of MuST-C dataset (hours and sentences for En$\\rightarrow$De, En$\\rightarrow$Es, En$\\rightarrow$Nl) are provided in Table 1 of the paper ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n### 4.2 Large-scale Pseudo Speech Translation Corpora\n### 4.3 Benchmarking and Evaluation Campaigns (IWSLT)\n## 5. Toolkits and Frameworks for Direct Speech-to-text Translation\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nExperiments are implemented using the ESPnet toolkit, an open-source toolkit for speech processing and end-to-end speech recognition and translation ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders'].\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nThe basic Transformer-based E2E-ST framework has similar settings with ESPnet-ST ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n### 5.1 Open-Source Toolkits vs. Commercial Toolkits for ST\n### 5.2 Feature Comparison of Popular ST Toolkits (e.g., ESPnet-ST, Fairseq-ST)\n### 5.3 Toolkits for Specific Tasks in Direct ST (e.g., Low-Resource ST Toolkits)\n## 6. Challenges and Future Directions in Direct Speech-to-text Translation\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThe paper identifies 'modeling deficiency' and 'representation inconsistency' as key challenges in direct ST encoder design, which SATE aims to address ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']. Future directions suggested include investigating the sequence length inconsistency issue in the adaptor module of SATE, potentially exploring methods to handle the length mismatch between acoustic and textual representations more explicitly ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders'].\n--------------------\nPaper bibkey: [a_comprehensive_survey_on_transfer_learning]\nDigest: \nThe survey concludes by pointing out several future directions in transfer learning research, some of which are relevant to direct speech-to-text translation ['a_comprehensive_survey_on_transfer_learning']. Key challenges include measuring transferability, avoiding negative transfer, and enhancing the interpretability of transfer learning models ['a_comprehensive_survey_on_transfer_learning']. These challenges are also pertinent to direct ST, where ensuring effective and positive knowledge transfer, understanding when and why transfer works, and making models more transparent are important research directions. Further exploration of transfer learning techniques in complex scenarios and privacy-preserving transfer learning are also mentioned as future research areas, which can inspire new research directions in direct ST ['a_comprehensive_survey_on_transfer_learning'].\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nThe scarcity of speech translation data is a challenge for end-to-end speech translation models, which FAT-MLM and FAT-ST aim to address by leveraging additional speech recognition and machine translation data ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. Future directions could explore extending FAT-MLM to incorporate more modalities beyond speech and text, and applying it to other multimodal spoken language processing tasks ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation'].\n### 6.1 6.x Ethical and Societal Implications\n## 7. Conclusion\nPaper bibkey: [stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders]\nDigest: \nThe paper concludes that the proposed Stacked Acoustic-and-Textual Encoding (SATE) method effectively addresses the challenges of pre-training in end-to-end speech translation by cascading ASR and MT encoders and using an adaptor module and multi-teacher knowledge distillation ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders']. SATE achieves state-of-the-art performance on LibriSpeech En-Fr and MuST-C En-De datasets, and importantly, demonstrates comparable or even better performance than cascaded ST systems when large-scale ASR and MT data are available, marking a significant advancement in end-to-end speech translation research ['stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders'].\n--------------------\nPaper bibkey: [a_comprehensive_survey_on_transfer_learning]\nDigest: \nThe survey concludes that transfer learning is a promising area in machine learning, offering advantages like reduced data and label dependency compared to traditional machine learning ['a_comprehensive_survey_on_transfer_learning']. It summarizes various transfer learning approaches from data and model perspectives, highlighting their mechanisms and strategies ['a_comprehensive_survey_on_transfer_learning']. The experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications ['a_comprehensive_survey_on_transfer_learning']. This underscores the potential of transfer learning for advancing direct speech-to-text translation by reducing reliance on large datasets and improving model generalization, provided that suitable techniques are chosen and adapted for the specific challenges of ST.\n--------------------\nPaper bibkey: [fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation]\nDigest: \nThe paper proposes Fused Acoustic and Text Masked Language Model (FAT-MLM) to learn a unified representation for text and speech from multimodal data ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. Based on FAT-MLM, the Fused Acoustic and Text Speech Translation model (FAT-ST) is proposed, which can learn from speech recognition and text-based machine translation data ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']. Experimental results show significant improvement on Must-C dataset and outperforms the cascaded baseline, achieving state-of-the-art performance ['fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation']."}, {"bibkey": "speech_translation_coupling_of_recognition_and_translation, a_comprehensive_survey_on_transfer_learning, statistical_phrase_based_speech_translation", "content": "#  Recent Advances in Direct Speech-to-text Translation\n## 1. Introduction\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nThis paper introduces a generative statistical model for speech-to-text translation, built as an extension of existing phrase-based text translation models ['statistical_phrase_based_speech_translation']. The motivation for direct speech translation stems from the limitations of pipeline approaches where ASR errors propagate to the MT system ['statistical_phrase_based_speech_translation']. Tightly coupled architectures, like the one presented, aim to allow the SMT system to search among multiple ASR hypotheses, potentially leading to better translations than relying on the single best ASR output ['statistical_phrase_based_speech_translation']. This approach is significant because it addresses the inherent imperfections of ASR systems by passing more information to the subsequent translation stages ['statistical_phrase_based_speech_translation'].\n--------------------\nPaper bibkey: [speech_translation_coupling_of_recognition_and_translation]\nDigest: \nThis paper addresses the core problem in speech translation: how to effectively couple the speech recognition and translation processes ['speech_translation_coupling_of_recognition_and_translation']. Traditional approaches often implement speech translation as a sequential operation, performing speech recognition first and then translating the recognized text ['speech_translation_coupling_of_recognition_and_translation']. This raises questions about handling recognition errors within the translation process and achieving a suitable interaction between recognition and translation ['speech_translation_coupling_of_recognition_and_translation']. The paper aims to derive a decision rule for speech translation and propose implementations that facilitate this interaction ['speech_translation_coupling_of_recognition_and_translation'].\n## 2. Background: Evolution of Speech Translation Paradigms\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nTraditional speech translation systems often adopt a 'pipeline' approach, where Automatic Speech Recognition (ASR) and Statistical Machine Translation (SMT) are separate components ['statistical_phrase_based_speech_translation']. In this cascaded system, the SMT system translates the transcription produced by the ASR as if it were written text ['statistical_phrase_based_speech_translation'].  While straightforward, this approach suffers because ASR systems are not flawless, and errors in transcription can negatively impact translation quality ['statistical_phrase_based_speech_translation']. To mitigate this, architectures with tighter coupling between ASR and SMT were developed, allowing the SMT to consider multiple ASR hypotheses, such as N-best lists or word lattices, to improve translation accuracy ['statistical_phrase_based_speech_translation']. Lattice-based translation, in particular, offers a larger search space and more detailed information compared to N-best lists, including word-level acoustic and language model scores ['statistical_phrase_based_speech_translation'].\n--------------------\nPaper bibkey: [speech_translation_coupling_of_recognition_and_translation]\nDigest: \nTraditional speech translation systems often follow a cascaded approach, involving separate speech recognition (ASR) and machine translation (MT) steps ['speech_translation_coupling_of_recognition_and_translation']. This sequential processing can lead to error propagation from the ASR component to the MT component ['speech_translation_coupling_of_recognition_and_translation']. The paper highlights the need to move beyond this simple sequential operation and explore methods for a more integrated approach to speech translation, emphasizing the statistical interaction between recognition and translation processes ['speech_translation_coupling_of_recognition_and_translation']. The paper argues that existing approaches have not fully addressed this direct interaction from a statistical perspective ['speech_translation_coupling_of_recognition_and_translation'].\n## 3. Techniques for Direct Speech-to-text Translation: Emphasizing Pre-training Strategies and Encoder Design Challenges\nPaper bibkey: [speech_translation_coupling_of_recognition_and_translation]\nDigest: \nThis paper proposes two new methods to address the interaction between recognition and translation processes in speech translation: local averaging approximation and monotone alignments ['speech_translation_coupling_of_recognition_and_translation']. These methods are presented in the context of Bayes decision rule for speech translation and aim to improve the integration of acoustic and linguistic information in the translation process ['speech_translation_coupling_of_recognition_and_translation'].\n--------------------\nPaper bibkey: [a_comprehensive_survey_on_transfer_learning]\nDigest: \nThis survey paper extensively discusses techniques for transfer learning, which is highly relevant to improving direct speech-to-text translation systems, especially in scenarios with limited data. The paper categorizes transfer learning approaches from data and model perspectives, providing a comprehensive overview of methods that can be adapted for direct ST. Pre-training strategies, data augmentation, and architectural innovations in transfer learning discussed in this survey can offer valuable insights for enhancing ST encoder design and overall system performance.  The survey emphasizes homogeneous transfer learning approaches, which are particularly applicable when the feature space of source and target domains in ST are similar, but distribution differences exist, for example, when adapting an ST model to a new accent or domain.  Cite: ['a_comprehensive_survey_on_transfer_learning']\n### 3.1 Challenges in ST Encoder Design\nPaper bibkey: [a_comprehensive_survey_on_transfer_learning]\nDigest: \nThe reference paper discusses the challenges in transfer learning when source and target domains differ. In the context of ST encoder design, similar challenges arise due to 'Modeling deficiency' and 'Representation inconsistency' when transferring knowledge from related tasks like ASR or MT.  While the paper doesn't directly mention these terms in the context of ST encoders, it provides a general background on domain discrepancy and negative transfer, which are relevant to these challenges. Understanding the factors leading to negative transfer, as discussed in the survey, is crucial for designing effective ST encoders that can benefit from transfer learning without performance degradation. Cite: ['a_comprehensive_survey_on_transfer_learning']\n### 3.2 Pre-training Strategies\nPaper bibkey: [a_comprehensive_survey_on_transfer_learning]\nDigest: \nPre-training strategies are essential for transfer learning, as highlighted in the reference paper. The survey reviews various pre-training methods in machine learning, including instance-based, feature-based, and parameter-based approaches, which can be adapted for direct ST.  Homogeneous transfer learning, a focus of the survey, is particularly relevant for pre-training ST models where the input feature space (speech) remains consistent, but domain-specific characteristics need to be addressed through distribution adaptation or parameter transfer.  The survey's discussion of different transfer learning paradigms (homogeneous, heterogeneous) provides a framework for categorizing and understanding pre-training strategies in ST. Cite: ['a_comprehensive_survey_on_transfer_learning']\n#### 3.2.1 Adversarial Pre-training with ASR and MT Data\nPaper bibkey: [a_comprehensive_survey_on_transfer_learning]\nDigest: \nThe survey discusses adversarial deep learning for domain adaptation, a technique that can be relevant to pre-training ST models using ASR and MT data.  Domain-Adversarial Neural Networks (DANNs), as described in the paper, aim to learn domain-invariant feature representations by playing an adversarial game between a feature extractor and a domain classifier.  This concept can be applied to pre-training ST models by using adversarial regularization to encourage the encoder to learn representations that are not only useful for translation but also domain-agnostic, potentially improving generalization across different speech domains or accents when leveraging ASR and MT pre-training data. Cite: ['a_comprehensive_survey_on_transfer_learning']\n#### 3.2.2 Self-Supervised Pre-training\nPaper bibkey: [a_comprehensive_survey_on_transfer_learning]\nDigest: \nThe survey does not explicitly discuss self-supervised pre-training in the context of speech, but the general principles of feature-based and model-based transfer learning are relevant. Self-supervised methods like Masked Acoustic Modeling (MAM) and wav2vec can be seen as feature-based transfer learning where representations learned from unlabeled speech (source domain) are transferred to improve ST (target task). The survey's discussion of feature encoding using autoencoders, such as Stacked Denoising Autoencoders (SDA), offers insights into how deep learning architectures can be used for self-supervised feature extraction and subsequent transfer to ST tasks. Cite: ['a_comprehensive_survey_on_transfer_learning']\n#### 3.2.3 Curriculum Pre-training\n#### 3.2.4 Multimodal Pre-training\n### 3.3 Data Augmentation Techniques\n### 3.4 Architectural Innovations and Key Components\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nThis paper introduces a novel approach to statistical phrase-based speech translation using a generative, source-channel model ['statistical_phrase_based_speech_translation']. This model extends phrase-based text translation by incorporating acoustic models from a target language ASR system ['statistical_phrase_based_speech_translation']. The core innovation lies in mapping ASR word lattices to lattices of phrase sequences, which are then translated using text translation operations, all within a Weighted Finite State Machine (WFSM) framework ['statistical_phrase_based_speech_translation']. The model comprises several steps: source language sentence generation, segmentation into source phrases, translation to target phrase sequences, phrase reordering and insertion, and finally, target phrase to word sequence transformation ['statistical_phrase_based_speech_translation']. This architecture allows for speech translation by using an acceptor for target language phrase sequences derived from an ASR word lattice, effectively integrating ASR and SMT processes ['statistical_phrase_based_speech_translation'].\n--------------------\nPaper bibkey: [speech_translation_coupling_of_recognition_and_translation]\nDigest: \nThe paper suggests two implementation methods for coupling speech recognition and translation:\n**Local Averaging Approximation:** This method introduces an auxiliary quantity $p(x_{j}|e)$ that directly links acoustic vectors $\\pmb{x}_{j}$ with a target word $e$ ['speech_translation_coupling_of_recognition_and_translation']. This quantity is approximated as:\n$$\n{p(x_{j}|e)} {:=} {\\displaystyle\\frac{\\sum_{f_{j}}p(x_{j}|f_{j})\\cdot p(f_{j}|\\tilde{f}_{j-1},e)\\cdot p(\\tilde{f}_{j+1}|f_{j})}{\\sum_{f_{j}}p(f_{j}|\\tilde{f}_{j-1},e)\\cdot p(\\tilde{f}_{j+1}|f_{j})}}\n$$\nThis approximation aims to capture the ambiguity from acoustic probabilities and replace lexicon probabilities in the search process ['speech_translation_coupling_of_recognition_and_translation']. It can be computed after recognition and before translation, potentially affecting search complexity due to less focused probabilities ['speech_translation_coupling_of_recognition_and_translation'].\n**Monotone Alignments:** This method uses dynamic programming (DP) search with monotone alignments ['speech_translation_coupling_of_recognition_and_translation']. In the maximum approximation and ignoring length models, the search criterion for a bigram language model $p(e_{i}|e_{i-1})$ is rewritten and solved using DP recursion ['speech_translation_coupling_of_recognition_and_translation']. For speech input, the search criterion is further adapted in the maximum approximation for alignments and source strings ['speech_translation_coupling_of_recognition_and_translation']. The DP recursion is given by:\n$$\n{Q(j,e)} {=} {p(f_{j}|e)\\cdot\\underset{\\delta,e^{\\prime}}{\\operatorname*{max}}\\left\\{p(\\delta)\\cdot p_{\\delta}(e|e^{\\prime})\\cdot Q(j-1,e^{\\prime})\\right\\}}\n$$\nThe computational complexity of this full search is $J \\cdot E^{2}$ (where E is target vocabulary size), which can be reduced by beam search ['speech_translation_coupling_of_recognition_and_translation'].\n--------------------\nPaper bibkey: [a_comprehensive_survey_on_transfer_learning]\nDigest: \nThe survey discusses various architectural innovations in deep learning for transfer learning, such as Deep Adaptation Networks (DANs) and Joint Adaptation Networks (JANs).  These networks incorporate mechanisms for distribution adaptation within deep architectures, which are relevant to architectural innovations in direct ST models. While the survey doesn't focus on ST specifically, the principles of discrepancy-based domain adaptation and adversarial learning in deep networks discussed can inspire innovations in ST architectures, particularly in designing modules and mechanisms to address the modality gap and improve cross-modal representation learning. Cite: ['a_comprehensive_survey_on_transfer_learning']\n#### 3.4.1 Addressing the Modality Gap\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nThe proposed model indirectly addresses the modality gap by incorporating target language acoustic models $P(A|t_{1}^{J})$ from the ASR system directly into the speech-to-text translation framework ['statistical_phrase_based_speech_translation']. The joint probability distribution is defined as $P(A,t_{1}^{J},v_{1}^{R},x_{1}^{K},u_{1}^{K},s_{1}^{I}) = P(A|t_{1}^{J})\\,P(t_{1}^{J}|v_{1}^{R})\\,P(v_{1}^{R}|x_{1}^{K},u_{1}^{K})\\,P(x_{1}^{K}|u_{1}^{K})\\,P(u_{1}^{K}|s_{1}^{I})\\,P(s_{1}^{I})$, where $\\mathcal{L}$ represents a weighted acceptor containing word sequences and acoustic scores from the ASR lattice ['statistical_phrase_based_speech_translation']. By using the ASR lattice as input, the model leverages acoustic information to guide the translation process from speech, effectively bridging the gap between acoustic and textual modalities at the phrase level ['statistical_phrase_based_speech_translation'].\n--------------------\nPaper bibkey: [a_comprehensive_survey_on_transfer_learning]\nDigest: \nThe survey mentions feature alignment strategies like Subspace Alignment (SA) and Geodesic Flow Kernel (GFK) for transfer learning, which can be conceptually linked to addressing the modality gap in ST.  While these methods are not directly applied to ST architectures in the survey, the underlying idea of aligning feature spaces from different domains is relevant.  Adaptor modules and mechanisms used in ST to bridge the modality gap can be seen as analogous to feature alignment techniques in transfer learning, aiming to create a common representation space that effectively integrates acoustic and textual information. Cite: ['a_comprehensive_survey_on_transfer_learning']\n#### 3.4.2 Efficient and Compact Models\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nThe paper utilizes Weighted Finite State Machines (WFSMs) to implement the translation model, which are known for their efficiency in representing and processing language models and translation operations ['statistical_phrase_based_speech_translation']. By formulating each component of the translation process (language model, phrase segmentation, phrase translation, reordering) as WFSMs, the system can perform translation through a series of compositions of these machines, potentially offering an efficient approach to speech translation ['statistical_phrase_based_speech_translation']. The use of WFSMs allows for operations like epsilon-removal, determinization, and minimization to reduce the size of lattices and transducers, contributing to computational efficiency ['statistical_phrase_based_speech_translation'].\n## 4. Datasets and Evaluation Metrics for Direct Speech-to-text Translation\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nThe performance of the proposed system was evaluated on the TC-STAR Chinese to English (C-E) Broadcast News translation task ['statistical_phrase_based_speech_translation']. The corpus consists of six Mandarin news broadcasts, segmented and transcribed into Chinese sentences, with two English translations for each sentence ['statistical_phrase_based_speech_translation']. The dataset is divided into Development and Evaluation sets, containing 525 and 494 sentences, respectively, with vocabulary statistics provided in Table 1 of the paper ['statistical_phrase_based_speech_translation']. Mandarin ASR lattices, generated by the LIMSI Mandarin Broadcast News System, were also used ['statistical_phrase_based_speech_translation']. Translation performance was measured using the BLEU metric ['statistical_phrase_based_speech_translation'].\n### 4.1 Multilingual Speech Translation Corpora\n### 4.2 Large-scale Pseudo Speech Translation Corpora\n### 4.3 Benchmarking and Evaluation Campaigns (IWSLT)\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nThe paper mentions the TC-STAR evaluation campaign as the context for the Mandarin Broadcast News translation task used for evaluation ['statistical_phrase_based_speech_translation']. The experiments were conducted using data and guidelines specified by the 2005 TC-STAR evaluation ['statistical_phrase_based_speech_translation']. The JHU/CU system submitted to the 2005 TC-STAR and NIST Chinese-English MT evaluations serves as the baseline for the reported experiments ['statistical_phrase_based_speech_translation'].\n## 5. Toolkits and Frameworks for Direct Speech-to-text Translation\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nThe system utilizes the GRM Library `grmcount` tool for extracting phrases from word lattices and AT&T FSM format for representing weighted finite state acceptors ['statistical_phrase_based_speech_translation']. The phrase-based SMT system used is the Translation Template Model (TTM) system developed by JHU/CU ['statistical_phrase_based_speech_translation']. Prior to translation, Chinese ASR lattices were converted into weighted finite state acceptors in AT&T FSM format and processed using operations like ϵ-removal, determinization, and minimization, potentially using tools available within WFST frameworks ['statistical_phrase_based_speech_translation'].\n### 5.1 Open-Source Toolkits vs. Commercial Toolkits for ST\n### 5.2 Feature Comparison of Popular ST Toolkits (e.g., ESPnet-ST, Fairseq-ST)\n### 5.3 Toolkits for Specific Tasks in Direct ST (e.g., Low-Resource ST Toolkits)\n## 6. Challenges and Future Directions in Direct Speech-to-text Translation\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nThe paper identifies several challenges and future directions. A key challenge is transforming ASR word lattices into phrase lattices, particularly extracting phrase sequences under the posterior distribution provided by the ASR system ['statistical_phrase_based_speech_translation']. Issues arise with handling silence markers, fillers, and sentence breaks in ASR lattices, as these are not typically present in bilingual text data used for training phrase extraction models ['statistical_phrase_based_speech_translation'].  A significant problem is the mismatch in tokenization and word segmentation between the ASR system and the SMT bitext, which complicates phrase pair extraction and negatively impacts translation performance ['statistical_phrase_based_speech_translation']. Future work includes improving phrase extraction from word lattices, potentially using metadata extraction techniques to better detect phrase and sentence boundaries, and integrating ASR and SMT systems constructed with consistent text formatting to address tokenization mismatches ['statistical_phrase_based_speech_translation']. The paper suggests that integrated development of ASR and SMT components is crucial for further performance improvements ['statistical_phrase_based_speech_translation'].\n--------------------\nPaper bibkey: [speech_translation_coupling_of_recognition_and_translation]\nDigest: \nThe paper highlights the challenge of statistically integrating speech recognition and translation processes ['speech_translation_coupling_of_recognition_and_translation']. It suggests that future work should focus on developing models and methods that more directly address the interaction between acoustic input and target language generation, moving beyond simple cascaded systems ['speech_translation_coupling_of_recognition_and_translation']. The proposed methods, local averaging and monotone alignments, are presented as initial steps towards a tighter coupling of recognition and translation, but further research is needed to refine and extend these approaches ['speech_translation_coupling_of_recognition_and_translation'].\n--------------------\nPaper bibkey: [a_comprehensive_survey_on_transfer_learning]\nDigest: \nThe survey concludes by discussing future directions in transfer learning, some of which are relevant to challenges and future directions in direct speech-to-text translation.  Specifically, the survey mentions the need for further research on measuring transferability and avoiding negative transfer, which are critical challenges in applying transfer learning to ST.  The survey also highlights the importance of interpretability in transfer learning models, a direction that is also relevant for understanding and improving the performance of direct ST systems.  Future research in ST could benefit from investigating methods to better quantify the transferability of knowledge from pre-training tasks and to design ST models that are more robust to negative transfer. Cite: ['a_comprehensive_survey_on_transfer_learning']\n### 6.1 6.x Ethical and Societal Implications\n## 7. Conclusion\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nThis paper presents a statistical modeling framework for direct speech-to-text translation, extending the phrase-based TTM text translation model and tightly coupling ASR and SMT subsystems using WFSMs ['statistical_phrase_based_speech_translation']. The approach demonstrates feasibility in Mandarin-to-English Broadcast News translation, showing improvements by translating ASR lattices compared to ASR 1-best hypotheses ['statistical_phrase_based_speech_translation']. The work highlights weaknesses in the initial formulation and implementation, particularly in phrase extraction from lattices and tokenization consistency, suggesting avenues for future improvements through integrated development of ASR and SMT systems ['statistical_phrase_based_speech_translation']. The overall outlook suggests that further refinement and integration of component systems can lead to enhanced translation performance in statistical speech translation ['statistical_phrase_based_speech_translation'].\n--------------------\nPaper bibkey: [speech_translation_coupling_of_recognition_and_translation]\nDigest: \nThis paper emphasizes the importance of coupling speech recognition and translation in speech translation systems and argues for a Bayes decision rule framework to guide the development of such systems ['speech_translation_coupling_of_recognition_and_translation']. It introduces local averaging approximation and monotone alignments as two methods to achieve a tighter integration of these processes ['speech_translation_coupling_of_recognition_and_translation']. The paper suggests that directly addressing the interaction between recognition and translation is crucial for advancing the field of speech translation beyond traditional cascaded models ['speech_translation_coupling_of_recognition_and_translation'].\n--------------------\nPaper bibkey: [a_comprehensive_survey_on_transfer_learning]\nDigest: \nThe survey concludes that transfer learning is a promising area in machine learning for reducing data and label dependency, and that selecting appropriate transfer learning models is crucial for practical applications. This conclusion is highly relevant to direct speech-to-text translation, where data scarcity and the need for efficient adaptation to new languages and domains are significant challenges.  The survey's emphasis on the advantages of transfer learning and the importance of model selection reinforces the rationale for exploring and applying transfer learning techniques to advance direct ST. Cite: ['a_comprehensive_survey_on_transfer_learning']"}, {"bibkey": "bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation, wav2vec_unsupervised_pre_training_for_speech_recognition, curriculum_pre_training_for_end_to_end_speech_translation", "content": "#  Recent Advances in Direct Speech-to-text Translation\n## 1. Introduction\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nThe paper explores unsupervised pre-training for speech recognition by learning representations of raw audio, aiming to improve acoustic model training, especially when transcribed data is limited ['wav2vec_unsupervised_pre_training_for_speech_recognition']. Pre-training is highlighted as an effective technique in scenarios where labeled data is scarce, which is particularly relevant for speech recognition due to the substantial effort required for data transcription ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The motivation is to leverage large amounts of unlabeled audio data, which is much easier to collect than labeled data, to learn general representations that can enhance downstream speech recognition tasks ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n--------------------\nPaper bibkey: [bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation]\nDigest: \nDirect speech-to-text translation (ST) is defined as translating audio into a target language in a single end-to-end model. It is motivated by scenarios like emergency calls and online courses where real-time translation is crucial. Direct ST offers advantages over traditional cascaded systems by reducing latency, streamlining the translation pipeline, and mitigating error propagation from automatic speech recognition (ASR) to machine translation (MT) components ['bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation']. The scope of this survey includes techniques to improve direct ST, particularly focusing on pre-training strategies.\n--------------------\nPaper bibkey: [curriculum_pre_training_for_end_to_end_speech_translation]\nDigest: \nDirect speech-to-text translation is defined as translating source language speech directly into target language text, aiming to overcome language barriers in communication ['curriculum_pre_training_for_end_to_end_speech_translation']. End-to-end ST models offer advantages over cascaded ASR+MT systems, including reduced latency and mitigation of error propagation ['curriculum_pre_training_for_end_to_end_speech_translation']. However, training end-to-end ST models requires large paired datasets, which are often scarce ['curriculum_pre_training_for_end_to_end_speech_translation']. Pre-training techniques are used to address this data scarcity issue ['curriculum_pre_training_for_end_to_end_speech_translation']. The scope of this survey includes techniques to improve end-to-end ST, particularly focusing on pre-training strategies for the encoder ['curriculum_pre_training_for_end_to_end_speech_translation'].\n## 2. Background: Evolution of Speech Translation Paradigms\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nThe paper builds upon the idea that pre-training has emerged as a powerful technique in computer vision and natural language processing, demonstrating significant improvements in various tasks by learning general representations from large datasets ['wav2vec_unsupervised_pre_training_for_speech_recognition']. In speech processing, while pre-training has been explored for tasks like emotion recognition and speaker identification, its application to improve supervised speech recognition directly has been less investigated before this work ['wav2vec_unsupervised_pre_training_for_speech_recognition']. This paper addresses this gap by applying unsupervised pre-training to enhance supervised ASR, enabling the use of readily available unlabeled audio data ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n--------------------\nPaper bibkey: [bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation]\nDigest: \nTraditional cascaded speech translation models consist of separate ASR and MT components. These systems suffer from error propagation, where errors from the ASR component are passed on to the MT component, and are trained with separate objectives for ASR and MT. End-to-end models, in contrast, offer theoretical and practical advantages by jointly training a single model to directly map speech to target text. This approach emphasizes joint training and direct mapping, aiming to reduce latency and rectify error propagation inherent in cascaded systems ['bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [curriculum_pre_training_for_end_to_end_speech_translation]\nDigest: \nTraditional speech translation paradigms rely on cascaded models, chaining ASR and MT systems ['curriculum_pre_training_for_end_to_end_speech_translation']. Cascaded models suffer from error propagation, where errors from the ASR component are passed on to the MT component ['curriculum_pre_training_for_end_to_end_speech_translation']. End-to-end models address these limitations by directly mapping speech to text in a single model, enabling joint training and avoiding error propagation, and reducing latency ['curriculum_pre_training_for_end_to_end_speech_translation']. End-to-end ST models have become a hot research topic due to these advantages, but they require substantial paired speech-text data for effective training ['curriculum_pre_training_for_end_to_end_speech_translation'].\n## 3. Techniques for Direct Speech-to-text Translation: Emphasizing Pre-training Strategies and Encoder Design Challenges\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nThis paper introduces wav2vec, a novel technique for unsupervised pre-training in speech recognition, which can be categorized under pre-training strategies for improving speech-to-text systems ['wav2vec_unsupervised_pre_training_for_speech_recognition']. Wav2vec focuses on learning representations of raw audio through a convolutional neural network, optimized via a noise contrastive binary classification task ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The pre-trained representations are then used to enhance acoustic model training for speech recognition ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n--------------------\nPaper bibkey: [bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation]\nDigest: \nTechniques to improve direct speech-to-text translation systems include pre-training strategies and multi-task learning. Pre-training is particularly emphasized to leverage large-scale ASR and MT datasets given the scarcity of speech-to-text translation data. The paper focuses on addressing the gap between pre-training and fine-tuning in end-to-end ST models ['bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [curriculum_pre_training_for_end_to_end_speech_translation]\nDigest: \nPre-training is a crucial technique for improving direct ST systems, especially in data-scarce scenarios ['curriculum_pre_training_for_end_to_end_speech_translation']. This paper focuses on curriculum pre-training as a strategy to enhance the ST encoder ['curriculum_pre_training_for_end_to_end_speech_translation']. The paper introduces a curriculum pre-training method with three courses: transcription learning (elementary), utterance understanding, and cross-lingual word mapping (advanced) ['curriculum_pre_training_for_end_to_end_speech_translation']. This staged approach aims to equip the encoder with necessary linguistic knowledge progressively ['curriculum_pre_training_for_end_to_end_speech_translation']. Experiments demonstrate that this curriculum pre-training method yields significant improvements on En-De and En-Fr ST benchmarks compared to conventional ASR pre-training ['curriculum_pre_training_for_end_to_end_speech_translation'].\n### 3.1 Challenges in ST Encoder Design\nPaper bibkey: [bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation]\nDigest: \nThe paper identifies three key challenges in pre-training and fine-tuning end-to-end ST models, which constitute the gap between these two stages:\n1. **Subnet Waste**: Previous methods often reuse only the ASR encoder and MT decoder, discarding other pre-trained subnets like the MT encoder, thus failing to leverage potentially valuable semantic information.\n2. **Role Mismatch**: The speech encoder's role changes between pre-training (pure acoustic modeling) and fine-tuning (acoustic and semantic/linguistic feature extraction), increasing learning difficulty.\n3. **Non-pre-trained Attention Module**: Traditional approaches train separate attention modules for ASR, MT, and ST, preventing the ST attention module from benefiting from pre-training ['bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [curriculum_pre_training_for_end_to_end_speech_translation]\nDigest: \nThe end-to-end ST encoder faces a heavy burden as it must perform transcription, understand source language syntax and semantics, and map to cross-lingual semantic space simultaneously ['curriculum_pre_training_for_end_to_end_speech_translation']. Traditional ASR pre-training only focuses on transcription, learning acoustic-phonetic alignment but lacking in capturing higher-level linguistic knowledge essential for translation ['curriculum_pre_training_for_end_to_end_speech_translation']. This modeling deficiency restricts the power of pre-trained representations for ST ['curriculum_pre_training_for_end_to_end_speech_translation'].\n### 3.2 Pre-training Strategies\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nThe paper proposes wav2vec, a self-supervised pre-training strategy that learns representations from raw audio data ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The core methodology involves training a multi-layer convolutional neural network to distinguish true future audio samples from negative samples using a contrastive loss ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\nThe wav2vec model architecture consists of two main networks: an encoder network and a context network ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The encoder network, $f: \\mathcal{X} \\mapsto \\mathcal{Z}$, is a five-layer convolutional network that embeds raw audio signal $\\mathbf{x}_{i} \\in \\mathcal{X}$ into a latent space, producing feature representations $\\mathbf{z}_{i} \\in \\mathcal{Z}$ ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The context network, $g: \\mathcal{Z} \\mapsto C$, further processes these latent representations by mixing multiple time-steps of the encoder output to obtain contextualized representations $\\mathbf{c}_{i} = g(\\mathbf{z}_{i} \\ldots \\mathbf{z}_{i-v})$ ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\nThe pre-training task is formulated as a contrastive loss, which is minimized for each step $k = 1, \\dots, K$:\n$$\n\\mathcal{L}_{k}=-\\sum_{i=1}^{T-k}\\Big(\\log\\sigma(\\mathbf{z}_{i+k}^{\\top}h_{k}(\\mathbf{c}_{i}))+\\lambda\\mathbb{E}_{\\widetilde{\\mathbf z} \\sim p_{n}}\\left\\Big)\n$$\nwhere $\\sigma$ is the sigmoid function, $h_{k}(\\mathbf{c}_{i}) = W_{k}\\mathbf{c}_{i} + \\mathbf{b}_{k}$ is a step-specific affine transformation, and $p_{n}$ is a proposal distribution from which distractor samples $\\widetilde{\\mathbf z}$ are drawn ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The overall loss is the sum of losses over different step sizes: $\\mathcal{L} = \\sum_{k=1}^{K}\\mathcal{L}_{k}$ ['wav2vec_unsupervised_pre_training_for_speech_recognition'].  After pre-training, the contextualized representations $\\mathbf{c}_{i}$ are used as input features for the acoustic model in place of log-mel filterbank features ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\nThis pre-training strategy is a homogeneous transfer learning paradigm as it pre-trains and fine-tunes models within the speech modality, focusing on learning better audio representations.\n--------------------\nPaper bibkey: [bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation]\nDigest: \nThis paper introduces the Tandem Connectionist Encoding Network (TCEN) as a pre-training strategy to bridge the gap between pre-training and fine-tuning. TCEN aims to reuse all subnets, maintain consistent subnet roles, and pre-train the attention module. The pre-training tasks involve CTC-based ASR pre-training for the speech encoder and standard MT pre-training for the text encoder and decoder. The transfer learning paradigm is homogeneous in the sense that it reuses pre-trained components (encoders and decoders) but innovatively connects them in a tandem fashion for the ST task. The key techniques include:\n- **TCEN Architecture**: Concatenates an ASR encoder and an MT encoder in tandem, feeding speech encoder outputs to the text encoder. This reuses both ASR and MT encoders, addressing subnet waste and role mismatch.\n- **Pre-training Attention**: Reuses the pre-trained MT attention module in the ST decoder, leveraging alignment information learned during MT pre-training.\n- **Semantic Consistency**: Shares the weight matrix between the CTC classification layer and the source word embedding layer to map speech encoder outputs and word embeddings to the same latent space.\n- **Length Consistency**:  Trains the text encoder with noisy MT data, where source sentences are lengthened to mimic CTC output sequences, making the text encoder more robust to the longer sequences from the speech encoder ['bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [curriculum_pre_training_for_end_to_end_speech_translation]\nDigest: \nPre-training strategies are vital for end-to-end ST to overcome data scarcity and encoder burden ['curriculum_pre_training_for_end_to_end_speech_translation']. This paper proposes curriculum pre-training as a novel strategy ['curriculum_pre_training_for_end_to_end_speech_translation'].\n#### 3.2.1 Adversarial Pre-training with ASR and MT Data\n#### 3.2.2 Self-Supervised Pre-training\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nWav2vec is a self-supervised pre-training technique that learns from unlabeled speech data ['wav2vec_unsupervised_pre_training_for_speech_recognition']. It employs Masked Acoustic Modeling (MAM) in a contrastive learning setup where the model is trained to predict future audio samples from context, effectively learning useful representations from raw audio without requiring transcriptions ['wav2vec_unsupervised_pre_training_for_speech_recognition']. This approach leverages large amounts of untranscribed speech data, which is particularly advantageous in low-resource scenarios where transcribed data is limited ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The model architecture involves convolutional networks, and the pre-training task is designed to capture temporal dependencies and audio characteristics directly from the waveform ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n#### 3.2.3 Curriculum Pre-training\nPaper bibkey: [curriculum_pre_training_for_end_to_end_speech_translation]\nDigest: \nThe proposed curriculum pre-training method consists of three courses with increasing difficulty ['curriculum_pre_training_for_end_to_end_speech_translation'].\n1. **Transcription (Elementary Course)**: Pre-trains the encoder using ASR task to learn basic transcription abilities, aligning acoustic features with phonemes or words. This is achieved by training an end-to-end ASR model with CTC and cross-entropy loss functions, $\\mathcal{L}_{ASR}=\\alpha\\mathcal{L}_{CTC}+(1-\\alpha)\\mathcal{L}_{CE}$ ['curriculum_pre_training_for_end_to_end_speech_translation'].\n2. **Understanding (Advanced Course - FMLM)**: Introduces a Frame-based Masked Language Model (FMLM) task to enable the encoder to understand sentence meaning. This task masks word-level speech segments and requires the encoder to predict the masked words using KL-Divergence loss, $\\mathcal{L}_{F M L M}=-\\sum_{y_{j}^{s}\\in\\tilde{\\pmb{y}}^{s}}\\sum q(y_{j}^{s})\\mathrm{log}\\frac{p(y_{j}^{s}|\\tilde{\\pmb{x}})}{q(y_{j}^{s})}$ ['curriculum_pre_training_for_end_to_end_speech_translation'].\n3. **Cross-lingual Mapping (Advanced Course - FBLT)**: Implements a Frame-based Bilingual Lexicon Translation (FBLT) task to teach cross-lingual word mapping. This task predicts target language words corresponding to source language speech segments, using KL-Divergence loss, $\\mathcal{L}_{F B L T}=-\\sum_{\\tilde{y}_{i}^{t}}\\sum q(\\tilde{y}_{i}^{t})\\mathrm{log}\\frac{p(\\tilde{y}_{i}^{t}|\\tilde{\\pmb{x}})}{q(\\tilde{y}_{i}^{t})}$ ['curriculum_pre_training_for_end_to_end_speech_translation'].\nThe pre-training is hierarchical, with the first 8 encoder layers for ASR and FMLM, and the next 4 layers for FBLT ['curriculum_pre_training_for_end_to_end_speech_translation']. This staged learning process gradually equips the encoder with transcription, understanding, and cross-lingual mapping abilities, leading to improved ST performance ['curriculum_pre_training_for_end_to_end_speech_translation'].\n#### 3.2.4 Multimodal Pre-training\n### 3.3 Data Augmentation Techniques\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nThe paper uses data augmentation during pre-training by cropping audio sequences to a maximum length within each batch to ensure equal length sequences on a GPU and to increase data variation ['wav2vec_unsupervised_pre_training_for_speech_recognition']. Cropping removes speech signal from either the beginning or end of the sequence with randomly decided offsets, effectively augmenting the training data and removing on average $25\\,\\%$ of the training data per epoch ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n--------------------\nPaper bibkey: [bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation]\nDigest: \nThe paper uses speed perturbation as a data augmentation technique during experiments, applying speed factors of 0.9 and 1.1 to raw audio signals to increase training data and improve model robustness ['bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [curriculum_pre_training_for_end_to_end_speech_translation]\nDigest: \nThe paper uses speed perturbation with factors 0.9 and 1.1 and SpecAugment during ASR pre-training to improve model robustness ['curriculum_pre_training_for_end_to_end_speech_translation']. SpecAugment includes frequency masking (F=30, mF=2) and time masking (T=40, mT=2) ['curriculum_pre_training_for_end_to_end_speech_translation'].\n### 3.4 Architectural Innovations and Key Components\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nWav2vec utilizes a fully convolutional architecture for both the encoder and context networks, which allows for efficient parallelization over time, compared to recurrent models ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The encoder network consists of five convolutional layers, and the context network comprises nine layers in the base model and twelve layers with increasing kernel sizes in the \"wav2vec large\" variant, incorporating skip connections for improved convergence in the larger model ['wav2vec_unsupervised_pre_training_for_speech_recognition']. Each layer in both networks includes a causal convolution with 512 channels, group normalization, and ReLU nonlinearity, with normalization across feature and temporal dimensions for each sample to ensure invariance to input scaling and offset ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n--------------------\nPaper bibkey: [bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation]\nDigest: \nThe Tandem Connectionist Encoding Network (TCEN) is the main architectural innovation proposed. It consists of a speech encoder, a text encoder, and a decoder with an attention module. The speech encoder is pre-trained with CTC loss, and the text encoder and decoder are pre-trained with MT loss. In fine-tuning for ST, the speech encoder's output is fed into the text encoder, and then to the decoder. This tandem connection and the weight sharing and noisy MT data techniques are key components designed to address the pre-training and fine-tuning gap ['bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [curriculum_pre_training_for_end_to_end_speech_translation]\nDigest: \nThe paper adopts the Transformer architecture for the end-to-end ST model ['curriculum_pre_training_for_end_to_end_speech_translation']. The encoder consists of 2D CNN layers for downsampling and Transformer encoder blocks ['curriculum_pre_training_for_end_to_end_speech_translation']. The decoder is composed of Transformer decoder blocks ['curriculum_pre_training_for_end_to_end_speech_translation']. Hierarchical encoder structure is used for curriculum pre-training, assigning different layers to different pre-training tasks ['curriculum_pre_training_for_end_to_end_speech_translation'].\n#### 3.4.1 Addressing the Modality Gap\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nThe wav2vec architecture, while not explicitly designed to address the modality gap between speech and text in speech translation, focuses on effectively modeling acoustic representations directly from raw audio ['wav2vec_unsupervised_pre_training_for_speech_recognition']. By learning high-quality representations from raw waveforms through unsupervised pre-training, wav2vec aims to bridge the gap between raw acoustic input and higher-level linguistic features needed for speech recognition ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The convolutional encoder and context networks are key components in extracting robust acoustic features that are beneficial for downstream ASR tasks, implicitly addressing the challenge of converting raw audio into a more linguistically meaningful representation ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n--------------------\nPaper bibkey: [bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation]\nDigest: \nThe TCEN architecture, along with semantic consistency (weight sharing between CTC layer and word embeddings) and length consistency (noisy MT data), are specifically designed to address the modality gap between speech and text. By sharing weights, the speech encoder's output space is aligned with the text embedding space. By using noisy MT data, the text encoder is made robust to the sequence length differences between word embeddings and speech encoder outputs ['bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation'].\n#### 3.4.2 Efficient and Compact Models\n## 4. Datasets and Evaluation Metrics for Direct Speech-to-text Translation\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nThe paper uses several datasets for pre-training and evaluation, including Wall Street Journal (WSJ), Librispeech, and TIMIT corpora ['wav2vec_unsupervised_pre_training_for_speech_recognition']. For pre-training, it utilizes the audio data from WSJ (81 hours), subsets and the full Librispeech (80 hours and 960 hours), and combinations thereof, which are large-scale unlabeled speech datasets ['wav2vec_unsupervised_pre_training_for_speech_recognition']. For evaluating the acoustic models, WSJ and TIMIT datasets are used, with standard train/dev/test splits, and the performance is measured using Word Error Rate (WER) and Letter Error Rate (LER) ['wav2vec_unsupervised_pre_training_for_speech_recognition']. These datasets and metrics are standard in the speech recognition field for benchmarking model performance ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n--------------------\nPaper bibkey: [bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation]\nDigest: \nThe paper uses the ST-TED En-De corpus and the augmented Librispeech En-Fr corpus for experiments. For ASR pre-training, TED-LIUM2 corpus is used. For MT pre-training, WMT2018 en-de and WIT3 datasets are used for En-De, and Librispeech transcription-translation pairs are used for En-Fr. Datasets vary in size, with ST-TED En-De containing 272 hours of speech and Librispeech En-Fr containing 236 hours. The IWSLT18 speech translation benchmark is used for evaluation. BLEU (case-insensitive) is used as the primary evaluation metric ['bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [curriculum_pre_training_for_end_to_end_speech_translation]\nDigest: \nThe paper uses LibriSpeech En-Fr and IWSLT En-De datasets for experiments ['curriculum_pre_training_for_end_to_end_speech_translation']. BLEU score is used as the primary evaluation metric, calculated using multi-bleu.perl script ['curriculum_pre_training_for_end_to_end_speech_translation'].\n### 4.1 Multilingual Speech Translation Corpora\nPaper bibkey: [bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation]\nDigest: \nST-TED En-De corpus is a multilingual corpus containing English speech and German translations, derived from TED Talks. It includes 272 hours of English speech and 171k segments, with aligned German translations. The corpus is used for both training and evaluation in the experiments ['bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [curriculum_pre_training_for_end_to_end_speech_translation]\nDigest: \nLibriSpeech En-Fr corpus is used, which is a subset of LibriSpeech ASR corpus aligned with French e-books, containing 236 hours of speech ['curriculum_pre_training_for_end_to_end_speech_translation']. For training, 100 hours clean training set and doubled ST size with Google Translate references are used, resulting in 90k training instances ['curriculum_pre_training_for_end_to_end_speech_translation'].\n### 4.2 Large-scale Pseudo Speech Translation Corpora\n### 4.3 Benchmarking and Evaluation Campaigns (IWSLT)\nPaper bibkey: [bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation]\nDigest: \nThe paper conducts experiments on the IWSLT18 speech translation benchmark, using the ST-TED En-De corpus which is part of this benchmark. The IWSLT evaluation campaign is a significant effort in speech translation, providing datasets and evaluation metrics for benchmarking ST systems ['bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [curriculum_pre_training_for_end_to_end_speech_translation]\nDigest: \nIWSLT En-De corpus with 271 hours of data is used, after removing low alignment quality utterances, resulting in 137k utterances ['curriculum_pre_training_for_end_to_end_speech_translation']. tst2013 is used as the test set ['curriculum_pre_training_for_end_to_end_speech_translation']. The paper also mentions IWSLT evaluation campaign in general as a benchmarking effort in ST field, but not specifically focuses on IWSLT campaign results ['curriculum_pre_training_for_end_to_end_speech_translation'].\n## 5. Toolkits and Frameworks for Direct Speech-to-text Translation\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nThe pre-training models of wav2vec are implemented in PyTorch using the fairseq toolkit ['wav2vec_unsupervised_pre_training_for_speech_recognition']. For training and evaluation of acoustic models, the wav2letter++ toolkit is used ['wav2vec_unsupervised_pre_training_for_speech_recognition']. These are open-source frameworks that support the development and experimentation of speech recognition models, providing functionalities for model building, training, and evaluation ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n--------------------\nPaper bibkey: [bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation]\nDigest: \nThe experiments are implemented using the ESPnet toolkit ['bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation']. ESPnet is an open-source toolkit that supports end-to-end speech processing, including speech translation.\n--------------------\nPaper bibkey: [curriculum_pre_training_for_end_to_end_speech_translation]\nDigest: \nESPnet is used as the toolkit for implementation and experiments ['curriculum_pre_training_for_end_to_end_speech_translation'].\n### 5.1 Open-Source Toolkits vs. Commercial Toolkits for ST\n### 5.2 Feature Comparison of Popular ST Toolkits (e.g., ESPnet-ST, Fairseq-ST)\n### 5.3 Toolkits for Specific Tasks in Direct ST (e.g., Low-Resource ST Toolkits)\n## 6. Challenges and Future Directions in Direct Speech-to-text Translation\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nThe paper concludes by suggesting that future work could explore different architectures to further improve the performance of unsupervised pre-training for speech recognition ['wav2vec_unsupervised_pre_training_for_speech_recognition']. This indicates that while wav2vec achieves state-of-the-art results, there is still room for improvement by investigating alternative model architectures in the context of self-supervised learning for speech representations ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n--------------------\nPaper bibkey: [bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation]\nDigest: \nThe paper addresses the challenge of the gap between pre-training and fine-tuning in direct ST, specifically subnet waste, role mismatch, and non-pre-trained attention. Unresolved issues may include further improving the consistency between speech and text representations and exploring more sophisticated pre-training tasks. Future research directions could focus on enhancing the proposed TCEN model, exploring other pre-training objectives, and investigating the performance in more diverse language pairs and low-resource scenarios ['bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [curriculum_pre_training_for_end_to_end_speech_translation]\nDigest: \nThe paper identifies the challenge of effectively utilizing unlabeled speech data and large bilingual text data to further improve end-to-end ST performance as a future direction ['curriculum_pre_training_for_end_to_end_speech_translation']. Exploring the application of curriculum pre-training to other NLP tasks is also suggested as a future research direction ['curriculum_pre_training_for_end_to_end_speech_translation'].\n### 6.1 6.x Ethical and Societal Implications\n## 7. Conclusion\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nThe paper introduces wav2vec, highlighting it as the first application of unsupervised pre-training to speech recognition using a fully convolutional model ['wav2vec_unsupervised_pre_training_for_speech_recognition']. A key takeaway is that wav2vec achieves a 2.43% WER on the WSJ test set, outperforming previous character-based speech recognition models with significantly less transcribed training data, demonstrating the effectiveness of unsupervised pre-training with raw audio ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The study shows that increasing the amount of data for pre-training improves performance and that this approach is beneficial not only in resource-poor setups but also in settings with abundant labeled data ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The overall outlook suggests that unsupervised pre-training is a promising direction for advancing speech recognition technology by effectively leveraging unlabeled audio data ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n--------------------\nPaper bibkey: [bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation]\nDigest: \nThe paper concludes that the proposed TCEN method effectively bridges the gap between pre-training and fine-tuning for end-to-end ST by reusing subnets, maintaining role consistency, and pre-training the attention module. Empirical results demonstrate significant improvements over baseline methods. The main contributions are identifying the pre-training/fine-tuning gap, proposing TCEN to alleviate these issues, and empirically validating TCEN's effectiveness on public datasets. The overall outlook suggests that TCEN offers a promising direction for advancing direct speech-to-text translation ['bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [curriculum_pre_training_for_end_to_end_speech_translation]\nDigest: \nThe paper concludes that curriculum pre-training, consisting of transcription, understanding, and bilingual lexicon translation courses, significantly improves end-to-end ST performance compared to conventional ASR pre-training ['curriculum_pre_training_for_end_to_end_speech_translation']. The proposed FMLM and FBLT tasks effectively teach the encoder source language understanding and target language meaning mapping ['curriculum_pre_training_for_end_to_end_speech_translation']. The curriculum pre-training method shows strong potential for incorporating linguistic knowledge into the ST encoder ['curriculum_pre_training_for_end_to_end_speech_translation']."}, {"bibkey": "speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are, effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data, librispeech_an_asr_corpus_based_on_public_domain_audio_books", "content": "#  Recent Advances in Direct Speech-to-text Translation\n## 1. Introduction\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nDirect Speech Translation (AST) aims to directly translate audio signals from a source language into text words in a target language, bypassing the intermediate transcription step ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. Direct models offer advantages over cascaded systems by reducing inference latency, streamlining the translation pipeline, and avoiding error propagation from Automatic Speech Recognition (ASR) to Machine Translation (MT) components ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. However, a key challenge for direct AST is the scarcity of available training resources ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data'].\n--------------------\nPaper bibkey: [speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are]\nDigest: \nDirect speech-to-text translation aims to translate speech signals directly into a foreign language text, bypassing the intermediate step of automatic speech recognition (ASR) ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. This approach offers advantages over traditional cascaded systems (ASR + MT) by potentially reducing latency, streamlining the translation pipeline, and mitigating error propagation from the ASR component to the MT component ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. The survey will cover the developments in speech translation, from cascaded models to end-to-end approaches, highlighting the challenges and potential solutions in the field ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are'].\n## 2. Background: Evolution of Speech Translation Paradigms\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nTraditional speech translation systems often employ a cascaded approach, combining ASR and MT components ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. While cascaded models can leverage large amounts of ASR and MT data, they suffer from error propagation between components and separate training objectives ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. End-to-end models for direct ST have emerged as a promising alternative, offering joint training and direct mapping from speech to text, potentially overcoming the limitations of cascaded systems ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data'].\n--------------------\nPaper bibkey: [speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are]\nDigest: \nThe evolution of speech translation paradigms began with loosely coupled cascaded models, which employed separate ASR and MT systems ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. These early systems were susceptible to error propagation from the ASR stage, especially when combined with interlingua-based MT ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. Subsequent efforts moved towards tighter integration of ASR and MT, aiming to address the limitations of error propagation and separate training objectives ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. End-to-end models have emerged as a promising alternative, offering joint training of all model components and direct mapping from speech to text, theoretically overcoming the issues of cascaded models ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. However, the empirical success of end-to-end models has been mixed, and many approaches still re-introduce elements of cascaded systems to address data scarcity ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are'].\n## 3. Techniques for Direct Speech-to-text Translation: Emphasizing Pre-training Strategies and Encoder Design Challenges\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nTo address the challenge of data scarcity in direct ST, pre-training strategies have become crucial ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. Pre-training the encoder with ASR tasks has shown to be effective in improving AST performance, especially in low-resource scenarios ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. This paper focuses on pre-training strategies, specifically investigating the effective use of MT data for pre-training the decoder component of direct ST systems ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data'].\n--------------------\nPaper bibkey: [speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are]\nDigest: \nThe paper categorizes modeling techniques by intermediate representations, inference strategies, training strategies, and end-to-end training data ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. It discusses the trade-offs between data efficiency and modeling power, suggesting that many end-to-end models compromise on truly end-to-end training to improve data efficiency by incorporating ASR and MT data ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are'].\n### 3.1 Challenges in ST Encoder Design\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nOne of the challenges in effectively pre-training ST models is the potential mismatch between encoder representations learned from different modalities, such as speech (ASR) and text (MT) ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. Simply transferring decoder parameters pre-trained on MT data to an ST model has not been consistently beneficial, suggesting a gap in the encoder representations between ASR and MT tasks ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. The decoder is optimized to work with the specific encoder it was trained with, and a modality gap can hinder effective transfer learning ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data'].\n--------------------\nPaper bibkey: [speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are]\nDigest: \nThe challenges of loosely coupled cascades include 'Erroneous early decisions', where committing to a potentially incorrect transcript from ASR leads to error propagation ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. 'Mismatched source-language' arises from ASR and MT components modeling source language priors differently due to different assumptions and training data, leading to stylistic and topical divergence ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. 'Information loss' occurs because the MT model is unaware of prosodic information from speech inputs, which is lost when using transcripts as intermediate representations ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are'].  Vanilla direct models, while avoiding these issues, suffer from 'Data efficiency' challenges due to the scarcity of end-to-end ST training corpora, making them unable to compete with cascaded models trained on abundant ASR and MT data ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are'].\n### 3.2 Pre-training Strategies\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nPre-training strategies aim to leverage data from related tasks like ASR and MT to improve the performance of direct ST models, particularly in data-scarce situations ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. This paper explores a novel pre-training strategy using adversarial regularization to bridge the gap between ASR and MT encoder representations, facilitating the effective use of MT decoder pre-training for ST ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data'].\n#### 3.2.1 Adversarial Pre-training with ASR and MT Data\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nThis paper proposes an adversarial pre-training method to align the encoder representations of ASR and NMT models, making a pre-trained NMT decoder more effective for speech translation ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. The approach involves simultaneously training ASR and NMT models with both Cross-Entropy loss ($L_{CE}$) and an adversarial regularization loss ($L_{DISC}$) ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. The final training objective is:\n$$L = L_{CE} + \\alpha L_{DISC}$$\nwhere $\\alpha$ is a constant parameter to control the regularizer's effect ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. A discriminator network is used to distinguish between encoder representations from ASR and NMT modalities, and the adversarial loss encourages the encoders to produce indistinguishable representations, thus bridging the modality gap ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. The discriminator is a three-layer feedforward network with 1024 hidden units and Leaky-ReLU activation ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data'].\n#### 3.2.2 Self-Supervised Pre-training\n#### 3.2.3 Curriculum Pre-training\n#### 3.2.4 Multimodal Pre-training\n### 3.3 Data Augmentation Techniques\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nThe paper mentions that data augmentation techniques can be applied to further improve translation quality, but they are not specifically studied in this paper ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data'].\n--------------------\nPaper bibkey: [speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are]\nDigest: \nData augmentation is mentioned as a technique to address data limitations in direct models ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. Specifically, augmenting ASR corpora with automatic translations or MT corpora with synthesized speech is discussed as a way to create more training data ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are'].  It is noted that augmented ASR corpora are found to be more effective than augmented MT corpora ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. Knowledge distillation is also mentioned as an extension of data augmentation ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are'].\n### 3.4 Architectural Innovations and Key Components\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nThe paper uses Transformer and S-Transformer architectures as the basis for their models ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. S-Transformer employs CNN layers and 2D self-attention for encoding audio features ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. The architectural innovation in this paper is primarily the introduction of the adversarial regularizer in the pre-training stage, rather than novel encoder-decoder structures or attention mechanisms.\n--------------------\nPaper bibkey: [speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are]\nDigest: \nThe paper discusses various modeling techniques, including different intermediate representations (transcripts, hidden representations, lattices, pre-segmented speech frames, unsupervised speech-unit clusters) ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. It also categorizes inference strategies into Committed Cascade (CC), Marginalizing Cascade (MC), Direct (Di), Committed Triangle (CT), Marginalizing Triangle (MT), and Joint (Jt) models, illustrating different ways of using intermediate representations and conditioning inputs and outputs ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are'].\n#### 3.4.1 Addressing the Modality Gap\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nThe adversarial pre-training method proposed in this paper is explicitly designed to address the modality gap between speech and text representations by aligning the encoder outputs of ASR and NMT models ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data'].\n#### 3.4.2 Efficient and Compact Models\n## 4. Datasets and Evaluation Metrics for Direct Speech-to-text Translation\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nThe paper uses the MuST-C corpus (English-German) and Libri-Trans corpus (English-French) for evaluating AST systems ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. For ASR pre-training, the LibriSpeech corpus is used ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. For MT pre-training, TED and Opensubtitle2018 corpora (En-De) and WMT14 (En-Fr) are used ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. Evaluation metrics include BLEU score for translation tasks (AST and MT) and Word Error Rate (WER) for ASR ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data'].\n--------------------\nPaper bibkey: [librispeech_an_asr_corpus_based_on_public_domain_audio_books]\nDigest: \nThis paper introduces the LibriSpeech corpus, a new corpus of read English speech suitable for training and evaluating speech recognition systems ['librispeech_an_asr_corpus_based_on_public_domain_audio_books']. The LibriSpeech corpus is derived from audiobooks from the LibriVox project and contains 1000 hours of speech sampled at 16 kHz ['librispeech_an_asr_corpus_based_on_public_domain_audio_books']. The corpus is freely available under the CC BY 4.0 license, along with language model training data and pre-built language models ['librispeech_an_asr_corpus_based_on_public_domain_audio_books']. The training portion of the corpus is split into three subsets with approximate sizes of 100, 360, and 500 hours ['librispeech_an_asr_corpus_based_on_public_domain_audio_books']. The corpus also includes development and test sets, each with \"clean\" and \"other\" subsets, designed to evaluate model performance under different conditions ['librispeech_an_asr_corpus_based_on_public_domain_audio_books']. The \"clean\" subsets are selected for higher recording quality and accents closer to US English, while \"other\" subsets include more challenging data ['librispeech_an_asr_corpus_based_on_public_domain_audio_books']. The corpus ensures gender balance at the speaker level and in terms of the amount of data available for each gender ['librispeech_an_asr_corpus_based_on_public_domain_audio_books'].\n--------------------\nPaper bibkey: [speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are]\nDigest: \nThe paper mentions the limited availability of end-to-end ST corpora as a major challenge, highlighting that for practical purposes, the use of separate ASR and MT corpora is often unavoidable ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. It notes efforts to compile ST corpora from web sources, but these are still limited in size and language coverage ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are'].\n### 4.1 Multilingual Speech Translation Corpora\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nThe MuST-C corpus, used for English-German experiments, consists of 408 hours of speech data aligned with 234K translated sentences, derived from TED Talks ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. The Libri-Trans corpus, used for English-French experiments, contains 230 hours of speech aligned with 131K French sentences ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data'].\n### 4.2 Large-scale Pseudo Speech Translation Corpora\n### 4.3 Benchmarking and Evaluation Campaigns (IWSLT)\nPaper bibkey: [librispeech_an_asr_corpus_based_on_public_domain_audio_books]\nDigest: \nThe paper evaluates acoustic models trained on LibriSpeech on both LibriSpeech dev and test sets and Wall Street Journal (WSJ) test sets, reporting Word Error Rates (WERs) as evaluation metrics ['librispeech_an_asr_corpus_based_on_public_domain_audio_books']. The experiments compare Speaker-Adapted Training (SAT) and Deep Neural Network (DNN) acoustic models trained on varying amounts of LibriSpeech data (100h, 460h, 960h) and WSJ data (si-284) ['librispeech_an_asr_corpus_based_on_public_domain_audio_books']. The results demonstrate that acoustic models trained on LibriSpeech achieve lower WER on WSJ test sets compared to models trained on WSJ itself, highlighting the benefit of the larger size of the LibriSpeech corpus ['librispeech_an_asr_corpus_based_on_public_domain_audio_books'].\n## 5. Toolkits and Frameworks for Direct Speech-to-text Translation\nPaper bibkey: [librispeech_an_asr_corpus_based_on_public_domain_audio_books]\nDigest: \nThe paper mentions that they are releasing Kaldi scripts that facilitate building speech recognition systems using the LibriSpeech corpus, indicating the toolkit support for this dataset in ASR research ['librispeech_an_asr_corpus_based_on_public_domain_audio_books'].\n### 5.1 Open-Source Toolkits vs. Commercial Toolkits for ST\n### 5.2 Feature Comparison of Popular ST Toolkits (e.g., ESPnet-ST, Fairseq-ST)\n### 5.3 Toolkits for Specific Tasks in Direct ST (e.g., Low-Resource ST Toolkits)\n## 6. Challenges and Future Directions in Direct Speech-to-text Translation\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nA challenge in direct ST is effectively leveraging pre-trained MT decoders due to the modality gap between speech and text encoder representations ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. Future directions include exploring data augmentation techniques to further improve the translation quality of end-to-end ST models ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data'].\n--------------------\nPaper bibkey: [librispeech_an_asr_corpus_based_on_public_domain_audio_books]\nDigest: \nThe paper points out that recordings from LibriVox are not completely ideal due to MP3 compression and inconsistent enforcement of noise removal and volume normalization guidelines, which introduces variability in audio quality within the corpus ['librispeech_an_asr_corpus_based_on_public_domain_audio_books']. This suggests a challenge in dealing with real-world audio data variability even in read speech corpora. Future directions could involve developing methods to improve robustness to audio quality variations or to automatically identify and filter segments based on audio quality for more consistent training data.\n--------------------\nPaper bibkey: [speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are]\nDigest: \nThe paper identifies several central challenges in ST research: erroneous early decisions, mismatched source-language, information loss (specifically prosody), and data efficiency ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. It proposes analyzing models across these dimensions to understand the trade-offs between data efficiency and modeling power ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. Future directions include exploring models that extend traditional cascaded models, such as end-to-end fine-tuning or rescoring, and addressing application-specific requirements like mode of delivery, output medium, and translation method ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are'].  Specifically, the paper highlights the need for more research into how prosody should inform textual translation decisions, suggesting examples where prosody disambiguates meaning and impacts translation ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. The paper also points out the unexplored area of joint inference models that output both transcripts and translations, especially in applications where both are displayed to the user ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are'].\n### 6.1 6.x Ethical and Societal Implications\n## 7. Conclusion\nPaper bibkey: [effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data]\nDigest: \nThis paper demonstrates that adversarial regularization can effectively align ASR and MT encoder representations, enabling more effective pre-training of AST decoders using MT data ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. The proposed method improves AST performance by approximately 1.5 BLEU points compared to conventional pre-training methods on English-German and English-French language pairs ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data']. The main contribution is bridging the modality gap through adversarial training to enhance the transfer of knowledge from MT decoders to ST models ['effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data'].\n--------------------\nPaper bibkey: [librispeech_an_asr_corpus_based_on_public_domain_audio_books]\nDigest: \nThe paper concludes that the LibriSpeech corpus, created through automatic alignment and segmentation of audiobooks, provides a large-scale resource for training speech recognition systems ['librispeech_an_asr_corpus_based_on_public_domain_audio_books'].  A key takeaway is that the larger size of the LibriSpeech corpus (1000 hours) allows acoustic models trained on it to outperform models trained on smaller datasets like WSJ (82 hours) even on the WSJ test sets, demonstrating the importance of data scale in speech recognition ['librispeech_an_asr_corpus_based_on_public_domain_audio_books']. The availability of the corpus and Kaldi scripts is intended to facilitate further research and reproducibility in the ASR field ['librispeech_an_asr_corpus_based_on_public_domain_audio_books'].\n--------------------\nPaper bibkey: [speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are]\nDigest: \nThe paper concludes by summarizing the historical development of ST research and emphasizing the key challenges, techniques, and requirements in the field ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. It highlights the significant modeling space and application-specific requirements that remain to be explored ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']. The paper encourages meaningful and generalizable comparisons of ST models to overcome long-standing issues and advance the field ['speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are']."}, {"bibkey": "revisiting_end_to_end_speech_to_text_translation_from_scratch, mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation, wav2vec_unsupervised_pre_training_for_speech_recognition", "content": "#  Recent Advances in Direct Speech-to-text Translation\n## 1. Introduction\nPaper bibkey: [mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation]\nDigest: \nDirect speech-to-text translation (ST) directly translates source language speech into target language text, which is useful in scenarios like international conferences and foreign-language video subtitling ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. Traditional cascaded approaches (ASR+MT) suffer from error propagation, which end-to-end (E2E) approaches aim to mitigate while also being computationally more efficient at inference time ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. Existing E2E-ST solutions often rely on source language transcriptions for pre-training or multi-task learning with ASR, which can be problematic for low-resource languages or scenarios with limited transcribed speech data ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation'].\n--------------------\nPaper bibkey: [revisiting_end_to_end_speech_to_text_translation_from_scratch]\nDigest: \nDirect end-to-end (E2E) speech-to-text translation (ST) is defined as the task of translating source-language audio directly into foreign text without intermediate steps, gaining popularity due to its advantages ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. The motivations for direct ST include reducing translation latency and bypassing transcription errors from ASR models, making it theoretically appealing ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. The significance of direct ST lies in its potential to simplify the pipeline and improve efficiency compared to traditional cascaded approaches ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. This survey will focus on techniques to improve direct ST, especially training from scratch without pre-training ['revisiting_end_to_end_speech_to_text_translation_from_scratch'].\n## 2. Background: Evolution of Speech Translation Paradigms\nPaper bibkey: [mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation]\nDigest: \nTraditional cascaded ST approaches first transcribe speech to text using Automatic Speech Recognition (ASR) and then translate the text using Machine Translation (MT) ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. These cascaded systems are prone to error propagation from the ASR component to the MT component ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. End-to-end ST models aim to overcome these limitations by directly mapping speech to translated text, offering advantages in terms of reduced latency, streamlined pipelines, and mitigation of error propagation ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. E2E-ST models can be trained jointly, directly learning the mapping from speech to text, contrasting with the separate training objectives of ASR and MT in cascaded models ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation'].\n--------------------\nPaper bibkey: [revisiting_end_to_end_speech_to_text_translation_from_scratch]\nDigest: \nTraditional cascaded speech translation models decompose the task into automatic speech recognition (ASR) and machine translation (MT), which are trained separately ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. Limitations of cascaded models include error propagation from ASR to MT and separate training objectives that may not be optimal for the overall translation task ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. End-to-end models offer advantages by jointly handling ASR and MT in a single neural network, allowing for joint training and direct mapping from speech to text ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. This theoretical advantage includes reduced latency and mitigation of ASR transcription errors ['revisiting_end_to_end_speech_to_text_translation_from_scratch'].\n## 3. Techniques for Direct Speech-to-text Translation: Emphasizing Pre-training Strategies and Encoder Design Challenges\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nThis paper introduces wav2vec, an unsupervised pre-training approach for speech recognition that learns representations from raw audio to improve acoustic model training ['wav2vec_unsupervised_pre_training_for_speech_recognition']. It employs a simple multi-layer convolutional neural network pre-trained on large amounts of unlabeled audio data using a noise contrastive binary classification task ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The pre-trained representations are then used to enhance supervised acoustic models ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The focus is on leveraging unlabeled audio data to overcome the need for large amounts of transcribed data in speech recognition ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n--------------------\nPaper bibkey: [mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation]\nDigest: \nTo improve E2E-ST, researchers have explored pre-training the ST encoder with ASR encoders or using multi-task learning (MTL) with ASR ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. These methods leverage source language transcriptions to provide training signals for the encoder to learn phonetic information and hidden representations ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation'].\n--------------------\nPaper bibkey: [revisiting_end_to_end_speech_to_text_translation_from_scratch]\nDigest: \nTechniques to improve direct ST systems often involve pre-training strategies and encoder design ['revisiting_end_to_end_speech_to_text_translation_from_scratch'].  Pre-training is commonly used to initialize the speech encoder on ASR data and/or the text decoder on MT data before fine-tuning on ST data ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. Data augmentation and architectural innovations are also crucial for enhancing performance ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. This paper focuses on techniques to improve direct ST from scratch, without pre-training, by revisiting existing techniques and proposing new methods ['revisiting_end_to_end_speech_to_text_translation_from_scratch'].\n### 3.1 Challenges in ST Encoder Design\nPaper bibkey: [mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation]\nDigest: \nE2E-ST is more challenging than MT or ASR because of the different modalities on source (speech) and target (text) sides, the longer input sequence of speech compared to text, and the need to learn global reordering between speech and translation, unlike the monotonic alignment in ASR ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. Obtaining corresponding phoneme or syllable segments from different languages adds to the difficulty ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation'].\n### 3.2 Pre-training Strategies\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nThe wav2vec model utilizes unsupervised pre-training to learn general audio representations, which are then transferred to improve supervised speech recognition ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The pre-training task is to distinguish true future audio samples from negative distractors using a contrastive loss, moving beyond frame-wise phoneme classification of previous works ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The model architecture is fully convolutional, allowing for efficient parallelization ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n--------------------\nPaper bibkey: [mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation]\nDigest: \nPre-training is a crucial technique for improving E2E-ST, especially for learning robust speech encoders ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. Existing pre-training strategies often rely on source language transcriptions, which limits their applicability in low-resource scenarios ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. The Masked Acoustic Modeling (MAM) technique is proposed as a self-supervised pre-training method that does not require transcriptions and can utilize unlabeled speech data or even arbitrary acoustic signals ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. MAM masks portions of the speech input and trains the encoder to recover the masked speech signals from context ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation'].\n--------------------\nPaper bibkey: [revisiting_end_to_end_speech_to_text_translation_from_scratch]\nDigest: \nPre-training is a widely adopted strategy in direct ST to improve translation quality, often pre-training the speech encoder on ASR data or the text decoder on MT data ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. This approach has become a standard practice in recent ST studies and toolkits ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. However, the paper argues that the significance of pre-training for E2E ST might be overestimated, and explores how far ST can go by training from scratch using speech-translation pairs alone ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. The paper does not focus on specific pre-training paradigms like homogeneous or heterogeneous transfer learning, SATE, or FATE, but rather investigates methods to enhance ST performance without relying on any pre-training ['revisiting_end_to_end_speech_to_text_translation_from_scratch'].\n#### 3.2.1 Adversarial Pre-training with ASR and MT Data\n#### 3.2.2 Self-Supervised Pre-training\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nWav2vec is a self-supervised pre-training technique where a convolutional neural network is trained to predict future audio samples from a given context using a contrastive objective ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The model takes raw audio as input and consists of an encoder network to embed the audio signal into a latent space and a context network to create contextualized representations ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The objective function is defined as:\n$$\n\\mathcal{L}_{k}=-\\sum_{i=1}^{T-k}\\Big(\\log\\sigma(\\mathbf{z}_{i+k}^{\\top}h_{k}(\\mathbf{c}_{i}))+\\lambda\\mathbb{E}_{\\mathbf{\\widetilde{z}}\\sim p_{n}}\\Big\\Big)\n$$\nwhere $\\mathbf{z}_{i}$ are latent representations from the encoder, $\\mathbf{c}_{i}$ are contextualized representations from the context network, and $h_{k}$ is a step-specific affine transformation ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The model is trained to minimize the loss $\\mathcal{L}=\\sum_{k=1}^{K}\\mathcal{L}_{k}$ over different step sizes $K$ ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n--------------------\nPaper bibkey: [mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation]\nDigest: \nMasked Acoustic Modeling (MAM) is a self-supervised pre-training technique where random masks are applied to the speech audio input, and the model is trained to reconstruct the original speech signal from the masked input using surrounding context ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. Unlike other self-supervised methods, MAM does not rely on transcriptions or forced alignment, making it applicable to untranscribed speech and even non-speech audio ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. MAM can be pre-trained on source language speech, multilingual speech, or arbitrary acoustic data ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. Pre-training tasks in MAM involve reconstructing masked speech frames using a deconvolutional reconstruction function and minimizing the mean squared error between original and recovered signals ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. Two masking strategies are proposed: Single Frame Masking (randomly mask individual frames) and Span Masking (randomly mask spans of consecutive frames) ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation'].\n#### 3.2.3 Curriculum Pre-training\n#### 3.2.4 Multimodal Pre-training\n### 3.3 Data Augmentation Techniques\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nThe paper uses cropping of audio sequences during pre-training as a form of data augmentation, where sequences are cropped to a maximum length, and cropping offsets are randomly chosen for each sample, re-sampled every epoch ['wav2vec_unsupervised_pre_training_for_speech_recognition']. This cropping ensures equal length sequences within a batch and removes on average 25% of the training data ['wav2vec_unsupervised_pre_training_for_speech_recognition']. Different crop sizes were evaluated, with 150k frames yielding the best performance on TIMIT ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n--------------------\nPaper bibkey: [mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation]\nDigest: \nSpecAugment, a data augmentation method involving random masking over input speech, is used in E2E-ST to improve performance ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. However, SpecAugment is used as a data augmentation during training, not as a pre-training framework like MAM ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation'].\n### 3.4 Architectural Innovations and Key Components\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nThe wav2vec model architecture consists of two main components: an encoder network and a context network, both built from convolutional neural networks ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The encoder network $f$ is a five-layer convolutional network that embeds raw audio into a latent space $\\mathcal{Z}$ ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The context network $g$ is a nine-layer convolutional network that takes the output of the encoder and mixes multiple latent representations into a contextualized tensor $\\mathbf{c}_{i}$ ['wav2vec_unsupervised_pre_training_for_speech_recognition']. Both networks use causal convolutions, group normalization, and ReLU nonlinearities ['wav2vec_unsupervised_pre_training_for_speech_recognition']. A larger variant, \"wav2vec large,\" incorporates deeper context networks with skip connections for increased capacity ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n--------------------\nPaper bibkey: [mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation]\nDigest: \nThe paper uses a Transformer-based E2E-ST framework with 2D convolutional downsampling and a 12-layer Transformer encoder-decoder ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. The MAM module is implemented by linearly projecting the encoder outputs and upsampling with 2-layer deconvolution to match the input signal size ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation'].\n--------------------\nPaper bibkey: [revisiting_end_to_end_speech_to_text_translation_from_scratch]\nDigest: \nThe paper proposes Neural Acoustic Feature Modeling (NAFM) as an architectural innovation ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. NAFM replaces heuristic rule-based acoustic feature extraction (like filterbanks) with trainable neural networks, aiming to learn ST-oriented acoustic features directly from raw waveforms ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. NAFM employs two feed-forward neural blocks to simulate the structure of filterbank extraction ['revisiting_end_to_end_speech_to_text_translation_from_scratch'].  The formula for NAFM is given by:\n$$\n\\begin{array}{r l}&{\\mathbf{x}^{(1)}=\\mathrm{LN}\\left(\\mathrm{FFN}\\left(\\mathbf{x}^{(0)}\\right)+\\mathbf{x}^{(0)}\\right),}\\\\ &{\\mathbf{x}^{(2)}=\\mathrm{LN}\\left(\\mathrm{FFN}\\left(\\mathbf{x}^{(1)}\\right)+\\mathbf{x}^{(1)}\\right),}\\end{array}\n$$\nwhere $\\mathbf{x}^{(0)}\\in\\mathbb{R}^{d_{s p e e c h}}$ is the raw speech frame ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. To regularize NAFM, an $L_2$ objective is added to leverage the inductive bias from filterbanks:\n$$\n\\mathcal{L}^{\\mathrm{NAFM}}(X,Y)=\\mathcal{L}(X,Y)+\\gamma\\frac{1}{\\lvert X\\rvert}(\\lvert\\lvert\\mathbf{X}^{(2)}-\\mathbf{X}^{f}\\rvert\\rvert^{2}),\n$$\nwhere $\\gamma$ is a hyperparameter and $\\mathbf{X}^{f}$ represents filterbank features ['revisiting_end_to_end_speech_to_text_translation_from_scratch'].\n#### 3.4.1 Addressing the Modality Gap\n#### 3.4.2 Efficient and Compact Models\n## 4. Datasets and Evaluation Metrics for Direct Speech-to-text Translation\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nThe wav2vec model was evaluated on speech recognition tasks using the WSJ and TIMIT datasets ['wav2vec_unsupervised_pre_training_for_speech_recognition']. For pre-training, the model used the audio data from WSJ, Librispeech (80h subset and full 960h), and a combination of WSJ and Librispeech ['wav2vec_unsupervised_pre_training_for_speech_recognition']. Evaluation metrics included Word Error Rate (WER) and Letter Error Rate (LER) for WSJ, and Phoneme Error Rate (PER) for TIMIT ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n--------------------\nPaper bibkey: [mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation]\nDigest: \nThe paper uses the MuST-C dataset for E2E-ST experiments, which is a multilingual dataset derived from TED Talks ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. For MAM pre-training, Libri-Light (English speech), Common Voice (multilingual speech), and Audioset (arbitrary acoustic data) corpora are used ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. BLEU score is used as the evaluation metric for translation accuracy ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation'].\n--------------------\nPaper bibkey: [revisiting_end_to_end_speech_to_text_translation_from_scratch]\nDigest: \nThe paper uses several datasets for training and evaluation, including MuST-C, CoVoST, LibriSpeech En-Fr, and Kosp2e ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. MuST-C is a multilingual corpus derived from TED talks, offering translations from English to 8 languages ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. CoVoST (version 2) is a large-scale multilingual ST corpus collected from Common Voice, providing translations for many language pairs ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. LibriSpeech En-Fr is an augmented dataset aligning French e-books with English LibriSpeech utterances ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. Kosp2e is a Korean-to-English ST corpus from various domains ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. Evaluation metric used is detokenized case-sensitive BLEU (Bilingual Evaluation Understudy) score, measured by SacreBLEU ['revisiting_end_to_end_speech_to_text_translation_from_scratch'].\n### 4.1 Multilingual Speech Translation Corpora\nPaper bibkey: [mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation]\nDigest: \nMuST-C dataset is used for evaluating E2E-ST models in the paper ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. It contains 408 hours of speech data ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation'].\n--------------------\nPaper bibkey: [revisiting_end_to_end_speech_to_text_translation_from_scratch]\nDigest: \nMuST-C is a multilingual ST corpus extracted from TED Talks, providing En-to-X translations for 8 languages with roughly 452 hours of training data per language ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. It contains approximately 252K utterances per language on average ['revisiting_end_to_end_speech_to_text_translation_from_scratch'].\n### 4.2 Large-scale Pseudo Speech Translation Corpora\n### 4.3 Benchmarking and Evaluation Campaigns (IWSLT)\n## 5. Toolkits and Frameworks for Direct Speech-to-text Translation\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nThe acoustic models for evaluation were trained and evaluated using the wav2letter++ toolkit ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The pre-training models were implemented in PyTorch using the fairseq toolkit ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n--------------------\nPaper bibkey: [mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation]\nDigest: \nThe experiments are conducted with settings similar to ESPnet-ST ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. Kaldi is used for feature extraction ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. Sentencepiece is used for subword tokenization ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation'].\n### 5.1 Open-Source Toolkits vs. Commercial Toolkits for ST\n### 5.2 Feature Comparison of Popular ST Toolkits (e.g., ESPnet-ST, Fairseq-ST)\n### 5.3 Toolkits for Specific Tasks in Direct ST (e.g., Low-Resource ST Toolkits)\n## 6. Challenges and Future Directions in Direct Speech-to-text Translation\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nThe paper highlights the effectiveness of unsupervised pre-training for speech recognition, particularly in low-resource scenarios, as a way to reduce the reliance on transcribed data ['wav2vec_unsupervised_pre_training_for_speech_recognition']. Future directions include investigating different architectures to further improve performance ['wav2vec_unsupervised_pre_training_for_speech_recognition']. A challenge remains in further improving performance with limited transcribed data and exploring the potential of even larger amounts of unlabeled data for pre-training.\n--------------------\nPaper bibkey: [mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation]\nDigest: \nCurrent E2E-ST solutions heavily rely on transcriptions, limiting their use in low-resource scenarios ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. Future directions include leveraging unlabeled speech data and arbitrary acoustic signals for pre-training, as demonstrated by MAM, to improve E2E-ST in data-scarce situations ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation'].\n--------------------\nPaper bibkey: [revisiting_end_to_end_speech_to_text_translation_from_scratch]\nDigest: \nThe paper identifies that pre-training still matters in low-resource scenarios and when large-scale external ASR or MT data is available ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. Future research directions include exploring how to leverage other types of data to improve pre-training for ST, especially in low-resource settings ['revisiting_end_to_end_speech_to_text_translation_from_scratch'].  Further investigation into optimizing ST with raw waveforms and improving Neural Acoustic Feature Modeling (NAFM) is also suggested for better acoustic feature extraction ['revisiting_end_to_end_speech_to_text_translation_from_scratch'].\n### 6.1 6.x Ethical and Societal Implications\n## 7. Conclusion\nPaper bibkey: [wav2vec_unsupervised_pre_training_for_speech_recognition]\nDigest: \nWav2vec introduces a novel unsupervised pre-training method for speech recognition using a fully convolutional model, achieving state-of-the-art results on WSJ with significantly less labeled data than previous best character-based systems ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The study demonstrates that pre-training on larger amounts of unlabeled audio data improves performance, benefiting both low-resource and full-resource settings in speech recognition ['wav2vec_unsupervised_pre_training_for_speech_recognition']. The main contribution is the successful application of unsupervised pre-training to speech recognition using a convolutional architecture and showing its effectiveness in improving ASR performance, especially in data-scarce situations ['wav2vec_unsupervised_pre_training_for_speech_recognition'].\n--------------------\nPaper bibkey: [mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation]\nDigest: \nThe paper introduces Masked Acoustic Modeling (MAM) as a novel self-supervised pre-training framework for E2E-ST that does not require transcriptions and can utilize arbitrary acoustic signals ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. MAM improves E2E-ST performance, especially in settings without transcriptions, and pre-training with arbitrary acoustic data is shown to be beneficial ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation']. MAM offers an alternative to transcription-dependent pre-training methods and has the potential to leverage vast amounts of unlabeled audio data for ST improvement ['mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation'].\n--------------------\nPaper bibkey: [revisiting_end_to_end_speech_to_text_translation_from_scratch]\nDigest: \nThe paper concludes that end-to-end ST models trained from scratch, when properly optimized with techniques like Parameterized Distance Penalty (PDP), CTC regularization, and specific architectural configurations, can achieve performance comparable to or even better than pre-training-based models in many scenarios ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. The main contribution is demonstrating that the performance gap between ST from scratch and pre-training-based ST has been overestimated, and that ST from scratch is a viable and promising direction ['revisiting_end_to_end_speech_to_text_translation_from_scratch']. The paper highlights the importance of revisiting and optimizing ST from scratch and offers a set of best practices for this approach ['revisiting_end_to_end_speech_to_text_translation_from_scratch']."}, {"bibkey": "investigating_self_supervised_pre_training_for_end_to_end_speech_translation, end_to_end_automatic_speech_translation_of_audiobooks, bridging_the_modality_gap_for_speech_to_text_translation", "content": "#  Recent Advances in Direct Speech-to-text Translation\n## 1. Introduction\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nDirect speech-to-text translation aims to translate speech in one language directly into text in another language, bypassing the intermediate step of source language transcription. This approach offers advantages over traditional cascaded systems, such as reduced latency and a streamlined pipeline, as it avoids error propagation from automatic speech recognition (ASR) to machine translation (MT) components. This paper investigates end-to-end speech-to-text translation in the context of audiobooks, focusing on scenarios where source language transcription may or may not be available during training and decoding. ['end_to_end_automatic_speech_translation_of_audiobooks']\n--------------------\nPaper bibkey: [bridging_the_modality_gap_for_speech_to_text_translation]\nDigest: \nDirect speech-to-text translation (ST) aims to translate speech from one language directly into text in another language, offering advantages over traditional cascaded systems by reducing latency, streamlining pipelines, and mitigating error propagation, which is valuable in scenarios like conference speeches, cross-border services, and international business talks ['bridging_the_modality_gap_for_speech_to_text_translation'].  The survey scope will cover recent advancements in this end-to-end approach.\n--------------------\nPaper bibkey: [investigating_self_supervised_pre_training_for_end_to_end_speech_translation]\nDigest: \nDirect speech-to-text translation (AST) is investigated in this paper, focusing on scenarios where source language recordings are untranscribed, limited training data is available, but a larger amount of unlabeled speech exists, which is common for languages with poor orthography or unwritten languages ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. The study explores leveraging unlabeled speech for end-to-end AST, aiming to reduce the dependence on labeled data by using acoustic representation learning ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation'].\n## 2. Background: Evolution of Speech Translation Paradigms\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nTraditional speech translation systems typically rely on a cascaded approach, combining ASR and MT modules. These systems first transcribe the source language speech into text using ASR, and then translate this text into the target language using MT.  End-to-end models, in contrast, aim to directly map speech from the source language to text in the target language, potentially offering a more efficient and robust approach by jointly training the entire translation process.  This paper builds upon previous works that explore end-to-end speech-to-text translation, including scenarios where source language transcriptions are not utilized during either training or decoding, and investigates a midway scenario where source language transcriptions are available only during training. ['end_to_end_automatic_speech_translation_of_audiobooks']\n--------------------\nPaper bibkey: [bridging_the_modality_gap_for_speech_to_text_translation]\nDigest: \nTraditional speech-to-text translation systems have relied on a pipeline approach, first converting speech to text using Automatic Speech Recognition (ASR) and then translating the text with Machine Translation (MT) models ['bridging_the_modality_gap_for_speech_to_text_translation']. This cascaded approach suffers from error propagation from the ASR to the MT stage and involves separate training objectives for each component ['bridging_the_modality_gap_for_speech_to_text_translation']. End-to-end models have emerged as a promising alternative, theoretically and practically advantageous due to joint training and direct mapping from speech to target text, potentially alleviating these limitations ['bridging_the_modality_gap_for_speech_to_text_translation'].\n--------------------\nPaper bibkey: [investigating_self_supervised_pre_training_for_end_to_end_speech_translation]\nDigest: \nPrevious AST systems relied on a two-step approach involving source language automatic speech recognition (ASR) followed by source-to-target text machine translation (MT) ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. Recent research explores end-to-end AST models that directly translate speech to text without explicit source language transcription during learning or decoding, though some use it at training time ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. Self-supervised learning from raw speech has emerged as a beneficial technique for improving ASR, and this paper investigates its impact on end-to-end AST performance ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation'].\n## 3. Techniques for Direct Speech-to-text Translation: Emphasizing Pre-training Strategies and Encoder Design Challenges\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThis paper explores end-to-end speech-to-text translation by training models with and without source language transcriptions available during training. The techniques employed involve encoder-decoder models with attention mechanisms for direct speech-to-text translation. The paper investigates the performance of end-to-end models in comparison to cascaded ASR+MT systems and explores the impact of pre-training and multi-task learning on the end-to-end approach. ['end_to_end_automatic_speech_translation_of_audiobooks']\n--------------------\nPaper bibkey: [bridging_the_modality_gap_for_speech_to_text_translation]\nDigest: \nThe paper introduces a novel technique called Speech-to-Text Adaptation for Speech Translation (STAST) to improve direct ST by addressing the modality gap between speech and text ['bridging_the_modality_gap_for_speech_to_text_translation']. The core methodology involves decoupling the ST encoder and employing a shrink mechanism to handle length inconsistencies, integrating a text-based MT model, and using cross-modal adaptation to bridge the semantic gap ['bridging_the_modality_gap_for_speech_to_text_translation']. Experiments on English-French and English-German datasets demonstrated state-of-the-art performance improvements using the STAST model ['bridging_the_modality_gap_for_speech_to_text_translation'].\n--------------------\nPaper bibkey: [investigating_self_supervised_pre_training_for_end_to_end_speech_translation]\nDigest: \nThis paper focuses on the pre-training strategy of self-supervised learning to improve direct speech-to-text translation ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. It uses contrastive predictive coding (CPC) pre-trained from unlabeled speech as a feature extractor for AST tasks ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation'].\n### 3.1 Challenges in ST Encoder Design\nPaper bibkey: [bridging_the_modality_gap_for_speech_to_text_translation]\nDigest: \nThe paper identifies key modality differences between speech and text that pose challenges for ST encoders ['bridging_the_modality_gap_for_speech_to_text_translation']. These include:\n1. **Length Inconsistency**: Speech feature sequences are much longer than corresponding text sequences, complicating alignment learning ['bridging_the_modality_gap_for_speech_to_text_translation'].\n2. **Semantic Gap**: Speech features, often hand-crafted, lack the rich semantic information present in text embeddings ['bridging_the_modality_gap_for_speech_to_text_translation'].\n3. **Robustness Issues**: Speech is more variable and susceptible to noise compared to clean text data, affecting model robustness ['bridging_the_modality_gap_for_speech_to_text_translation'].\nThese differences overload the ST encoder, requiring it to simultaneously learn acoustic and semantic information, thus hindering effective representation learning ['bridging_the_modality_gap_for_speech_to_text_translation'].\n### 3.2 Pre-training Strategies\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThe paper investigates pre-training strategies by initializing the encoder and decoder of the end-to-end speech translation model with pre-trained ASR and MT models respectively. This pre-trained model is then compared to end-to-end models trained from scratch. The motivation behind pre-training is to leverage the knowledge learned from related tasks (ASR and MT) to improve the performance and convergence speed of the speech translation model. This can be considered a form of heterogeneous transfer learning, where models trained on different but related tasks are used to initialize the speech translation model. ['end_to_end_automatic_speech_translation_of_audiobooks']\n--------------------\nPaper bibkey: [investigating_self_supervised_pre_training_for_end_to_end_speech_translation]\nDigest: \nThis paper investigates self-supervised pre-training using contrastive predictive coding (CPC) for AST ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. The CPC model consists of an encoder network converting audio to latent representations and a context network aggregating time steps into contextualized representations ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. The model is trained to distinguish a future audio frame from negative samples using a contrastive loss ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. The study shows that fine-tuning CPC models on AST training data further enhances performance, particularly in low-resource settings, and ensembling models with filter-bank and CPC features achieves near state-of-the-art results without ASR pre-training ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. The representations learned are analyzed to show improved phone discrimination, source-target sequence alignment, and robustness to speaker variability ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation'].\n#### 3.2.1 Adversarial Pre-training with ASR and MT Data\n#### 3.2.2 Self-Supervised Pre-training\nPaper bibkey: [investigating_self_supervised_pre_training_for_end_to_end_speech_translation]\nDigest: \nThe paper uses a contrastive predictive coding (CPC) model for self-supervised pre-training ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. The CPC model, based on wav2vec, is pre-trained on unlabeled speech data from the LibriSpeech corpus ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. The encoder network converts raw speech samples $x$ into feature representations $z$, and the context network builds contextualized representations $c_i$ from sequences of $z$ ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. The model is trained end-to-end to minimize a contrastive loss, distinguishing a future sample $z_{i+k}$ from negative samples $\\tilde{z}$ ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. Fine-tuning the pre-trained wav2vec model on the AST training data (How2 corpus) further improves performance ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. Experiments show that models trained with wav2vec representations converge better and faster, especially in low resource settings (28 and 56 hours) compared to filter-bank features ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation'].\n#### 3.2.3 Curriculum Pre-training\nPaper bibkey: [bridging_the_modality_gap_for_speech_to_text_translation]\nDigest: \nCurriculum pre-training, as proposed by Wang et al. (2020b), is mentioned as a method that integrates courses to enable encoders to understand sentence meaning and map words across languages ['bridging_the_modality_gap_for_speech_to_text_translation']. However, the paper critiques its reliance on force-alignment and potential introduction of alignment errors due to the need for an extra ASR model ['bridging_the_modality_gap_for_speech_to_text_translation'].\n#### 3.2.4 Multimodal Pre-training\n### 3.3 Data Augmentation Techniques\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThe paper utilizes an augmented LibriSpeech corpus for end-to-end speech translation. This augmentation process involves aligning French e-books with English utterances from the LibriSpeech corpus. Additionally, English transcriptions from LibriSpeech are translated into French using Google Translate. This process effectively creates a pseudo-speech translation corpus by leveraging existing ASR data and MT techniques to generate parallel speech-translation pairs. The training dataset is further doubled by concatenating aligned references with Google Translate references. This data augmentation strategy addresses the scarcity of parallel speech-text data for speech translation by creating a larger training corpus. ['end_to_end_automatic_speech_translation_of_audiobooks']\n--------------------\nPaper bibkey: [investigating_self_supervised_pre_training_for_end_to_end_speech_translation]\nDigest: \nData augmentation using speed perturbation with factors of 0.9, 1.0, and 1.1 is applied to the training data in the experiments ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation'].\n### 3.4 Architectural Innovations and Key Components\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThe paper employs an encoder-decoder model with attention for end-to-end speech translation. The speech encoder combines convolutional layers and bidirectional LSTMs.  Specifically, it uses two non-linear tanh layers followed by two convolutional layers with stride (2,2) to reduce the time length of the input features, and then three bidirectional LSTM layers. The character-level decoder consists of two LSTM layers with a global attention mechanism and a dense output layer. The architecture is designed to be compact and efficient for speech translation. The encoder architecture is a mix of convolutional encoders and pyramidal LSTMs, aiming to capture both local and long-range dependencies in the speech signal. ['end_to_end_automatic_speech_translation_of_audiobooks']\n--------------------\nPaper bibkey: [bridging_the_modality_gap_for_speech_to_text_translation]\nDigest: \nThe STAST model itself is an architectural innovation designed for end-to-end ST ['bridging_the_modality_gap_for_speech_to_text_translation']. Key components include:\n1. **Decoupled Encoder**: The ST encoder is separated into an acoustic encoder, a shrink mechanism, and a semantic encoder to distribute the learning burden ['bridging_the_modality_gap_for_speech_to_text_translation'].\n2. **Shrink Mechanism**: This mechanism filters redundant hidden states from the acoustic encoder based on CTC label probabilities to reduce the length of speech representation and improve alignment with text ['bridging_the_modality_gap_for_speech_to_text_translation'].\n3. **Integrated NMT Model**: A complete text-based NMT model is integrated into STAST by sharing the semantic encoder and decoder, ensuring training in a shared latent space and leveraging MT knowledge ['bridging_the_modality_gap_for_speech_to_text_translation'].\n4. **Cross-Modal Adaptation**: This method further bridges the gap by transferring semantic knowledge from text representation to speech representation through sentence-level or word-level adaptation, minimizing the distance between their representations ['bridging_the_modality_gap_for_speech_to_text_translation'].\n--------------------\nPaper bibkey: [investigating_self_supervised_pre_training_for_end_to_end_speech_translation]\nDigest: \nThe speech encoder architecture consists of a stack of two VGG-like CNN blocks followed by five 1024-dimensional BLSTM layers ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. Each VGG block reduces the time and frequency dimensions of input speech features by a factor of 2 ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. A projection block is added to reduce the dimension of wav2vec features from 512 to 83 to maintain a similar parameter budget compared to filter-bank features ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. Bahdanau's attention mechanism is used, and the decoder is a stack of two 1024-dimensional LSTM layers ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation'].\n#### 3.4.1 Addressing the Modality Gap\nPaper bibkey: [bridging_the_modality_gap_for_speech_to_text_translation]\nDigest: \nThe STAST model is explicitly designed to address the modality gap ['bridging_the_modality_gap_for_speech_to_text_translation']. The decoupled encoder, shrink mechanism, integrated NMT model, and cross-modal adaptation are all components aimed at aligning speech and text representations and improving semantic knowledge transfer from text MT to speech translation ['bridging_the_modality_gap_for_speech_to_text_translation'].\n#### 3.4.2 Efficient and Compact Models\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThe paper emphasizes the training of compact and efficient end-to-end AST models. The model architecture is designed to be relatively shallow, with a focus on parameter efficiency. For instance, the speech encoder uses a stack of three bidirectional LSTMs and convolutional layers to process the input speech features.  The decoder is also composed of two LSTM layers. The parameter size of the end-to-end model is reported to be 6.7 million for BTEC and 9.4 million for LibriSpeech, which is smaller than the cascaded models. Experimental results suggest that these compact end-to-end models can achieve performance close to cascaded systems, demonstrating efficiency in terms of model size and computational cost. ['end_to_end_automatic_speech_translation_of_audiobooks']\n## 4. Datasets and Evaluation Metrics for Direct Speech-to-text Translation\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThe paper introduces and utilizes the Augmented LibriSpeech corpus, a large English to French corpus created for direct speech translation. The corpus is derived from the LibriSpeech ASR corpus and augmented with French translations aligned from e-books and machine translation. The corpus is split into training, development, and test sets.  The paper also evaluates models on the BTEC synthetic speech corpus.  Evaluation metrics used are BLEU for translation tasks (MT and AST) and WER for speech recognition (ASR). These metrics are standard in the speech translation and machine translation fields for evaluating translation quality and transcription accuracy. ['end_to_end_automatic_speech_translation_of_audiobooks']\n--------------------\nPaper bibkey: [bridging_the_modality_gap_for_speech_to_text_translation]\nDigest: \nThe paper uses two datasets for experiments: Augmented LibriSpeech English-French and MuST-C English-German ['bridging_the_modality_gap_for_speech_to_text_translation']. Evaluation is performed using case-insensitive BLEU score, computed with the multi-bleu.pl script ['bridging_the_modality_gap_for_speech_to_text_translation'].\n--------------------\nPaper bibkey: [investigating_self_supervised_pre_training_for_end_to_end_speech_translation]\nDigest: \nThe How2 corpus is used for main experiments, containing about 297.6 hours of speech translated from English to Portuguese ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. Subcorpora of 10%, 20%, 30%, and 60% of the full corpus are created to simulate lower resource scenarios ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. MuST-C English-to-German and English-to-French datasets are used to validate results in low resource settings ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. TIMIT, LibriSpeech, and VoxCeleb1 datasets are used for analyzing learnt representations in phoneme recognition and speaker verification tasks ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. BLEU score is used to evaluate translation quality on How2 and MuST-C datasets ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. Phone Error Rate (PER) is used for phoneme recognition on TIMIT ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. Equal Error Rate (EER) is used for speaker verification on VoxCeleb1 and LibriSpeech ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation'].\n### 4.1 Multilingual Speech Translation Corpora\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThe Augmented LibriSpeech corpus is a multilingual speech translation corpus, specifically for English to French. It contains 236 hours of English speech aligned with French translations.  The corpus is created by aligning French e-books with English utterances from LibriSpeech and further augmented with MT translations of English transcripts.  It is split into train (100h clean, 130h extended), dev (2h), and test (4h) sets.  This corpus addresses the lack of large publicly available parallel speech-text corpora for speech translation, providing a valuable resource for training and evaluating multilingual speech translation systems. ['end_to_end_automatic_speech_translation_of_audiobooks']\n--------------------\nPaper bibkey: [bridging_the_modality_gap_for_speech_to_text_translation]\nDigest: \nThe paper utilizes the MuST-C English-German corpus, derived from TED Talks, which includes English speech, transcriptions, and translations in multiple languages ['bridging_the_modality_gap_for_speech_to_text_translation']. This corpus contains 408 hours of speech and 234K translation pairs, recorded from live presentations, making it noisier than audiobook datasets ['bridging_the_modality_gap_for_speech_to_text_translation'].\n--------------------\nPaper bibkey: [investigating_self_supervised_pre_training_for_end_to_end_speech_translation]\nDigest: \nMuST-C corpus is used for validation with English-to-German and English-to-French language pairs in low resource settings ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation'].\n### 4.2 Large-scale Pseudo Speech Translation Corpora\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThe Augmented LibriSpeech corpus can be considered a large-scale pseudo speech translation corpus because of its creation methodology. While it leverages aligned e-books, it also incorporates machine translation (Google Translate) to generate French translations of English transcripts.  This use of MT in the corpus creation process introduces a pseudo-parallel aspect, as some translations are not human-generated but machine-generated.  Despite this, the corpus is large (236 hours) and provides a valuable resource for training end-to-end speech translation models, especially when large amounts of parallel human-translated speech data are unavailable. ['end_to_end_automatic_speech_translation_of_audiobooks']\n### 4.3 Benchmarking and Evaluation Campaigns (IWSLT)\nPaper bibkey: [investigating_self_supervised_pre_training_for_end_to_end_speech_translation]\nDigest: \nThe development set from the authors' participation in IWSLT2019 is reused for validation, and the evaluation is conducted in conditions similar to IWSLT 2019 shared task ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation'].\n## 5. Toolkits and Frameworks for Direct Speech-to-text Translation\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThe models in this paper are implemented using TensorFlow as part of the LIG-CRIStAL NMT toolkit. This indicates the use of an open-source framework (TensorFlow) and a research-oriented toolkit (LIG-CRIStAL NMT toolkit) for developing and experimenting with direct speech-to-text translation models. The LIG-CRIStAL NMT toolkit provides functionalities for neural machine translation and related tasks, facilitating the implementation of encoder-decoder models with attention and other components used in the paper. ['end_to_end_automatic_speech_translation_of_audiobooks']\n### 5.1 Open-Source Toolkits vs. Commercial Toolkits for ST\n### 5.2 Feature Comparison of Popular ST Toolkits (e.g., ESPnet-ST, Fairseq-ST)\nPaper bibkey: [bridging_the_modality_gap_for_speech_to_text_translation]\nDigest: \nESPnet is mentioned as a toolkit used for baseline comparisons, with both encoder and decoder pre-trained in their experiments ['bridging_the_modality_gap_for_speech_to_text_translation'].\n### 5.3 Toolkits for Specific Tasks in Direct ST (e.g., Low-Resource ST Toolkits)\n## 6. Challenges and Future Directions in Direct Speech-to-text Translation\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThe paper concludes that while cascaded ASR+MT systems still achieve the best performance, end-to-end methods are approaching their performance levels, especially with pre-training and multi-task learning. This suggests that a challenge in direct speech-to-text translation remains in closing the performance gap with cascaded systems. Future research directions could focus on further improving end-to-end models to surpass the performance of cascaded models, potentially through advancements in model architectures, training techniques, and utilization of larger and higher-quality datasets. The paper also highlights the Augmented LibriSpeech corpus as a challenging benchmark for future end-to-end AST systems, suggesting that improving performance on this corpus is a direction for future work. ['end_to_end_automatic_speech_translation_of_audiobooks']\n--------------------\nPaper bibkey: [bridging_the_modality_gap_for_speech_to_text_translation]\nDigest: \nThe paper identifies the challenge of handling noisy speech as an area needing further research, as end-to-end ST models still lag pipeline systems in robustness to noise, particularly evident in the MuST-C experiments ['bridging_the_modality_gap_for_speech_to_text_translation']. Future directions include exploring more effective knowledge transfer from NMT models to ST models and extending the modality gap bridging approach to other tasks ['bridging_the_modality_gap_for_speech_to_text_translation'].\n--------------------\nPaper bibkey: [investigating_self_supervised_pre_training_for_end_to_end_speech_translation]\nDigest: \nThe paper indicates that self-supervised representations, particularly CPC, significantly improve AST results compared to filter-bank features, especially in low-medium resource conditions (train < 100h) ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. The benefit is attributed to better phone discrimination, source-target alignments, and speaker robustness of self-supervised representations ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation'].  Further research could explore how to effectively leverage self-supervised pre-training in high-resource settings and for more complex translation tasks.\n### 6.1 6.x Ethical and Societal Implications\n## 7. Conclusion\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThis paper demonstrates the feasibility of training compact end-to-end automatic speech translation models, achieving performance close to cascaded ASR+MT systems on both audiobook and synthetic speech corpora.  The study highlights the benefits of pre-training and multi-task learning for end-to-end AST.  The introduction of the Augmented LibriSpeech corpus provides a valuable large-scale resource for benchmarking end-to-end AST systems on real speech. The paper suggests that while cascaded systems currently offer superior performance, end-to-end methods are a promising direction and are rapidly improving. ['end_to_end_automatic_speech_translation_of_audiobooks']\n--------------------\nPaper bibkey: [bridging_the_modality_gap_for_speech_to_text_translation]\nDigest: \nThe paper concludes that the proposed STAST model effectively improves end-to-end ST by bridging the modality gap through encoder decoupling, length shrink mechanism, NMT integration, and cross-modal adaptation ['bridging_the_modality_gap_for_speech_to_text_translation']. The STAST model achieves state-of-the-art results and demonstrates the benefits of addressing modality differences in direct ST ['bridging_the_modality_gap_for_speech_to_text_translation']. The core idea of bridging the representation gap between modalities can be potentially applied to other cross-modal tasks ['bridging_the_modality_gap_for_speech_to_text_translation'].\n--------------------\nPaper bibkey: [investigating_self_supervised_pre_training_for_end_to_end_speech_translation]\nDigest: \nThis paper concludes that self-supervised pre-training using CPC is effective for end-to-end AST, especially in low-resource scenarios, by providing better speech representations ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. Ensembling models trained with both filter-bank and self-supervised features achieves near state-of-the-art performance without ASR pre-training ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']. The study highlights the potential of self-supervised learning to reduce the dependence on labeled data for AST and to improve performance by learning representations robust to speaker variability and better at capturing phonetic information and alignments ['investigating_self_supervised_pre_training_for_end_to_end_speech_translation']."}, {"bibkey": "analyzing_asr_pretraining_for_low_resource_speech_to_text_translation, end_to_end_automatic_speech_translation_of_audiobooks, self_supervised_representations_improve_end_to_end_speech_translation", "content": "#  Recent Advances in Direct Speech-to-text Translation\n## 1. Introduction\nPaper bibkey: [self_supervised_representations_improve_end_to_end_speech_translation]\nDigest: \nDirect speech-to-text translation models offer a simpler and computationally more efficient alternative to traditional cascaded models by preserving more acoustic information and avoiding error propagation from the speech recognition component ['self_supervised_representations_improve_end_to_end_speech_translation']. However, achieving good performance in direct ST systems requires large amounts of annotated data, which are often limited, especially compared to the availability of unlabeled data ['self_supervised_representations_improve_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [analyzing_asr_pretraining_for_low_resource_speech_to_text_translation]\nDigest: \nDirect speech-to-text translation (ST) is gaining attention, especially for low-resource languages and languages without written forms, or endangered languages where translations into high-resource languages are easier to obtain than transcriptions ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. End-to-end approaches are appealing in these scenarios ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation'].  Pre-training techniques are explored to improve low-resource direct ST ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. This survey will focus on monolingual pretraining for low-resource AST and investigate the impact of language relatedness and pretraining data size on AST performance, and the combination of pretraining with data augmentation methods ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation'].\n--------------------\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nDirect speech-to-text translation, also known as end-to-end speech translation (ST), aims to directly translate speech in a source language into text in a target language without relying on intermediate source language transcripts. This approach offers several advantages over traditional cascaded systems, including reduced latency due to the elimination of sequential processing, streamlined pipelines by integrating speech recognition and machine translation into a single model, and mitigation of error propagation from the automatic speech recognition (ASR) component to the machine translation (MT) component. The application of direct ST is particularly relevant in scenarios requiring efficient and compact translation models. This survey will explore the recent advancements in direct speech-to-text translation, focusing on techniques and methodologies, and will consider the contributions of this paper in the context of audiobook translation. ['end_to_end_automatic_speech_translation_of_audiobooks']\n## 2. Background: Evolution of Speech Translation Paradigms\nPaper bibkey: [self_supervised_representations_improve_end_to_end_speech_translation]\nDigest: \nTraditional cascaded speech translation models typically consist of Automatic Speech Recognition (ASR) and Machine Translation (MT) components. End-to-end speech translation models have emerged as a simpler and more efficient approach, potentially reducing error propagation and preserving acoustic information better than cascaded systems ['self_supervised_representations_improve_end_to_end_speech_translation']. End-to-end models aim to directly map speech to text in the target language through joint training, contrasting with the separate training objectives in cascaded models ['self_supervised_representations_improve_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nTraditional speech translation systems typically follow a cascaded approach, which involves two separate stages: first, automatic speech recognition (ASR) to transcribe the source speech into text, and then machine translation (MT) to translate the transcribed text into the target language. These cascaded models, while effective, suffer from limitations such as error propagation, where errors from the ASR component are passed on to the MT component, potentially degrading the overall translation quality. Furthermore, the separate training objectives of ASR and MT components may not be optimally aligned for the speech translation task. In contrast, end-to-end models offer a more direct approach by jointly training a single model to map speech directly to target text, theoretically leading to improved efficiency and potentially better translation quality by avoiding error accumulation and optimizing for the end task. ['end_to_end_automatic_speech_translation_of_audiobooks'] The evolution towards end-to-end models represents a paradigm shift aiming to overcome the inherent limitations of cascaded systems.\n## 3. Techniques for Direct Speech-to-text Translation: Emphasizing Pre-training Strategies and Encoder Design Challenges\nPaper bibkey: [self_supervised_representations_improve_end_to_end_speech_translation]\nDigest: \nThis paper focuses on pre-training strategies, specifically self-supervised learning, to improve direct speech-to-text translation, especially in data-scarce settings ['self_supervised_representations_improve_end_to_end_speech_translation']. The core methodology involves leveraging self-supervised pre-trained speech representations, such as wav2vec, vq-wav2vec, and BERT features, to enhance the performance of end-to-end ST models in both high-resource and low-resource scenarios ['self_supervised_representations_improve_end_to_end_speech_translation']. The experimental results demonstrate that these self-supervised features consistently improve translation performance and exhibit good cross-lingual transferability ['self_supervised_representations_improve_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [analyzing_asr_pretraining_for_low_resource_speech_to_text_translation]\nDigest: \nThis paper investigates pre-training strategies and data augmentation to improve direct ST, focusing on low-resource scenarios ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. It examines the impact of pretraining data size and language relatedness on the performance of direct ST systems, and whether pretraining can be combined with data augmentation to further improve results ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. The core methodology involves pre-training the encoder of an end-to-end ASR model on various datasets and then transferring the encoder parameters to initialize a direct ST model ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. The paper uses Spanish-English parallel data from the Fisher corpus for AST and pre-trains on ASR datasets of varying sizes and language relatedness to Spanish, including AISHELL Chinese corpus and GlobalPhone languages ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. Experimental results show that pretraining on a larger dataset from an unrelated language can be more effective than pretraining on a smaller dataset from a related language ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. The Word Error Rate (WER) of the pretrained ASR model is found to be a better predictor of final AST performance than language relatedness or pretraining data size ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. Data augmentation using speed perturbation is also shown to yield complementary benefits when combined with pretraining ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation'].\n--------------------\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThis paper explores end-to-end speech-to-text translation techniques by focusing on model architecture and training methodologies. The study investigates a scenario where source language transcription is available during training, representing a midway point between scenarios without source transcripts and traditional cascaded approaches. The core methodology involves training encoder-decoder models with attention mechanisms for direct speech-to-text translation. The paper experiments with different model configurations, including end-to-end models trained from scratch, pre-trained models (initialized with ASR and MT models), and multi-task models (jointly trained for AST, ASR, and MT). The datasets used include a synthetic BTEC corpus and a newly augmented LibriSpeech audiobook corpus. Experimental results demonstrate the feasibility of training compact end-to-end ST models, with pre-training and multi-task learning showing improvements in performance. However, the cascaded ASR+MT approach still achieves the best performance in their experiments. The paper also highlights the creation and evaluation of a new audiobook corpus for end-to-end speech translation. ['end_to_end_automatic_speech_translation_of_audiobooks']\n### 3.1 Challenges in ST Encoder Design\n### 3.2 Pre-training Strategies\nPaper bibkey: [self_supervised_representations_improve_end_to_end_speech_translation]\nDigest: \nPre-training strategies, particularly self-supervised learning, are explored in this paper to address data scarcity challenges in direct ST ['self_supervised_representations_improve_end_to_end_speech_translation']. The paper investigates the effectiveness of different self-supervised pre-training methods for speech representations, including wav2vec, vq-wav2vec, and BERT features learned from discretized representations ['self_supervised_representations_improve_end_to_end_speech_translation']. These methods leverage unlabeled speech data to learn representations through tasks like contrastive predictive coding or masked language modeling ['self_supervised_representations_improve_end_to_end_speech_translation']. The transfer learning paradigm is heterogeneous, transferring representations learned from English speech data to speech translation tasks involving other languages ['self_supervised_representations_improve_end_to_end_speech_translation']. The reported performance gains in ST are significant, especially in low-resource scenarios, demonstrating the benefit of self-supervised pre-training ['self_supervised_representations_improve_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [analyzing_asr_pretraining_for_low_resource_speech_to_text_translation]\nDigest: \nThis paper focuses on ASR pre-training strategies for direct ST ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. The pre-training task is ASR, using various datasets including Mandarin Chinese (AISHELL-1) and several GlobalPhone languages (Croatian, Czech, French, Polish, Portuguese, Swedish) ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. The types of pre-training data include transcribed speech from these ASR datasets ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. The model architecture used is an encoder-decoder model with CNN and LSTM layers for both ASR and AST tasks, and only the encoder parameters are transferred from the pretrained ASR model to the AST model ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. This is a homogeneous transfer learning paradigm as it transfers from ASR to ST, both sequence-to-sequence tasks with speech input ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. Performance gains in ST are observed from various ASR pre-training approaches, particularly in the low-resource Spanish-English ST scenario, with BLEU score improvements ranging from 0.2 to 4.3 depending on the pre-training dataset ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. The paper does not explicitly discuss techniques to reuse subnets or ensure consistency in semantic representation and sequence length, or SATE and FATE methods ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation'].\n--------------------\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThe paper investigates pre-training strategies by initializing the encoder and decoder of the end-to-end AST model with pre-trained ASR and MT models, respectively. This pre-training approach leverages existing models trained on related tasks to potentially improve the performance and convergence speed of the direct ST model. The pre-trained model is identical in architecture to the end-to-end model but benefits from informed initialization of its encoder and decoder components. This can be considered a form of heterogeneous transfer learning, where knowledge from ASR and MT tasks is transferred to the ST task. Experimental results on both BTEC and Augmented LibriSpeech datasets show that pre-training leads to improved BLEU scores in AST and faster convergence during training. For instance, on the LibriSpeech dataset, the pre-trained model achieves better performance compared to the end-to-end model trained from scratch and converges much faster. ['end_to_end_automatic_speech_translation_of_audiobooks']\n#### 3.2.1 Adversarial Pre-training with ASR and MT Data\n#### 3.2.2 Self-Supervised Pre-training\nPaper bibkey: [self_supervised_representations_improve_end_to_end_speech_translation]\nDigest: \nThis paper investigates self-supervised pre-training techniques, specifically wav2vec, vq-wav2vec, and BERT features, for direct ST ['self_supervised_representations_improve_end_to_end_speech_translation'].\n- **wav2vec**: This method learns speech representations by predicting future samples through optimizing a contrastive loss, using an encoder and an aggregator network ['self_supervised_representations_improve_end_to_end_speech_translation'].\n- **vq-wav2vec**: Similar to wav2vec, but it includes a quantization module to discretize the encoder's outputs before feeding them to the aggregator network, resulting in discrete tokens ['self_supervised_representations_improve_end_to_end_speech_translation'].\n- **BERT features**: These features are learned on top of the discretized representations from vq-wav2vec, leveraging the pre-trained BERT model for speech tasks ['self_supervised_representations_improve_end_to_end_speech_translation'].\nThese models are pre-trained on unlabeled English speech data from the Librispeech corpus (960h) ['self_supervised_representations_improve_end_to_end_speech_translation']. Experiments show that using these pre-trained features improves ST performance compared to using log-mel filterbank features, particularly in low-resource settings and cross-lingual transfer scenarios ['self_supervised_representations_improve_end_to_end_speech_translation']. For example, wav2vec features brought improvements of 4.28/6.37 BLEU on Fr-En ST on CoVoST/Tatoeba datasets without ASR pre-training ['self_supervised_representations_improve_end_to_end_speech_translation'].\n#### 3.2.3 Curriculum Pre-training\n#### 3.2.4 Multimodal Pre-training\n### 3.3 Data Augmentation Techniques\nPaper bibkey: [self_supervised_representations_improve_end_to_end_speech_translation]\nDigest: \nThe paper employs SpecAugment for both ASR and ST tasks, using the LD policy without time warping ['self_supervised_representations_improve_end_to_end_speech_translation']. When training with learned features, the SpecAugment policy along the frequency dimension is adjusted proportionally to the embedding size, acting as a form of dropout on the input features ['self_supervised_representations_improve_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [analyzing_asr_pretraining_for_low_resource_speech_to_text_translation]\nDigest: \nThe paper explores data augmentation using speed perturbation in direct ST ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. Specifically, Kaldi's 3-way speed perturbation is used, creating versions of the AST data with audio sped up and sped down by factors of 0.9 and 1.1 respectively ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. This technique is used to improve model robustness and performance, especially when combined with pre-training ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. Experiments show that data augmentation with speed perturbation yields complementary benefits to pre-training, further improving AST results ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. For a 20-hour AST system, data augmentation alone improves BLEU score from 10.3 to 13.3 on the test set, and when combined with pre-training on large Chinese ASR data, the BLEU score reaches 15.8 ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. There is no mention of GigaST corpus in this paper.\n### 3.4 Architectural Innovations and Key Components\nPaper bibkey: [self_supervised_representations_improve_end_to_end_speech_translation]\nDigest: \nThe paper utilizes a sequence-to-sequence BiLSTM attention-based encoder-decoder architecture with a 3-layer decoder for both ST and ASR tasks ['self_supervised_representations_improve_end_to_end_speech_translation']. For low-resource ST, a hybrid BERT-backbone architecture is also explored, where a BERT model pre-trained on discretized speech features is reused as the encoder, while keeping the same BiLSTM decoder architecture ['self_supervised_representations_improve_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [analyzing_asr_pretraining_for_low_resource_speech_to_text_translation]\nDigest: \nThe paper uses a standard encoder-decoder architecture adapted from prior work, consisting of CNN layers for feature extraction and LSTM layers for sequence modeling in both encoder and decoder ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. The encoder comprises two CNN layers and three bidirectional LSTM layers, while the decoder includes a 128-dimensional embedding layer, three LSTM layers, and an attention mechanism ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation'].  The specific architecture is illustrated in Figure 1 of the paper ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. The focus of the paper is not on architectural innovations but on pre-training strategies and data augmentation techniques ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. There is no mention of decoupled encoders, shrink mechanisms, neural acoustic feature modeling, or adaptor modules in the context of architectural innovations in this paper.\n--------------------\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThe paper utilizes an encoder-decoder architecture with attention for direct speech-to-text translation. The speech encoder is a hybrid design, combining convolutional layers and bidirectional LSTM layers. It starts with two non-linear tanh layers followed by two convolutional layers with stride, which reduces the time length of the feature sequence by a factor of 4, speeding up training. These convolutional layers are followed by three bidirectional LSTM layers to further encode the speech features. The character-level decoder consists of a conditional LSTM with a global attention mechanism and a dense output layer. The attention mechanism helps the decoder focus on relevant parts of the encoded speech features when generating the target text. The decoder is character-level, outputting French characters. This architecture is designed to directly map speech features to target language characters in an end-to-end fashion. ['end_to_end_automatic_speech_translation_of_audiobooks']\n#### 3.4.1 Addressing the Modality Gap\n#### 3.4.2 Efficient and Compact Models\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThe research aims to develop compact and efficient end-to-end AST models. The model architecture described, with convolutional and LSTM layers, is designed to be relatively compact. The parameter counts for different models are provided, showing that the end-to-end and pre-trained models have fewer parameters compared to the cascaded model. For example, the end-to-end model on LibriSpeech has 9.4 million parameters, while the cascaded model has 6.3 + 15.9 million parameters. The paper demonstrates that these compact end-to-end models can achieve performance close to cascaded models, suggesting efficiency in terms of model size and computational cost. ['end_to_end_automatic_speech_translation_of_audiobooks']\n## 4. Datasets and Evaluation Metrics for Direct Speech-to-text Translation\nPaper bibkey: [self_supervised_representations_improve_end_to_end_speech_translation]\nDigest: \nThe paper uses several datasets for training and evaluating direct ST models ['self_supervised_representations_improve_end_to_end_speech_translation']. For English-to-X ST, the MuST-C dataset, comprising English TED talks translated into 8 languages, is used ['self_supervised_representations_improve_end_to_end_speech_translation']. For X-to-English ST, the CoVoST dataset, covering 11 languages translated to English with crowd-sourced speech, is employed ['self_supervised_representations_improve_end_to_end_speech_translation']. English ASR pre-training uses data from the Common Voice dataset ['self_supervised_representations_improve_end_to_end_speech_translation']. Evaluation is performed on the CoVoST test set and, when available, the Tatoeba test set ['self_supervised_representations_improve_end_to_end_speech_translation']. Word Error Rate (WER) is used for ASR evaluation, and BLEU score (case-insensitive and tokenized) is used for ST evaluation ['self_supervised_representations_improve_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [analyzing_asr_pretraining_for_low_resource_speech_to_text_translation]\nDigest: \nThe paper uses several datasets for training and evaluating direct ST and ASR models ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. For direct ST, it uses the Fisher corpus Spanish-English parallel data, downsampled to 20 hours to simulate low-resource settings, with dev and test sets each comprising 4.5 hours of speech ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. For ASR pretraining, it employs the AISHELL-1 corpus of Mandarin Chinese (150 hours and a 20-hour subset) and GlobalPhone corpora for seven languages (around 20 hours each: Chinese, Croatian, Czech, French, Polish, Portuguese, Swedish) ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. Dataset characteristics include size (hours of speech), language pairs (Spanish-English for ST), language domain (telephone speech for Fisher, read speech for GlobalPhone and AISHELL) ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. Evaluation metrics used are Word Error Rate (WER) for ASR performance and 4-gram BLEU score for AST performance, calculated on four reference translations ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. The paper does not focus on dataset creation methodologies or the role of specific corpora like MuST-C or GigaST, or IWSLT evaluation campaigns in detail.\n--------------------\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThe paper introduces a newly created dataset, Augmented LibriSpeech, for end-to-end speech translation. This corpus is derived from the LibriSpeech ASR corpus and augmented with French translations. The augmentation process involves automatically aligning French e-books with English utterances from LibriSpeech. For each utterance, the corpus provides English speech signal, English transcription, and two French translations: one from e-book alignment and another from Google Translate of English transcripts. The corpus comprises 236 hours of English speech aligned with French text, split into train, development, and test sets. The paper uses BLEU score as the primary evaluation metric for machine translation and speech translation tasks, and Word Error Rate (WER) for the ASR task. These metrics are standard in the respective fields and allow for comparison with other systems. ['end_to_end_automatic_speech_translation_of_audiobooks']\n### 4.1 Multilingual Speech Translation Corpora\nPaper bibkey: [self_supervised_representations_improve_end_to_end_speech_translation]\nDigest: \nThe paper utilizes the MuST-C dataset, which contains audio recordings of English TED Talks translated into 8 languages, with sentence-level aligned transcriptions and translations ['self_supervised_representations_improve_end_to_end_speech_translation']. The CoVoST dataset, another multilingual ST dataset, is used for X-to-English ST experiments, featuring 11 source languages and English as the target, with crowd-sourced speech on diverse topics ['self_supervised_representations_improve_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThe Augmented LibriSpeech corpus is a multilingual speech translation corpus, specifically for English to French. It provides a large-scale resource (236 hours) for training and evaluating English-French speech translation systems. The creation methodology involves leveraging the existing LibriSpeech corpus and aligning it with French e-books, making it a relatively cost-effective approach to create a large parallel speech-text corpus. The corpus is divided into different subsets (train, dev, test) with varying levels of alignment quality, with the test set being manually verified for alignment accuracy. ['end_to_end_automatic_speech_translation_of_audiobooks']\n### 4.2 Large-scale Pseudo Speech Translation Corpora\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThe Augmented LibriSpeech corpus can be considered a pseudo speech translation corpus due to the automatic alignment process used to create the parallel data. While the test set is manually checked, the training and development sets contain automatically aligned data, which may include some level of noise or inaccuracies in the alignment. The corpus is large-scale (236 hours), making it suitable for training deep learning models for speech translation. The use of automatically aligned e-books as translations can be seen as a form of pseudo-data generation, as it may not perfectly reflect human-quality translations but provides a substantial amount of training data. ['end_to_end_automatic_speech_translation_of_audiobooks']\n### 4.3 Benchmarking and Evaluation Campaigns (IWSLT)\n## 5. Toolkits and Frameworks for Direct Speech-to-text Translation\nPaper bibkey: [self_supervised_representations_improve_end_to_end_speech_translation]\nDigest: \nExperiments in this paper are conducted using the fairseq framework ['self_supervised_representations_improve_end_to_end_speech_translation']. Training and inference are implemented within this framework, which supports sequence-to-sequence models and provides necessary tools for ASR and ST tasks ['self_supervised_representations_improve_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThe models in this paper are implemented using TensorFlow as part of the LIG-CRIStAL NMT toolkit. This indicates the use of an open-source framework for developing the end-to-end speech translation system. TensorFlow is a widely used deep learning framework that provides the necessary tools and functionalities for building and training neural network models. The LIG-CRIStAL NMT toolkit suggests a specific in-house toolkit built upon TensorFlow, potentially offering modularity and specific functionalities tailored for neural machine translation and related tasks like speech translation. ['end_to_end_automatic_speech_translation_of_audiobooks']\n### 5.1 Open-Source Toolkits vs. Commercial Toolkits for ST\n### 5.2 Feature Comparison of Popular ST Toolkits (e.g., ESPnet-ST, Fairseq-ST)\n### 5.3 Toolkits for Specific Tasks in Direct ST (e.g., Low-Resource ST Toolkits)\n## 6. Challenges and Future Directions in Direct Speech-to-text Translation\nPaper bibkey: [self_supervised_representations_improve_end_to_end_speech_translation]\nDigest: \nThe paper points out that further research could investigate the robustness of pre-trained features under different data conditions and explore multilingual settings to enhance cross-lingual transferability and broaden the applicability of self-supervised pre-training in speech translation ['self_supervised_representations_improve_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [analyzing_asr_pretraining_for_low_resource_speech_to_text_translation]\nDigest: \nThe paper identifies limitations in understanding the factors influencing pretraining effectiveness in low-resource AST ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. Open challenges include further investigation into why WER of the pretrained ASR model is a strong predictor of AST performance and how to leverage this finding to choose optimal pretraining data and methods ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. Potential future research directions suggested by the paper include exploring different configurations of multilingual pretraining to potentially outperform monolingual pretraining ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. Further analysis of neural network representations to understand how pretraining and fine-tuning affect the encoding of phonetic and semantic information is also suggested ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. The paper does not explicitly mention ethical and societal implications.\n--------------------\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThe paper identifies that while end-to-end models achieve performance close to cascaded models, cascaded ASR+MT systems still yield the best results in their experiments. This suggests that challenges remain in fully bridging the performance gap between end-to-end and cascaded approaches for speech translation. Future research directions could focus on further improving end-to-end model architectures, exploring more effective pre-training strategies, and leveraging larger and higher-quality datasets. Specifically, enhancing the ability of end-to-end models to capture long-range dependencies and handle the modality gap between speech and text could be crucial. The paper also highlights the Augmented LibriSpeech corpus as a valuable resource for future benchmarking and development of end-to-end AST systems, encouraging further research to challenge their established baselines on this corpus. ['end_to_end_automatic_speech_translation_of_audiobooks']\n### 6.1 6.x Ethical and Societal Implications\n## 7. Conclusion\nPaper bibkey: [self_supervised_representations_improve_end_to_end_speech_translation]\nDigest: \nThis paper concludes that self-supervised representations, such as wav2vec, vq-wav2vec, and BERT features, effectively improve the performance of direct speech translation systems ['self_supervised_representations_improve_end_to_end_speech_translation']. These pre-trained features exhibit good transferability to other languages and can be effectively combined with ASR pre-training to further enhance performance, especially in low-resource scenarios ['self_supervised_representations_improve_end_to_end_speech_translation']. Fine-tuning pre-trained features on target languages or leveraging multilingual ASR pre-training can further improve cross-lingual transfer ['self_supervised_representations_improve_end_to_end_speech_translation']. The main contribution is demonstrating the consistent benefits of self-supervised pre-trained speech representations for ST and providing insights into effective transfer learning strategies ['self_supervised_representations_improve_end_to_end_speech_translation'].\n--------------------\nPaper bibkey: [analyzing_asr_pretraining_for_low_resource_speech_to_text_translation]\nDigest: \nKey takeaways from this paper include that the WER of the pretrained ASR model is a better predictor of AST performance than language relatedness or pretraining data size, and data augmentation with speed perturbation provides additional benefits when combined with ASR pretraining for low-resource AST ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. The main contribution is the finding that minimizing WER in ASR pretraining is crucial for maximizing gains in downstream AST, and the demonstration of complementary benefits from pretraining and data augmentation ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation']. The overall outlook suggests that focusing on improving ASR pretraining and combining it with data augmentation are promising directions for advancing low-resource direct ST ['analyzing_asr_pretraining_for_low_resource_speech_to_text_translation'].\n--------------------\nPaper bibkey: [end_to_end_automatic_speech_translation_of_audiobooks]\nDigest: \nThis paper concludes that it is feasible to train compact end-to-end automatic speech translation models, achieving performance close to cascaded ASR+MT systems, especially with pre-training and multi-task learning. The study introduces and evaluates a new audiobook corpus, Augmented LibriSpeech, for benchmarking end-to-end AST systems. The findings suggest that while end-to-end models are promising, cascaded models currently still offer superior performance. The paper contributes a detailed analysis of end-to-end AST on a real speech corpus and provides baseline results on the Augmented LibriSpeech dataset, encouraging future research to build upon these findings and further advance the field of direct speech-to-text translation. ['end_to_end_automatic_speech_translation_of_audiobooks']"}, {"bibkey": "must_c_a_multilingual_speech_translation_corpus, pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation", "content": "#  Recent Advances in Direct Speech-to-text Translation\n## 1. Introduction\nPaper bibkey: [pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation]\nDigest: \nDirect speech-to-text translation (ST) is motivated by applications in low-resource languages, such as language documentation and crisis relief, where traditional cascaded systems relying on ASR and MT components are often hindered by the lack of transcribed source audio and parallel text data ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. End-to-end ST offers advantages by directly mapping speech to text, streamlining the pipeline and potentially reducing error propagation inherent in cascaded systems ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. This paper investigates leveraging high-resource ASR data to improve low-resource direct ST, particularly focusing on pre-training strategies ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation'].\n--------------------\nPaper bibkey: [must_c_a_multilingual_speech_translation_corpus]\nDigest: \nThe development of neural end-to-end approaches in spoken language translation (SLT), similar to automatic speech recognition (ASR) and machine translation (MT), is hindered by the limited availability of large, publicly accessible training corpora ['must_c_a_multilingual_speech_translation_corpus']. This data scarcity impedes the progress and widespread adoption of end-to-end SLT models, despite their potential advantages ['must_c_a_multilingual_speech_translation_corpus']. While cascaded ASR+MT systems can leverage abundant task-specific data, end-to-end SLT models face challenges due to the lack of sizeable training datasets ['must_c_a_multilingual_speech_translation_corpus']. This corpus scarcity is a significant obstacle in the field of direct speech-to-text translation, limiting the ability to train robust and generalizable neural models.\n## 2. Background: Evolution of Speech Translation Paradigms\nPaper bibkey: [pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation]\nDigest: \nTraditional speech translation systems typically follow a cascaded approach, involving automatic speech recognition (ASR) to transcribe the source speech, followed by machine translation (MT) to translate the transcription into the target language ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. This pipeline suffers from error propagation, where errors from the ASR component are passed on to the MT component, potentially degrading the overall translation quality ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. Furthermore, training ASR and MT components separately requires substantial resources like transcribed source audio and parallel text, which are often scarce for low-resource languages ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. End-to-end ST models offer a potential solution by jointly training a single model to directly translate speech into text, theoretically mitigating error propagation and enabling training with only speech-text parallel data, which may be more readily available in low-resource scenarios ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation'].\n--------------------\nPaper bibkey: [must_c_a_multilingual_speech_translation_corpus]\nDigest: \nTraditional cascaded speech translation systems rely on separate ASR and MT components, each trained independently with task-specific data ['must_c_a_multilingual_speech_translation_corpus'].  These systems, while benefiting from abundant ASR and MT data, suffer from error propagation between stages and are not jointly optimized for the speech translation task ['must_c_a_multilingual_speech_translation_corpus']. In contrast, end-to-end models theoretically offer advantages by directly mapping speech to text in a single model, allowing for joint training and potentially mitigating error propagation ['must_c_a_multilingual_speech_translation_corpus']. However, the practical realization of end-to-end SLT is challenged by the scarcity of large-scale speech-to-text translation corpora, unlike the data-rich environments of ASR and MT ['must_c_a_multilingual_speech_translation_corpus'].\n## 3. Techniques for Direct Speech-to-text Translation: Emphasizing Pre-training Strategies and Encoder Design Challenges\nPaper bibkey: [pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation]\nDigest: \nThis paper focuses on pre-training strategies to improve direct speech-to-text translation, especially in low-resource settings ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. The core technique investigated is pre-training the ST model on a high-resource automatic speech recognition (ASR) task and then fine-tuning it for the ST task ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. The datasets used include Switchboard for English ASR (300 hours), GlobalPhone for French ASR (20 hours), Fisher Spanish corpus for Spanish-English ST (up to 160 hours, with subsets for low-resource simulation), and Mboshi-French parallel corpus for Mboshi-French ST (4 hours) ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. Experiments demonstrate significant BLEU score improvements in low-resource Spanish-English ST (from 10.8 to 20.2 with 20 hours data, pre-trained on 300h English ASR) and Mboshi-French ST (from 3.5 to 7.1 with 4 hours data, pre-trained on English and French ASR) ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. The pre-training strategy is heterogeneous when the ASR language is different from the ST source language (e.g., French ASR for Spanish-English ST), and can be considered homogeneous when the ASR target language is the same as the ST target language (e.g., English ASR for Spanish-English ST, sharing target language English) ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation'].\n### 3.1 Challenges in ST Encoder Design\nPaper bibkey: [pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation]\nDigest: \nThe paper implicitly addresses the challenge of acoustic variability in ST encoder design.  The authors hypothesize that pre-training the encoder, even on a different language, helps the model better normalize over acoustic variability such as speaker and channel differences ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. This suggests that learning robust acoustic representations that are invariant to these variabilities is a key difficulty in ST encoder design, especially in low-resource scenarios where the model may struggle to learn these representations from limited data ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation'].\n### 3.2 Pre-training Strategies\nPaper bibkey: [pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation]\nDigest: \nThis paper primarily investigates ASR pre-training as a strategy to improve direct ST, particularly in low-resource scenarios ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. The pre-training task is ASR, using high-resource transcribed speech data in languages like English and French ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. The pre-training data consists of transcribed speech from corpora like Switchboard (300 hours English) and GlobalPhone (20 hours French) ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. The model architecture is an encoder-decoder with attention, used for both ASR pre-training and ST fine-tuning, allowing for parameter transfer ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. The transfer learning methodology involves initializing the ST model with parameters pre-trained on the ASR task, followed by fine-tuning on ST data ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. The performance gains are significant, especially in low-resource settings, with BLEU score improvements of up to 9 points for Spanish-English ST and nearly doubling the BLEU score for Mboshi-French ST ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. The paper explores both homogeneous (English ASR for Spanish-English ST) and heterogeneous (French ASR for Spanish-English ST) transfer learning paradigms, finding that both are effective, but homogeneous transfer may have slight additional benefits due to potential code-switching effects ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. Ablation studies reveal that transferring encoder parameters contributes most significantly to the performance gains, suggesting that pre-training primarily helps in learning better acoustic representations ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation'].\n#### 3.2.1 Adversarial Pre-training with ASR and MT Data\n#### 3.2.2 Self-Supervised Pre-training\n#### 3.2.3 Curriculum Pre-training\n#### 3.2.4 Multimodal Pre-training\n### 3.3 Data Augmentation Techniques\nPaper bibkey: [pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation]\nDigest: \nThe paper employs data augmentation techniques during training, including adding Gaussian noise (standard deviation 0.25) to MFCC features and dropping frames with a probability of 0.10 ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. These techniques are used to improve model robustness and generalization ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation'].\n### 3.4 Architectural Innovations and Key Components\nPaper bibkey: [pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation]\nDigest: \nThe paper uses a standard encoder-decoder architecture with attention, adapted from prior work in ST and ASR ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. The encoder consists of CNN layers followed by bidirectional LSTM layers to process speech features, and the decoder uses LSTM layers with attention to generate text ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation'].  The attention mechanism is a global attentional model with a general score function and input feeding ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. No novel architectural innovations are explicitly proposed in this paper; the focus is on the pre-training strategy applied to this architecture ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation'].\n#### 3.4.1 Addressing the Modality Gap\n#### 3.4.2 Efficient and Compact Models\n## 4. Datasets and Evaluation Metrics for Direct Speech-to-text Translation\nPaper bibkey: [pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation]\nDigest: \nThe paper utilizes several datasets for training and evaluating direct ST models. For Spanish-English ST, the Fisher Spanish speech corpus is used, simulating low-resource conditions by using subsets of 2.5h to 50h, along with a development and test set each around 4.5 hours ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. For Mboshi-French ST, the Mboshi-French parallel corpus is used, which is a truly low-resource corpus of around 4 hours ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. For ASR pre-training, the Switchboard Telephone speech corpus (300h English) and the French speech corpus from the GlobalPhone collection (20h French) are used ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. Evaluation metrics include BLEU score and METEOR (word-level unigram precision and recall) ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. BLEU is the primary metric, while METEOR is used to provide a more nuanced evaluation in low-resource settings where BLEU can be less informative ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation'].\n--------------------\nPaper bibkey: [must_c_a_multilingual_speech_translation_corpus]\nDigest: \nDatasets play a crucial role in advancing direct speech-to-text translation, particularly for data-hungry neural models ['must_c_a_multilingual_speech_translation_corpus']. The availability of large, high-quality datasets is essential for training robust and generalizable end-to-end SLT systems ['must_c_a_multilingual_speech_translation_corpus'].  The characteristics of these datasets, including size, language pairs, and domain, directly influence the performance and applicability of trained models ['must_c_a_multilingual_speech_translation_corpus']. Evaluation metrics, such as BLEU and WER, are used to assess the quality of MT, SLT and ASR models respectively ['must_c_a_multilingual_speech_translation_corpus'].\n### 4.1 Multilingual Speech Translation Corpora\nPaper bibkey: [must_c_a_multilingual_speech_translation_corpus]\nDigest: \nMuST-C is a multilingual speech translation corpus designed to address the scarcity of resources for training end-to-end SLT models ['must_c_a_multilingual_speech_translation_corpus']. It provides English to 8 target languages: German, Spanish, French, Italian, Dutch, Portuguese, Romanian, and Russian ['must_c_a_multilingual_speech_translation_corpus']. Each language pair in MuST-C contains at least 385 hours of audio recordings from English TED Talks, sentence-level aligned with manual transcriptions and translations ['must_c_a_multilingual_speech_translation_corpus']. The corpus creation methodology involves downloading TED Talks videos and HTML files, aligning transcriptions and translations using Gargantua, and then aligning English text with audio using Gentle forced-aligner ['must_c_a_multilingual_speech_translation_corpus']. Quality verification includes filtering out noisy segments and talks based on word alignment rates ['must_c_a_multilingual_speech_translation_corpus']. MuST-C is significantly larger than existing publicly available SLT corpora, aiming to facilitate the training of data-intensive neural SLT models ['must_c_a_multilingual_speech_translation_corpus']. Table 2 in the paper provides detailed statistics for each language section, including the number of talks, sentences, hours of speech, and word counts ['must_c_a_multilingual_speech_translation_corpus'].\n### 4.2 Large-scale Pseudo Speech Translation Corpora\n### 4.3 Benchmarking and Evaluation Campaigns (IWSLT)\nPaper bibkey: [must_c_a_multilingual_speech_translation_corpus]\nDigest: \nThe paper compares MuST-C to the IWSLT18 English-German corpus, which is also derived from TED Talks but created using a different alignment methodology based on time information from SRT files ['must_c_a_multilingual_speech_translation_corpus']. Experiment 1 in the paper uses the IWSLT18 corpus for comparison to empirically assess the quality of MuST-C ['must_c_a_multilingual_speech_translation_corpus']. The results show that models trained on MuST-C outperform those trained on IWSLT18 in ASR, MT, and SLT tasks, indicating a higher quality of alignments in MuST-C ['must_c_a_multilingual_speech_translation_corpus'].\n## 5. Toolkits and Frameworks for Direct Speech-to-text Translation\nPaper bibkey: [pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation]\nDigest: \nThe experiments in this paper are implemented using the Chainer toolkit ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. The authors mention that their code will be made freely available ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation'].\n### 5.1 Open-Source Toolkits vs. Commercial Toolkits for ST\n### 5.2 Feature Comparison of Popular ST Toolkits (e.g., ESPnet-ST, Fairseq-ST)\n### 5.3 Toolkits for Specific Tasks in Direct ST (e.g., Low-Resource ST Toolkits)\n## 6. Challenges and Future Directions in Direct Speech-to-text Translation\nPaper bibkey: [pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation]\nDigest: \nThe paper highlights the challenge of low-resource speech-to-text translation and demonstrates that pre-training on high-resource ASR data is an effective approach to address this challenge ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. Future directions suggested by the authors include exploring multilingual ASR pre-training, using pre-trained multilingual speech features, and incorporating pre-trained language models for the decoder through techniques like language model fusion ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. They also suggest further investigation into speech features targeted for low-resource multispeaker settings ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation'].\n--------------------\nPaper bibkey: [must_c_a_multilingual_speech_translation_corpus]\nDigest: \nA major challenge in direct speech-to-text translation is the scarcity of large, high-quality training corpora, which hinders the development and application of end-to-end neural models ['must_c_a_multilingual_speech_translation_corpus']. The creation of resources like MuST-C addresses this challenge by providing a substantial multilingual corpus for SLT research ['must_c_a_multilingual_speech_translation_corpus']. Future directions include expanding the language coverage and size of MuST-C, leveraging the scalable corpus creation procedure described in the paper to incorporate more TED Talks data and potentially new languages ['must_c_a_multilingual_speech_translation_corpus'].\n### 6.1 6.x Ethical and Societal Implications\n## 7. Conclusion\nPaper bibkey: [pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation]\nDigest: \nThis paper concludes that pre-training end-to-end speech translation systems on high-resource ASR data significantly improves performance, especially for low-resource languages ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation'].  A key takeaway is that transferring encoder parameters, which learn acoustic representations, is particularly beneficial ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. The study shows substantial BLEU improvements in both Spanish-English and Mboshi-French low-resource ST tasks through ASR pre-training, demonstrating the effectiveness and flexibility of this approach, even when the ASR language is different from the ST source and target languages ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation']. The outlook is positive for leveraging readily available high-resource ASR data to advance low-resource speech translation ['pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation'].\n--------------------\nPaper bibkey: [must_c_a_multilingual_speech_translation_corpus]\nDigest: \nMuST-C is presented as a significant contribution to the field of speech translation by providing a large, high-quality multilingual corpus to facilitate the training of data-hungry neural SLT models ['must_c_a_multilingual_speech_translation_corpus'].  It is currently the largest publicly available corpus of its kind, featuring at least 385 hours of transcribed and translated speech per language across 8 target languages ['must_c_a_multilingual_speech_translation_corpus']. The corpus creation methodology is scalable, allowing for future expansions in language coverage and data volume, promising to further support advancements in direct speech-to-text translation research ['must_c_a_multilingual_speech_translation_corpus']."}, {"bibkey": "gigast_a_10000_hour_pseudo_speech_translation_corpus", "content": "#  Recent Advances in Direct Speech-to-text Translation\n## 1. Introduction\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nDirect speech-to-text translation (ST) is defined as directly translating speech in the source language into sentences in the target language without generating source language transcription. A typical baseline model is speech-transformer, which is similar to the MT model Transformer but with a pre-processing down-sampling module for speech signals. Training high-quality end-to-end ST models requires large datasets. Compared to MT, ST datasets like MuST-C En-De (234k samples, 408 hours) are much smaller than MT parallel text data (e.g., >4M for En-De). This paper explores the impact of increasing ST training data size to the scale of MT data on translation performance. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n## 2. Background: Evolution of Speech Translation Paradigms\n## 3. Techniques for Direct Speech-to-text Translation: Emphasizing Pre-training Strategies and Encoder Design Challenges\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThis paper evaluates the performance of Speech-Transformer and SSL-Transformer models for direct ST. SSL-Transformer incorporates pre-trained speech encoders like Wav2vec2 and HuBERT, replacing the Fbank features used in Speech-Transformer. Increasing training data size with GigaST improves BLEU scores for both model types. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n### 3.1 Challenges in ST Encoder Design\n### 3.2 Pre-training Strategies\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThis paper explores the use of pre-trained speech encoders (Wav2vec2 and HuBERT) in SSL-Transformer models for direct ST. These pre-trained encoders are used as feature extractors, replacing Fbank features in the Speech-Transformer model. The SSL-Transformer models are fine-tuned on ST data, with the pre-trained speech encoders also being updated during training. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n#### 3.2.1 Adversarial Pre-training with ASR and MT Data\n#### 3.2.2 Self-Supervised Pre-training\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThe paper utilizes self-supervised pre-trained models Wav2vec2 and HuBERT as speech encoders in the SSL-Transformer architecture for direct ST. These models are pre-trained using self-supervised learning techniques on large amounts of unlabeled speech data and have shown strong performance on speech recognition tasks. In this work, they are adapted for ST by fine-tuning them as part of the end-to-end ST model. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n#### 3.2.3 Curriculum Pre-training\n#### 3.2.4 Multimodal Pre-training\n### 3.3 Data Augmentation Techniques\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThe paper introduces GigaST, a large-scale pseudo speech-to-text translation corpus created by translating the transcript of the GigaSpeech ASR corpus. GigaST is up to 25 times larger than existing open-source datasets like MuST-C. The training set translations are generated using a strong MT system, while test set translations are manually annotated. This large pseudo ST corpus serves as a form of data augmentation, significantly increasing the training data size for direct ST models. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n### 3.4 Architectural Innovations and Key Components\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThe paper uses Speech-Transformer as a benchmark model, which consists of CNN layers for down-sampling and a Transformer encoder-decoder module. SSL-Transformer replaces the Fbank feature input of Speech-Transformer with representations from pre-trained speech encoders (Wav2vec2, HuBERT) followed by convolutional subsamplers and a Transformer encoder-decoder. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n#### 3.4.1 Addressing the Modality Gap\n#### 3.4.2 Efficient and Compact Models\n## 4. Datasets and Evaluation Metrics for Direct Speech-to-text Translation\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThis paper introduces GigaST, a large-scale pseudo ST corpus in English-German (En-De) and English-Chinese (En-Zh) directions. The corpus is derived from the GigaSpeech ASR corpus (10,000 hours of English speech). The training set is translated using MT, and the test set is translated by humans. The paper also uses MuST-C En-De dataset and evaluates on GigaST test sets and an in-house En-Zh test set and MuST-C tst-COMMON set for En-De. BLEU score is used as the evaluation metric. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n### 4.1 Multilingual Speech Translation Corpora\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThe paper uses MuST-C English-German dataset in experiments, which is a widely used benchmark for multilingual speech translation. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n### 4.2 Large-scale Pseudo Speech Translation Corpora\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nGigaST is a large-scale pseudo ST corpus created by translating the transcript of GigaSpeech. The training set translations are generated by a strong Transformer-based MT model trained on WMT2021 and OPUS data. Human evaluation shows that the MT-generated translations are of good quality, especially for En-De, approaching human translation quality. The test sets are created by human translators. GigaST En-De and En-Zh training sets are significantly larger than MuST-C, providing up to 10,000 hours of pseudo ST data. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n### 4.3 Benchmarking and Evaluation Campaigns (IWSLT)\n## 5. Toolkits and Frameworks for Direct Speech-to-text Translation\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThe models in this paper are implemented using NeurST toolkit. The training scripts are also released to facilitate replication. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n### 5.1 Open-Source Toolkits vs. Commercial Toolkits for ST\n### 5.2 Feature Comparison of Popular ST Toolkits (e.g., ESPnet-ST, Fairseq-ST)\n### 5.3 Toolkits for Specific Tasks in Direct ST (e.g., Low-Resource ST Toolkits)\n## 6. Challenges and Future Directions in Direct Speech-to-text Translation\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThe paper points out that cascade systems with a powerful MT model can still outperform end-to-end models, suggesting that there is room to improve end-to-end ST performance. Future work could explore pre-training of the decoder and multitask training to bridge this gap. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']\n### 6.1 6.x Ethical and Societal Implications\n## 7. Conclusion\nPaper bibkey: [gigast_a_10000_hour_pseudo_speech_translation_corpus]\nDigest: \nThis paper introduces GigaST, a large-scale pseudo ST corpus, and demonstrates that training ST models with this corpus improves performance, achieving state-of-the-art results on MuST-C En-De. The release of GigaST dataset and training scripts is expected to facilitate further research in speech translation. The paper highlights the benefit of large-scale training data and pre-trained speech encoders for direct ST. ['gigast_a_10000_hour_pseudo_speech_translation_corpus']"}, {"bibkey": "findings_of_the_iwslt_2022_evaluation_campaign", "content": "#  Recent Advances in Direct Speech-to-text Translation\n## 1. Introduction\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nThe 19th International Conference on Spoken Language Translation (IWSLT) 2022 evaluation campaign featured eight shared tasks related to spoken language translation, addressing scientific challenges in the field ['findings_of_the_iwslt_2022_evaluation_campaign']. This campaign highlights the ongoing research and development in spoken language translation, a field that aims to bridge the gap between spoken languages ['findings_of_the_iwslt_2022_evaluation_campaign']. The tasks included simultaneous speech translation and offline speech translation, which are relevant to direct speech-to-text translation, as they involve translating speech directly into text without relying on intermediate representations like source language transcripts in the traditional cascaded approach ['findings_of_the_iwslt_2022_evaluation_campaign'].\n## 2. Background: Evolution of Speech Translation Paradigms\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nThe IWSLT evaluation campaign, as reported in this paper, serves as a platform to compare different paradigms in speech translation, particularly cascaded and end-to-end approaches ['findings_of_the_iwslt_2022_evaluation_campaign'].  The offline speech translation task explicitly aimed to compare cascaded systems (integrating ASR and MT components) and end-to-end models (single neural networks directly translating audio to target text) ['findings_of_the_iwslt_2022_evaluation_campaign']. The campaign acknowledges the evolution towards end-to-end models in recent years, noting their ability to reduce the performance gap with cascaded approaches ['findings_of_the_iwslt_2022_evaluation_campaign']. This reflects the broader trend in speech translation research to move from traditional pipelines to more integrated, direct models.\n## 3. Techniques for Direct Speech-to-text Translation: Emphasizing Pre-training Strategies and Encoder Design Challenges\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nThe IWSLT 2022 Evaluation Campaign showcased various techniques employed by participating teams to advance direct speech-to-text translation, particularly in the offline and simultaneous speech translation tasks ['findings_of_the_iwslt_2022_evaluation_campaign']. Pre-training strategies and architectural innovations were prominent themes in the submitted systems. Many teams leveraged pre-trained models, both for speech and text, to initialize and enhance their speech translation systems ['findings_of_the_iwslt_2022_evaluation_campaign']. Data augmentation and ensemble methods were also commonly used to improve the robustness and performance of the models ['findings_of_the_iwslt_2022_evaluation_campaign'].\n### 3.1 Challenges in ST Encoder Design\n### 3.2 Pre-training Strategies\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nPre-training was a significant focus in the IWSLT 2022 submissions, with many teams utilizing pre-trained models to boost performance in direct ST ['findings_of_the_iwslt_2022_evaluation_campaign']. The campaign highlighted the use of various pre-trained models, including Wav2vec 2.0 and HuBERT as pre-trained audio encoders, and mBART, mBART50, M2M100, Delta LM, and T5 as pre-trained text decoders ['findings_of_the_iwslt_2022_evaluation_campaign']. These pre-trained models were often integrated into both cascaded and end-to-end ST systems to improve translation quality, especially in scenarios with limited speech translation data ['findings_of_the_iwslt_2022_evaluation_campaign']. The integration of these models suggests a trend towards leveraging transfer learning from related tasks like ASR and MT to enhance direct ST.\n#### 3.2.1 Adversarial Pre-training with ASR and MT Data\n#### 3.2.2 Self-Supervised Pre-training\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nSelf-supervised pre-training techniques were represented by the use of models like Wav2vec 2.0 and HuBERT in several submissions to the IWSLT 2022 campaign ['findings_of_the_iwslt_2022_evaluation_campaign']. For example, CUNI-KIT leveraged a pre-trained wav2vec 2.0 encoder in their end-to-end simultaneous speech-to-text system ['findings_of_the_iwslt_2022_evaluation_campaign']. Similarly, UPC used wav2vec 2.0 and HuBERT as pre-trained speech encoders for their end-to-end offline speech translation system ['findings_of_the_iwslt_2022_evaluation_campaign']. These models, pre-trained on large amounts of unlabeled speech data, serve as effective feature extractors, improving the performance of ST models, particularly in low-resource scenarios by leveraging untranscribed speech data ['findings_of_the_iwslt_2022_evaluation_campaign'].\n#### 3.2.3 Curriculum Pre-training\n#### 3.2.4 Multimodal Pre-training\n### 3.3 Data Augmentation Techniques\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nData augmentation techniques were employed by several teams in the IWSLT 2022 campaign to enhance the training data and improve model robustness ['findings_of_the_iwslt_2022_evaluation_campaign']. XIAOMI used data augmentation methods including tagged backtranslation, knowledge distillation, and iterative backtranslation in their text-to-text simultaneous translation system ['findings_of_the_iwslt_2022_evaluation_campaign']. AISP-SJTU also utilized data augmentation methods like knowledge distillation and tagged backtranslation for their English-Mandarin simultaneous translation system ['findings_of_the_iwslt_2022_evaluation_campaign']. Furthermore, ON-TRAC employed speed perturbation and SpecAugment for training their end-to-end dialect speech translation system ['findings_of_the_iwslt_2022_evaluation_campaign']. These techniques help to address data scarcity and improve the generalization of direct ST models.\n### 3.4 Architectural Innovations and Key Components\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nThe IWSLT 2022 campaign highlighted various architectural innovations and key components in direct ST systems. Conformer architecture was adopted by FBK in their end-to-end speech translation system, aiming to reduce computational requirements ['findings_of_the_iwslt_2022_evaluation_campaign']. Transformer-based architectures were widely used, often in combination with other architectures like Conformer, as seen in submissions from USTC-NELSLIP and XIAOMI ['findings_of_the_iwslt_2022_evaluation_campaign'].  MLLP-VRAIN adopted a cascaded approach with a chunking-based DNN-HMM ASR model and a multi-path wait-k transformer-based MT model for simultaneous ST ['findings_of_the_iwslt_2022_evaluation_campaign']. CMU explored the MultiDecoder architecture for dialect ST, enhancing it with hierarchical speech encoder and joint CTC/Attention ST decoding ['findings_of_the_iwslt_2022_evaluation_campaign']. These innovations reflect ongoing efforts to improve the efficiency, accuracy, and robustness of direct ST models.\n#### 3.4.1 Addressing the Modality Gap\n#### 3.4.2 Efficient and Compact Models\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nFBK's submission to the simultaneous speech translation task focused on reducing computation requirements, aiming to democratize participation in the task ['findings_of_the_iwslt_2022_evaluation_campaign']. They demonstrated methods to avoid ASR encoder pre-training and used a Conformer architecture with a CTC loss to improve efficiency ['findings_of_the_iwslt_2022_evaluation_campaign']. This work contributes to the development of more efficient and accessible direct ST models.\n## 4. Datasets and Evaluation Metrics for Direct Speech-to-text Translation\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nThe IWSLT 2022 Evaluation Campaign utilized a range of datasets for training and evaluating direct ST models across different tasks ['findings_of_the_iwslt_2022_evaluation_campaign']. For offline and simultaneous speech translation tasks, the MuST-C V2 corpus was used for English-German, English-Chinese, and English-Japanese language pairs ['findings_of_the_iwslt_2022_evaluation_campaign'].  Additional allowed training corpora included MuST-C V1, CoVoST, WIT3, Speech-Translation TED corpus, How2, LibriVoxDeEn, Europarl-ST, TED LIUM v2 and v3, WMT, OpenSubtitles 2018, Augmented LibriSpeech, Mozilla Common Voice, LibriSpeech ASR corpus, and VoxPopuli ['findings_of_the_iwslt_2022_evaluation_campaign']. The evaluation metrics primarily used were BLEU for translation quality and latency metrics like Average Proportion (AP), Average Lagging (AL), and Differentiable Average Lagging (DAL) for simultaneous translation ['findings_of_the_iwslt_2022_evaluation_campaign']. Human evaluation, using MQM and Direct Assessment, was also conducted to complement automatic metrics and provide a more comprehensive assessment of translation quality ['findings_of_the_iwslt_2022_evaluation_campaign'].\n### 4.1 Multilingual Speech Translation Corpora\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nThe MuST-C corpus, particularly MuST-C V2, was a central multilingual speech translation corpus used in the IWSLT 2022 Offline Speech Translation task, extending to language directions like English-Chinese and English-Japanese, in addition to the traditional English-German ['findings_of_the_iwslt_2022_evaluation_campaign']. MuST-C V2 provided training, development, and test sets for these language pairs, maintaining a consistent structure across languages, facilitating multilingual speech translation research ['findings_of_the_iwslt_2022_evaluation_campaign'].\n### 4.2 Large-scale Pseudo Speech Translation Corpora\n### 4.3 Benchmarking and Evaluation Campaigns (IWSLT)\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nThe IWSLT evaluation campaign itself serves as a major benchmarking effort for speech translation technologies, including direct ST ['findings_of_the_iwslt_2022_evaluation_campaign']. The 2022 campaign featured shared tasks on simultaneous, offline, speech-to-speech, low-resource, multilingual, dialect, formality control, and isometric speech translation ['findings_of_the_iwslt_2022_evaluation_campaign']. The campaign used specific datasets for each task, such as TED talks for offline ST and newly created datasets for low-resource and dialect ST, and employed metrics like BLEU, chrF, latency metrics, and human evaluations to assess system performance ['findings_of_the_iwslt_2022_evaluation_campaign']. Key findings included progress in offline ST, challenges in low-resource and dialect ST, and initial explorations in speech-to-speech and isometric ST ['findings_of_the_iwslt_2022_evaluation_campaign'].\n## 5. Toolkits and Frameworks for Direct Speech-to-text Translation\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nThe IWSLT 2022 Evaluation Campaign saw the use of several toolkits and frameworks for developing direct ST systems. ESPnet was used by teams like CMU and ON-TRAC for their submissions, particularly for tasks like dialect ST and end-to-end ST ['findings_of_the_iwslt_2022_evaluation_campaign']. Fairseq was also a popular framework, used by JHU and ON-TRAC, especially for MT components and end-to-end ST models ['findings_of_the_iwslt_2022_evaluation_campaign'].  The SIMULEvAL toolkit was specifically used for evaluating simultaneous speech translation systems, providing metrics for both translation quality and latency ['findings_of_the_iwslt_2022_evaluation_campaign']. SpeechBrain was mentioned as the toolkit used by ON-TRAC for their ASR component in the dialect ST task ['findings_of_the_iwslt_2022_evaluation_campaign']. Marian-NMT was used by NUV for their isometric ST system ['findings_of_the_iwslt_2022_evaluation_campaign'].\n### 5.1 Open-Source Toolkits vs. Commercial Toolkits for ST\n### 5.2 Feature Comparison of Popular ST Toolkits (e.g., ESPnet-ST, Fairseq-ST)\n### 5.3 Toolkits for Specific Tasks in Direct ST (e.g., Low-Resource ST Toolkits)\n## 6. Challenges and Future Directions in Direct Speech-to-text Translation\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nThe IWSLT 2022 Evaluation Campaign highlighted several ongoing challenges and future directions in direct speech-to-text translation research. In simultaneous speech translation, future directions include improving latency metrics to support long unsegmented input and extending the task to support speech output, potentially consolidating the task to speech-to-text only ['findings_of_the_iwslt_2022_evaluation_campaign']. For offline speech translation, further investigation into the balance between cascaded and end-to-end approaches is needed, as is continued exploration of pre-trained models' role in improving translation quality ['findings_of_the_iwslt_2022_evaluation_campaign']. Low-resource and dialect speech translation remain extremely challenging, requiring more effective methods for leveraging limited data and transferring knowledge from related languages or dialects ['findings_of_the_iwslt_2022_evaluation_campaign'].\n### 6.1 6.x Ethical and Societal Implications\n## 7. Conclusion\nPaper bibkey: [findings_of_the_iwslt_2022_evaluation_campaign]\nDigest: \nThe IWSLT 2022 Evaluation Campaign showcased significant advances in direct speech-to-text translation and related areas, as evidenced by the participation of 27 teams across eight shared tasks ['findings_of_the_iwslt_2022_evaluation_campaign']. Key takeaways include the continued progress in offline ST, the increasing importance of pre-trained models, and the ongoing challenges in low-resource and dialectal speech translation ['findings_of_the_iwslt_2022_evaluation_campaign']. The campaign also highlighted the first explorations into speech-to-speech and isometric translation, paving the way for future research directions and innovations in the field of spoken language translation ['findings_of_the_iwslt_2022_evaluation_campaign']. The overall outlook for direct ST is positive, with continuous improvements in translation quality and efficiency being driven by advancements in techniques and resources."}, {"bibkey": "espnet_st_all_in_one_speech_translation_toolkit", "content": "#  Recent Advances in Direct Speech-to-text Translation\n## 1. Introduction\nPaper bibkey: [espnet_st_all_in_one_speech_translation_toolkit]\nDigest: \nDirect speech-to-text translation (ST) converts speech signals in one language directly into text in another language, offering advantages over traditional cascaded systems. These advantages include reduced latency, streamlined pipelines, and mitigation of error propagation, making it suitable for time-critical applications like simultaneous interpretation. End-to-end ST models directly map speech to text, contrasting with cascaded approaches that combine automatic speech recognition (ASR) and machine translation (MT) modules. The development of unified toolkits is crucial for advancing research in both end-to-end and cascaded ST approaches. This survey aims to explore recent advancements in direct ST, focusing on toolkits like ESPnet-ST that facilitate research in this field. ['espnet_st_all_in_one_speech_translation_toolkit']\n## 2. Background: Evolution of Speech Translation Paradigms\nPaper bibkey: [espnet_st_all_in_one_speech_translation_toolkit]\nDigest: \nTraditional speech translation systems have historically relied on cascaded models, involving separate ASR and MT modules. These Cascade-ST systems, while effective, suffer from error propagation between modules and increased latency. In contrast, end-to-end speech translation (E2E-ST) has emerged as a promising alternative, directly mapping speech from a source language to text in a target language using a single sequence-to-sequence model. This approach theoretically and practically offers advantages like reduced latency and joint training, mitigating error propagation inherent in cascaded systems. The evolution towards E2E-ST is driven by the convergence of sequence-to-sequence models in ASR and MT, paving the way for unified models for direct speech translation. ['espnet_st_all_in_one_speech_translation_toolkit']\n## 3. Techniques for Direct Speech-to-text Translation: Emphasizing Pre-training Strategies and Encoder Design Challenges\nPaper bibkey: [espnet_st_all_in_one_speech_translation_toolkit]\nDigest: \nThe paper does not explicitly categorize techniques for direct ST into pre-training strategies, data augmentation, and architectural innovations in a dedicated section. However, it implicitly touches upon data augmentation and architectural aspects within the context of the ESPnet-ST toolkit and the example models it supports. The paper emphasizes the importance of multi-task learning and transfer learning as techniques to improve E2E-ST performance. Pre-training using ASR and MT tasks is highlighted as a method to initialize E2E-ST models, demonstrating a transfer learning paradigm. Data augmentation techniques like speed perturbation and SpecAugment are mentioned as methods implemented in ESPnet-ST to enhance model robustness. Architectural innovations are represented by the Transformer-based models used within ESPnet-ST for ASR, MT, and E2E-ST components. The experimental results across various datasets showcase the effectiveness of these techniques and the toolkit in achieving state-of-the-art or competitive performance in direct ST. ['espnet_st_all_in_one_speech_translation_toolkit']\n### 3.1 Challenges in ST Encoder Design\n### 3.2 Pre-training Strategies\nPaper bibkey: [espnet_st_all_in_one_speech_translation_toolkit]\nDigest: \nPre-training is a crucial technique mentioned in the paper to improve the performance of E2E-ST systems. The paper highlights transfer learning from ASR and MT tasks as a valuable pre-training strategy. In ESPnet-ST, the parameters of the ST encoder can be initialized with a pre-trained ASR encoder, and the ST decoder can be initialized with a pre-trained MT decoder. This transfer learning approach leverages knowledge learned from related tasks (ASR and MT) to enhance the performance of the ST model, especially in scenarios where ST data might be limited. The paper demonstrates this strategy by initializing E2E-ST models with pre-trained ASR and MT components within the ESPnet-ST toolkit and showing performance improvements in experiments. This represents a heterogeneous transfer learning paradigm, transferring knowledge from different but related tasks (ASR and MT) to the target ST task. ['espnet_st_all_in_one_speech_translation_toolkit']\n#### 3.2.1 Adversarial Pre-training with ASR and MT Data\n#### 3.2.2 Self-Supervised Pre-training\n#### 3.2.3 Curriculum Pre-training\n#### 3.2.4 Multimodal Pre-training\n### 3.3 Data Augmentation Techniques\nPaper bibkey: [espnet_st_all_in_one_speech_translation_toolkit]\nDigest: \nESPnet-ST incorporates speech data augmentation techniques to improve the robustness of ST models. Specifically, the paper mentions two techniques implemented within the toolkit: speed perturbation and SpecAugment. Speed perturbation involves augmenting speech data by altering the speed with factors of 0.9, 1.0, and 1.1, effectively tripling the data size. This technique is found to be important for stabilizing E2E-ST training. SpecAugment, which applies time and frequency masking blocks to log mel-filterbank features, is also implemented. Originally proposed for ASR, SpecAugment has been shown to be effective for E2E-ST as well. These data augmentation techniques within ESPnet-ST are aimed at addressing data scarcity and improving the generalization of direct ST models. ['espnet_st_all_in_one_speech_translation_toolkit']\n### 3.4 Architectural Innovations and Key Components\nPaper bibkey: [espnet_st_all_in_one_speech_translation_toolkit]\nDigest: \nESPnet-ST utilizes Transformer-based architectures for its ASR, MT, and E2E-ST components, representing a significant architectural choice. The toolkit employs a Transformer-based hybrid CTC/attention framework for ASR, known for its effectiveness. For MT and E2E-ST, Transformer architectures with self-attention blocks are used for both encoder and decoder. A key aspect of ESPnet-ST's design is the unified definition of parameter names across ASR, MT, and ST components. This deliberate design choice facilitates easy parameter sharing and transfer learning between pre-trained ASR and MT models and E2E-ST models. The toolkit's architecture, based on Transformers and a unified framework, contributes to its flexibility and performance in direct ST. ['espnet_st_all_in_one_speech_translation_toolkit']\n#### 3.4.1 Addressing the Modality Gap\n#### 3.4.2 Efficient and Compact Models\n## 4. Datasets and Evaluation Metrics for Direct Speech-to-text Translation\nPaper bibkey: [espnet_st_all_in_one_speech_translation_toolkit]\nDigest: \nThe paper evaluates ESPnet-ST on several benchmark speech translation corpora, showcasing datasets used for training and evaluating direct ST models. These datasets include Fisher-CallHome Spanish (Es->En), Libri-trans (En->Fr), How2 (En->Pt), and Must-C (En -> 8 languages). These corpora vary in size, language pairs, and domain (e.g., conversational telephone speech, read speech, YouTube videos, TED talks). The paper details the size and characteristics of some datasets, such as Fisher-CallHome containing 170 hours of Spanish conversational speech and Libri-trans containing 236 hours of English read speech.  For evaluation, the paper primarily uses the BLEU (Bilingual Evaluation Understudy) metric, reporting 4-gram BLEU scores. It mentions using the multi-bleu.perl script in Moses for BLEU calculation. The datasets and BLEU metric are crucial for benchmarking and comparing the performance of direct ST models, as demonstrated in the experimental section of the paper. ['espnet_st_all_in_one_speech_translation_toolkit']\n### 4.1 Multilingual Speech Translation Corpora\nPaper bibkey: [espnet_st_all_in_one_speech_translation_toolkit]\nDigest: \nThe paper utilizes the Must-C corpus, a multilingual speech translation corpus derived from TED talks, which supports translation from English to 8 target languages (De, Pt, Fr, Es, Ro, Ru, Nl, It).  The corpus is used to evaluate the multilingual capabilities of ESPnet-ST. While the paper does not detail the creation methodology or quality verification for Must-C, it uses this corpus to demonstrate the toolkit's performance across multiple language pairs, indicating its importance in multilingual ST research.  The size of the corpus and specific details about language coverage and alignment process are not elaborated upon in this paper. ['espnet_st_all_in_one_speech_translation_toolkit']\n### 4.2 Large-scale Pseudo Speech Translation Corpora\nPaper bibkey: [espnet_st_all_in_one_speech_translation_toolkit]\nDigest: \nThe paper mentions augmenting the Libri-trans corpus by adding translation references using Google Translate. While not explicitly termed \"pseudo ST corpora,\" this implicitly acknowledges the use of machine translation to enhance training data.  The paper does not focus on large-scale pseudo ST corpora like GigaST in detail, but the use of Google Translate to augment Libri-trans suggests an awareness of leveraging MT for data augmentation in ST, a concept related to pseudo-data generation.  Details about creation processes, source ASR corpora, or quality verification of large-scale pseudo ST corpora are not provided in this paper. ['espnet_st_all_in_one_speech_translation_toolkit']\n### 4.3 Benchmarking and Evaluation Campaigns (IWSLT)\nPaper bibkey: [espnet_st_all_in_one_speech_translation_toolkit]\nDigest: \nThe paper includes an MT experiment using the IWSLT 2016 En-De dataset to validate the performance of ESPnet-ST's MT modules. While not directly focusing on speech translation benchmarking in IWSLT, the paper uses the IWSLT dataset as a benchmark for MT performance, which is relevant to cascaded ST systems and the MT component within ESPnet-ST. It compares ESPnet-ST's MT performance to Fairseq on IWSLT, highlighting the toolkit's competitive performance in machine translation tasks within a standard benchmarking campaign dataset. The paper doesn't detail shared tasks, evaluation metrics, or key findings from IWSLT specifically related to ST, but uses IWSLT for MT evaluation as a related task. ['espnet_st_all_in_one_speech_translation_toolkit']\n## 5. Toolkits and Frameworks for Direct Speech-to-text Translation\nPaper bibkey: [espnet_st_all_in_one_speech_translation_toolkit]\nDigest: \nThis paper is primarily focused on introducing ESPnet-ST, an open-source toolkit designed for direct speech-to-text translation. ESPnet-ST is presented as an all-in-one toolkit that integrates ASR, MT, and TTS functionalities within the broader ESPnet framework, specifically tailored for ST tasks. The paper details its functionalities, ease of use, modularity, and support for various tasks including E2E-ST and Cascade-ST. It emphasizes the toolkit's recipes for reproducible experiments, stage-by-stage processing, and features like multi-task learning and transfer learning. The availability of pre-trained models and example scripts for various ST corpora further underscores ESPnet-ST as a significant toolkit for direct ST development. The paper positions ESPnet-ST as a tool to facilitate research and development in both end-to-end and cascaded ST approaches. ['espnet_st_all_in_one_speech_translation_toolkit']\n### 5.1 Open-Source Toolkits vs. Commercial Toolkits for ST\nPaper bibkey: [espnet_st_all_in_one_speech_translation_toolkit]\nDigest: \nThe paper implicitly positions ESPnet-ST as an open-source toolkit by highlighting its public availability on GitHub and its nature as a community-driven project within the ESPnet framework. While it doesn't explicitly compare open-source and commercial toolkits, the paper's emphasis on open-source availability, customizability, and community support inherent in ESPnet suggests the advantages of open-source solutions for ST research. The paper does not discuss commercial toolkits directly, but by presenting ESPnet-ST as a freely available and customizable option, it implicitly advocates for the benefits of open-source toolkits in the field. ['espnet_st_all_in_one_speech_translation_toolkit']\n### 5.2 Feature Comparison of Popular ST Toolkits (e.g., ESPnet-ST, Fairseq-ST)\nPaper bibkey: [espnet_st_all_in_one_speech_translation_toolkit]\nDigest: \nThe paper includes a table (Table 1) comparing ESPnet-ST with other frameworks like Lingvo, OpenSeq2Seq, NeMo, RETURNN, SLT.KIT, and Fairseq based on supported tasks (ASR, LM, E2E-ST, Cascade-ST, MT, TTS) and the availability of example recipes and pre-trained models.  This table serves as a feature comparison, highlighting ESPnet-ST's unique offering of integrating ASR, MT, TTS, and ST within a single toolkit with comprehensive recipes and pre-trained models. While it includes Fairseq in the comparison table, the feature comparison is not deeply detailed for each toolkit but rather provides a high-level overview of task support and resource availability. The comparison emphasizes ESPnet-ST's all-in-one nature and broad task coverage relative to other toolkits in the landscape. ['espnet_st_all_in_one_speech_translation_toolkit']\n### 5.3 Toolkits for Specific Tasks in Direct ST (e.g., Low-Resource ST Toolkits)\nPaper bibkey: [espnet_st_all_in_one_speech_translation_toolkit]\nDigest: \nThe paper does not specifically focus on toolkits designed or optimized for particular tasks like low-resource ST, multilingual ST, or real-time ST in a dedicated section. However, it implicitly addresses multilingual ST by demonstrating ESPnet-ST's performance on the multilingual Must-C corpus.  The paper showcases the toolkit's capabilities across multiple language pairs, suggesting its potential for multilingual ST.  While not explicitly categorized as a \"multilingual ST toolkit,\" ESPnet-ST's features and experimental results indicate its applicability to multilingual scenarios. Specific toolkits tailored for low-resource or real-time ST are not discussed in detail in this paper. ['espnet_st_all_in_one_speech_translation_toolkit']\n## 6. Challenges and Future Directions in Direct Speech-to-text Translation\nPaper bibkey: [espnet_st_all_in_one_speech_translation_toolkit]\nDigest: \nThe paper concludes by mentioning future directions for ESPnet-ST, indicating broader challenges in the field.  It states the intention to support more corpora and implement novel techniques to bridge the gap between end-to-end and cascaded approaches. This implicitly acknowledges the ongoing challenge of achieving consistent superiority of E2E-ST over Cascade-ST and suggests future research directions aimed at improving E2E-ST to match or surpass cascaded systems in translation quality across various conditions. The paper's future direction of bridging the gap between E2E and cascaded approaches points to a key unresolved issue in direct ST research: optimizing E2E-ST to fully realize its theoretical advantages in practical scenarios, especially in terms of translation quality compared to well-established cascaded systems. ['espnet_st_all_in_one_speech_translation_toolkit']\n### 6.1 6.x Ethical and Societal Implications\n## 7. Conclusion\nPaper bibkey: [espnet_st_all_in_one_speech_translation_toolkit]\nDigest: \nESPnet-ST is presented as an all-in-one toolkit designed to accelerate the development of both end-to-end and cascaded speech translation systems. Key advances highlighted are the integration of ASR, MT, TTS, and ST recipes and models within a single codebase, the provision of recipes for various ST corpora, and the open-sourcing of the toolkit and pre-trained models. The toolkit aims to lower the barrier for researchers to enter and contribute to ST research by offering a unified and customizable framework. The overall outlook suggests that ESPnet-ST will contribute to future advancements in direct ST by facilitating experimentation and implementation of novel techniques, particularly in bridging the performance gap between end-to-end and cascaded approaches. ['espnet_st_all_in_one_speech_translation_toolkit']"}, {"bibkey": "statistical_phrase_based_speech_translation", "content": "#  Recent Advances in Direct Speech-to-text Translation\n## 1. Introduction\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nThis paper introduces a generative statistical model for speech-to-text translation, extending phrase-based text translation models to handle speech input. The motivation is to improve upon pipeline approaches (ASR + SMT) by tightly coupling ASR and SMT to leverage information from multiple ASR hypotheses, aiming for better translation quality by searching a larger space of possibilities than just the single best ASR output. The paper focuses on developing a system that translates ASR word lattices, which contain richer information than N-best lists, using phrase-based translation techniques. The scope is limited to Mandarin Broadcast News to English translation. ['statistical_phrase_based_speech_translation']\n## 2. Background: Evolution of Speech Translation Paradigms\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nThe paper discusses the limitations of the 'pipeline' approach where ASR transcription is treated as standard text for SMT. It highlights that in imperfect systems like ASR, it's crucial to pass on as much information as possible to subsequent stages. Tightly coupled architectures, translating ASR N-best lists or word lattices, are presented as advancements to address this limitation by allowing the SMT system to consider multiple ASR hypotheses. Lattice-based translation is theoretically advantageous due to larger search spaces and detailed sub-sentential information, but realizing these gains practically has been challenging. This work builds upon prior efforts to translate lattices and confusion networks to improve translation quality. ['statistical_phrase_based_speech_translation']\n## 3. Techniques for Direct Speech-to-text Translation: Emphasizing Pre-training Strategies and Encoder Design Challenges\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nThis paper proposes a novel approach to statistical phrase-based speech translation based on a generative, source-channel model, drawing inspiration from HMM-based ASR. The core technique involves extending phrase-based models used in text translation to incorporate acoustic models from a target language ASR system. The approach translates speech by mapping ASR word lattices to lattices of phrase sequences, which are then translated using text translation operations. This method aims to directly process the uncertainty in speech recognition within the translation process. ['statistical_phrase_based_speech_translation']\n### 3.1 Challenges in ST Encoder Design\n### 3.2 Pre-training Strategies\n#### 3.2.1 Adversarial Pre-training with ASR and MT Data\n#### 3.2.2 Self-Supervised Pre-training\n#### 3.2.3 Curriculum Pre-training\n#### 3.2.4 Multimodal Pre-training\n### 3.3 Data Augmentation Techniques\n### 3.4 Architectural Innovations and Key Components\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nThe key architectural innovation is the adaptation of the Translation Template Model (TTM), a phrase-based generative model, to process ASR word lattices. The process involves several steps formulated as Weighted Finite State Machines (WFSM): Source Language Model (G), Source Phrase Segmentation (W), Phrase Translation (R), Phrase Movement and Insertion ($\\Phi$), and Target Phrase Segmentation ($\\Omega$).  The translation process is described as a series of WFSM compositions acting on a phrase lattice.  For speech translation, the input is an ASR word lattice acceptor ($\\mathcal{L}$), which is transformed into a phrase lattice (Q) by applying the Target Phrase Segmentation transducer ($\\Omega \\circ \\mathcal{L}$). The final translation lattice ($\\mathcal{T}$) is generated by composing $G \\circ W \\circ R \\circ \\Phi \\circ Q$. The translation is then extracted as the minimum cost path through $\\mathcal{T}$.  This approach effectively integrates ASR acoustic models $P(A|t_{1}^{J})$ within the translation model $\\bar{P}(A,t_{1}^{J},v_{1}^{\\bar{R}},x_{1}^{\\bar{K}},u_{1}^{K},s_{1}^{I}) = P(A|t_{1}^{J})\\,P(t_{1}^{J}|v_{1}^{R})\\,P(v_{1}^{R}|x_{1}^{K},u_{1}^{K})\\,P(x_{1}^{K}|u_{1}^{K})\\,P(u_{1}^{K}|s_{1}^{I})\\,P(s_{1}^{I})$. ['statistical_phrase_based_speech_translation']\n#### 3.4.1 Addressing the Modality Gap\n#### 3.4.2 Efficient and Compact Models\n## 4. Datasets and Evaluation Metrics for Direct Speech-to-text Translation\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nThe system is evaluated on the TC-STAR Chinese to English Broadcast News translation task, using the 2005 evaluation set. The dataset consists of six Mandarin news broadcasts, manually transcribed and translated into English. The development and evaluation sets contain 525 and 494 sentences, respectively.  Mandarin ASR lattices, generated by the LIMSI Mandarin Broadcast News System, are also used. The ASR Character Error Rate (CER) on the Dev set is reported as 8.7%. Translation performance is measured using the BLEU metric. Document-level BLEU (dBLEU) is primarily reported for speech translation experiments due to segmentation differences between ASR and manual transcriptions. ['statistical_phrase_based_speech_translation']\n### 4.1 Multilingual Speech Translation Corpora\n### 4.2 Large-scale Pseudo Speech Translation Corpora\n### 4.3 Benchmarking and Evaluation Campaigns (IWSLT)\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nThe paper mentions the TC-STAR evaluation campaign as the context for their experiments in 2005. ['statistical_phrase_based_speech_translation']\n## 5. Toolkits and Frameworks for Direct Speech-to-text Translation\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nThe translation experiments are based on the TTM phrase-based SMT system developed by JHU/CU. The system utilizes WFSM operations for translation, implemented using the GRM Library.  Specific tools like `grmcount` from the GRM Library are used for extracting phrases from word lattices. For re-casing, the SRILM disambig tool is mentioned. ['statistical_phrase_based_speech_translation']\n### 5.1 Open-Source Toolkits vs. Commercial Toolkits for ST\n### 5.2 Feature Comparison of Popular ST Toolkits (e.g., ESPnet-ST, Fairseq-ST)\n### 5.3 Toolkits for Specific Tasks in Direct ST (e.g., Low-Resource ST Toolkits)\n## 6. Challenges and Future Directions in Direct Speech-to-text Translation\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nThe paper identifies weaknesses in their initial formulation and implementation, particularly in handling disfluent ASR-hypothesized phrases and the mismatch in tokenization and word segmentation between the ASR system and SMT bitext. Future directions include improving phrase extraction from word lattices, incorporating metadata extraction for better phrase and sentence boundary detection, and integrated development of ASR and SMT systems with consistent text formatting to address tokenization mismatches.  The paper also notes the need for different modeling procedures to handle phrases from ASR systems, which are often disfluent and less likely to be in training bitext. ['statistical_phrase_based_speech_translation']\n### 6.1 6.x Ethical and Societal Implications\n## 7. Conclusion\nPaper bibkey: [statistical_phrase_based_speech_translation]\nDigest: \nThe paper concludes by summarizing the development of a modeling framework for statistical speech-to-text translation as an extension of phrase-based TTM text translation. It highlights the tight coupling of ASR and SMT subsystems in their approach, both in model formulation and WFSM implementation. The feasibility of the approach is demonstrated through Mandarin-to-English Broadcast News translation experiments, with anticipated further improvements through integrated development of ASR and SMT components.  The main contribution is the formulation and initial implementation of a lattice-based phrase translation system, showing improvements over 1-best ASR translation. The outlook is positive, expecting further gains from addressing identified weaknesses. ['statistical_phrase_based_speech_translation']"}], "papers": [{"title": "Speech translation: coupling of recognition and translation", "authors": null, "bibkey": "speech_translation_coupling_of_recognition_and_translation", "bibitem": "@article{Ney_IEEE1999,\n  title = {Speech translation: coupling of recognition and translation}\n}", "url": "", "latex_url": null, "latex_path": null, "pdf_url": "https://sci.bban.top/pdf/10.1109/icassp.1999.758176.pdf", "pdf_path": "output/download_papers/icassp.1999.758176/icassp.1999.758176.pdf", "md_url": null, "latex_length": 0, "latex": "", "abstract": "In speech translation, we are faced with the problem of how to couple the speech recognition process and the translation process. Starting from the Bayes decision rule for speech translation, we analyze how the interaction between the recognition process and the translation process can be modelled. In the light of this decision rule, we discuss the already existing approaches to speech translation. None of the existing approaches seems to have addressed this direct interaction. We suggest two new methods, the local averaging approximation and the monotone alignments. $P r(e_{1}^{I})$ is the language model of the target language, whereas $P r(f_{1}^{J}|e_{1}^{I})$ is the string translation model. The argmax operation denotes the search problem, i. e. the generation of the output sentence in the target language. The overall architecture of the statistical translation approach is summarized in Fig. 1. Here, we have assumed suitable transformation steps [2, 6] and a decomposition of the string translation model into alignment models and lezical models (see Section 3.1). The notational convention will be as follows.We use thesymbol $\\pmb{P r}(.)$ to denote general probability distributions with (nearly) no specific assumptions. In contrast, for model-based probability distributions, we use the generic symbol $p(.)$", "abstract_length": 1331, "abstract_token": 275, "introduction": "Recently, the statistical approach to text translation has been adopted by a number of research groups [1, 2, 3, 6, 8, 13]. In addition, the translation approach has been extended by using speech input rather than text input [8, 10]. In this case of speech translation, however, there are two processes to be considered: speech recognition of the source language and translation into the target language. Therefore the question arises: What is the correct decision rule for speech translation? Often, speech translation is simply implemented as a sequential operation by first performing speech recognition and then translation of the recognized text. But then, there is the question of how recognition errors should be handled by the translation process. Ultimately, the problem boils down to the question of how to arrive at a suitable interaction of the recognition process and the translation process. In this paper, we will attempt to derive a suitable decision rule for speech translation and to present suitable implementations.", "introduction_length": 1035, "introduction_token": 194, "reference": "# REFERENCES  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname{arg\\,max}_{\\epsilon_{1}^{I}}P r(e_{1}^{I}|x_{1}^{T})\\;=}}\\\\ &{=}&{\\arg\\operatorname*{max}_{\\epsilon_{1}^{I}}\\Big\\{P r(e_{1}^{I})\\,\\cdot\\,\\operatorname*{max}_{\\bar{f}_{1}^{I},\\,a_{1}^{I}}\\prod_{j}\\Big[p(a_{j}|a_{j-1},I)\\,\\cdot}\\\\ &{}&{\\qquad\\qquad\\qquad\\quad\\cdot p(f_{j}|f_{j-1},e_{a_{j}})\\cdot p(x_{j}|f_{j})\\Big]\\Big\\}}\\\\ &{=}&{\\arg\\operatorname*{max}_{\\epsilon_{1}^{I}}\\operatorname*{max}_{\\bar{f}_{1}^{I},\\delta_{1}^{I}}\\prod_{j}\\big[p(\\delta_{j})\\cdot p_{\\delta_{j}}(e_{j}|e_{j-1})\\,\\cdot}\\\\ &{}&{\\qquad\\qquad\\quad\\cdot p(f_{j}|f_{j-1},e_{j})\\cdot p(x_{j}|f_{j})\\big]}\\end{array}\n$$  \n\nHere, we have used the same re-formulation as for text input and thus obtain the DP recursion:  \n\n$$\n\\begin{array}{r l}{Q(j,e,f)\\;=\\;p(x_{j}|f)\\;\\cdot\\underset{f^{\\prime}}{\\operatorname*{max}}\\left\\lbrace p(f|f^{\\prime},e)\\;\\cdot\\right.}&{}\\\\ &{\\left.\\quad\\quad\\cdot\\underset{\\delta,e^{\\prime}}{\\operatorname*{max}}\\;\\left\\lbrace p(\\delta)\\cdot p_{\\delta}(e|e^{\\prime})\\cdot Q(j-1,e^{\\prime},f^{\\prime})\\right\\rbrace\\right\\rbrace}\\end{array}\n$$  \n\nFor full search, the computational complexity of this recursion is $J\\cdot{\\dot{E}}^{2}\\cdot F^{2}$ ${\\bar{\\boldsymbol{F}}}=$ vocabulary size of the source language vocabulary). In addition to beam search and accelerated LM recombination, the complexity can be reduced by considering only promising word pairs $(e,f)$  \n\nThe monotonicity requirement can be satisfied by suitably re-ordering the words of either the source or the target language [6]. For speech input, the preferred language is the target language due to the possible errors in recognition and prosodic segmentation. Therefore, re-ordering the target words, maybe in connection with some grammarbased language model, could then be performed as part of the search strategy [9]. As an additional advantage, the monotone DP search could be directly applied to the word graph as provided by the speech recognizer.  \n\n# Acknowledgment  \n\nThe problem studied in this paper was the topic of many discussions with E. Vidal when the author was a visiting scientist at Universidad Politecnica de Valencia, Valencia, Spain, in 1996.  \n\nThis work has been partly carried out in the framework of the Verbmobil project (01 IV 701 T4) funded by the German Federal Ministry of Education, Science, Research and Technology and of the Eutrans project (ESPRIT 30268) funded by the European Community.  \n\n[1] H. Alshawi, F. Xiang: English-to-Mandarin Speech Translation with Head Transducers. Spoken Language Translation Workshop (SLT-97), Madrid, Spain, pp. 54-60, July 1997.   \n[2] A. L. Berger, P. F. Brown et al.: The Candide System for Machine Translation.. ARPA Human Language Technology Workshop, Plainsboro,NJ, Morgan Kaufmann Publishers, San Mateo, CA, pp. 152-157, March 1994.   \n[3] P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mercer: The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, Vol. 19, No. 2, pp. 263-311, 1993.   \n[4] F. J. Och, C. Tillmann: Unpublished Results. Computer Science Department, RWTH Aachen, Germany, July 1998.   \n[5]  N. Reithinger, E. Maier: Using Statistical Dialogue Act Processing in Verbmobil. 33rd Annual Meeting of the Association for Computational Linguistics, Cambridge, MA, pp. 116-121, June 1995.   \n[6] C. Tillmann, S. Vogel, H. Ney, A. Zubiaga: A DP. based Search Using Monotone Alignments in Statistical Translation. 35th Annual Conf. of the Association for Computational Linguistics, Madrid, Spain, pp. 289-296, July 1997.   \n[7] C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, H. Sawaf: Accelerated DP Based Search for Statistical Translation. Fifth European Conf. on Speech Communication and Technology, p.2667-2670, Rhodos, Gre, Sep. 1997.   \n[8] E. Vidal: Finite-State Speech-to-Speech Translation. Int.Conf.onAcoustic Speech and SignalProcessing, Munich, Germany, pp. 1i1-114, April 1997.   \n[9]  J. M. Vilar, E. Vidal, J. C. Amengual: Learning Extended Finite State Models for Language Translation. 12th European Conf. on Artifcial Intelligence, Budapest, Hungary, 1996.   \n[10]  A. Lavie, L. Levin, A. Waibel, D. Gates, M. Gavalda, L. Mayfield: JANUS: Multi-lingual translation of spontaneous speech in a limited domain. 2nd Conf. of the Association for Machine Translation in the Americas pp. 252-255, Montreal, Quebec, Oct. 1995.   \n11  Y-Y. Wang, A. Waibel: Decoding Algorithm in Statistical Translation. 35th Annual Conf. of the Association for Computational Linguistics, Pp. 366-372, Madrid, Spain, July 1997.   \n12] F. Wessel, W. Macherey, R. Schluiter: Using Probabilities as Confidence Measures. Int. Conf. on Acoustics, Speech and Signal Processing, Seattle, WA, pp. 225- 228, May 1998.   \n13] D. Wu: A Polynomial-Time Algorithm for Statistical Machine Translation. 34th Annual Conf. of the Association for Computational Linguistics, pp. 152-158, Santa Cruz, CA, June 1996.", "reference_length": 4951, "reference_token": 1528, "txt_length": 16619, "txt_token": 5031, "txt": "# SPEECH TRANSLATION: COUPLING OF RECOGNITION AND TRANSLATION  \n\nHermann Ney Lehrstuhl fir Informatik VI, RWTH Aachen - University of Technology, D-52056 Aachen, Germany  \n\n# ABSTRACT  \n\nIn speech translation, we are faced with the problem of how to couple the speech recognition process and the translation process. Starting from the Bayes decision rule for speech translation, we analyze how the interaction between the recognition process and the translation process can be modelled. In the light of this decision rule, we discuss the already existing approaches to speech translation. None of the existing approaches seems to have addressed this direct interaction. We suggest two new methods, the local averaging approximation and the monotone alignments.  \n\n$P r(e_{1}^{I})$ is the language model of the target language, whereas $P r(f_{1}^{J}|e_{1}^{I})$ is the string translation model. The argmax operation denotes the search problem, i. e. the generation of the output sentence in the target language. The overall architecture of the statistical translation approach is summarized in Fig. 1. Here, we have assumed suitable transformation steps [2, 6] and a decomposition of the string translation model into alignment models and lezical models (see Section 3.1).  \n\nThe notational convention will be as follows.We use thesymbol $\\pmb{P r}(.)$ to denote general probability distributions with (nearly) no specific assumptions. In contrast, for model-based probability distributions, we use the generic symbol $p(.)$  \n\n# 1.INTRODUCTION  \n\nRecently, the statistical approach to text translation has been adopted by a number of research groups [1, 2, 3, 6, 8, 13]. In addition, the translation approach has been extended by using speech input rather than text input [8, 10]. In this case of speech translation, however, there are two processes to be considered: speech recognition of the source language and translation into the target language.  \n\nTherefore the question arises: What is the correct decision rule for speech translation? Often, speech translation is simply implemented as a sequential operation by first performing speech recognition and then translation of the recognized text. But then, there is the question of how recognition errors should be handled by the translation process. Ultimately, the problem boils down to the question of how to arrive at a suitable interaction of the recognition process and the translation process. In this paper, we will attempt to derive a suitable decision rule for speech translation and to present suitable implementations.  \n\n# 2. BAYES DECISION RULE  \n\n# 2.1. Review: Text Input  \n\nTo pave the ground, we review the Bayes decision rule for text translation. We are given a source ('French') string $f_{1}^{J}\\,=\\,f_{1}...f_{j}...f_{J}$ , which is to be translated into a target ('English') string $e_{1}^{I}=e_{1}...e_{i}...e_{I}$ . In this paper, the term word always refers to a full-form word. Among all possible target strings, we will choose the string with the highest probability which is given by Bayes’ decision rule [3]:  \n\n$$\n\\begin{array}{r c l}{\\hat{e}_{1}^{I}}&{=}&{\\arg\\underset{e_{1}^{I}}{\\operatorname*{max}}\\left\\{P r(e_{1}^{I}|f_{1}^{J})\\right\\}}\\\\ &{=}&{\\arg\\underset{e_{1}^{I}}{\\operatorname*{max}}\\left\\{P r(e_{1}^{I})\\cdot P r(f_{1}^{J}|e_{1}^{I})\\right\\}}\\end{array}\\,.\n$$  \n\n![](images/6d5be82e9f108fd3438697fcf19dd8b3a69ff5766114aab7928caf16563a8543.jpg)  \nFigure 1. Bayes decision rule for text trans  \n\n# 2.2. Speech Input  \n\nThus far we have assumed written input,i.e.perfect input with no errors. When trying to apply the same translation concept to spoken input, we are faced with the additional complication of speech recognition errors. So the question comesupofhowtointegratetheprobabilitiesofthe speech recognition process into the translation process. Although there have been activities in speech translation at several places [1, 8, 10], there has been no work on this question of recognition/translation integration.  \n\nConsidering the problem of speech input rather than text input for translation, we can distinguish three levels, namely the acoustic vectors $\\pmb{x}_{1}^{T}\\:=\\:\\pmb{x}_{1}...\\pmb{\\bar{x}}_{t}...\\pmb{x}_{T}$ overtime $t=1...T$ ,thesourcewords $f_{1}^{J}$ and thetargetwords $e_{1}^{I}$  \n\n$$\nx_{1}^{T}\\rightarrow f_{1}^{J}\\rightarrow e_{1}^{I}\n$$  \n\nFrom a strict point of view, the source words $f_{1}^{J}$ arenotof direct interest for the speech translation task.Mathematically, this is captured by introducing the possible source wordstrings $f_{1}^{J}$ as hidden variables into the Bayes decision rule:  \n\n$$\n\\begin{array}{r l}&{\\underset{t=1}{\\operatorname{arg\\,max}}\\,{\\operatorname*{Pr}}(e_{1}^{t}|\\hat{\\mathbf{z}}|)=}\\\\ {=}&{\\,\\underset{t\\leq1}{\\operatorname*{arg\\,max}}\\,\\Big\\{P r(e_{1}^{t})\\cdot\\,P r(\\alpha_{1}^{t}|e_{1}^{t})\\Big\\}}\\\\ {=}&{\\,\\underset{t\\leq1}{\\operatorname*{arg\\,max}}\\,\\Bigg\\{P r(e_{1}^{t})\\cdot\\sum_{l_{l}^{\\prime}}P r(f_{1}^{t},z_{1}^{t}|e_{1}^{t})\\Bigg\\}}\\\\ {=}&{\\,\\underset{t\\leq1}{\\operatorname*{arg\\,max}}\\,\\Bigg\\{P r(e_{1}^{t})\\cdot\\sum_{l_{l}^{\\prime}}P r(f_{1}^{t}|e_{1}^{t})\\cdot P r(z_{1}^{t}|f_{1}^{t},e_{1}^{t})\\Bigg\\}}\\\\ {=}&{\\,\\underset{t\\leq1}{\\operatorname*{arg\\,max}}\\,\\Bigg\\{P r(e_{1}^{t})\\cdot\\sum_{l_{l}^{\\prime}}P r(f_{1}^{t}|e_{1}^{t})\\cdot P r(z_{1}^{t}|f_{1}^{t},e_{1}^{t})\\Bigg\\}}\\\\ {=}&{\\,\\underset{t\\leq1}{\\operatorname*{arg\\,max}}\\,\\Bigg\\{P r(e_{1}^{t})\\cdot\\sum_{l_{l}^{\\prime}}P r(f_{1}^{t}|e_{1}^{t})\\cdot P r(z_{1}^{t}|f_{1}^{t})\\Bigg\\}}\\\\ {\\approx}&{\\,\\underset{t\\leq1}{\\operatorname*{arg\\,max}}\\,\\Bigg\\{P r(e_{1}^{t})\\cdot\\underset{t=1}{\\operatorname*{max}}\\,\\Big\\{P r(f_{1}^{t}|e_{1}^{t})\\cdot P r(z_{1}^{t}|f_{1}^{t})\\Big\\}\\Bigg\\}}\\end{array}\n$$  \n\nthe target language model $P r(e_{1}^{I})$ is sufficiently strong or, tobemore exact,ifits strength is comparableto that of the sourcelanguagemodel $P r(\\breve{f}_{1}^{J})$ .We mention the following approaches:  \n\n·In many systems,the method of n-best lists is used. The recognizer produces a list of n best source sentences,and the translation system works as a filter that selects one out of the n sentences using some suitable criterion. This joint generation and filtering process can be viewed as a crude approximation of the joint probability $P r(f_{1}^{J},e_{1}^{I})$   \n· When using finite-state methodology rather than a fully stochastic approach, the probability $P r(f_{1}^{J},e_{1}^{I})$ is modelled by the finite-state network of the corresponding transducer, which is typically refined by domain and range restrictions [8].   \n·In the extreme case, we might be only interested in the meaning of the target translation. Such an approach was used in [5] for the Verbmobil task. In Bayes decision rule, this case is captured by putting most emphasis on a semantically constrained language model $\\mathbf{\\dot{\\boldsymbol{P}}}\\mathbf{\\boldsymbol{r}}(e_{1}^{\\boldsymbol{I}})$  \n\nHowever, it is clear that none of these approaches has fully covered the recognition-translation interaction from a statistical point of view.  \n\n# 3.ALIGNMENT AND LEXICON MODELS  \n\nTo convert the Bayes decision rule derived above into a practical algorithm, we have to introduce specific modelling assumptions.  \n\n# 3.1. Alignment Models  \n\nHere, we have made no special modelling assumption, apartfrom thereasonable assumption that  \n\n$$\nP r(x_{1}^{T}|f_{1}^{J},e_{1}^{I})=P r(x_{1}^{T}|f_{1}^{J})\\ ,\n$$  \n\ni. e. the target string $e_{1}^{I}$ does not help to predict the acoustic vectors (in the source language) if the source string $f_{1}^{J}$ is given. In addition, in the last equation, we have used the maximum approximation. Only in that special case of speech translation, at least from a strict point of view, there is the notion of a 'recognized’ source word sequence $f_{1}^{J}$ . However, this word sequence is very much determined by the combination of the language model $P r(e_{1}^{I})$ ofthe target language and the translation model $P r(\\dot{f}_{1}^{\\bar{J}}|e_{1}^{\\bar{I}})$ .In contrast, in recognition, there would be only the language model $P r(f_{1}^{J})$  \n\nIt is instructive to re-interpret already existing approaches for handling speech input in a translation task in the light of the Bayes decision rule for speech translation, even if these approaches are not based on stochastic modelling. The key issue in all these approaches is the question of how the requirement of having both a well-formed source sentence $f_{1}^{J}$ and a well-formed target sentence $e_{1}^{I}$ atthe same time is satisfied. From the statistical point of view, this question is captured by finding suitable models for the jointprobability $\\bar{P}r(f_{1}^{J},e_{1}^{I})=P r(\\Breve{e}_{1}^{I})\\cdot P r(f_{1}^{J}|e_{1}^{I})$  \n\nFrom the decision rule, it is clear that the translation process will have an effect on the recognition process only if  \n\nA key issue in modeling the string translation probability $P r(\\dot{f}_{1}^{J}|e_{1}^{I})$ is the question of how we define the correspondencebetweenthewordsof thetargetsentenceandthe words of the source sentence. To this purpose, alignment models have been introduced [3]. The alignment model used here [6] is similar to the concept of Hidden Markov models (HMM) in speech recognition. The alignment is a mapping $j\\to i=a_{j}$ from source position $_j$ to target position $\\pmb{i}=\\pmb{a}_{j}$ Later, we will limit ourselves to so-called monotone HMM alignments as shown in Fig. 2.  \n\nDenoting the alignment probabilities by $p(a_{j}\\{a_{j-1},I\\}$ and the lexicon probability by $p(f_{j}|e_{i})$ , we re-write the string translation probability:  \n\n$$\n\\begin{array}{r c l}{P r(f_{1}^{J}|e_{1}^{I})}&{=}&{\\displaystyle\\sum_{a_{1}^{J}}\\prod_{j}\\left[p(a_{j}|a_{j-1},I)\\cdot p(f_{j}|e_{a_{j}})\\right]}\\end{array}\n$$  \n\n# 3.2. Speech Input  \n\nTo simplify the Bayes decision rule for speech translation, we will consider two modelling assumptions:  \n\n·Acoustic modelling: We assume that the speech recognizer produces a word graph as output. Each arc of the word graph represents awordhypothesis $f_{j}$ which covers the portion $\\pmb{x}_{j}$ of the acoustic vectors (slightly abusing notation). The acoustic probabilities provided by the speech recognizer aredenotedby $p(x_{j}|f_{j})$ . Thus we have:  \n\n![](images/73cea0b0fbead2124fc9905d7db9466ee4689ab5c5c4ceb3304d29dd9fa12e81.jpg)  \nFigure 2. Illustration of monotone HMM alignments.  \n\n$$\nP r(x_{1}^{T}|f_{1}^{J})\\;\\;\\;=\\;\\;\\;\\prod_{j=1}^{J}p(x_{j}|f_{j})\n$$  \n\nThis assumption is without serious loss of generality.  \n\n· Lexicon modelling:  \n\nWhen presenting the statistical approach to translation, the tacit assumption had been that the source sentence $f_{1}^{J}$ was well formed. However, for speech input, this assumption is no more valid. Therefore, to take into account the requirement of 'well-formedness', we use a more complex translation model by including the dependence on the predecessor word:  \n\n$$\np(f_{j}|f_{j-1},e_{a_{j}})\\quad\\mathrm{in~lieu~of}\\quad p(f_{j}|e_{a_{j}})\n$$  \n\n$$\n{\\cal P}r(f_{1}^{J}|e_{1}^{I})\\;\\;\\;=\\;\\;\\;\\sum_{a_{1}^{J}}\\prod_{j}\\left[p(a_{j}|a_{j-1},I)\\cdot p(f_{j}|f_{j-1},e_{a_{j}})\\right]\\;\n$$  \n\nFor the sake of simplicity, here we do not go beyond the bigram dependence.  \n\n# 4.IMPLEMENTATIONS  \n\nTo reduce the computational complexity, we will present two methods in detail, namely the local averaging approximation and the monotone alignments.  \n\n# 4.1.Local Averaging Approximation  \n\nUsing the above two modelling assumptions, we re-write the term $\\overbrace{P r(x_{1}^{T}|e_{1}^{I})}$ in the Bayes decision rule:  \n\n$$\n\\begin{array}{r l}{\\displaystyle\\sum_{f_{1}^{J}}P r(f_{1}^{J}|e_{1}^{I})\\cdot P r(x_{1}^{T}|f_{1}^{J})\\(=}&{}\\\\ {=}&{\\displaystyle\\sum_{f_{1}^{J}}\\sum_{a_{1}^{J}}\\prod_{j}\\left[p(a_{j}|a_{j-1},I)\\cdot p(f_{j}|f_{j-1},e_{a_{j}})\\cdot p(x_{j}|f_{j})\\right]}\\\\ {\\cong}&{\\displaystyle\\sum_{a_{1}^{J}}\\prod_{j}\\left[p(a_{j}|a_{j-1},I)\\cdot p(x_{j}|e_{a_{j}})\\right]\\quad.}\\end{array}\n$$  \n\nHere, in the last equation, we have used the approximation that the averaging process over $f_{1}^{J}$ results in a local effect that can be captured by an auxiliary quantity $p(x_{j}|e)$ . This quantity that directly links the acoustic vectors $\\pmb{x}_{j}$ with the targetworde of the corresponding source word $\\pmb{f}$ .Before justifying and computing the auxiliary quantity $\\pmb{p}(\\pmb{x}_{j}|e)$ ,we study its impact on the system architecture. The ambiguity caused by the acoustic probabilities is captured by the auxiliary quantity $p(x_{j}|e)$ and replaces the lexicon probabilities $p(f_{j}\\{e\\}$ in the search process [2, 6, 11, 13]. This computation can be done after the recognition process and before the translation process. Therefore it does not affect the complexity of a full search strategy in translation. However, the new quantities $p(x_{j}|e)$ may be less focussed than the original lexicon probabilities $p(f|e)$ and thus may result in more searchhypotheses when a beam search or some other non-full search method is applied.  \n\nThe reason why the exact averaging over $f_{1}^{J}$ cannotbe carried out in a straightforward way is that, for each word hypothesis $f_{j}$ , there is a dependence on the preceding word $f_{j-1}$ and the subsequent word $f_{j+1}$ (assuming a bigram language model). To avoid this complication, we consider  \n\nthe single best recognized sentence $\\tilde{f}_{1}^{J}:=\\tilde{f}_{1}...\\tilde{f}_{j}...\\tilde{f}_{J}$ Then the joint effect of the acoustic probabilities $p(x_{j}|f_{j})$ and the bigram LM probabilities $p(f|f^{\\prime})$ can be approximately taken into account by defining:  \n\n$$\n\\begin{array}{r c l}{p(x_{j}|e)}&{:=}&{\\displaystyle\\sum_{f_{j}}p(x_{j}|f_{j})\\cdot p(f_{j}|e,\\tilde{f}_{1}^{J}\\setminus\\tilde{f}_{j})}\\\\ &&{=}&{\\displaystyle\\frac{\\sum_{f_{j}}p(x_{j}|f_{j})\\cdot p(f_{j}|\\tilde{f}_{j-1},e)\\cdot p(\\tilde{f}_{j+1}|f_{j})}{\\sum_{f_{j}}p(f_{j}|\\tilde{f}_{j-1},e)\\cdot p(\\tilde{f}_{j+1}|f_{j})}}\\end{array}\n$$  \n\nNote that this quantity is intended to serve as the direct link between the acoustic segment $\\pmb{x}_{j}$ and any target word hypothesis $^e$ . The approximation can be improved by making use of the so-called forward-backward probabilities of the word graph [12] so that not only the single best, but all 'good’ word sequences with high enough probability scores can be taken into account. This is achieved by computing 'posterior'probabilities for each word hypothesis $f_{j}$ that are based on the observations $\\pmb{x}_{1}...\\pmb{x}_{j-1}\\pmb{x}_{j+1}...\\pmb{x}_{J}$ rather than the recognition hypothesis $\\tilde{f}_{1}...\\tilde{f}_{j-1}\\tilde{f}_{j+1}...\\tilde{f}_{J}$  \n\n# 4.2. Monotone Alignments  \n\nFirst, we review the dynamic programming (DP) search for monotone alignments as described in [6, 7]. The monotonicity requirement will be discussed later. In the maximum approximation (applied to the alignments $a_{1}^{J}$ )andignoring thelengthmodel $p(I|J)$ , we re-write the search criterion for a bigram language model $\\pmb{p}(e_{i}|e_{i-1})$  \n\n$$\n\\begin{array}{r l}{\\lefteqn{\\arg\\operatorname*{max}_{e_{1}^{I}}P_{T}(e_{1}^{I}|f_{1}^{J})}=}\\\\ {=}&{\\arg\\underset{e_{1}^{I}}{\\operatorname*{max}}\\prod_{j}\\ \\left[p(a_{j}|a_{j-1},I)\\cdot p(e_{a_{j}}|e_{a_{j}-1})\\cdot p(f_{j}|e_{a_{j}})\\right]}\\\\ {=}&{\\arg\\underset{e_{1}^{J},\\delta_{1}^{J}}{\\operatorname*{max}}\\ \\prod_{j}\\ \\left[p(\\delta_{j})\\cdot p_{\\delta_{j}}(e_{j}|e_{j-1})\\cdot p(f_{j}|e_{j})\\right]\\ .}\\end{array}\n$$  \n\nFor the last equation, we have re-formulated the search criterion as described in [4, 6]. Stretching notation, we have switched from the sequence $e_{1}^{I}$ along the target positions to a sequence $e_{1}^{J}$ along the source positions using the jump width $\\delta_{j}:=a_{j}-a_{j-1}$ and suitable defined alignment quantities $\\pmb{p}(\\delta_{j})$ and LM quantities $\\pmb{p}_{\\delta_{j}}(e_{j}|e_{j-1})$  \n\nFor the above criterion, there is a closed-form solution by the dynamic programming (DP) recursion (with the auxiliary function $Q(\\bar{j},e)]$  \n\n$$\n\\begin{array}{r l r}{Q(j,e)}&{=}&{p(f_{:}|e)\\cdot\\underset{\\delta,e^{\\prime}}{\\operatorname*{max}}\\left\\{p(\\delta)\\cdot p_{\\delta}(e|e^{\\prime})\\cdot Q(j-1,e^{\\prime})\\right\\}}\\end{array}\n$$  \n\nFor full search, the computational complexity of this recursion is $J\\cdot E^{2}$ $\\ln=$ vocabulary size of the target language vocabulary), which can be reduced by beam search and accelerated LM recombination [7].  \n\nNow, we consider speech input. Again in the maximum approximation(appliedtothealignments $\\pmb{a}_{1}^{J}$ andthesource strings $f_{1}^{J}$ ),we re-write the search criterion:", "appendix": ""}, {"title": "Effectively pretraining a speech translation decoder with Machine Translation data", "authors": "Alinejad, Ashkan and Sarkar, Anoop", "bibkey": "effectively_pretraining_a_speech_translation_decoder_with_machine_translation_data", "bibitem": "@article{Alinejad_EMNLP2020,\n  url = {https://aclanthology.org/2020.emnlp-main.644/},\n  title = {Effectively pretraining a speech translation decoder with Machine Translation data},\n  authors = {Alinejad, Ashkan and Sarkar, Anoop},\n  abstract = {Directly translating from speech to text using an end-to-end approach is still challenging for many language pairs due to insufficient data. Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the results in low resource settings, attempting to use pretrained parameters from the Neural Machine Translation (NMT) task has been largely unsuccessful in previous works. In this paper, we will show that by using an adversarial regularizer, we can bring the encoder representations of the ASR and NMT tasks closer even though they are in different modalities, and how this helps us effectively use a pretrained NMT decoder for speech translation.},\n  year = {2020},\n  editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},\n  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},\n  address = {Online},\n  publisher = {Association for Computational Linguistics},\n  doi = {10.18653/v1/2020.emnlp-main.644},\n  pages = {8014--8020},\n  month = {nov},\n  entrytype = {inproceedings}\n}", "url": "https://aclanthology.org/2020.emnlp-main.644/", "latex_url": null, "latex_path": null, "pdf_url": "https://aclanthology.org/2020.emnlp-main.644.pdf", "pdf_path": "output/download_papers/2020.emnlp-main.644/2020.emnlp-main.644.pdf", "md_url": null, "latex_length": 0, "latex": "", "abstract": "Directly translating from speech to text using an end-to-end approach is still challenging for many language pairs due to insufficient data. Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the results in low resource settings, attempting to use pretrained parameters from the Neural Machine Translation (NMT) task has been largely unsuccessful in previous works. In this paper, we will show that by using an adversarial regularizer, we can bring the encoder representations of the ASR and NMT tasks closer even though they are in different modalities, and how this helps us effectively use a pretrained NMT decoder for speech translation.", "abstract_length": 693, "abstract_token": 128, "introduction": "Automatic Speech Translation (AST) aims to directly translate audio signals in the source language into the text words in the target language. For many years, the pipeline of transcribing speech with ASR and then translating with the MT component was a standard method to address the speech translation problem. Having access to lots of data in many language pairs, the cascaded model for speech translation can benefit from well-trained ASR and MT components and generate high-quality translations. In recent years, it has shown that we can remove the transcription step and build an end-to-end model that is strong enough to compete with the cascaded model (Pino et al., 2019). Such models not only have lower inference latency, but they also do not suffer from the problem of errors that propagate from one component to the next. However, the scarcity of available resources is the main challenge in this task, and a variety of methods are proposed to address this problem. One of the most effective approaches to increase the performance of AST systems is to pretrain the encoder using an ASR model (Bansal et al., 2018). While pretraining the encoder by an ASR model even in different languages shows promising results (Bansal et al., 2019), using a pretrained MT decoder is not beneficial (Berard et al., 2018; Bansal et al., 2018) or slightly improve the result (Sperber et al., 2019) and even in some cases may worsen the results (Bahar et al., 2019). One explanation for this phenomenon is that the decoder works well only if its input comes from an encoder that it was trained with (Lample et al., 2018). To solve the problem of invariant encoder representations, we make use of an adversarial regularizer in our loss function to bring the output of the ASR encoder closer to the input of MT decoder. We show that this modification can improve the BLEU score by $+2.0$ BLEU points.", "introduction_length": 1891, "introduction_token": 409, "reference": "# References  \n\nOliver Adams, Graham Neubig, Trevor Cohn, Steven Bird, Quoc Truong Do, and Satoshi Nakamura. 2016. Learning a lexicon and translation model from phoneme lattices. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2377–2382, Austin, Texas. Association for Computational Linguistics.  \n\nAntonios Anastasopoulos and David Chiang. 2017. A case study on using speech-to-translation alignments for language documentation. In Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 170–178, Honolulu. Association for Computational Linguistics.  \n\nAntonios Anastasopoulos and David Chiang. 2018. Tied multitask learning for neural speech translation. In Proceedings of the 2018 Conference of the North  \n\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 82–91, New Orleans, Louisiana. Association for Computational Linguistics.  \n\nAntonios Anastasopoulos, David Chiang, and Long Duong. 2016. An unsupervised probability model for speech-to-translation alignment of low-resource languages. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1255–1263, Austin, Texas. Association for Computational Linguistics.  \n\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat, Roee Aharoni, Melvin Johnson, and Wolfgang Macherey. 2019. The missing ingredient in zero-shot neural machine translation. ArXiv, abs/1903.07091.  \n\nParnia Bahar, Tobias Bieschke, and Hermann Ney. 2019. A comparative study on end-to-end speech to text translation. 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 792–799.  \n\nSameer Bansal, Herman Kamper, Karen Livescu, Adam Lopez, and Sharon Goldwater. 2018. Low-resource speech-to-text translation. CoRR, abs/1803.09164.  \n\nSameer Bansal, Herman Kamper, Karen Livescu, Adam Lopez, and Sharon Goldwater. 2019. Pretraining on high-resource speech recognition improves low-resource speech-to-text translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2- 7, 2019, Volume 1 (Long and Short Papers), pages 58–68. Association for Computational Linguistics.  \n\nSameer Bansal, Herman Kamper, Adam Lopez, and Sharon Goldwater. 2017. Towards speech-to-text translation without speech recognition. CoRR, abs/1702.03856.  \n\nAlexandre Berard, Laurent Besacier, Ali Kocabiyikoglu, and Olivier Pietquin. 2018. End-to-end automatic speech translation of audiobooks. pages 6224–6228.  \n\nAlexandre Be´rard, Olivier Pietquin, Laurent Besacier, and Christophe Servan. 2016. Listen and Translate: A Proof of Concept for End-to-End Speech-to-Text Translation. In NIPS Workshop on end-to-end learning for speech and audio processing, Barcelona, Spain.  \n\nOndˇrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Alesˇ Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12–58, Baltimore, Maryland, USA. Association for Computational Linguistics.  \n\nEunah Cho, Jan Niehues, and Alex Waibel. 2017. Nmtbased segmentation and punctuation insertion for real-time spoken language translation. In Proc. Interspeech 2017, pages 2645–2649.  \n\nMattia A. Di Gangi, Roldano Cattoni, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2019a. MuST-C: a Multilingual Speech Translation Corpus. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2012–2017, Minneapolis, Minnesota. Association for Computational Linguistics.  \n\nMattia A. Di Gangi, Matteo Negri, Viet Nhat Nguyen, Amirhossein Tebbifakhr, and Marco Turchi. 2019b. Data augmentation for end-to-end speech translation: Fbk@iwslt ’19.  \n\nLong Duong, Antonios Anastasopoulos, David Chiang, Steven Bird, and Trevor Cohn. 2016. An attentional model for speech translation without transcription. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 949–959, San Diego, California. Association for Computational Linguistics.  \n\nMattia A. Di Gangi, Matteo Negri, and Marco Turchi. 2019. Adapting Transformer to End-to-End Spoken Language Translation. In Proc. Interspeech 2019, pages 1133–1137.  \n\nHirofumi Inaguma, Kevin Duh, Tatsuya Kawahara, and Shinji Watanabe. 2019. Multilingual end-to-end speech translation. 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 570–577.  \n\nYe Jia, Melvin Johnson, Wolfgang Macherey, Ron Weiss, Yuan Cao, Chung-Cheng Chiu, Naveen Ari, Stella Laurenzo, and Yonghui Wu. 2019. Leveraging weakly supervised data to improve end-to-end speech-to-text translation. pages 7180–7184.  \n\nTakatomo Kano, Sakriani Sakti, and Satoshi Nakamura. 2020. End-to-end speech translation with transcoding by multi-task learning for distant language pairs. IEEE/ACM Trans. Audio, Speech and Lang. Proc., 28:1342–1355.  \n\nDiederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. International Conference on Learning Representations.  \n\nAli Can Kocabiyikoglu, Laurent Besacier, and Olivier Kraif. 2018. Augmenting librispeech with French translations: A multimodal corpus for direct speech translation evaluation. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).  \n\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. 2018. Unsupervised machine translation using monolingual corpora only. In International Conference on Learning Representations.   \nY. Lecun, L. Bottou, Y. Bengio, and P. Haffner. 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324.   \nA. D. McCarthy, L. Puzon, and J. Pino. 2020. Skinaugment: Auto-encoding speaker conversions for automatic speech translation. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7924–7928.   \nH. Ney. 1999. Speech translation: coupling of recognition and translation. In 1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258), volume 1, pages 517–520 vol.1.   \nV. Panayotov, G. Chen, D. Povey, and S. Khudanpur. 2015. Librispeech: An asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5206–5210.   \nKishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.   \nDaniel S. Park, William Chan, Yu Zhang, ChungCheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le. 2019. Specaugment: A simple data augmentation method for automatic speech recognition. In INTERSPEECH.   \nNgoc-Quan Pham, Jan Niehues, Thanh-Le Ha, and Alex Waibel. 2019. Improving zero-shot translation with language-independent constraints. In Proceedings of the Forth Conference on Statistical Machine Translation (WMT 2019).   \nJuan Pino, Liezl Puzon, Jiatao Gu, Xutai Ma, Arya D. McCarthy, and Deepak Gopinath. 2019. Harnessing indirect training data for end-to-end automatic speech translation: Tricks of the trade.   \nMatthias Sperber, Graham Neubig, Jan Niehues, and Alex Waibel. 2019. Attention-passing models for robust and data-efficient end-to-end speech translation. Transactions of the Association for Computational Linguistics, 7:313–325.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio,  \n\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc.  \n\nRon J. Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, and Zhifeng Chen. 2017. Sequence-tosequence models can directly translate foreign speech. In INTERSPEECH.", "reference_length": 8653, "reference_token": 2276, "txt_length": 17423, "txt_token": 4475, "txt": "# Effectively pretraining a speech translation decoder with Machine Translation data  \n\nAshkan Alinejad, Anoop Sarkar Simon Fraser University, Burnaby, BC, Canada {aalineja,anoop}@sfu.ca  \n\n# Abstract  \n\nDirectly translating from speech to text using an end-to-end approach is still challenging for many language pairs due to insufficient data. Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the results in low resource settings, attempting to use pretrained parameters from the Neural Machine Translation (NMT) task has been largely unsuccessful in previous works. In this paper, we will show that by using an adversarial regularizer, we can bring the encoder representations of the ASR and NMT tasks closer even though they are in different modalities, and how this helps us effectively use a pretrained NMT decoder for speech translation.  \n\n# 1 Introduction  \n\nAutomatic Speech Translation (AST) aims to directly translate audio signals in the source language into the text words in the target language. For many years, the pipeline of transcribing speech with ASR and then translating with the MT component was a standard method to address the speech translation problem. Having access to lots of data in many language pairs, the cascaded model for speech translation can benefit from well-trained ASR and MT components and generate high-quality translations.  \n\nIn recent years, it has shown that we can remove the transcription step and build an end-to-end model that is strong enough to compete with the cascaded model (Pino et al., 2019). Such models not only have lower inference latency, but they also do not suffer from the problem of errors that propagate from one component to the next. However, the scarcity of available resources is the main challenge in this task, and a variety of methods are proposed to address this problem. One of the most effective approaches to increase the performance of AST systems is to pretrain the encoder using an ASR model (Bansal et al., 2018). While pretraining the encoder by an ASR model even in different languages shows promising results (Bansal et al., 2019), using a pretrained MT decoder is not beneficial (Berard et al., 2018; Bansal et al., 2018) or slightly improve the result (Sperber et al., 2019) and even in some cases may worsen the results (Bahar et al., 2019).  \n\nOne explanation for this phenomenon is that the decoder works well only if its input comes from an encoder that it was trained with (Lample et al., 2018). To solve the problem of invariant encoder representations, we make use of an adversarial regularizer in our loss function to bring the output of the ASR encoder closer to the input of MT decoder. We show that this modification can improve the BLEU score by $+2.0$ BLEU points.  \n\n# 2 Models  \n\n# 2.1 End-to-End Speech Translation  \n\nSimilar to conventional MT models, the speech translation task generates translated words in the target language, representing as $\\hat{Y}=(\\hat{y}_{1},\\dots,\\hat{y}_{m})$ , given the sequence of source speech features $X=$ $(x_{1},\\ldots,x_{n})$ . The translation model then minimizes the Cross-Entropy loss $\\mathrm{L}_{C E}\\,=\\,\\Delta(\\hat{Y},Y)$ , where $\\Delta$ is the sum of character-level CrossEntropy losses.  \n\nWe use character-level encoding and decoding using Transformer (Vaswani et al., 2017) as the basic architecture of all our models. For the AST and ASR models, we use similar architecture to (Di Gangi et al., 2019b) with an S-Transformer (Gangi et al., 2019). The main difference between transformer and S-Transformer is the way it encodes the input features. S-Transformer encodes the audio features by passing them into two stacked layers of Convolutional Neural Nets (CNN). Then, it uses a 2D self Attention layer to compute the attention matrix using the second CNN’s output. We followed the architecture of (Vaswani et al., 2017) in our MT model.  \n\n![](images/49290bc00f5581f1ac49c1bf0cab02d03fdb12a7bdd23b2dd4db2f4005d86d0b.jpg)  \nFigure 1: The proposed pretraining method using an adversarial loss.  \n\nThe conventional method for training an AST model is to pretrain ASR and NMT models separately and then transferring parameters of the encoder from ASR and the decoder from MT to the AST model, before starting to train via speech translation data.  \n\n# 2.2 Aligning encoder representations  \n\nSince we are training the encoder representations of the ASR model and the decoder parameters of the NMT system to work with their own encoder and decoder, pretraining the parameters of the AST model with a speech encoder from ASR and a text decoder from NMT is not ideal. Therefore, we propose to use adversarial training to bring NMT encoder and ASR encoder representations closer together.  \n\nAn overview of our model is depicted in Figure 1. Instead of separately pretraining the ASR and NMT, we propose to update their parameters simultaneously. In order to add explicit incentives to learn multi-modal representations in the encoder, we will train our NMT and ASR models on both Cross-Entropy loss and a new regularization loss. The final training objective for each task can be formulated as:  \n\nwhere $\\mathrm{L}_{C E}$ is the Cross-Entropy loss, $L_{D I S C}$ is the newly added regularization term, and $\\alpha$ is the constant parameter to control the effect of our regularizer. Since LDISC is a smaller number compared to $L_{C E}$ , we set $\\alpha$ to 5 in all our experiments to make the regularizer loss more perceptible during backward propagation. We are also sharing the parameters of the transformer layers in the encoder between AST and MT models. In the following section, we describe the regularizer.  \n\n# 2.3 Adversarial regularizer  \n\nGiven the embeddings of inputs $x_{i}$ in each modalities (speech features for ASR or character embeddings for NMT), the encoder computes the encoder representations $Z_{x_{i}}$ . By passing $Z_{x_{i}}$ to the discriminator, we can train its network by minimizing the loss function $L o s s_{D}\\,=\\,-\\mathbb{E}_{(x_{i},m_{i})}[\\log P_{D}(m_{i}|Z_{x_{i}})]$ , where $m_{i}$ is the modality of $x_{i}$ , with $m_{i}\\in\\{{\\mathrm{ASR}},{\\mathrm{NMT}}\\}$ and $P_{D}$ is the probability of choosing the right modality given the output of encoder.  \n\nThe encoder of NMT or ASR will be trained in order to deceive the discriminator by minimizing the loss:  \n\n$$\n\\mathsf{L}_{D I S C}=-\\mathbb{E}_{(x_{i},m_{i})}[\\log P_{D}(m_{j}|Z_{x_{i}})]\n$$  \n\nwhere $m_{j}\\,=\\,{\\bf A}{\\bf S}{\\bf R}$ if $m_{i}=\\mathrm{NMT}$ and vice versa. By incorporating this regularizer, we ensure that the encoder representations from different modalities (speech and text) become indistinguishable during training.  \n\nOur discriminator consists of a three-layer feedforward network with 1024 hidden units, followed by a Leaky-ReLU activation function (Lample et al., 2018).  \n\n# 3 Experiments  \n\n# 3.1 Dataset  \n\nTo evaluate our AST systems, we conducted our experiments on two datasets. For the EnglishGerman language pair, we use the MuST-C corpus (Di Gangi et al., 2019a), which consists of 408 hours of speech data aligned with 234K translated sentences. For the English-French language pair, we use the full training set of Translation  \n\nAugmented Librispeech (Libri-Trans) corpus (Kocabiyikoglu et al., 2018) with 230 hours of speech aligned with 131K french sentences.  \n\nWe use LibriSpeech corpus (Panayotov et al., 2015) with 960h of English speeches in order to train our ASR system. Since the test and dev sets of Libri-Trans corpus is part of the ASR LibriSpeech dataset, we remove all utterances from ASR LibriSpeech that share the same (chapter-id, readerid) pairs with the test and dev sets in the LibriTrans corpus. For En-De MT training, we use the combination of TED and Opensubtitle2018 corpora 1 2 which contains more than 18M sentences pairs after filtering noisy pairs. The MT training of the English-French language pair uses the En-Fr portion of the WMT14 competition (Bojar et al., 2014).  \n\n# 3.2 Preprocessing and Evaluation  \n\nFor each speech utterance, we extract $40\\ {\\mathrm{Mel}}.$ filterbank energy features with a step size of 10 ms and a window size of $25\\mathrm{ms}$ . For features extracted from $\\mathrm{MuSt-C}$ and ASR LibriSpeech, we apply mean and variance normalization for each speaker.  \n\nWe keep all the texts in our experiments truecase and tokenize them using Moses tokenizer3. We remove the punctuation from all English texts (both from the target side of ASR and the source side of MT).  \n\nFor translation tasks (AST and MT), we report BLEU score (Papineni et al., 2002) on tokenized sentences4. We evaluate our ASR systems using Word Error Rate (WER)5.  \n\n# 3.3 Model settings  \n\nFor both En-De and En-Fr tasks, we followed the architecture in (Di Gangi et al., 2019b). We use six Transformer layers of size 512 in the encoder and decoder with eight attention heads. The size of feed-forward mechanism is 1024. The embedding layer in the encoder for the AST task contains two layers of 2D CNNs (Lecun et al., 1998) followed by a ReLU activation function. Each CNN layer has 16 output channels, with a stride of (2, 2). We run all our models on two GeForce GTX 1080 GPUs with 12GB RAM each. The total number of parameters and run-time of our models in Table 1.  \n\n<html><body><table><thead><tr><td rowspan=\"3\"></td><td colspan=\"2\"><b>En-De</b></td><td colspan=\"2\"><b>En-Fr</b></td></tr><tr><td><b>#param</b></td><td><b>#hours</b></td><td><b>#param</b></td><td><b>#hours</b></td></tr></thead><tbody><tr><td>cascaded NMT</td><td>45M</td><td>27</td><td>45M</td><td>13</td></tr><tr><td>cascaded ASR</td><td>31M</td><td>22.5</td><td>31M</td><td>18.2</td></tr><tr><td>AST</td><td>31M</td><td>34</td><td>31M</td><td>18</td></tr></tbody></table></body></html>  \n\nTable 1: The number of parameters and run-time of our models on MuSt-C dataset (En-De) and Libri-Trans dataset (En-Fr).   \nTable 2: Results of AST models trained only with AST data. The performance is measured with BLEU score on MuST-C test set.   \n\n\n<html><body><table><thead><tr><td><b>Task</b></td><td><b>En-De</b></td><td><b>En-Fr</b></td></tr></thead><tbody><tr><td>cascaded</td><td>18.76</td><td>15</td></tr><tr><td>AST + ASR pre</td><td>18.71</td><td>14.7</td></tr><tr><td>AST + ASR pre + MT pre</td><td>19.05</td><td>15.3</td></tr><tr><td>AST + regularizer</td><td>20.24</td><td>17.01</td></tr></tbody></table></body></html>  \n\n# 3.4 Training settings  \n\nIn all our models, we use the Adam optimizer (Kingma and Ba, 2014) with an initial learning rate of 0.00005. During the first 6000 warm-up updates, we increase it linearly to 0.003, then decrease it with inverse square root decay (Vaswani et al., 2017). The number of warm-up updates in our MT systems is 8000.  \n\n# 4 Results  \n\nIn this section, we analyze the effect of our regularizer on two different settings: (A) When we only have access to AST data (section 4.1) and (B) When we can benefit from External data (section 4.2). For each setting, we run experiments on four different models:  \n\n1. The cascaded model  \n\n2. AST model with pretrained ASR encoder  \n\n3. AST model with pretrained ASR encoder and MT decoder  \n\n4. Our proposed model with adversarial loss.  \n\nTable 3: BLEU scores of AST models, trained with both AST and external ASR and MT data.   \n\n\n<html><body><table><thead><tr><td><b>Task</b></td><td><b>En-De</b></td><td><b>En-Fr</b></td></tr></thead><tbody><tr><td>cascaded</td><td>21.06</td><td>19.21</td></tr><tr><td>AST + ASR pre</td><td>19.01</td><td>16.13</td></tr><tr><td>AST + ASR pre + MT pre</td><td>19.12</td><td>16.27</td></tr><tr><td>AST + regularizer</td><td>20.81</td><td>17.7</td></tr></tbody></table></body></html>  \n\n# 4.1 Using only AST data  \n\nTable 2 shows the performance of AST models for En-De and En-Fr language pairs. When the cascaded model is restricted to use small AST datasets merely, the model will not be strong enough to beat an AST model with a pretrained encoder and decoder. We should also note that unlike (Bansal et al., 2019; Bahar et al., 2019), where transferring decoder parameters were not effective, in all our AST models, we could only beat the cascaded model by pretraining the decoder.  \n\nThe last row in the table gives the AST model results, which uses adversarial regularizer during the pretrain step. As we can see, training the NMT and the ASR models simultaneously can help pretrained components be compatible with each other and improve the final performance by 1.2 and 1.7 BLEU scores for En-De and En-Fr language pairs respectively.  \n\n# 4.2 Using both AST and External data  \n\nLimiting the training data for the speech translation models to AST datasets is not a realistic assumption for many language pairs, and in practice, the cascaded model can greatly benefit from the large amounts of NMT and ASR corpora.  \n\nTable 3 summarizes the effects of adding external training data to our experiments. Adding external data can boost the performance of the cascaded model and by comparing Table 2 and 3, we can see that the additional NMT and ASR data can improve the translation quality of the cascaded model by $^{+2}$ BLEU scores, while it can barely affect the AST model with pretrained encoder and the decoder. Consequently, the gap between the AST model and the cascaded system increases by around $+3$ BLEU scores for En-Fr and $^{+2}$ BLEU scores for the En-De language pair.  \n\nAs we can see in the last row of Table 3, adding our proposed pretraining step can help the model perform better during training, and compared to the conventional pretraining step, we can see an increase of more than 1 BLEU point in each language pair. Although the cascaded model by having access to all the pretrained parameters (the encoder and decoder of both NMT and ASR) still has better translation quality, we can bring the performance of an end-to-end model closer to it by adding the new regularizer. It is also important to note that since we are not changing the final structure of the AST model, most of the other techniques for further improving the translation quality, such as data augmentation, which was examined in previous studies (McCarthy et al., 2020; Park et al., 2019) can also be applied. But we won’t study them in this paper.  \n\n# 5 Related Work  \n\nThe cascaded pipeline of transcribing speech signals and then translating them using an MT component (Ney, 1999; Cho et al., 2017) was for many years the standard design of speech translation systems (Inaguma et al., 2019). The idea of having an end-to-end structure for this task showed promising results in the works of (Adams et al., 2016; Duong et al., 2016; B´erard et al., 2016; Anastasopoulos et al., 2016; Anastasopoulos and Chiang, 2017; Bansal et al., 2017). After the success of (Weiss et al., 2017) in creating a powerful model for ST systems, more recent studies focused on exploring their power, and one of the main approaches to boost the performance of such models is to make use of available data from other tasks, such as ASR and NMT. (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019) show that multitask learning can be effective and (Jia et al., 2019; Pino et al., 2019; Park et al., 2019; McCarthy et al., 2020) investigate various data augmentation techniques. The impact of pretraining the encoder with ASR model is also studied in (Berard et al., 2018; Bansal et al., 2018, 2019). In experiments of (Bahar et al., 2019; Bansal et al., 2019) the performance gain of pretraining the decoder with an MT model was marginal.  \n\n(Kano et al., 2020) addresses the ASR encoder and MT decoder gap problem by proposing a “Transcoder” and use smooth-L1 loss to bring ASR hidden representation close to MT encoder hidden representation.  \n\nThe idea of modifying loss function in AST models was also discussed in (Sperber et al., 2019). Their formulation of the additional loss is different from ours, and they use their additional loss function in a different NMT architecture from ours.  \n\nThe idea of adding adversarial regularizer was discussed in other tasks such as unsupervised MT (Lample et al., 2018) or zero-shot translation (Pham et al., 2019). The closest research to our work is (Arivazhagan et al., 2019), which uses a similar adversarial network to bring encoder representations closer together. However, they apply their model to the zero-shot machine translation task, with a different architecture. They also apply their regularizer to the representations of the different languages with the same modalities.  \n\n# 6 Conclusion  \n\nIn this paper, we study the impact of pretraining an AST decoder using an MT model and propose a method to make the pretraining step more effective. We show that we can align the latent representations of different modalities by using adversarial loss and make the ASR encoder more compatible with the MT decoder. Our experiments demonstrate that we can improve the performance by around 1.5 BLEU points on two language pairs compared to conventional pretraining methods.  \n\n# Acknowledgments  \n\nWe would like to thank the anonymous reviewers for their helpful comments. The research was partially supported by the Natural Sciences and Engineering Research Council of Canada grants NSERC RGPIN-2018-06437 and RGPAS-2018- 522574 and a Department of National Defence (DND) and NSERC grant DGDND-2018-00025", "appendix": "."}, {"title": "Findings of the IWSLT 2022 Evaluation Campaign", "authors": "Anastasopoulos, Antonios and Barrault, Lo\\\"\\i c and Bentivogli, Luisa and Zanon Boito, Marcely and Bojar, Ond\\v rej and Cattoni, Roldano and Currey, Anna and Dinu, Georgiana and Duh, Kevin and Elbayad, Maha and Emmanuel, Clara and Est\\`eve, Yannick and Federico, Marcello and Federmann, Christian and Gahbiche, Souhir and Gong, Hongyu and Grundkiewicz, Roman and Haddow, Barry and Hsu, Benjamin and Javorsk\\'y, D\\'avid and Kloudov\\'a, V\\u era and Lakew, Surafel and Ma, Xutai and Mathur, Prashant and McNamee, Paul and Murray, Kenton and N\\v adejde, Maria and Nakamura, Satoshi and Negri, Matteo and Niehues, Jan and Niu, Xing and Ortega, John and Pino, Juan and Salesky, Elizabeth and Shi, Jiatong and Sperber, Matthias and St\\\"uker, Sebastian and Sudoh, Katsuhito and Turchi, Marco and Virkar, Yogesh and Waibel, Alexander and Wang, Changhan and Watanabe, Shinji", "bibkey": "findings_of_the_iwslt_2022_evaluation_campaign", "bibitem": "@article{anastasopoulos-etal-2022-findings,\n  url = {https://aclanthology.org/2022.iwslt-1.10/},\n  title = {Findings of the IWSLT 2022 Evaluation Campaign},\n  authors = {Anastasopoulos, Antonios and Barrault, Lo\\\"\\i c and Bentivogli, Luisa and Zanon Boito, Marcely and Bojar, Ond\\v rej and Cattoni, Roldano and Currey, Anna and Dinu, Georgiana and Duh, Kevin and Elbayad, Maha and Emmanuel, Clara and Est\\`eve, Yannick and Federico, Marcello and Federmann, Christian and Gahbiche, Souhir and Gong, Hongyu and Grundkiewicz, Roman and Haddow, Barry and Hsu, Benjamin and Javorsk\\'y, D\\'avid and Kloudov\\'a, V\\u era and Lakew, Surafel and Ma, Xutai and Mathur, Prashant and McNamee, Paul and Murray, Kenton and N\\v adejde, Maria and Nakamura, Satoshi and Negri, Matteo and Niehues, Jan and Niu, Xing and Ortega, John and Pino, Juan and Salesky, Elizabeth and Shi, Jiatong and Sperber, Matthias and St\\\"uker, Sebastian and Sudoh, Katsuhito and Turchi, Marco and Virkar, Yogesh and Waibel, Alexander and Wang, Changhan and Watanabe, Shinji},\n  abstract = {The evaluation campaign of the 19th International Conference on Spoken Language Translation featured eight shared tasks: (i) Simultaneous speech translation, (ii) Offline speech translation, (iii) Speech to speech translation, (iv) Low-resource speech translation, (v) Multilingual speech translation, (vi) Dialect speech translation, (vii) Formality control for speech translation, (viii) Isometric speech translation. A total of 27 teams participated in at least one of the shared tasks. This paper details, for each shared task, the purpose of the task, the data that were released, the evaluation metrics that were applied, the submissions that were received and the results that were achieved.},\n  year = {2022},\n  bibkey = {anastasopoulos-etal-2022-findings},\n  editor = {Salesky, Elizabeth and Federico, Marcello and Costa-juss\\`a, Marta},\n  booktitle = {Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022)},\n  address = {Dublin, Ireland (in-person and online)},\n  publisher = {Association for Computational Linguistics},\n  doi = {10.18653/v1/2022.iwslt-1.10},\n  pages = {98--157},\n  month = {may},\n  entrytype = {inproceedings}\n}", "url": "https://aclanthology.org/2022.iwslt-1.10/", "latex_url": null, "latex_path": null, "pdf_url": "https://aclanthology.org/2022.iwslt-1.10.pdf", "pdf_path": "output/download_papers/2022.iwslt-1.10/2022.iwslt-1.10.pdf", "md_url": null, "latex_length": 0, "latex": null, "abstract": "The evaluation campaign of the 19th International Conference on Spoken Language Translation featured eight shared tasks: (i) Simultaneous speech translation, (ii) Offline speech translation, (iii) Speech to speech translation, (iv) Low-resource speech translation, (v) Multilingual speech translation, (vi) Dialect speech translation, (vii) Formality control for speech translation, (viii) Isometric speech translation. A total of 27 teams participated in at least one of the shared tasks. This paper details, for each shared task, the purpose of the task, the data that were released, the evaluation metrics that were applied, the submissions that were received and the results that were achieved.", "abstract_length": 698, "abstract_token": 144, "introduction": "The International Conference on Spoken Language Translation (IWSLT) is the premier annual scientific conference for all aspects of spoken language translation. IWSLT is organized by the Special Interest Group on Spoken Language Translation, which is supported by ACL, ISCA and ELRA. Like in all previous editions (Akiba et al., 2004; Eck and Hori, 2005; Paul, 2006; Fordyce, 2007; Paul, 2008, 2009; Paul et al., 2010; Federico et al., 2011,2012; Cettolo et al.,2013,2014,2015, 2016, 2017; Niehues et al.,2018, 2019; Ansari et al., 2020; Anastasopoulos et al.,2021), this year's conference was preceded by an evaluation campaign featuring shared tasks addressing scientific challenges in spoken language translation. This paper reports on the 2022 IWSLT Evaluation Campaign, which offered eight shared tasks: · Simultaneous speech translation, addressing low latency speech translation either streamed by a speech recognition (ASR) system or directly from the audio source. The translation directions for both conditions are: English to German, English to Japanese, and English to Mandarin Chinese. · Offline speech translation, proposing speech translation of talks from English to German, English to Japanese, and English to Mandarin Chinese, using either cascade architectures or end-to-end models able to directly translate source speech into target text; <html><body><table><thead><tr><td><b>Team</b></td><td><b>Organization</b></td></tr></thead><tbody><tr><td>AISP-SJTU</td><td>Aispeech and Shanghai Jiao Tong University, China (Zhu et al., 2022)</td></tr><tr><td>ALEXA AI</td><td>Amazon Alexa AI, USA (Shanbhogue et al., 2022)</td></tr><tr><td>ALEXA AI</td><td>Amazon Alexa AI, USA (Zhang et al., 2022a)</td></tr><tr><td>APPTEK</td><td>AppTek, Germany (Wilken and Matusov, 2022)</td></tr><tr><td>CMU</td><td>Carnegie Mellon University, USA (Yan et al., 2022)</td></tr><tr><td>CUNI-KIT</td><td>Charles University, Czech Republic, and KIT, Germany (Polak et al., 2022)</td></tr><tr><td>FBK</td><td>Fondazione Bruno Kessler, Italy (Gaido et al., 2022)</td></tr><tr><td>GMU</td><td>George Mason University, USA</td></tr><tr><td>HW-TSC</td><td>Huawei Translation Services Center, China (Li et al.; Wang et al.; Guo et al.; Li et al.)</td></tr><tr><td>JHU</td><td>Johns Hopkins University, USA (Yang et al., 2022)</td></tr><tr><td>KIT</td><td>Karlsruhe Institute of Technology, Germany (Pham et al., 2022; Polak et al., 2022)</td></tr><tr><td>MLLP-VRAIN</td><td>Universitat Politecnica de Valencia, Spain (Iranzo-Sanchez et al., 2022)</td></tr><tr><td>NA</td><td>Neural.AI, China</td></tr><tr><td>NAIST</td><td>Nara Institute of Science and Technology, Japan (Fukuda et al., 2022)</td></tr><tr><td>NIUTRANS</td><td>NiuTrans, China (Zhang et al., 2022c)</td></tr><tr><td>NUV</td><td>Navrachana University, India (Bhatnagar et al., 2022)</td></tr><tr><td>NEMO</td><td>NVIDIA NeMo, USA(Hrinchuk et al., 2022)</td></tr><tr><td>ON-TRAC</td><td>ON-TRAC Consortium, France (Boito et al., 2022b)</td></tr><tr><td>Uos</td><td>University of Sheffield, UK (Vincent et al., 2022)</td></tr><tr><td>TALTECH</td><td>Tallinn University of Technology, Estonia</td></tr><tr><td>UMD</td><td>University of Maryland, USA (Rippeth et al., 2022)</td></tr><tr><td>UPC</td><td>Universitat Politecnica de Catalunya, Spain (Tsiamas et al., 2022a)</td></tr><tr><td>USTC-NELSLIP</td><td> University of Science and Technology of China (Zhang et al., 2022b)</td></tr><tr><td>XIAOMI</td><td>Xiaomi AI Lab, China (Guo et al., 2022a)</td></tr><tr><td>Y1</td><td>Yi, China (Zhang and Ao, 2022)</td></tr></tbody></table></body></html> · Speech to speech translation, investigating for the first time automatic translation of human speech in English into synthetic speech in German, either with cascaded or direct neural models. · Low-resource speech translation, focusing on resource-scarce settings for translating input speech in Tamasheq into French text, and input speech in Tunisian Arabic into English text. · Multilingual speech translation, analyzing the performance of multi-lingual versus bilingual translation models for the Offline speech translation tasks (discussed in the Offline task section); · Dialect speech translation, addressing speech translation from Tunisian into English under three training data conditions: (i) only with limited dialect-specific training data (provided by the organizers); (ii) with also larger amount of related-language data (Modern Standard Arabic); (ii) with any kind of publicly available data. · Formality control for SLT, addressing the formality level (formal vs. informal) in spoken language translation from English into German, Spanish, Hindi, Japanese, Italian and Russian. The task focuses in particular on zero-shot learning in multilingual models, given that for the last two directions no formality-annotated training data is provided. · Isometric SLT, addressing the generation of translations similar in length to the source, from English into French, German and Spanish. The shared tasks attracted 27 participants (see Table 1) from both academic and industrial organizations. The following sections report on each shared task in detail, in particular: the goal and automatic metrics adopted for the task, the data used for training and testing data, the received submissions and the summary of results. Detailed results for some of the shared tasks are reported in a corresponding appendix.", "introduction_length": 5419, "introduction_token": 1516, "reference": "# References  \n\nFarhad Akhbardeh, Arkady Arkhangorodsky, Magdalena Biesialska, Ondrej Bojar, Rajen Chatterjee, Vishrav Chaudhary, Marta R. Costa-jussa, Cristina Espana-Bonet, Angela Fan, Christian Federmann, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Leonie Harter, Kenneth Heafield, Christopher Homan, Matthias Huck, Kwabena Amponsah-Kaakyire, Jungo Kasai, Daniel Khashabi, Kevin Knight, Tom Kocmi, Philipp Koehn, Nicholas Lourie, Christof Monz, Makoto Morishita, Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Allahsera Auguste Tapo, Marco Turchi, Valentin Vydrin, and Marcos Zampieri. 2021. Findings of the 2021 conference on machine translation (WMT21). In Proceedings of the Sixth Conference on Machine Translation, pages 1-88, Online. Association for Computational Linguistics.  \n\nYasuhiro Akiba, Marcello Federico, Noriko Kando, Hiromi Nakaiwa, Michael Paul, and Jun'ichi Tsujii. 2004. Overview of the IWSLTO4 Evaluation Campaign. In Proceedings of the International Workshop on Spoken Language Translation, pages 1-12, Kyoto, Japan.  \n\nAntonios Anastasopoulos, Ondrej Bojar, Jacob Bremerman, Roldano Cattoni, Maha Elbayad, Marcello Federico, Xutai Ma, Satoshi Nakamura, Matteo Negri, Jan Niehues, Juan Pino, Elizabeth Salesky, Sebastian Stiker, Katsuhito Sudoh, Marco Turchi, Alexander Waibel, Changhan Wang, and Matthew Wiesner.2021.FINDINGS OF THE IWSLT 2021 EVALUATION CAMPAIGN. In Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021), pages 1-29, Bangkok, Thailand (online). Association for Computational Linguistics.  \n\nEbrahim Ansari, Amittai Axelrod, Nguyen Bach, Ondrej Bojar, Roldano Cattoni, Fahim Dalvi, Nadir Durrani, Marcello Federico, Christian Federmann, Jiatao Gu, Fei Huang, Kevin Knight, Xutai Ma, Ajay Nagesh, Matteo Negri, Jan Niehues, Juan Pino, Elizabeth Salesky, Xing Shi, Sebastian Stiker, Marco Turchi, and Changhan Wang. 2020. Findings of the IWSLT 2020 Evaluation Campaign. In Proceedings of the17thInternational Conference onSpokenLanguage Translation (IWSLT 2020), Seattle,USA.  \n\nRosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber. 2020. Common voice: A massivelymultilingual speech corpus. In LREC.  \n\nArun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, et al. 2021. XLS-R: Self-supervised cross-lingual speech representation learning at scale. arXiv preprint arXiv:2111.09296.  \n\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020a. wav2vec 2.0: A framework for self-supervised learning of speech representations.Advances inNeuralInformationProcessing Systems,33:12449-12460.  \n\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020b. wav2vec 2.0: A framework for self-supervised learning of speech representations.InAdvances in Neural InformationProcessing Systems, volume 33, pages 12449-12460. Curran Associates, Inc.  \n\nBBC.2019. BBC Subtitle Guidelines. BBC $\\copyright$ 2018 Version 1.1.8.  \n\nBenjamin Beilharz and Xin Sun. 2019. LibriVoxDeEn - A Corpus for German-to-English Speech Translation and Speech Recognition.  \n\nLuisa Bentivogli, Mauro Cettolo, Marco Gaido, Alina Karakanta, Alberto Martinelli, and Marco Turchi Matteo Negri. 2021. Cascade versus Direct Speech Translation: Do the Differences Still Make a Difference? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics, Bangkok, Thailand. Association for Computational Linguistics.   \nAakash Bhatnagar, Nidhir Bhavsar, Muskaan Singh, and Petr Motlicek. 2022. Hierarchical Multi-task learning framework for Isometric-Speech Language Translation. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT).   \nMarcely Zanon Boito, Fethi Bougares, Florentin Barbier, Souhir Gahbiche, Loic Barrault, Mickael Rouvier, and Yannick Estéve. 2022a. Speech resources in the tamasheq language. Language Resources and Evaluation Conference (LREC).   \nMarcely Zanon Boito, John Ortega, Hugo Riguidel, Antoine Laurent, Loic arrault, Fethi ugare, ras Chaabani, Ha Nguyen, Florentin Barbier, Souhir Gahbiche, and Yannick Esteve. 2022b. ON-TRAC Consortium Systems for the IWSLT 2022 Dialect and Low-resource Speech Translation Tasks. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT).   \nRoldano Cattoni, Mattia Antonino Di Gangi, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2021. Must-c:A multilingual corpus for end-to-end speech translation. Computer Speech & Language, 66:101155.   \nMauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian Stuiker, K. Sudoh, K. Yoshino, and Christian Federmann. 2017. Overview of the IWSLT 2017 Evaluation Campaign. In Proceedings of the 14th International Workshop on Spoken Language Translation (IWSLT 2017), pages 2-14, Tokyo, Japan.   \nMauro Cettolo, Christian Girardi, and Marcello Federico. 2012. WIT3: Web Inventory of Transcribed and Translated Talks. In Proceedings of the Annual Conference of the European Association for Machine Translation (EAMT), Trento, Italy.   \nMauro Cettolo, Jan Niehues, Sebastian Stuiker, Luisa Bentivogli, Roldano Cattoni, and Marcello Federico. 2015. The IWSLT 2015 Evaluation Campaign. In Proceedings of the 12th International Workshop on Spoken Language Translation (IWSLT 2015), Da Nang, Vietnam.   \nMauro Cettolo, Jan Niehues, Sebastian Stuiker, Luisa Bentivogli, and Marcello Federico. 2013. Report on the 10th IWSLT Evaluation Campaign. In Proceedings of the TenthInternational Workshop on Spoken Language Translation (IWSLT 2013), Heidelberg, Germany.  \n\nMauro Cettolo, Jan Niehues, Sebastian Stiker, Luisa Bentivogli, and Marcello Federico. 2014. Report on the 11th IWSLT Evaluation Campaign, IWSLT 2014. In Proceedings of the Eleventh International Workshop on Spoken Language Translation (IWSLT 2014), Lake Tahoe, USA.  \n\nMauro Cettolo, Jan Niehues, Sebastian Stiker, Luisa Bentivogli, and Marcello Federico. 2016. The IWSLT 2016 Evaluation Campaign. In Proceedings of the13thInternationalWorkshop onSpokenLanguage Translation (IWSLT 2016), Seattle, USA.  \n\nColin Cherry and George Foster. 2019. Thinking slow about latency evaluation for simultaneous machine translation. arXiv preprint arXiv:1906.00048.  \n\nJoon Son Chung, Arsha Nagrani, and Andrew Zisserman. 2018. VoxCeleb2: Deep Speaker Recognition. In Interspeech, pages 1086-1090.  \n\nAlexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and Michael Auli. 2021. Unsupervised Cross-Lingual Representation Learning for Speech Recognition. In Proc. Interspeech 2021, pages2426-2430.  \n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale.   In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440- 8451, Online. Association for Computational Linguistics.  \n\nSiddharth Dalmia, Brian Yan, Vikas Raunak, Florian Metze, and Shinji Watanabe. 2021. Searchable hidden intermediates for end-to-end models of decomposable sequence tasks. In Proceedings of the 2021 ConferenceoftheNorthAmericanChapterofthe AssociationforComputational Linguistics:Human Language Technologies, pages 1882-1896, Online. Association for Computational Linguistics.  \n\nMattia A. Di Gangi, Roldano Cattoni, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2019. MuST-C: a Multilingual Speech Translation Corpus. In Proceedingsofthe2019ConferenceoftheNorthAmerican Chapter of theAssociationfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2012-2017, Minneapolis, Minnesota.  \n\nMatthias Eck and Chiori Hori. 2005. Overview of the IWSLT 2005 evaluation campaign. In Proceedings of the International Workshop on Spoken Language Translation, pages 1-22, Pittsburgh, PA.  \n\nJohanes Effendi, Yogesh Virkar, Roberto BarraChicote, and Marcello Federico. 2022.Duration modeling of neural tts for automatic dubbing. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8037-8041.  \n\nSolene Evain, Ha Nguyen, Hang Le, Marcely Zanon Boito, Salima Mdhaffar, Sina Alisamir, Ziyi Tong, Natalia Tomashenko, Marco Dinarelli, Titouan Parcollet, et al. 2021. Task agnostic and task specific self-supervised learning from speech with LeBenchmark.InThirty-fifthConferenceonNeuralInformation Processing Systems Datasets and Benchmarks Track (Round 2).  \n\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. 2021. Beyond english-centric multilingual machine translation. Journal of Machine Learning Research, 22(107):1-48.  \n\nM. Federico, Y. Virkar, R. Enyedi, and R. BarraChicote. 2020a. Evaluating and optimizing prosodic alignment for automatic dubbing. In Proceedings of Interspeech, page 5.  \n\nMarcello Federico, Luisa Bentivogli, Michael Paul, and Sebastian Stiker. 2011.Overview of theIWSLT 2011 Evaluation Campaign. In Proceedings of the International Workshop on Spoken LanguageTranslation, pages 11-27, San Francisco, USA.  \n\nMarcello Federico, Mauro Cettolo, Luisa Bentivogli, Michael Paul, and Sebastian Stuker. 2012. Overview of the IWSLT 2012 Evaluation Campaign. In Proceedings of the International Workshop on Spoken Language Translation, pages 11-27, Hong Kong, HK.  \n\nMarcello Federico, Robert Enyedi, Roberto BarraChicote, Ritwik Giri, Umut Isik, Arvindh Krishnaswamy, and Hassan Sawaf. 2020b. From Speechto-Speech Translation to Automatic Dubbing. In Proc. of IWSLT, pages 257-264, Online. ACL.  \n\nChristian Federmann.2018. Appraiseevaluation framework for machine translation. In Proceedings of the27thInternational Conference onComputational Linguistics: System Demonstrations, pages 86-88, Santa Fe, New Mexico. Association for Computational Linguistics.  \n\nWeston Feely, Eva Hasler, and Adria de Gispert. 2019. Controlling Japanese honorifics in Englishto-Japanese neural machine translation. In Proceedings of the6thWorkshop onAsianTranslation, pages 45-53, Hong Kong, China. Association for Computational Linguistics.  \n\nCameron Shaw Fordyce. 2007. Overview of the IWSLT 2007 evaluation campaign. In Proceedings of the International Workshop on Spoken Language Translation, pages 1-12, Trento, Italy.  \n\nMarkus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactionsof theAssociationforComputational Linguistics,9:1460-1474.  \n\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ondrej Bojar. 2021b. Results of the WMT21 metrics shared task: Evaluating metrics with expertbased human evaluations on TED and news domain. In Proceedings of the Sixth Conference on Machine Translation, pages 733-774, Online. Association for Computational Linguistics.  \n\nRyo Fukuda, Yuka Ko, Yasumasa Kano, Kosuke Doi, Hirotaka Tokuyama, Sakriani Sakti, Katsuhito Sudoh, and Satoshi Nakamura. 2022.NAIST Simultaneous Speech-to-Text Translation System for IWSLT 2022. In Proceedings of the 19th International Conference on Spoken LanguageTranslation (IWSLT).  \n\nMarco Gaido, Sara Papi, Dennis Fucci, Giuseppe Fiameni, Matteo Negri, and Marco Turchi. 2022. Efficient yet  Competitive Speech Translation: FBK $@$ IWSLT2022. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT).  \n\nKarthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu Venkatesh, Raefer Gabriel, and Dilek Hakkani-Tuir. 2019. Topical-Chat: Towards knowledge-grounded open-domain conversations. In Proc. Interspeech 2019, pages 1891-1895.  \n\nYvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2013. Continuous measurement scales in human evaluation of machine translation. In Proceedings of the7th Linguistic AnnotationWorkshop and Interoperability with Discourse, pages 33-41, Sofia, Bulgaria. Association for Computational Linguistics.  \n\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. 2020.Conformer: Convolution-augmented transformer for speech recognition. In Proceedings ofInterspeech2020,21stAnnualConferenceofthe International Speech Communication Association, pages 5036—-5040, Shanghai, China.  \n\nBao Guo, Mengge Liu, Wen Zhang, Hexuan Chen, Chang Mu, Xiang Li, Jianwei Cui, Bin Wang, and Yuhang Guo. 2022a. The Xiaomi Text-to-Text Simultaneous Speech Translation System for IWSLT 2022.In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT).  \n\nJiaxin Guo, Yinglu Li, Minghan Wang, Xiaosong Qiao, Yuxia Wang, Hengchao Shang, Chang Su, Yimeng Chen, Min Zhang, Shimin Tao, Hao Yang, and Ying Qin. 2022b. The HW-TSC's Speech to Speech Translation System for IWSLT 2022 Evaluation. In Proceedings of the19th International Conference on Spoken Language Translation (IWSLT).  \n\nAndrew Hayes and Klaus Krippendorff. 2007. Answering the call for a standard reliability measure for coding data. Communication Methods and Measures, 1:77-89.  \n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770- 778.  \n\nFrancois Hernandez, Vincent Nguyen, Sahar Ghannay, Natalia A. Tomashenko, and Yannick Esteve. 2018. TED-LIUM 3: twice as much data and corpus repartition for experiments on speaker adaptation. CoRR, abs/1805.04699.  \n\nOleksii Hrinchuk, Vahid Noroozi, Abhinav Khattar, Anton Peganov, Sandeep Subramanian, Somshubra Majumdar, and Oleksii Kuchaiev. 2022. NVIDIA NeMo Ofline Speech Translation Systems for IWSLT 2022. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT).  \n\nJavier Iranzo-Sanchez, Jorge Civera Saiz, and Alfons Juan. 2021. Stream-level latency evaluation for simultaneous machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 664-670, Punta Cana, Dominican Republic. Association for Computational Linguistics.  \n\nJavier Iranzo-Sanchez, Javier Jorge Cano, Alejandro Pérez-Gonzalez de Martos, Adrian Giménez Pastor, Goncal Garcés Diaz-Munio, Pau Baquero-Arnal, Joan Albert Silvestre-Cerda, Jorge Civera Saiz, Albert Sanchis, and Alfons Juan. 2022. MLLPVRAIN UPV systems for the IWSLT 2022 Simultaneous Speech Translation and Speech-to-Speech Translation tasks. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT).  \n\nJavier Iranzo-Sanchez, Joan Albert Silvestre-Cerda, Javier Jorge, Nahuel Rosello, Adria Giménez, Albert Sanchis, Jorge Civera, and Alfons Juan. 2020. Europarl-st: A multilingual corpus for speech translation of parliamentary debates. In Proc. of45th Intl. Conf. on Acoustics, Speech, and Signal Processing (ICASSP 2020), pages 8229-8233, Barcelona (Spain).  \n\nWon Jang, Dan Lim, Jaesam Yoon, Bongwan Kim, and Juntae Kim. 2021.UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation. In Interspeech,pages 2207-2211.  \n\nDavid Javorsky, Dominik Machacek, and Ondrej Bojar. 2022. Comprehension of subtitles from retranslating simultaneous speech translation.  \n\nJapan Translation Federation JTF. 2018. JTF Translation Quality Evaluation Guidelines, 1st Edition (in Japanese).  \n\nJaehyeon Kim, Jungil Kong, and Juhee Son. 2021. Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech. In ICML.  \n\nAli Can Kocabiyikoglu, Laurent Besacier, and Olivier Kraif. 2018. Augmenting Librispeech with French Translations: A Multimodal Corpus for Direct Speech Translation Evaluation. In Proceedings of LREC 2018, Miyazaki, Japan.  \n\nPhilipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 conference on empirical methods in natural language processing, pages 388-395.  \n\nSurafel Lakew, Marcello Federico, Yue Wang, Cuong Hoang, Yogesh Virkar, Roberto Barra-Chicote, and Robert Enyedi. 2021a. Machine translation verbosity control for automatic dubbing. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).  \n\nSurafel M Lakew, Yogesh Virkar, Prashant Mathur, and Marcello Federico. 2021b. Isometric mt: Neural machine translation for automatic dubbing. arXiv preprint arXiv:2112.08682.  \n\nSurafel Melaku Lakew, Mattia Di Gangi, and Marcello Federico. 2019. Controlling the output length of neural machine translation. In Proc. IWSLT.  \n\nYinglu Li, Minghan Wang, Jiaxin Guo, Xiaosong Qiao, Yuxia Wang, Daimeng Wei, Chang Su, Yimeng Chen, Min Zhang, Shimin Tao, Hao Yang, and Ying Qin. 2022a. The HW-TSC's Offline Speech Translation System for IWSLT 2022 Evaluation. In Proceedings of the19thInternational Conference on Spoken Language Translation (IWSLT).  \n\nZongyao Li, JiaXin Guo, Daimeng Wei, Hengchao Shang, Minghan Wang, Ting Zhu, Zhanglin Wu, Zhengzhe Yu, Xiaoyu Chen, Lizhi Lei, Hao Yang, and Ying Qin. 2022b. HW-TSC's Participation in the IWSLT 2022 Isometric Spoken Language Translation. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT).  \n\nPierre Lison, Jorg Tiedemann, and Milen Kouylekov. 2018. OpenSubtitles2018: Statistical rescoring of sentence alignments in large, noisy parallel corpora. In Proceedings of theEleventh International ConferenceonLanguageResourcesandEvaluation(LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).  \n\nDan Liu, Mengge Du, Xiaoxi Li, Ya Li, and Enhong Chen. 2021. Cross attention augmented transducer networks for simultaneous translation.In Proceedings of the2021Conference onEmpirical Methods in Natural Language Processing, pages 39-55, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.  \n\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation. Transactions of the Associationfor Computational Linguistics,8:726-742.  \n\nShuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, Alexandre Muzio, Saksham Singhal, Hany Hassan Awadalla, Xia Song, and Furu Wei. 2021. DeltaLM: Encoder-decoder pre-training for language generation and translation by augmenting pretrained multilingual encoders. arXiv.  \n\nXutai Ma, Mohammad Javad Dousti, Changhan Wang, Jiatao Gu, and Juan Pino. 2020a. SIMULEVAL: An evaluation toolkit for simultaneous translation.In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 144-150, Online. Association for Computational Linguistics.  \n\nXutai Ma, Juan Pino, and Philipp Koehn. 2020b. SimulMT to SimulST: Adapting simultaneous text translation to end-to-end simultaneous speech translation.In Proceedings of the 1st Conference of the Asia-PacificChapter of the Associationfor ComputationalLinguisticsandthe1othInternationalJoint Conference on Natural Language Processing, pages 582-587, Suzhou, China. Association for Computational Linguistics.  \n\nDominik Machacek,  Jonas Kratochvil,Tereza Vojtechova, and Ondrej Bojar. 2019. A speech test set of practice business presentations with additional relevant texts. In Statistical Language and Speech Processing, pages 151-161, Cham, Switzerland. Springer Nature Switzerland AG.  \n\nEvgeny Matusov, Patrick Wilken, and Yota Georgakopoulou. 2019.  Customizing neural machine translation for subtitling.  In Proceedings of the FourthConferenceonMachineTranslation(Volume 1: Research Papers), pages 82-93, Florence, Italy. Association for Computational Linguistics.  \n\nJ. Niehues, R. Cattoni, S. Stiker, M. Negri, M. Turchi, T. Ha, E. Salesky, R. Sanabria, L. Barrault, L. Specia, and M. Federico. 2019. The IWSLT 2019 Evaluation Campaign. In Proceedings of the 16th InternationalWorkshoponSpokenLanguageTranslation (IWSLT 2019), Hong Kong, China.  \n\nJan Niehues. 2020. Machine translation with unsupervised length-constraints. In Proceedings of the 14th Conferenceof theAssociationforMachineTranslation in the Americas (AMTA 2020), pages 21-35.  \n\nJan Niehues, Roldano Cattoni, Sebastian Stiker, Mauro Cettolo, Marco Turchi, and Marcello Federico. 2018.The IWSLT 2018 Evaluation Campaign.In Proceedings of the 15th International Workshop on Spoken Language Translation (IWSLT 2018), pages 2-6, Bruges, Belgium.  \n\nXing Niu, Sudha Rao, and Marine Carpuat. 2018. Multi-task neural models for translating between styles within and across languages. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1008-1021, Santa Fe, New Mexico, USA. Association for Computational Linguistics.  \n\nMaria Nadejde, Anna Currey, Benjamin Hsu, Xing Niu, Marcello Federico, and Georgiana Dinu. 2022. CoCoA-MT: A dataset and benchmark for Contrastive Controlled MT with application to formality. In Findings of the Association for Computational Linguistics: NAACL 2022,Seattle,USA.Association for Computational Linguistics.  \n\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations.  \n\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015.Librispeech:An asr corpus based on public domain audio books. 2015IEEEInternationalConferenceonAcoustics, Speech and Signal Processing (ICASSP), pages 5206-5210.  \n\nKishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 4Oth annual meeting on association for computational linguistics. Association for Computational Linguistics.  \n\nKyubyong Park and Thomas Mulc. 2019. Css10: A collection of single speaker speech datasets for 10 languages. Interspeech.  \n\nMichael Paul. 2006.Overview of the IWSLT 2006 Evaluation Campaign. In Proceedings of the International Workshop on Spoken Language Translation, pages 1-15, Kyoto, Japan.  \n\nMichael Paul. 2008.Overview of the IWSLT 2008 Evaluation Campaign. In Proceedings of the International Workshop on Spoken Language Translation, pages 1-17, Waikiki, Hawaii.  \n\nMichael Paul. 2009. Overview of the IWSLT 2009 Evaluation Campaign. In Proceedings of the International Workshop on Spoken Language Translation, pages 1-18, Tokyo, Japan.  \n\nMichael Paul, Marcello Federico, and Sebastian Stiker. 2010.Overview of the IWSLT 2010 Evaluation Campaign. In Proceedings of the International Workshop on Spoken Language Translation, pages 3-27, Paris, France.  \n\nNgoc-Quan Pham, Tuan Nam Nguyen, Thai-Binh Nguyen, Danni Liu, Carlos Mullov, Jan Niehues, and Alexander Waibel. 2022. Efficient yet Competitive Speech Translation: FBK $@$ IWSLT2022.In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT).  \n\nPeter Polak, Ngoc-Quan Pham, Tuan Nam Nguyen, Danni Liu, Carlos Mullov, Jan Niehues, Ondrej Bojar, and Alexander Waibel. 2022. System for Simultaneous Speech Translation Task at IWSLT 2022. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT).  \n\nMaja Popovic. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392-395, Lisbon, Portugal. Association for Computational Linguistics.   \nMatt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186- 191, Belgium, Brussels. Association for Computational Linguistics.   \nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.   \nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. Comet: A neural framework for mt evaluation. arXiv preprint arXiv:2009.09025.   \nElijah Rippeth, Sweta Agrawal, and Marine Carpuat. 2022. Controlling Translation Formality Using Pretrained Multilingual Language Models. In Proceedings of the19th International Conference onSpoken Language Translation (IWSLT).   \nAnthony Rousseau, Paul Deléglise, and Yannick Esteve. 2014. Enhancing the ted-lium corpus with selected data for language modeling and more ted talks. In LREC.   \nM. Rouvier, G. Dupuy, P. Gay, E. Khoury, T. Merlin, and S. Meignier. 2013. An Open-source State-ofthe-art Toolbox for Broadcast News Diarization. In Proceedings of the Interspeech.   \nAshutosh Saboo and Timo Baumann. 2019. Integration of Dubbing Constraints into Machine Translation. In Proc. of WMT, pages 94-101, Florence, Italy. ACL.   \nElizabeth Salesky, Julian Mader, and Severin Klinger. 2021. Assessing evaluation metrics for speech-tospeech translation. In ASRU.   \nRamon Sanabria, Ozan Caglayan, Shruti Palaskar, Desmond Elliott, Loic Barrault, Lucia Specia, and Florian Metze. 2018. How2: a large-scale dataset for multimodal language understanding. In Proceedings of the Workshop on Visually Grounded Interaction and Language (ViGIL). NeurIPS.   \nAndrea Schioppa, David Vilar, Artem Sokolov, and Katja Filippova. 2021la. Controlling machine translation for multiple attributes with additive interventions. In Proceedings of the 2021 Conference on EmpiricalMethodsinNatural LanguageProcessing, pages 6676-6696, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.  \n\nAndrea Schioppa, David Vilar, Artem Sokolov, and Katja Filippova. 2021b. Controlling machine translation for multiple attributes with additive interventions. In Proceedings of the 2021 Conference on Empirical MethodsinNatural LanguageProcessing,pages 6676-6696.  \n\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016.  Controlling politeness in neural machine translation via side constraints. In Proceedings of the 2016 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, pages 35-40, San Diego, California. Association for Computational Linguistics.  \n\nAkshaya Vishnu Kudlu Shanbhogue, Ran Xue, ChingYun Chang, and Sarah Campbell. 2022. Amazon Alexa AI's System for IWSLT 2022 Ofline Speech Translation Shared Task.  In Proceedings of the 19th International Conference on Spoken Language Translation (IwSLT).  \n\nGabriel Synnaeve, Qiantong Xu, Jacob Kahn, Edouard Grave, Tatiana Likhomanenko, Vineel Pratap, Anuroop Sriram, Vitaliy Liptchinsky, and Ronan Collobert. 2020. End-to-end asr: from supervised to semi-supervised learning with modern architectures. InICML.  \n\nSho Takase and Naoaki Okazaki. 2019. Positional Encoding to Control Output Sequence Length. Proc. of NAACL.  \n\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2020. Multilingual translation with extensible multilingual pretraining and finetuning.  \n\nAmirhossein Tebbifakhr, Ruchit Agrawal, Matteo Negri, and Marco Turchi. 2018. Multi-source transformer with combined losses for automatic post editing. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 846- 852.  \n\nJorg Tiedemann, Santhosh Thottingal, et al. 2020. Opus-mt-building open translation services for the world. In Proceedings of the 22nd Annual Conferenceof theEuropeanAssociationfor Machine Translation. European Association for Machine Translation.  \n\nIoannis Tsiamas, Gerard I. Gallego, Carlos Escolano, José A. R. Fonollosa, and Marta R. Costa-jussa. 2022a.  Pretrained Speech Encoders and Efficient Fine-tuning Methods for Speech Translation: UPC at IWSLT 2022. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT).  \n\nIoannis Tsiamas, Gerard I. Gallego, José A. R. Fonollosa, and Marta R. Costa-jussa. 2022b. Shas: Approaching optimal segmentation for end-to-end speech translation.  \n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You Need. In Proceedings of NIPS 2017.   \nSebastian Vincent, Loic Barrault, and Carolina Scarton. 2022. Controlling Formality in Low-Resource NMT with Domain Adaptation and Re-Ranking: SLTCDT-UoS at IWSLT2022. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT).   \nYogesh Virkar, Marcello Federico, Robert Enyedi, and Roberto Barra-Chicote. 2021. Improvements to Prosodic Alignment for Automatic Dubbing. In ICASSP 2021- 2021IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7543-7574. ISSN: 2379-190X.   \nYogesh Virkar, Marcello Federico, Robert Enyedi, and Barra-Chicote Roberto. 2022. Prosodic alignment for offsreen automatic dubbing. arXiv preprint arXiv:2204.02530.   \nAditi Viswanathan, Varden Wang, and Antonina Kononova. 2019. Controlling formality and style of machine translation output using AutoML. In SIMBig, volume 1070 of Communications in Computer and Information Science, pages 306-313. Springer   \nChanghan Wang, Juan Pino, Anne Wu, and Jiatao Gu. 2020a. Covost: A diverse multilingual speech-totext translation corpus. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 4197-4203.   \nChanghan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. 2021. VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. In Proceedings of the 59th Annual Meting of theAssociation forComputational Linguistics and the 1lth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 993-1003, Online. Association for Computational Linguistics.   \nChanghan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, and Juan Pino. 2020b. fairseq s2t:Fast speech-to-text modeling with fairseq. arXiv preprint arXiv:2010.05171.   \nMinghan Wang, Jiaxin GUO, Yinglu Li, Xiaosong Qiao, Yuxia Wang, Zongyao Li, Chang Su, Yimeng Chen, Min Zhang, Shimin Tao amd Hao Yang, and Ying Qin. 2022. The HW-TSC's Simultaneous Speech Translation System for IWSLT 2022 Evaluation. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT).   \nPatrick Wilken and Evgeny Matusov. 2022. AppTek's Submission to the IWSLT 2022 Isometric Spoken Language Translation Task. In Proceedings of the 19thIntenationalConferenceonSpokenLanguage Translation (IWSLT).  \n\nLijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, Tie-Yan Liu, et al. 2021. Rdrop: regularized dropout for neural networks. Advances in Neural Information Processing Systems, 34.  \n\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mt5: A massively multilingual  pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934.  \n\nBrian Yan, Patrick Fernandes, Siddharth Dalmia, Jiatong Shi, Yifan Peng, Dan Berrebbi, Xinyi Wang, Graham Neubig, and Shinji Watanabe. 2022. CMU's IWSLT 2022 Dialect Speech Translation System. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT).  \n\nJinyi Yang, Amir Hussein, Matthew Wiesner, and Sanjeev Khudanpur. 2022. JHU IWSLT 2022 Dialect Speech Translation System Description.  In Proceedings of the 19th International Conference on SpokenLanguage Translation(IWSLT).  \n\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019. Paws-x: A cross-lingual adversarial dataset for paraphrase identification. arXiv preprint arXiv:1908.11828.  \n\nLei Yu, Laurent Sartran, Wojciech Stokowiec, Wang Ling, Lingpeng Kong, Phil Blunsom, and Chris Dyer. 2020. Better document-level machine translation with bayes’ rule. Transactions of the Association for Computational Linguistics, 8(0):346-360.  \n\nDaniel Zhang, Jiang Yu, Pragati Verma, Ashwinkumar Ganesan, and Sarah Campbell. 2022a. Improving Machine Translation Formality Control with Weakly-Labelled Data Augmentation and Post Editing Strategies. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT).  \n\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert.  arXiv preprint arXiv:1904.09675.  \n\nWeitai Zhang, Zhongyi Ye, Haitao Tang, Xiaoxi Li, Xinyuan Zhou, Jing Yang, Jianwei Cui, Dan Liu, Junhua Liu, and Lirong Dai. 2022b. The USTCNELSLIP Offline Speech Translation Systems for IWSLT 2022.In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT).  \n\nYuhao Zhang, Canan Huang, Chen Xu, Xiaoqian Liu, Bei Li, Anxiang Ma, Tong Xiao, and Jingbo Zhu. 2022c. The NiuTrans's Submission to the IWSLT22 English-to-Chinese Offline Speech Translation Task. In Proceedings of the 19th International Conference on SpokenLanguage Translation (IWSLT).  \n\nZiqiang Zhang and Junyi Ao. 2022. The YiTrans Neural Speech Translation Systems for IWSLT 2022 Offline Shared Task. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT).  \n\nQinpei Zhu, Renshou Wu, Guangfeng Liu, Xinyu Zhu, Xingyu Chen, Yang Zhou, Qingliang Miao, Rui Wang, and Kai Yu. 2022. The AISP-SJTU Simultaneous Translation System for IWSLT 2022.In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT).", "reference_length": 33972, "reference_token": 9321, "txt_length": 200709, "txt_token": 60920, "txt": "FINDINGS OF THEIWSLT 2022EVALUATION CAMPAIGN   \n\n\n<html><body><table><thead><tr><td><b>Antonios Anastasopoulos George Mason U.</b></td><td colspan=\"2\"><b>Loic Barrault  Le Mans University</b></td><td><b>Luisa Bentivogli FBK</b></td></tr></thead><tbody><tr><td>Marcely Zanon Boito</td><td>Ondrej Bojar</td><td>Roldano Cattoni</td><td>Anna Currey</td></tr><tr><td>Georgiana Dinu U. Avignon</td><td>Kevin Duh Charles U.</td><td>FBK Maha Elbayad</td><td>Clara Emmanuel AWS</td></tr><tr><td>AWS Yannick Esteve</td><td>JHU Marcello Federico</td><td>Meta Christian Federmann</td><td>Apple Souhir Gahbiche</td></tr><tr><td>Avignon University Hongyu Gong</td><td>AWS</td><td>Microsoft</td><td>Airbus</td></tr><tr><td>Avignon University</td><td>Roman Grundkiewicz</td><td>Barry Haddow</td><td>Benjamin Hsu</td></tr><tr><td>Meta</td><td>Microsoft</td><td>U. of Edinburgh Surafel M. Lakew</td><td>AWS Xutai Ma</td></tr><tr><td>Charles U.</td><td>Charles U. Vera Kloudova</td><td>AWS</td><td>JHU/Meta</td></tr><tr><td>Charles U.</td><td>Charles U.</td><td>AWS</td><td>JHU/Meta</td></tr><tr><td>Prashant Mathur</td><td>Paul McNamee</td><td>Kenton Murray</td><td>Maria Nadejde</td></tr><tr><td>AWS</td><td>JHU</td><td>JHU</td><td>AWS</td></tr><tr><td>Satoshi Nakamura</td><td>Matteo Negri</td><td>Jan Niehues</td><td>Xing Niu</td></tr><tr><td>Satoshi Nakamura NAIST</td><td>FBK</td><td>KIT</td><td>AWS</td></tr><tr><td>John Ortega</td><td> Juan Pino</td><td>Elizabeth Salesky</td><td>Jiatong Shi</td></tr><tr><td>Le Mans University</td><td>Meta</td><td>JHU</td><td>CMU</td></tr><tr><td>David Javorsky</td><td>Sebastian Stiker</td><td>Katsuhito Sudoh</td><td>Marco Turchi</td></tr><tr><td>Matthias Sperber</td><td>Zoom</td><td>NAIST</td><td>FBK</td></tr><tr><td>Apple</td><td>Zoom</td><td>NAIST</td><td>David Javorsky</td></tr><tr><td>Yogesh Virkar</td><td>Alex Waibel</td><td>Changhan Wang</td><td>Shinji Watanabe CMU</td></tr><tr><td>AWS</td><td>CMU/KIT</td><td>Meta</td><td>CMU</td></tr><tr><td>AWS</td><td>CMU/KIT</td><td>Meta</td><td>CMU</td></tr><tr><td>AWS</td><td>CMU/KIT</td><td>Meta</td><td>CMU</td></tr></tbody></table></body></html>  \n\n# Abstract  \n\nThe evaluation campaign of the 19th International Conference on Spoken Language Translation featured eight shared tasks: (i) Simultaneous speech translation, (ii) Offline speech translation, (i) Speech to speech translation, (iv) Low-resource speech translation, (v) Multilingual speech translation, (vi) Dialect speech translation, (vii) Formality control for speech translation, (vi) Isometric speech translation. A total of 27 teams participated in at least one of the shared tasks. This paper details, for each shared task, the purpose of the task, the data that were released, the evaluation metrics that were applied, the submissions that were received and the results that were achieved.  \n\n# 1 Introduction  \n\nThe International Conference on Spoken Language Translation (IWSLT) is the premier annual scientific conference for all aspects of spoken language translation. IWSLT is organized by the Special Interest Group on Spoken Language Translation, which is supported by ACL, ISCA and ELRA. Like in all previous editions (Akiba et al., 2004; Eck and Hori, 2005; Paul, 2006; Fordyce, 2007; Paul, 2008, 2009; Paul et al., 2010; Federico et al., 2011,2012; Cettolo et al.,2013,2014,2015, 2016, 2017; Niehues et al.,2018, 2019; Ansari et al., 2020; Anastasopoulos et al.,2021), this year's conference was preceded by an evaluation campaign featuring shared tasks addressing scientific challenges in spoken language translation.  \n\nThis paper reports on the 2022 IWSLT Evaluation Campaign, which offered eight shared tasks:  \n\n· Simultaneous speech translation, addressing low latency speech translation either streamed by a speech recognition (ASR) system or directly from the audio source. The translation directions for both conditions are: English to German, English to Japanese, and English to Mandarin Chinese.  \n\n· Offline speech translation, proposing speech translation of talks from English to German, English to Japanese, and English to Mandarin Chinese, using either cascade architectures or end-to-end models able to directly translate source speech into target text;  \n\n<html><body><table><thead><tr><td><b>Team</b></td><td><b>Organization</b></td></tr></thead><tbody><tr><td>AISP-SJTU</td><td>Aispeech and Shanghai Jiao Tong University, China (Zhu et al., 2022)</td></tr><tr><td>ALEXA AI</td><td>Amazon Alexa AI, USA (Shanbhogue et al., 2022)</td></tr><tr><td>ALEXA AI</td><td>Amazon Alexa AI, USA (Zhang et al., 2022a)</td></tr><tr><td>APPTEK</td><td>AppTek, Germany (Wilken and Matusov, 2022)</td></tr><tr><td>CMU</td><td>Carnegie Mellon University, USA (Yan et al., 2022)</td></tr><tr><td>CUNI-KIT</td><td>Charles University, Czech Republic, and KIT, Germany (Polak et al., 2022)</td></tr><tr><td>FBK</td><td>Fondazione Bruno Kessler, Italy (Gaido et al., 2022)</td></tr><tr><td>GMU</td><td>George Mason University, USA</td></tr><tr><td>HW-TSC</td><td>Huawei Translation Services Center, China (Li et al.; Wang et al.; Guo et al.; Li et al.)</td></tr><tr><td>JHU</td><td>Johns Hopkins University, USA (Yang et al., 2022)</td></tr><tr><td>KIT</td><td>Karlsruhe Institute of Technology, Germany (Pham et al., 2022; Polak et al., 2022)</td></tr><tr><td>MLLP-VRAIN</td><td>Universitat Politecnica de Valencia, Spain (Iranzo-Sanchez et al., 2022)</td></tr><tr><td>NA</td><td>Neural.AI, China</td></tr><tr><td>NAIST</td><td>Nara Institute of Science and Technology, Japan (Fukuda et al., 2022)</td></tr><tr><td>NIUTRANS</td><td>NiuTrans, China (Zhang et al., 2022c)</td></tr><tr><td>NUV</td><td>Navrachana University, India (Bhatnagar et al., 2022)</td></tr><tr><td>NEMO</td><td>NVIDIA NeMo, USA(Hrinchuk et al., 2022)</td></tr><tr><td>ON-TRAC</td><td>ON-TRAC Consortium, France (Boito et al., 2022b)</td></tr><tr><td>Uos</td><td>University of Sheffield, UK (Vincent et al., 2022)</td></tr><tr><td>TALTECH</td><td>Tallinn University of Technology, Estonia</td></tr><tr><td>UMD</td><td>University of Maryland, USA (Rippeth et al., 2022)</td></tr><tr><td>UPC</td><td>Universitat Politecnica de Catalunya, Spain (Tsiamas et al., 2022a)</td></tr><tr><td>USTC-NELSLIP</td><td> University of Science and Technology of China (Zhang et al., 2022b)</td></tr><tr><td>XIAOMI</td><td>Xiaomi AI Lab, China (Guo et al., 2022a)</td></tr><tr><td>Y1</td><td>Yi, China (Zhang and Ao, 2022)</td></tr></tbody></table></body></html>  \n\n· Speech to speech translation, investigating for the first time automatic translation of human speech in English into synthetic speech in German, either with cascaded or direct neural models.  \n\n· Low-resource speech translation, focusing on resource-scarce settings for translating input speech in Tamasheq into French text, and input speech in Tunisian Arabic into English text.  \n\n· Multilingual speech translation, analyzing the performance of multi-lingual versus bilingual translation models for the Offline speech translation tasks (discussed in the Offline task section);  \n\n· Dialect speech translation, addressing speech translation from Tunisian into English under three training data conditions: (i) only with limited dialect-specific training data (provided by the organizers); (ii) with also larger amount of related-language data (Modern Standard Arabic); (ii) with any kind of publicly available data.  \n\n· Formality control for SLT, addressing the formality level (formal vs. informal) in spoken language translation from English into German, Spanish, Hindi, Japanese, Italian and Russian. The task focuses in particular on zero-shot learning in multilingual models, given that for the last two directions no formality-annotated training data is provided.  \n\n· Isometric SLT, addressing the generation of translations similar in length to the source, from English into French, German and Spanish.  \n\nThe shared tasks attracted 27 participants (see Table 1) from both academic and industrial organizations. The following sections report on each shared task in detail, in particular: the goal and automatic metrics adopted for the task, the data used for training and testing data, the received submissions and the summary of results. Detailed results for some of the shared tasks are reported in a corresponding appendix.  \n\n# 2 Simultaneous Speech Translation  \n\nSimultaneous translation is the task of generating translations incrementally given partial text or speech input only. Such capability enables multilingual live communication and access to multilingual multimedia content in real time. The goal of this challenge, organized for the third consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency.  \n\n# 2.1 Challenge  \n\nParticipants were given two parallel tracks to enter and encouraged to enter all tracks:  \n\n· text-to-text:  translating the output of a streaming ASR system in real time from English to German, English to Japanese, and English to Mandarin Chinese. speech-to-text: translating speech into text in real time from English to German, English to Japanese, and English to Mandarin Chinese.  \n\nFor the speech-to-text track, participants were encouraged to submit systems either based on cascaded or end-to-end approaches. Participants were required to upload their system as a Docker image so that it could be evaluated by the organizers in a controlled environment.  We also provided example implementations and baseline systems for English-German speech-to-text translation, English-Japanese speech-to-text translation and English-Japanese text-to-text translation.  \n\n# 2.2  Data and Metrics  \n\nThe training and development data conditions were identical as in the Offline Speech Translation track. More details are available in $\\S3.2$  \n\nSystems were evaluated with respect to quality and latency. Quality was evaluated with the standard BLEU metric (Papineni et al., 2002) and, as a first trial this year, also manually. Latency was evaluated with metrics developed for simultaneous machine translation, including average proportion (AP), average lagging (AL) and differentiable average lagging (DAL, Cherry and Foster 2019), and later extended to the task of simultaneous speech translation (Ma et al., 2020b).  \n\nThe evaluation was run with the SIMULEvAL toolkit (Ma et al., 2020a). For the latency measurement of all systems, we contrasted computationaware and non computation-aware latency metrics. Computation-aware latency was also computed for text-to-text systems by taking into account the timestamps obtained from the ASR transcript generated by a streaming AsR model. The latency was calculated at the word level for English-German systems and at the character level for English-Japanese and English-Mandarin systems. BLEU was computed via sacrebleu (Post, 2018) (as integrated into SIMULEvAL) with default options for English-German, with the \"zh\" option for English-Mandarin and with the MeCab tokenizer for English-Japanese.  \n\nThe systems were ranked by the translation quality (measured by BLEU) in different latency regimes, low, medium and high. Each regime was determined by a maximum latency threshold measured by AL on the Must-C tst-COMMON set. The thresholds were set to 1000, 2000 and 4000 for English-German, 2500, 4000 and 5000 for English-Japanese and 2000, 3000 and 4000 for English-Japanese, and were calibrated by the baseline system. Participants were asked to submit at least one system per latency regime and were encouraged to submit multiple systems for each regime in order to provide more data points for latency-quality trade-off analyses. The organizers confirmed the latency regime by rerunning the systems on the tst-COMMON set.  \n\nThe systems were run on the test set segmented in three ways: the first segmentation, called gold, leverages the transcript to force align and segment the audio; the second and third segmentations, called Segmentation 1 and Segmentation 2, use a voice activity detection tool to segment the input audio without relying on the transcript.  \n\n# 2.3  Novelties for the Third Edition  \n\nText-to-text track moving closer to the speechto-text track  This year, we used the output of a streaming ASR system as input instead of the gold transcript. As a result, both text-to-text and speech-to-text systems can be ranked together for a given language pair.  \n\nLanguage pairs We added Mandarin Chinese as a target language, resulting in three pairs: EnglishGerman, English-Japanese and English-Mandarin.  \n\nHuman Evaluation and Human Interpretation Benchmark We added an experimental manual evaluation for the English-to-German speech-totext track as well as a human interpretation benchmark (Section 2.6.1). Independently, English-toJapanese speech-to-text track outputs were also manually scored, using the MQM setup, see Section 2.6.2.  \n\nSegmentation We reverted to the setting of the first edition where we only used segmented input in order to reduce the number of conditions and also because we noticed that existing latency metrics were not well adapted to long unsegmented input. However, recent improvements to the latency metrics (Iranzo-Sanchez et al., 2021) could allow to work with unsegmented input in the future.  \n\n# 2.4  Submissions  \n\nThe simultaneous task received submissions from 7 teams, the highest number to date. 5 teams entered the English-German speech-to-text track, 3 teams entered the English-Mandarin speechto-text track and 3 teams entered the EnglishJapanese speech-to-text track. For text-to-text, there were 3 teams for English-Mandarin, 1 team for English-German and 1 team for EnglishJapanese. Given that the majority of submissions were on the speech-to-text track, we are considering consolidating the task into speech-to-text only in future editions.  \n\nXIAOMI (Guo et al., 2022a) entered the textto-text track for English-Mandarin. Their model is transformer-based and leverages R-Drop and a deep architecture. Data augmentation methods include tagged backtranslation, knowledge distillation and iterative backtranslation. Simultaneous models use the multi-path wait-k algorithm. Finally, two error correction models are introduced in order to make the systems more robust to ASR errors.  \n\nMLLP-VRAIN (Iranzo-Sanchez et al., 2022) entered the speech-to-text track for EnglishGerman. They adopt a cascaded approach, with a chunking-based DNN-HMM ASR model, followed by a multi-path wait-k transformer-based MT model. Speculative beam search is employed at inference time.  \n\nHW-TSC (Wang et al., 2022) entered all tracks, i.e. speech-to-text and text-to-text for EnglishGerman, English-Japanese and English-Mandarin. Moreover, the authors contrasted cascaded and end-to-end methods for the speech-to-text track.  \n\nCUNI-KIT (Polak et al., 2022) entered the speech-to-text track for English-German, EnglishJapanese and English-Mandarin. They propose a method for converting an ofline model to a simultaneous model without adding modifications to the original model. The offline model is an end-to-end multilingual speech-to-text model that leverages a pretrained wav2vec 2.0 encoder and a pretrained mBART decoder. The input is broken down into chunks and decoding is run for each new chunk. Once a stable hypothesis is identified, that hypothesis is displayed. Various stable hypothesis detection methods are investigated.  \n\nAISP-SJTU (Zhu et al., 2022) entered the speech-to-text and text-to-text tracks for EnglishMandarin. Their model is based on an $\\mathrm{ASR}+\\mathrm{MT}$ cascade. They propose dynamic-CAAT, an improvement over CAAT (Liu et al., 2021) that uses multiple right context window sizes during training. The proposed method is compared to wait-k and multi-path wait-k. Data augmentation methods include knowledge distillation, tagged backtranslation and marking data with lowercased and non punctuated input with a special token.  \n\nFBK (Gaido et al., 2022) entered the speech-totext track for English-German with an end-to-end model. The authors’ main goal is to reduce computation requirements in order to democratize the task to more academic participants. First, they show how to avoid ASR encoder pretraining by using a conformer architecture and a CTC loss on top of an intermediate layer in the encoder. In addition, they use the same model for the offline task as for the simultaneous task. The auxiliary CTC loss is used to predict word boundaries and informs a wait-k policy. The latency is also controlled by the speech segment size. Finally, two data filtering methods based on negative log likelihood of an initial model and length ratio are investigated in order to make training more efficient.  \n\nNAIST(Fukuda et al.,2022） entered the speech-to-text  track for  English-German and English-Japanese. The proposed model applies decoding each time a new input speech segment is detected and to constrain the decoder on previously output predictions. An offline model is trained first and then finetuned on prefix pairs. The prefix pairs are extracted by translating prefixes and checking that the generated target is a prefix of the translation of the entire input. Prefixes with length imbalance are filtered out. An input segment boundary predictor is trained as a classifier by considering all prefixes and giving a positive labels to those prefixes that were extracted previously.  \n\n# 2.5 Results  \n\nResults are summarized in Figure 1, Figure 2 and Figure 3. We also present the text-to-text results on English-Mandarin in Figure 4. More details are available in the appendix. The results include both text-to-text systems and speech-to-text systems. When participants submitted both a text-totext system and a speech-to-text system, we retain the best system. The only participant with only a text-to-text system is XIAOMI and we can see that the system is at a disadvantage due to the noise introduced by the provided streaming ASR model. The ranking are consistent across the medium and high latency regime. However, for the low latency regime, we note a degradation from the FBK system and we observe that the NAIST system is robust to lower latency.  \n\n# 2.6 Human Evaluation  \n\nWe conducted a human evaluation for English-to.   \nGerman and English-to-Japanese independently.  \n\n# 2.6.1  English-to-German  \n\nFor English-to-German, the human evaluation was inspired by Javorsky et al. (2022). This evaluation examined (1) the best system from each latency regime selected by BLEU score, and (2) transcription of human interpretation by a professional English-German interpreter (German native speaker, certified German conference interpreter and sworn translator and interpreter) in February 2022. The interpreting was carried out remotely and transcribed by students of German for Intercultural Communication at the Institute of Translation Studies, Charles University, Faculty of Arts.2  \n\nThe English-to-German task used two parts of the test set: (1) the Common part is used as the blind test set in the automatic evaluation and also in the Offline speech translation task, and (2) the Non-Native part comes from IWSLT 2019 NonNative Translation Task.  \n\nDetails of the human evaluation are provided in Section A.1.1 of the Appendix and results are shown in Table 18. BLEU scores correlate very well with the human judgements for both parts of the test set, as can be seen in Figure 5.  \n\nThe Common part of the test set is kept confidential for future use. For the Non-Native part, we release system outputs as well as manual judgements on the corresponding IWSLT page.  \n\n# 2.6.2 English-to-Japanese  \n\nFor English-to-Japanese, we used JTF Translation Quality Evaluation Guidelines (JTF, 2018) based on Multidimensional Quality Metrics (MQM). We chose four systems for the evaluation and asked a professional translator to evaluate the translations for one talk in the blind test set. We followed the error weighting by a previous study (Freitag et al., 2021a) to calculate error scores. Details of the human evaluation are provided in A.1.2 in Appendix.  \n\nThe results are shown in Table 16, and we can find the error scores positively correlate with BLEU.  \n\n# 2.7 Future Editions  \n\nPossible changes to future editions include:  \n\n· changing the latency metric in order to support long unsegmented input. · extending the task to support speech output. removing the text-to-text track in order to consolidate tracks.  \n\n# 3  Offline Speech Translation  \n\nOffline speech translation, defined in various forms over the years, is one of the speech tasks with the longest tradition at the IWSLT campaign. This year,4 it focused on the translation of English audio data extracted from TED talks? into text in one of the three target languages comprising the 2022 sub-tasks, i.e. German, Japanese, and Mandarin Chinese.  \n\n![](images/c647c48844b5b7f6d04c985c90272a1fed75b6c4ba2036619eef831df7635694.jpg)  \nFigure 1: Latency-quality tradeoff curves for English-German.  \n\n![](images/2997d522e8bbb6576957ab96bcccf64bff9d5f3d18b63a3a87795bd8d29e8517.jpg)  \nFigure 2: Latency-quality tradeoff curves for English-Japanese.  \n\n![](images/55ef91eb9632514d3f6d301d3804551d3dc012beb12c1d80140be969b623cd3a.jpg)  \nFigure 3: Latency-quality tradeoff curves for English-Mandarin  \n\n![](images/8093b5727aa670f7a805671ee4f5b03f3844ce0c44a60d12c1901cafb50c9376.jpg)  \nFigure 4: Latency-quality tradeoff curves for English-Mandarin (text-to-text track).  \n\n# 3.1 Challenge  \n\nIn recent years, offline speech translation (ST) has seen a rapid evolution, characterized by the steady advancement of direct end-to-end models (building on a single neural network that directly translates the input audio into target language text) that were able to significantly reduce the performance gap with respect to the traditional cascade approach (integrating ASR and MT components in a pipelined architecture). In light of the IWSLT results of the last two years (Ansari et al., 2020; Anastasopoulos et al., 2021) and of the findings of recent work attesting that the gap between the two paradigms has substantially closed (Bentivogli et al., 2021), also this year a key element of the evaluation was to set up a shared framework for their comparison. For this reason, and to reliably measure progress with respect to the past rounds, the general evaluation setting was kept unchanged.  \n\nOn the architecture side, participation was allowed both with cascade and end-to-end (also known as direct) systems. In the latter case, valid submissions had to be obtained by models that: i) do not exploit intermediate symbolic representations (e.g., source language transcription or hypotheses fusion in the target language), and $i i$ )rely on parameters that are all jointly trained on the end-to-end task.  \n\nOn the test set provision side, also this year participants could opt for processing either a precomputed automatic segmentation of the test set or a version of the same test data segmented with their own approach. This option was maintained not only to ease participation (by removing one of the obstacles in audio processing) but also to gain further insights into the importance of properly segmenting the input speech. As shown by the results of recent IWSLT campaigns, effective preprocessing to reduce the mismatch between the provided training material (often “clean\"” corpora split into sentence-like segments) and the supplied unsegmented test data is in fact a common trait of top-performing systems.  \n\nConcerning the types of submission, also this year two conditions were offered to participants: constrained, in which only a pre-defined list of resources is allowed, and unconstrained.  \n\nMultiple submissions were allowed, but participants had to explicitly indicate their “primary\" (one at most) and “contrastive\" runs, together with the corresponding type of system (cascade/end-to-end), training data condition (constrained/unconstrained), and test set segmentation (own/given).  \n\nNovelties of the 2022 offline ST task. Within this consolidated overall setting, the organization of this year's task took into consideration new emerging challenges, namely: i) the availability of new data covering more language directions, $i i_{\\cdot}$ the development of new and gigantic pre-trained models, and ii) the need for more accurate evaluations. Accordingly, three main differences with respect to previous editions characterize this year's edition:  \n\n![](images/6510e93561ed95aea948ce0cf158639f0d60df856a9734f172b719e49e2b4c71.jpg)  \nFigure 5: Relation between automatic and manual scoring for English-to-German simultaneous translation on the Common and Non-native part of the test set in the three latency regimes.  \n\n· To measure systems performance in different language settings, two new target languages have been added, extending the number of offline ST sub-tasks to three: English-German (the traditional one), English-Chinese, and English-Japanese.  \n\n· To understand the effect of exploiting popular pre-trained models in state-of-the-art ST systems, participants were given the possibility to exploit some of them in addition to the allowed training resources for the constrained condition.  \n\n· To shed light on the reliability of system ranking based on automatic metrics, and to align our task with other evaluation campaigns (e.g. $\\mathrm{WMT}^{6\\cdot}$ ), the outputs of all the submitted primary systems have been manually evaluated by professional translators. On this basis, a new ranking based on direct human assessments was also produced.  \n\n# 3.2  Data and Metrics  \n\nTraining and development data. Also this year, participants had the possibility to train their systems using several resources available for ST, ASR and MT.  \n\nTo extend the language directions covered by the offline task, new data was selected from the English-Chinese and English Japanese sections of the MuST-C V2 corpus?. For both languages, they include training, dev, and test (Test Common), in the same structure of the MuST-C V2 EnglishGerman section (Cattoni et al., 2021) used last year.  \n\nBesides the two new language directions of MuST-C V2, also this year the allowed training corpora include:  \n\n· MuST-C V1 (Di Gangi et al., 2019);   \n· CoVoST (Wang et al., 2020a);   \n· WIT3 (Cettolo et al., 2012) ;   \n· Speech-Translation TED corpus8;   \n· How2 (Sanabria et al., 2018)9;   \n· LibriVoxDeEn (Beilharz and Sun, 2019)10;   \n· Europarl-ST (Iranzo-Sanchez et al., 2020);   \n· TED LIUM v2 (Rousseau et al., 2014) and v3 (Hernandez et al., 2018);   \n· WMT $2019^{11}$ and $2020^{12}$   \n· OpenSubtitles 2018 (Lison et al., 2018);   \n·Augmented LibriSpeech (Kocabiyikoglu et al., 2018)13   \n· Mozilla Common Voice14 ;   \n· LibriSpeech ASR corpus (Panayotov et al., 2015);   \n· VoxPopuli15 (Wang et al., 2021).  \n\nThe only addition over last year is the VoxPopuli dataset.  \n\nSimilarly to the training data, participants were also provided with a list of pre-trained models that can be used in the constrained condition. The list includes:  \n\n·Wav2vec $2.0^{16}$ (Baevski et al., 2020a);   \n· Hubert17;   \n· MBART18 (Liu et al.,2020);   \n· MBART $50^{19}$ (Tang et al., 2020);   \n· $\\mathbf{M}2\\mathbf{M}100^{20}$ (Fan et al., 2021);   \n· Delta ${\\cal L}{\\bf M}^{21}$ (Ma et al., 2021);   \n$\\mathrm{T}5^{22}$ (Raffel et al., 2020).  \n\nThe development data allowed under the constrained condition consist of the dev set from IWSLT 2010, as well as the test sets used for the 2010,2013,2014,2015,2018,2019,and  \n\n2020 IWSLT campaigns. Using other  training/development resources was allowed but, in this case, participants were asked to mark their submission as unconstrained.  \n\nTest data. For each language direction, namely En-De, En-Zh and En-Ja, a new test set was created. The new test sets were built from 17 TED talks for En-De, 16 for En-Zh and 13 for En-Ja. None of these talks is included in the current public release of MuST-C. Similar to last year, participants were presented with the option of processing either an unsegmented version (to be split with their preferred segmentation method) or an automatically segmented version of the audio data. For the segmented version, the resulting number of segments is 2,059 (corresponding to about $3\\mathrm{h}34\\mathrm{m}$ of translated speech from 17 talks) for En-De, 1,874 $(3\\mathrm{h}17\\mathrm{m})$ for En-Zh and 1,758 $(2\\mathrm{h}38\\mathrm{m})$ for En-Ja. The details of the three test sets are reported in Table 2.  \n\nTable 2: Statistics of the official test sets for the ofline speech translation task (tst2022).   \n\n\n<html><body><table><thead><tr><td><b>Talks</b></td><td><b>Sentences</b></td><td><b>Duration</b></td></tr></thead><tbody><tr><td>En-De 17</td><td>2,059</td><td>3h34m</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>Lang</td><td>16</td><td>16</td></tr><tr><td>Lang</td><td>1,874</td><td>3h17m</td></tr><tr><td>Lang</td><td>16</td><td>16</td></tr><tr><td>En-Zh</td><td>16</td><td>16</td></tr><tr><td>En-Zh</td><td>16</td><td>16</td></tr><tr><td>Lang</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>En-Ja</td><td>1,768</td><td>16</td></tr><tr><td>Lang</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>2h38m</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>13</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr><tr><td>16</td><td>16</td><td>16</td></tr></tbody></table></body></html>  \n\nTo measure technology progress with respect to last year's round, participants were asked to process also the undisclosed 2021 En-De test set that, in the segmented version, consists of 2,037 segments (corresponding to about 4.1 hours of translated speech from 17 talks).  \n\nMetrics. The systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references.  This similarity is measured using the BLEU metric, computed with SacreBLEU (Post, 2018) with default settings.  \n\nSimilar to the 2021 edition,we consider two different types of target-language references, namely:  \n\n· The original TED translations. Since these references come in the form of subtitles, they are subject to compression and omissions to adhere to the TED subtitling guidelines.23  \n\nThis makes them less literal compared to standard, unconstrained translations;  \n\n· Unconstrained translations. These references were created from scratch24 by adhering to the usual translation guidelines. They are hence exact translations (i.e. literal and with proper punctuation).  \n\nTable 3: Statistics of the official test set for the offline speech translation task (tst2022). \\* statistics are reported in terms of characters for Chinese and Japanese.   \n\n\n<html><body><table><thead><tr><td><b>Lang Pair</b></td><td><b>Lang</b></td><td><b>Sentences</b></td><td><b>Words</b></td></tr></thead><tbody><tr><td></td><td>En</td><td>2,059</td><td>39,814</td></tr><tr><td>En-De</td><td>De - Orig</td><td>2,059</td><td>32,361</td></tr><tr><td></td><td>De - Uncon.</td><td>2,059</td><td>36,655</td></tr><tr><td></td><td>En</td><td>1,874</td><td>36,736</td></tr><tr><td>En-Zh</td><td>Zh - Orig</td><td>1,874</td><td>63,876*</td></tr><tr><td></td><td>Zh - Uncon.</td><td>1,874</td><td>64,767*</td></tr><tr><td></td><td>En</td><td>1,768</td><td>30,326</td></tr><tr><td>En-Ja</td><td> Ja - Orig</td><td>1,768</td><td>62,778*</td></tr><tr><td></td><td> Ja - Uncon.</td><td>1,768</td><td>74,637*</td></tr></tbody></table></body></html>  \n\nAs shown in Table 3, the different approaches to generate the human translations led to significantly different references. For En-De, while the unconstrained translation has a similar length (counted in words) compared to the corresponding source sentence, the original is $\\sim\\!15\\%$ shorter in order to fulfil the additional constraints for subtitling. For En-Ja and En-Zh, it is difficult to make a proper comparison with the source data as the Japanese and Chinese data are counted in characters while the English one is counted in words. However, it is evident that the unconstrained translations have more characters than the original ones following a similar trend seen for En-De.  \n\nBesides considering separate scores for the two types of references, results were also computed by considering both of them in a multi-reference setting. Similar to last year, the submitted runs were ranked based on case-sensitive BLEU calculated on the test set by using automatic re-segmentation of the hypotheses based on the reference translations by mwerSegmenter.25  \n\n# 3.3 Submissions  \n\nOverall, 10 different teams submitted at total of 29 primary submissions. For the English-to-German task 8 teams submitted 10 runs, for English-toChinese 9 teams 11 runs and for the English-toJapanese task 6 teams participated with 8 primary runs. For all the language pairs two teams submitted a primary cascaded and a primary end-toend system. Overall, most teams participated in all 3 language directions, partly with individual systems and partly with multi-lingual systems.  \n\nWe encouraged the submission of end-to-end as well as cascaded systems. Several participants experimented with both types of architectures and in two instances primary end-to-end and cascaded systems were submitted. In total, we had 4 cascaded and 6 end-to-end submissions for the English-to-German tasks, 5 cascaded and 6 endto-end for English-to-Chinese and 3 cascaded and 5 end-to-end submissions for English-to-Japanese.  \n\nOne additional change in this year's evaluation campaign was that the use of a list of pre-trained models. Most of the teams investigated this research direction and integrated pre-trained models into their final submission. Both, the integration of pre-trained speech models as well as text models were successfully investigated. In addition, several teams focused on audio segmentation approaches.  \n\n·HW-TSC (Li et al., 2022a) submission is built in the cascaded form, including three types of ASR models and one type of translation model. Before performing the speech translation, the LIUM SpkDiarization tool (Rouvier et al., 2013), provided to the participants, was used to cut off the test set wav files into segments. For the ASR part, they use conformer, U2T-transformer and U2-conformer, and all of them are trained on a combination of the MUST-C, COVOST, LibriSpeech, TedLIUM datasets. The system is adapted to the TED domain using domain tags. For the translation model, they trained a Transformer-large on the WMT21- news dataset, and fine-tuned it on the MUSTC and IWSLT datasets. The output of the different ASR models has been re-ranked and the best combination selected as primary submission.  \n\n· FBK (Gaido et al., 2022) focused in their submission on reducing model training costs without sacrificing translation quality. They submitted an end-to-end  speech  translation system model using the conformerarchitecture without pre-trained models. The model is trained on specifically filtered and resegmented parts of the corpus. The final submission is an ensemble of several models.  \n\n· USTC-NELSLIP (Zhang et al., 2022b) submitted primary end-to-end and cascaded systems for all three language directions which ensemble several individual models. In the cascaded condition, the ASR models combined transformer and conformer architectures and the MT models are trained on synthetic data to be robust against ASR errors. The end-to-end models also combine conformer and transformer encoders and are partly initialized from ASR systems.  \n\n· ALEXA AI (Shanbhogue et al., 2022) submitted an end-to-end speech translation system that leverages pretrained models and cross modality transfer learning for all three language directions. They used encoders for text as well as speech and initialized the models using pretrained speech and text models. The work mainly focused on improving knowledge transfer. In addition, a special focus was put on segmentation strategies.  \n\n· N1UTRANS (Zhang et al., 2022c) submission to the English-Chinese track is an end-to-end speech translation system composed of different pre-trained acoustic models and machine translation models. The models were combined by two kinds of adapters and the final submission is an ensemble of three individual speech translation models.  \n\n· UPC (Tsiamas et al., 2022a) submission is an end-to-end speech translation model which combines pre-trained speech encoder and text decoder for all the three language directions of the task. As a speech encoder wav2vec 2.0 and HuBERT are used, both already finetuned on English ASR data. As a text decoder an mBART50 fine-tuned on multilingual MT (one-to-many) is used. These two modules are coupled with a length adaptor block and in the end-to-end training, additional adapters are trained. For the final submission several initial models are combined.  \n\n·KIT (Pham et al., 2022) submitted an endto-end system using pre-trained audio and text models to all the three language directions. The systems were trained on the initial training data as well as on additional synthetic data. Furthermore, sentence segmentation strategies were investigated. The final submission is an ensemble of several models.  \n\n·Y1 (Zhang and Ao, 2022)) submitted primary end-to-end and cascaded systems for all three language directions using large-scale pre-trained models. Starting from pre-trained speech and language models, the authors investigated a multi-stage pre-training and the use of a task dependent fine-tuning for ASR, MT and speech translation. In addition, various efforts to perform data preparation was carried out. Finally, an ensemble of several models was submitted as the primary submission.  \n\n· NEURAL.AI submitted a cascaded speech translation system to the English-to-Chinese speech translation task.  The ASR system consists of a conformer encoder and a transformer decoder. The MT system is a finedtuned deltalm-base.  \n\n# 3.4 Results  \n\nThis year, the submissions to the IWSLT Offline translation task were not only evaluated using automatic metrics, but also a human evaluation was carried out. All results are shown in detail in the appendix.  \n\n# 3.4.1  Automatic Evaluation  \n\nThe results for each of the language pairs are shown in the tables in section A.5. For Englishto-German we show the results for this year's test set (Table 19) as well as for last year's test set (Table 20). This enables us to also show the progress compared to last year. For the two new language pairs, English-to-Chinese (Table 21) and Englishto-Japanese (Table 22), we present the numbers of this year's test set.  \n\nFirst, all the submissions are distributed in a range from 4 to 7 BLEU points. The only exception is Chinese, where one system performed significantly worse than the others. This large  \n\nBLEU score range is significantly different than last year's ranking where all the submissions were close to each other. The overall 2022 ranking for the English-German task is quite similar to the ranking obtained for the test set 2021.  \n\nProgress  The comparison between this year's submissions and last year's submission on test set 2021 in the English-to-German task allows us to measure the progress since last year. As shown in Table 20, 7 out of 9 systems performed better than the best system last year. This year's best system is 4 BLEU points better than last year's system. So, we are seeing a clear improvement in translation quality. One possible reason for the improvement is the additional allowed resources (the VoxPopuli dataset and the pre-trained models). However, also teams not using the additional resources (FBK) outperformed last year's system.  \n\nEnd-to-end vs. cascade As in previous years, we received cascaded and end-to-end submissions. While in the last years, end-to-end systems were able to close the gap to cascaded systems, we do not see this trend since last year. In this year, for all conditions, a cascaded system performed best. Furthermore, when looking at the participants who submitted both, a primary end-to-end and a primary cascaded system, in 6 out of 8 times, the cascaded system performed better than the end-to-end system. Whether this is partly due to the integration of pre-trained models has to be evaluated in further experiments.  \n\nPre-trained models  It is difficult to measure the impact of pre-trained models since there is no participant submitting both, a translation system with and without pre-trained models. However, there are some indications of the usefulness of pre-trained models. First, nearly all participants submitted systems with pre-trained models. Typically, these are audio encoders like wav2vec or Hubert for the encoder and text models like mBart for the decoder. Secondly, all winning systems are using this technology. And finally, we see large gains in translation quality compared to last year, where this technique was not allowed. Consequently, these models seem to be an interesting knowledge source. However, it should be noted that the models are rather large and therefore can also be a limiting factor for teams to participate in the evaluation campaign.  \n\nMulti-lingual models For the first time, since several years, this year's edition of the offline task included several language directions. Interestingly, this did not lead to a partition of participants into different language pairs, but most participants submitted translations for all three language pairs. While the best performing systems were individually optimized for each language, we also see multilingual models submitted to the tasks. Especially, the integration of pre-trained models, which are typically multi-lingual, made it easier to build translation systems for all three conditions. While the ranking between the languages is not the same, it is still very similar. This indicates that a good system in one language direction typically will also result in good performance in the other directions. While the amount of training resources is at least comparable, this is interesting since the languages are rather different.  \n\n# 3.4.2 Human Evaluation  \n\nWe conducted a human evaluation of primary submissions based on a random selection of 1,350 segments from the test set of each language pair. Human graders were asked for a direct assessment, expressed through scores between 0 and 100. To minimize the impact of errors in the automatic segmentation, graders were also shown system output for the previous and the following sentence and asked not to let segmentation issues infuence their scores. We used Appraise to compute system scores, statistical significance, and rankings. Details of the human evaluation are provided in Section A.2.  \n\nAs for the results (Tables 23, 24, 25), the ranking of systems matches that of the automatic evaluation when accounting for statistical significance for English to German and English to Chinese, but not for English to Japanese. The scores indicate clear differences between systems (that usually persist across language pairs), but also significant overlap in the translation quality of different systems.  \n\n# 3.4.3  Final remarks  \n\nBy inspecting this year's results, we can make three final observations.  \n\nThe first is about the relation between the cascade and end-to-end technology. According to the automatic metrics, and in contrast to last year's campaign, cascade systems achieve the best performance in all the language directions. However, human evaluation does not validate automatic results for En-De and En-Jp, where the best cascade and end-to-end systems are in the same cluster and not statistically different. This outcome further confirms the findings of Bentivogli et al. (2021) for En-De but extends them to one new language pair out of the two addressed (En-Jp and En-Zh). For this reason, more investigation about the two technologies is still needed and will be further carried out in the next editions of this task.  \n\nThe other observation is about the introduction of human evaluation in our task. While largely confirming the rankings obtained with automatic metrics, it provides the most reliable picture of the real differences between the systems, showing that they are not so evident as they were detected by automatic metrics. Given the importance of human evaluation to accurately assess state-of-theart technologies, we plan to rely on it also in the next edition of the task.  \n\nThelastobservationisaboutthenoticeable jump in performance on the progress test set compared to last year's systems. All the current systems have been able to outperform the best 2021 system, with gains reaching up to 6 BLEU score points when using multiple references. While it is difficult to ascribe this improvement to a single factor, it is worth to note that the main change in this year's task setting is the availability of pretrained models. We suggest that these models can have an important role in the final translation quality, and we plan to further investigate their usefulness in the next edition.  \n\n# 4  Speech to Speech Translation  \n\nSpeech-to-speech translation is the task of translating audio input in a language into audio output in a target language. In the offline setting, systems are able to take into account an entire input audio segment in order to translate, similar to a consecutive interpreter. This is in contrast to streaming or simultaneous settings where systems are only exposed to partial input as in simultaneous interpretation. The goal of this task is to foster the development of automatic methods for ofline speechto-speech translation.  \n\n# 4.1 Challenge  \n\nParticipants built speech-to-speech translation systems from English into German using any possible method, for example with a cascade system (speech recognition $^+$ machine translation $^+$ speech synthesis or end-to-end speech-to-text translation $^+$ speech synthesis) or an end-to-end or direct system.  \n\n# 4.2  Data and Metrics  \n\nData. This task allowed the same training and testing data from the Offline task on EnglishGerman speech-to-text translation to more directly compare Offline S2T and S2ST systems. More details are available in $\\S3.2$ .We note that while the evaluation data between the two tasks was the same, it was not directly parallel, as different sentence-level segmentation was used. For this task, gold sentence segmentation was used. This means that scores are not directly comparable between the two tasks, though we do evaluate a direct comparison for a subset of submissions.  \n\nIn addition to the Offline task data, the following training data was allowed to help build German TTS and English-German speech-to-speech models:  \n\n·Synthesized MuST-C: Target speech for the German target text of MuST-C V2 (Cattoni et al., 2021) which was synthesized for this task using a VITS model (Kim et al., 2021) trained on the German portion of CSS10.  \n\nCSS10: A single-speaker German TTS dataset (Park and Mulc, 2019)  \n\n· Pretrained German TTS model: A pretrained German VITS (Kim et al., 2021) TTS model to facilitate cascaded models and dual submission with the Offline task.  \n\nWe note that several datasets allowed for the Offine task including Common Voice  (Ardila et al., 2020) and LibriVoxDeEn (Beilharz and Sun, 2019) also contain multi-speaker German speech and text data, enabling their use for this task as well.  \n\nMetrics. While we evaluate with both automatic and human evaluation scores, systems were ranked according to the human evaluation.  \n\nAutomatic metrics. To automatically evaluate translation quality, the speech output was automatically transcribed with an ASR system (Conneau et al., 2021),26 and then BLEU (Papineni et al., 2002) was computed between the generated transcript and the human-produced text reference. Previous work (Salesky et al., 2021) has shown evaluating synthesized speech with ASR and chrF can be more robust than ASR and BLEU, so we additionally score with chrF (Popovic, 2015). All scores were computed using SacreBLEU (Post, 2018).  \n\nHuman evaluation. Output speech translations were evaluated with respect to translation quality and speech quality.  \n\n· Translation quality: Bilingual annotators were presented with the source audio and the target audio, and gave scores on the translation quality between 1 and 5. There were 3 annotators per sample and we retained the median score.  \n\n· Output speech quality: In addition to translation quality (capturing meaning), the quality of the speech output was also humanevaluated along three dimensions: naturalness (voice and pronunciation), clarity of speech (understandability), and sound quality (noise and other artifacts). These axes are more fine-grained than the traditional overall MOS score.  \n\nThe detailed guidelines for output speech quality were as follows:  \n\n· Naturalness: Recordings that sound humanlike, with natural-sounding pauses, stress, and intonation, should be given a high score. Recordings that sound robotic, flat, or otherwise unnatural should be given a low score.  \n\n· Clarity of speech: Recordings with clear speech and no mumbling and unclear phrases should be given a high score. Recordings with a large amount of mumbling and unclear phrases should be given a low score.  \n\n· Sound quality: Recordings with clean audio and no noise and static in the background should be given a high score. Recordings with a large amount of noise and static in the background should be given a low score.  \n\n# 4.3  Submissions  \n\nWe received submissions from four teams, one of which was withdrawn due to submission errors.  \n\nWe also compare two submissions to the Offline task which were retranslated with the gold segmentation and synthesized using the TTS model provided by the organizers.  \n\nMLLP-VRAIN (Iranzo-Sanchez et al., 2022) submitted a cascaded system of separate ASR, MT, and TTS models. They use the same ASR and MT models developed for the Simultaneous ST task, with a less restrictive pruning setup to allow a wider search space for the ASR model and without the multi-path wait-k policy used there for MT. They include a speaker-adaptive module in their TTS system to produce a high quality voice that mimics voice characteristics of the source speaker. Their TTS model is a typical two-stage approach, combining a Conformer-based model (Gulati et al., 2020) to produce spectrograms with a multi-band UnivNet (Jang et al., 2021) model to then produce speech waveforms. They include a speaker encoder, a modified ResNet-34 residual network architecture (He et al., 2016) from (Chung et al., 2018) more widely used for speaker recognition tasks and trained on the TED-LIUM v3 dataset (Hernandez et al., 2018), which is combined with the Conformer output to produce more faithful voices.  \n\nHW-TSC (Guo et al., 2022b) submitted a cascaded system of separate ASR, MT, and TTS models. The ASR model ensembles Conformer (Gulati et al., 2020) and S2T-Transformer models (Synnaeve et al., 2020), and is cleaned with the U2 model. The MT model is pretrained on news corpora and finetuned to MuST-C and IWSLT data, with context-aware MT reranking inspired by Yu et al. (2020). They use the provided pretrained VITS TTS model. They use domain tags for each training data source to improve performance. They submitted one primary and three contrastive systems, which ablate individual components. Contrastive1 includes the ASR ensemble but removes reranking for both ASR and MT. Constrastive2 uses the Conformer ASR model only without reranking. Contrastive3 uses the S2TTransformer ASR model only without reranking.  \n\nUPC (Tsiamas et al.,2022a) submitted a cascaded system, extending their direct speech-totext model submitted to the Offline task with the provided German VITS TTS model for S2ST. Their final speech-to-text model combined initialization using HuBERT models, LayerNorm and Attention finetuning (LNA), and knowledge distillation from mBART. For both tasks, they used SHAS segmentation during training (Tsiamas et al., 2022b) for consistent improvements. Data filtering and augmentation were also key aspects of their submission.  \n\nA direct S2ST model built upon the VITS synthesis model was submitted but withdrawn due to errors.  \n\n# 4.4   Results  \n\nResults as scored by automatic metrics are shown in Table 26 and human evaluation results are shown in Table 27 and Table 28 in the Appendix.  \n\nOverall results. From the automatic metric perspective, MLLP-VRAIN obtains the highest ASR-BLEU score, followed by HW-TSC and UPC. Note that there is a disagreement between BLEU and chrF ranking for MLLP-VRAIN and HW-TSC. For human evaluation along the speech quality perspective, MLLP-VRAIN obtains a higher quality system compared to the other systems. This is expected as HW-TSC, UPC and the reference system all use the default provided TTS system. It is interesting to note that for these 3 systems, all scores are close to each other on speech quality even though the output content is different. We thus hypothesize that speech quality is orthogonal to translation quality. Finally, for human evaluation along the translation quality perspective, HW-TSC obtained the highest score, followed by MLLP-VRAIN and UPC. Note that this ranking is consistent with the ASR-chrF but not with ASR-BLEU. Surprisingly, the reference system obtains the lowest score. We hypothesize that this may be due to misalignments in the test set between the source audio and the source transcript (rather than between the source transcript and the target translation since the target translations were generated by human translator given the source text transcripts). In addition, we found variance between raters, which could account for this. We will go through a review process for those instances prior to releasing the human judgments.  \n\nS2ST Approaches. This year, all systems except the withdrawn submission were cascaded systems, with two systems adopting an $\\mathrm{ASR}+\\mathrm{MT}+$ TTS approach and one system adopting an endto-end $\\mathrm{S2T+TTS}$ approach. This does not allow us to draw meaningful conclusions on various approaches to the task and we will encourage more direct and/or end-to-end submissions in future editions.  \n\nAutomatic scoring.   To compute automatic metrics, we apply several steps, which may affect quality assessment. The final row of Table 26 shows chrF and BLEU computed on normalized text translations and references; normalizing system output and references reduces scores slightly, by 0.8 BLEU and 0.3 chrF. The larger potential for degradation comes from the synthesis (TTS) and transcription (ASR) roundtrip, which we can directly evaluate the effects of using the reference translations and cascaded systems. Synthesizing the gold reference translation and transcribing with the wav2vec2-large-xlsr-53-german ASR model gives a BLEU score of 68.46 and chrF of 88.78 - degradation of 31.5 BLEU and 11.2 chrF. This confirms errors are introduced by imperfect TTS and ASR models when scoring S2ST systems in this way, and also shows the greater impact of slight variations introduced by TTS and ASR on word-level BLEU than on chrF, which does not necessarily reflect differences in human evaluation (see results in Section B.3). When synthesizing and transcribing machine translation output, there is also degradation in metric scores compared to directly evaluating the text output, but it is considerably smaller. For example, the FBK Offline submission $^+$ TTS scores are reduced by 6 BLEU and 4.6 chrF. We see comparing the FBK, KIT, and UPC submissions here, which were all also submitted to the Offline task as speech-to-text systems and then the translations synthesized with the same TTS model, that though there are degradations in performance from synthesis, the relative performance of these models is partly maintained.  While the submissions from KIT and FBK both outperform UPC, the relative performance between KIT and FBK reverses according to BLEU - but not according to chrF. This suggests that a finer granularity translation metric may better reflect translation quality after synthesis.  \n\n# 4.5 Conclusion  \n\nThis is the first time that speech output is introduced in one of the IWSLT shared tasks. The speech-to-speech task serves as a pilot for this kind of task and we plan to run future editions of this task. Possible future extensions include extending the task to the simultaneous setting and running human evaluations dedicated to additional aspects of the speech output (e.g. preservation of some non-lexical aspects of the input).  \n\n# 5  Low-Resource Speech Translation  \n\nThis shared task focuses on the problem of developing speech transcription and translation tools for under-resourced languages. For the vast majority of the world's languages there exist little speech-translation parallel data at the scale needed to train speech translation models. Instead, in a real-world situation one might have access to limited, disparate resources (e.g. word-level translations, speech recognition, small parallel text data, monolingual text, raw audio, etc).  \n\nBuilding on last year's task that focused on two varieties of Swahili (Anastasopoulos et al., 2021), the shared task invited participants to build speech translation systems for translating out of two predominantly oral languages, Tamasheq and Tunisian Arabic, and into the linguae francae of the respective regions (English and French). The use of any pre-trained machine translation, speech recognition, speech synthesis, or speech translation model was allowed, as did unconstrained submissions potentially using data other than the ones the organizers provided.  \n\n# 5.1 Data and Metrics  \n\nTwo datasets were shared for this year's lowresource speech translation track: the TamasheqFrench translation corpus (Boito et al., 2022a), and the Tunisian Arabic-English dataset from the Dialect Translation track (unconstrained condition). In this section we will focus on the Tamasheq corpus, leaving the results for Tunisian Arabic to be presented in Section 6.  \n\nThe Tamasheq-French translation corpus27 contains $17\\,\\mathrm{h}$ of speech in the Tamasheq language, which corresponds to 5,829 utterances translated to French. Additional audio data was also made available through the Niger-Mali audio collection: $224\\,\\mathrm{h}$ in Tamasheq and $417\\,\\mathrm{h}$ in geographically close languages (French from Niger, Fulfulde, Hausa, and Zarma).28 For all this data, the speech style is radio broadcasting, and the dataset presents no transcription.  \n\nFor this track, the main evaluation metric was lower-cased BLEU4 computed over the produced French translation.29 We also shared with participants results for $\\mathrm{chrF++}$ . Both are computed on SacreBLEU (Post, 2018).30  \n\n# 5.2  Submissions  \n\nFor the Tamasheq language, we received submissions from three teams: ON-TRAC, TALTECH and GMU. We now detail their speech translations models.  \n\nON-TRAC: Boito et al. (2022b) submitted primary and contrastive end-to-end ST systems. Their primary submission focuses on the leveraging of intermediate representations produced by a pre-trained wav2vec 2.0 (Baevski et al., 2020b) base model trained on $234\\,\\mathrm{h}$ of Tamasheq audio. Their end-to-end ST system comprises: a partial wav2vec 2.0 module (in which the last 6 encoder layers were removed), a linear layer for downprojecting the output of the wav2vec 2.0 encoder, and a Transformer decoder with 3 heads, 4 layers and dimensionality of 256. Their contrastive model does not consider SSL features: it uses as input 512-dimensional mel filterbank features. This model leverages approximate transcriptions in Tamasheq produced by a French phonemic ASR model. These are used to train an end-to-end ST conformer model that jointly optimizes ASR, MT and ST losses. The model is made of 12 conformer layers of dimensionality 1024, and three transformer decoder layers of dimensionality 2048.  \n\nTalTech: Their system is an encoder-decoder ST model with a pretrained XLS-R (Babu et al., 2021) as encoder, and a mBART-50 (Tang et al., 2020) as decoder. For the encoder, they used all the 24 layers of the XLS-R 300M model implemented in fai rseq (Ott et al., 2019), fine-tuning it on the provided unlabeled raw audio files in Tamasheq (224h) for 5 epochs. For the decoder, they used the last 12 decoding layers available in the mBART-50 pretrained model.31 The cross attention layers in the decoder were pointed to the XLS-R's hidden state output to mimic the original cross attention mechanism for text-to-text translation.  \n\nGMU: Their model uses the fairseq S2T extension (Wang et al., 2020b), using the transformer architecture. They first fine-tune the pretrained XLS-R 300M encoder on French and Arabic ASR, using portions of the Multilingual TEDx dataset, and then train the whole model on the speech translation task using all provided data.  \n\n# 5.3  Results  \n\nAll results are presented in Table 4. We observe that the dataset is very challenging: the best achieved BLEU is only 5.7 (ON-TRAC). This challenging setting inspired the teams to leverage pre-trained models: all submissions apply pretrained initialization for reducing the cold start in direct ST in low-resource settings.  \n\nDetailing these, ON-TRAC submissions included the training of a wav2vec 2.0 model on target data, and the training of a phonetic French ASR. TalTech used massive multilingual off-theshelf pre-trained models, and GMU pre-trained their speech encoder on French and Arabic. This illustrates the current trend for ST systems of incorporating pre-trained models. It is nonetheless noticeable that, even with the incorporation of powerful representation extractors (wav2vec 2.0, XLS-R, mBART-50), the achieved results are rather low.  \n\nThis year's best submission (primary, ONTRAC) leveraged a Tamasheq wav2vec 2.0 model trained on $234\\,\\mathrm{h}$ . In their post-evaluation results, they included a comparison with different larger wav2vec 2.0 models: XLSR-53 (Conneau et al., 2020), LeBenchmark-7K (Evain et al., 2021), and a multilingual wav2vec 2.0 trained on the NigerMali audio collection.  Their results hint that smaller pre-trained models focused on the target data seemed to perform better in these lowresource settings. This might be due to the existing domain mismatch between pre-training data (from the off-the-shelf models) and the target data.32  \n\nThe second best submission (contrastive, ONTRAC) illustrates how even approximate transcriptions can attenuate the challenge of the direct ST task. The authors trained a phonetic French ASR model, and used the produced transcriptions as additional supervision for joint ASR, MT and ST optimization. This solution is very attractive for low-resource settings, as off-the-shelf ASR models - and annotated data to train new ones - are largely available for high-resourced languages.  \n\nFinally, we find that TalTech submission illustrates how the application of off-the-box pretrained multilingual models can be challenging. A similar point can be made about the GMU submission, which despite multilingual finetuning failed to produce meaningful outputs for this challenging task.  \n\nIn summary, this year's submissions focused on the application of large pre-trained models for end-to-end ST in low-resource settings. They illustrated how low-resource ST remains extremely challenging, even when leveraging powerful speech feature extractors (wav2vec 2.0), and massive multilingual decoders (mBART-50). In such settings, we find that the training of selfsupervised models on target data, and the production of artificial supervision (approximate phonemic transcriptions) were the most effective approaches for translating $17\\,\\mathrm{h}$ of Tamasheq audio into French text.  \n\n# 6 Dialect Speech Translation  \n\nIn some communities, two dialects of the same language are used by speakers under different settings. For example, in the Arabic-speaking world, Modern Standard Arabic (MSA) is used as spoken and written language for formal communications (e.g., news broadcasts, official speeches, religion), whereas informal communication is carried out in local dialects such as Egyptian, Moroccan, and Tunisian. This diglossia phenomenon poses unique challenges to speech translation. Often only the “high\" dialect for formal communication has sufficient training data for building strong ASR and MT systems; the “low dialect for informal communication may not even be commonly written. With this shared task (new for 2022), we hope to bring attention the unique challenges of dialects in diglossic scenarios.  \n\n# 6.1 Challenge  \n\nThe goal of this shared task is to advance dialectal speech translation in diglossic communities. Specifically, we focus on Tunisian-to-English speech translation (ST), with additional ASR and MT resources in Modern Standard Arabic.  \n\nThe ultimate goal of this shared task is to explore how transfer learning between “high\" and “low” dialects can enable speech translation in diglossic communities. Diglossia  is a common phenomenon in the world. Besides Arabic vs. its dialects, other examples  include Mandarin  Chinese  vs. Cantonese/Shanghainese/Taiwanese/etc., Bahasa Indonesia  vs. Javanese/Sundanese/Balinese/etc., Standard German vs.  Swiss German, and Katharevousa vs.  Demotic Greek. With this shared task, we imagine that techniques from multilingual speech translation and low-resource speech translation will be relevant, and hope that new techniques that specifically exploit the characteristics of diglossia can be explored.  \n\n<html><body><table><thead><tr><td><b>Team</b></td><td><b>System</b></td><td><b>Pre-trained Models</b></td><td><b>BLEU</b></td><td><b>chrF++</b></td></tr></thead><tbody><tr><td>ON-TRAC</td><td> primary contrastive</td><td>wav2vec 2.0 (Tamasheq) ASR (French)</td><td>5.7 5.0</td><td>31.4 26.7</td></tr><tr><td>TalTech</td><td>primary</td><td>XLS-R, mBART-50</td><td>2.7</td><td>24.3</td></tr><tr><td>GMU</td><td>primary</td><td>XLS-R (Arabic, French)</td><td>0.5</td><td>16.9</td></tr></tbody></table></body></html>\n\nTable 4: Summary of results for the Tamasheq-french corpus for the low-resource shared task.  \n\n# 6.2 Data and Metrics  \n\nParticipants were provided with the following datasets:  \n\n·(a) 160 hours of Tunisian conversational speech (8kHz), with manual transcripts  \n\n(b) $200\\mathbf{k}$ lines of manual translations of the above Tunisian transcripts into English, making a three-way parallel data (i.e. aligned audio, transcript, translation) that supports endto-end speech translation models  \n\n(c) 1200 hours of Modern Standard Arabic (MSA) broadcast news with transcripts for ASR, available from MGB-2 (Specifically, MGB-2 contains an estimated $70\\%$ MSA, with the rest being a mix of Egyptian, Gulf, Levantine, and North African dialectal Arabic. All of the MGB-2 train data is allowed.)  \n\n·Approximately $42{,}000\\mathrm{k}$ lines  of bitext in MSA-English for MT from OPUS (specifically: Opensubtitles, UN, QED, TED, GlobalVoices, News-Commentary).  \n\nDatasets (a) and (b) are new resources developed by the LDC, and have been manually segmented at the utterance level. This three-way parallel data (Tunisian speech, Tunisian text, English text) enables participants to build end-to-end or cascaded systems that take Tunisian speech as input and generate English text as final output. The main evaluation metric is lower-cased BLEU on the final English translation33.  \n\nParticipants can build systems for evaluation in any of these conditions:  \n\n· Basic condition: train on datasets (a) and (b) only. This uses only Tunisian-English resources; the smaller dataset and simpler setup makes this ideal for participants starting out in speech translation research.  \n\n· Dialect adaptation condition:  train on datasets (a), (b), (c), (d). The challenge is to exploit the large MSA datasets for transfer learning while accounting for lexical, morphological, and syntactic differences between dialects. This condition may be an interesting way to explore how multilingual models work in multi-dialectal conditions.  \n\n· Unconstrained condition: participants may use public or private resources for English and more Arabic dialects besides Tunisian (e.g., CommonVoice, TEDx, NIST OpenMT, MADAR, GALE). Multilingual models beyond Arabic and English are allowed. This condition is cross-listed with the low-resource shared task.  \n\nThe data and conditions available to participants are summarized in Table 5.From the LDC-provided dataset LDC2022E01, we create official train/dev/testl splits for the basic condition34 and encourage participants to compare results on “testl. The official blind evaluation set LDC2022E02 is referred to as “test2\"; it is collected in the same way as LDC2022E01 and utterance segmentation is given.  \n\nTable 5: Datasets for Dialect Shared Task.   \n\n\n<html><body><table><thead><tr><td rowspan=\"2\"><b>Dataset</b></td><td rowspan=\"2\"><b>Speech (#hours)</b></td><td colspan=\"3\"><b>Text (#lines)</b></td><td rowspan=\"2\"><b>Use</b></td></tr><tr><td><b>Tunisian</b></td><td><b>MSA</b></td><td><b>English</b></td></tr></thead><tbody><tr><td>LDC2022E01 train</td><td>160</td><td>200k</td><td>—</td><td>200k</td><td>Basic condition</td></tr><tr><td>LDC2022E01 dev</td><td>3</td><td>3833</td><td>—</td><td>3833</td><td>Basic condition</td></tr><tr><td>LDC2022E01 testl</td><td>3</td><td>4204</td><td>-</td><td>4204</td><td>Unofficial evaluation</td></tr><tr><td>LDC2022E02 test2</td><td>3</td><td>4288</td><td>—</td><td>4288</td><td>Official evaluation for 2022</td></tr><tr><td>MGB2</td><td>1100</td><td>-</td><td>1.1M</td><td>-</td><td>Dialect adaptation; mostly MSA</td></tr><tr><td>OPUS</td><td>-</td><td>-</td><td>42M</td><td>42M</td><td>Dialect adaptation condition</td></tr></tbody></table></body></html>  \n\n# 6.3 Submissions  \n\nWe received submissions from three teams (CMU, JHU, ON-TRAC). Each team explored very different architectures and adaptation techniques. We recommend referring to the system descriptions for details; below is just a brief summary of their contributions:  \n\nCMU (Yan et al., 2022) focuses on the MultiDecoder architecture (Dalmia et al., 2021) implemented in ESPnet, which is an end-to-end ST model that decomposes into ASR and MT subnets  while  maintaining differentiability. Intuitively, hidden states found by beam search from the ASR decoder are fed as input to the ST encoder.  New enhancements on this architecture using hierarcharchical speech encoder and joint CTC/Attention ST decoding are introduced, with gains in BLEU.  \n\nAdditionally, different approaches to integrating end-to-end and cascaded systems are examined in detailed; for example, one approach uses one system to generate N-best candidates, and the other system to help compute minimum Bayes risk. This resulted in the strongest system for this year's shared task.  \n\nIn terms of dialect adaptation, the CMU team explored (a) using a Tunisian ASR model select similar MGB2 data by cross-entropy, and (b) using MSA-EN MT trained on OPUS to synthetically augment MGB2 with translations.  \n\nJHU (Yang et al., 2022) uses a cascaded architecture, where the AsR component is a conformerbased hybrid attention/CTC model implemented in ESPnet and the MT component is a Transformer model implemented in fairseq. ASR pre-training using wave2vec 2 (XLSR-53) is explored for the unconstrained condition. There is also an emphasis on text normalization to reduce variation in the  \n\nTunisian transcripts, which resulted in considerable BLEU gains.  \n\nIn terms of dialect adaptation, the JHU team investigated a novel data augmentation technique for the MT component: First, a ${\\mathrm{EN}}{\\rightarrow}{\\mathrm{MSA}}$ MT model is trained on OPUS and applied to decode LDC2022E01 train set (treating English as source input), synthesizing a paired MSA-Tunisian bitext. With this, a ${\\mathrm{MSA}}{\\rightarrow}^{\\prime}$ Tunisian MT model is trained and applied on OPUS, synthesizing a large Tunisian-English bitext. This can be then used in a fine-tuning setup with the original LDC2022E01 data.  \n\nON-TRAC (Boito et al., 2022b) compares both end-to-end and cascaded systems. The end-toend ST system is a conformer model trained with speed pertubation and SpecAugment, implemented in ESPnet. The cascaded system consists of an ASR component implemented in SpeechBrain, and MT component implemented in fairseq (either biLSTM or convolutional model). Specifically, the ASR component is composed of a wav2vec 2 module, followed by a dense hidden layer and a softmax output of 34 character vocabulary. The use of character outputs in the ASR component is unique to ON-TRAC; other teams employ sub-word units (1000 units for CMU, 400- 1000 units for JHU).  \n\nIn terms of dialect adaptation, the ON-TRAC team explored fine-tuning on the ASR component: first, the ASR model is trained on the MGB2 data; then the model is fine-tuned on the LDC2022E01 data, with the wav2vec portion fixed and the final two layers randomly initialized.  \n\n# 6.4  Results  \n\n# 6.4.1 Automatic evaluation  \n\nWe are interested in two main scientific questions:  \n\n1. For speech translation of primarily spoken dialects, is it beneficial to incorporate data from related dialects with larger written resources? If so, what is the best way to incorporate these resources in training?  \n\n2. Does the inherent imbalance and heterogeneity of resources in different dialects favor end-to-end or cascaded architectures? Specifically, there are separate MSA datasets (MGB2, OPUS) that correspond to ASR and MT sub-tasks, but no single MSA dataset that corresponds to an end-to-end speech translation task like the Tunisian-English LDC2022E01 dataset.  \n\nTable 29 in the Appendix presents the full results on test2 and test1 sets. Table 6 here presents a summary of select systems in terms of the architecture and training data employed. First, we observe that mixing in MSA/English data tends to improve results over the basic condition of using only the Tunisian/English data.  For example, CMU's E2 system obtains 20.8 BLEU, a 0.4 improvement over the E1 system; these are both multi-decoder ensembles, the difference being the training data used. Similarly, JHU's dialect adapt primary system outperforms its basic condition counterpart by 1.8 BLEU. While dialect adaptation is promising, some of the system description papers observe a plateauing effect with additional data, so more work may be needed.  \n\nSecond, the comparison between end-to-end architectures (directing generating English text from Tunisian speech) vs. cascaded $\\mathrm{ASR+MT}$ architectures (two stage Tunisian speech to text, followed Tunisian text to English text) is more complex. On one hand, the ON-TRAC system description reports stronger results from its cascaded architecture which exploits wav2vec and additional MGB2 data in its ASR component; on the other hand, the current best-performing model on this task is CMU's E2 system (20.8 BLEU on test2), which mixes both end-to-end and cascaded systems in a Minimum Bayes Risk (MBR) framework. We are not able to make a clear verdict regarding the best architecture for this task, but believe the distinction between end-to-end and cascade architecture may become more blurred in the future.  \n\nIn summary, we conclude that (1) dialectal adaptation is a promising direction that deserves more research, and (2) the decision between endto-end vs. cascaded architectures most likely will depend on complicated factors, and both should be pursued during development.  \n\n# 6.4.2 Human evaluation  \n\nFor the text-based human evaluation in this task, we employed the Direct Assessment (DA) with document context and extended with Scalar Quality Metric (SQM). The overview of the $\\scriptstyle\\mathrm{DA}+\\mathrm{SOM}$ is provided in Section A.4. In this section we only highlight adaptations specific to the task and discuss the results. Since the test set consisted of a few long conversations, human evaluation was run on a subset of it: we sampled 92 excerpts including 10 consecutive segments and used them as document context. We also adapted annotator guidelines for this task asking for judging correct meaning preservation more than grammatical inconsistencies that may appear in informal conversations, as presented on Figure 6.  \n\nWe have collected 13,860 assessment scores for this task, after excluding quality control items (Table 7). The official results of the human evaluation are presented in Table 31. Systems from each participating teams are significantly different from other teams, but none of the systems was able to provide translation quality competing with the human reference. From the post-annotation survey, some translation issues noticed by annotators were mostly related to incorrect translation of terminology terms and colloquial phrases as well as grammatical and fluency inconsistencies. A few annotators mentioned that in some cases the context of 10 consecutive segments was insufficient and having an access to the original video or audio would help them with the assessment decisions. We will take this feedback into account in next editions of the human evaluation.  \n\n# 7  Formality Control for SLT  \n\nMachine translation (MT) models typically return one single translation for each input segment.  Specific problems can arise for spoken language translation from English into languages that have multiple levels of formality expressed through honorifics or “grammatical register?\" For example, the sentence ‘Are you sure?' can have two possible correct translations in German: ‘Sind Sie sicher? for the formal register and ‘Bist du sicher? for the informal one. Leaving the model to choose between different valid translation options can lead to translations with inconsistent tone that are perceived as inappropriate by users depending on their demographics and cultural backgrounds, in particular for certain use cases (e.g. customer service, business, gaming chat). Most prior research addressing this problem has been tailored to individual languages and proposed custom models trained on data with consistent formality (Viswanathan et al., 2019), or through side constraints to control politeness or formality (Sennrich et al., 2016; Niu et al., 2018; Feely et al., 2019; Schioppa et al., 2021a).  \n\n<html><body><table><thead><tr><td><b>Team / Condition / System</b></td><td><b>Architecture</b></td><td><b>Training Data</b></td><td><b>BLEU</b></td><td><b>△</b></td></tr></thead><tbody><tr><td>CMU / basic / E1</td><td>Mix Mix</td><td>TA/EN</td><td>20.4 20.8</td><td>-</td></tr><tr><td>CMU / dialect adapt / E2</td><td>Mix</td><td>TA/EN + MSA/EN</td><td>20.8</td><td>0.4</td></tr><tr><td>JHU / basic / primary</td><td>Cascaded</td><td>TA/EN TA/EN + MSA/EN</td><td>17.1</td><td>-</td></tr><tr><td> JHU / dialect adapt / primary</td><td>Cascaded</td><td>TA/EN + MSA/EN</td><td>18.9</td><td>1.8</td></tr><tr><td>ON-TRAC / basic /primary</td><td>End-to-End</td><td>TA/EN</td><td>12.4</td><td>-</td></tr><tr><td>ON-TRAC / unconstrained / post-eval</td><td>Cascaded</td><td>TA/EN + MSA/EN</td><td>14.4</td><td>2.0</td></tr></tbody></table></body></html>  \n\nTable 7: Amount of human assessments collected in the text-based evaluation for the Dialect Speech Translation Task run in Appraise. Counts after removing documents with quality control items.   \n\n\n<html><body><table><thead><tr><td><b>Language pair</b></td><td><b>Sys.</b></td><td><b>Ass.</b></td><td><b>Ass./Sys.</b></td></tr></thead><tbody><tr><td>Tunisian→English</td><td>7</td><td>13,860</td><td>1,980</td></tr></tbody></table></body></html>  \n\nTable 6: Summary of select systems for Dialect Shared Task (BLEU on test2). We highlight the BLEU improvements $(\\Delta)$ obtained when training with additional MSA/English data compared with just the Tunisian/English (TA/EN) in the basic condition.   \n\n\n<html><body><table><thead><tr><td><b>Source</b></td><td><b>Could you provide your first name please?</b></td></tr></thead><tbody><tr><td>Informal</td><td>Konntest du bitte deinen Vornamen angeben?</td></tr><tr><td>Formal</td><td>Konnten Sie bitte Ihren Vornamen angeben?</td></tr><tr><td>Source</td><td>OK, then please follow me to your table.</td></tr><tr><td>Informal</td><td></td></tr><tr><td>Formal</td><td></td></tr><tr><td>Respectful</td><td></td></tr></tbody></table></body></html>  \n\n# 7.1 Challenge  \n\nThe goal of this task was to advance research on controlling formality for spoken language translation across multiple diverse target languages and domains.35 How formality distinctions are expressed grammatically and lexically can vary widely by language. In many Indo-European languages (e.g., German, Hindi, Italian, Russian, and Spanish), the formal and informal registers are distinguished by the second person pronouns and/or corresponding verb agreement. In Japanese, distinctions that express polite, respectful, and humble speech can be more extensive, including morphological markings on the main verb, as well as on some nouns and adjectives; specific lexical choices; and longer sentences. For this task we  \n\nTable 8: Contrastive translations for EN-DE and ENJA with different formality. Phrases in bold were annotated by professional translators as marking formality. Example reproduced from Nadejde et al. (2022).  \n\nconsidered two formality levels: formal and informal. For Japanese, where more than two formality levels are possible, informal was mapped to kudaketa and formal to teineigo. We give examples of these phenomena in Table 8.  \n\nThe task focused on text-to-text translation of spoken language with a special theme of zeroshot learning in multilingual models. The task covered supervised and zero-shot settings, both with constrained and unconstrained training data requirements. For the supervised setting, participants were provided with a formality-annotated dataset for training and development for four language pairs: English $\\rightarrow$ German, Spanish, Hindi, Japanese. For the zero-shot task, which covered English $\\rightarrow$ Italian, Russian, only targeted test data was provided after system submission period.  \n\nAs this was the first shared task organized on formality control, one objective was to establish a standard benchmark including: formalityannotated train and test sets, an evaluation metric, pre-trained baseline models and human evaluation guidelines. To encourage further research in this area and improve the task definition, we will release all these resources (including system outputs and human evaluation annotations) under a shared repository.36  \n\n# 7.2  Data and Metrics  \n\n# 7.2.1 Formality-annotated data  \n\nFor this task, the organizers provided formalityannotated parallel data comprising of source segments paired with two contrastive reference translations, one for each formality level (informal and formal). The dataset (CoCoA-MT), released by Nadejde et al. (2022), includes phrase-level annotations of formality markers in the target segments in order to facilitate evaluation and analysis (shown in bold in Table 8). Formality distinctions are expressed by the use of grammatical register or honorific language. The training set provided to participants comprises segments sourced from two domains: Topical-Chat (Gopalakrishnan et al., 2019) and Telephony. For the test set, organizers additionally included segments sourced from a third held-out domain: Call-Center.  \n\nTable 9 reports the number of source segments used for training and evaluation and the overlap between the references (informal vs. formal) as measured by BLEU. The lowest overlap is for Japanese and the highest overlap is for Hindi, indicating that the task of controlling formality is more challenging for Japanese than for Hindi.  \n\nTable 9: Number of segments in the training and test data, and overlap between the references in the test set as measured by BLEU (informal vs. formal). Table adapted from Nadejde et al. (2022).   \n\n\n<html><body><table><thead><tr><td><b>Setting</b></td><td><b>Target</b></td><td><b>#train</b></td><td><b>#test</b></td><td><b>overlap</b></td></tr></thead><tbody><tr><td rowspan=\"4\"> Supervised</td><td>DE</td><td>400</td><td>600</td><td>75.1</td></tr><tr><td>ES</td><td>400</td><td>600</td><td>79.0</td></tr><tr><td>HI</td><td>400</td><td>600</td><td>81.1</td></tr><tr><td>JA</td><td>1,000</td><td>600</td><td>74.6</td></tr><tr><td rowspan=\"2\">Zero-shot</td><td>IT</td><td>0</td><td>600</td><td>78.8</td></tr><tr><td>RU</td><td>0</td><td>600</td><td>二</td></tr></tbody></table></body></html>  \n\n# 7.2.2  Task definition  \n\nParticipants were allowed to submit systems under the constrained and unconstrained data settings. To train their systems, participants were allowed to use the formality-labeled dataset provided by the organizers as well as the additional resources described below.  \n\nConstrained  task:  Textual  MuST-C vl.2 data (Di Gangi et al., 2019) (for EN-DE, EN-ES, EN-IT, EN-RU), data released for the WMT news translation tasks (WMT2137 for EN-JA;  \n\nWMT1438 for EN-HI), multilingual data from the same dataset (e.g. using EN-FR MuST-C data for training EN-ES models). Participants were not allowed to use external auxiliary tools (e.g., morphological analysers) or pre-trained models (e.g., BERT).  \n\nUnconstrained task: Pre-trained models (e.g.. mBERT, mBART), additional annotations from morphological analysers, data released by the WMT news translation tasks (WMT21 for ENDE, EN-RU; WMT1 $3^{39}$ for EN-ES; News Commentary $\\mathrm{v}16^{40}$ and Europarl41 for EN-IT) and ParaCrawl v9.42 For EN-HI, EN-JA, participants were allowed to use any other publicly available textual datasets such as WikiMatrix43 and JParaCraw1.4  \n\nIn both settings, no additional manually created formality-labeled data was allowed. For the unconstrained setting, obtaining additional annotations automatically was allowed as long as the code and data would be publicly released.  \n\nEvaluation sets Systems were evaluated for overall quality on MuST-C v1.2 test sets (tstCOMMON) (Di Gangi et al., 2019) for $\\mathrm{EN{\\rightarrow}D E}$ ES, IT, RU. For $\\mathrm{EN{\\rightarrow}H I}$ , JA, systems were evaluated on WMT newstest2014 and 2020, respectively. Formality control accuracy was evaluated on the CoCoA-MT formality-annotated test set.  \n\nAutomatic  metrics   Overall quality was measured by sacreBLEU (Post, 2018) and COMET (Rei et al., 2020). Formality control accuracywasmeasured using thereferenced-based corpus-level metric released with the CoCoA-MT dataset. The metric relies on the contrastive reference translations to  automatically assign, with high precision, formality labels (formal vs. informal) to each hypothesis. The segment-level labels are then aggregated to compute the corpus level Matched-Acccuracy (M-ACC). For further details on and evaluation of the M-ACC automatic metric, we refer the reader to the corresponding CoCoA-MT paper (Nadejde et al., 2022).  \n\n# 7.3 Submissions  \n\nWe received submissions from three teams. We briefy summarize their methodologies below and refer the reader to their system description papers for details.  \n\nALEXAAI (Zhang et al., 2022a) focused on using data augmentation to generate additional formality data and on using post-editing strategies to convert outputs from a generic NMT system into the desired formality level. They participated in the unconstrained supervised setting for $\\mathrm{EN{\\rightarrow}H I}$ JA. The authors made use of the limited amount of formality data released for the shared task to fine-tune mBART to classify segments as formal or informal. The formality classifier was then used to augment the available training data with additional formal/informal examples which they used to fine-tune a generic NMT system. The final system output from this fine-tuned model was then post-edited using a variety of strategies that the authors examine.  \n\nFor $\\mathrm{EN{\\rightarrow}H I}$ , the post-editing strategy was a rule-based approach which turned informal pronouns to formal pronouns. For $\\mathrm{EN}{\\rightarrow}\\mathrm{JA}$ ,the authors focused on a rule-based method for conjugating verbs. Finally, the authors addressed expansion of their methods to something languageagnostic and examined a seq2seq model used to transform formal outputs into informal outputs (they assumed that the output from the fine-tuned model was formal already and the seq2seq model was only used to generate informal translations). Generally, the authors found that the rule-based approaches worked better than the seq2seq postediting model.  \n\nUoS (Vincent et al., 2022) focused on using data augmentation to generate additional formality data and on re-ranking translations from a generic NMT system for a given formality level. They trained systems for all four settings: {constrained, unconstrained} $\\times\\ \\{$ supervised, zero-shot}. For the supervised settings, they submitted models for $\\mathrm{EN{\\rightarrow}D E}$ , ES. For the zero-shot settings, they submitted models for $\\mathrm{EN}{\\rightarrow}\\mathrm{IT}_{}$ RU.  \n\nIn order to augment the formality data, the authors fine-tuned a language model which they used to rank sentences from the available parallel corpora (depending on the constrained or unconstrained setting) by their similarity with the released formal and informal data. Most similar sentences were extracted using a relative position difference algorithm. For the zero-shot case, they noted that a smaller subset of sentences were considered formal (or informal) across the supervised sets for $\\mathrm{EN{\\rightarrow}D E}$ , ES. They considered these segments to be strongly formal/informal and used this to find pairs in the zero-shot languages.  \n\nThey fine-tuned their generic NMT system using the augmented and released formality data. At inference time, they used a large beam width $k$ for beam search and generated $k$ -best hypotheses. The resulting set of hypotheses were re-ranked using a relative frequency model trained on the released formality data (or, for the zero-shot case, using the similar sentences extracted earlier).  \n\nUMD (Rippeth et al., 2022) proposed training a single multilingual model that can cover all target languages and formality levels, and experimented with both mBART and mT5 as this model. They also worked with different fine-tuning strategies using both the gold labeled data from the shared task and formality-labeled data extracted from the unlabeled parallel data through rule-based methods or through automatic classification. As finetuning strategies they compared using pre-trained models with adapted vector-valued interventions proposed by Schioppa et al. (2021a) against bilingual models optimized towards one formality level (formal or informal) by fine-tuning all model parameters. For automatically labeling data, the authors also relied on fine-tuning a pre-trained multilingual model (XLM-R) for binary classification.  \n\n# 7.4 Results  \n\n# 7.4.1 Automatic Evaluation  \n\nIn Table 10 and Table 11, we report the formality control accuracy scores (M-ACC) defined in $\\S7.2$ for the unconstrained and constrained tracks respectively.45 For the supervised language arcs (i.e. $\\mathrm{EN{\\rightarrow}D E}$ , ES, HI, JA) and unconstrained setting, submitted systems were successfully able to control formality. Average scores across formality settings range from 99.4 for $\\mathrm{EN{\\rightarrow}H I}$ to 92.9 for $\\mathrm{EN}{\\rightarrow}\\mathrm{JA}$ $\\mathrm{EN}{\\rightarrow}\\mathrm{JA}$ was the language pair with the largest gap between formal and informal accuracy, with both submitted systems doing an average of 11.0 points better on informal translations than formal translations. Finally, we observed that the ALEXA AI and UoS teams generally performed better on the supervised unconstrained task than UMD, possibly due to the former's use of highquality parallel training data as opposed to the latter's use of multilingual pre-trained models.  \n\n<html><body><table><thead><tr><td><b>Language Pair</b></td><td><b>System</b></td><td><b>F</b></td><td><b>I</b></td></tr></thead><tbody><tr><td>EN→DE</td><td>UMD Uos</td><td>99.4 100.0</td><td>96.5 100.0</td></tr><tr><td>EN→ES</td><td>UMD Uos</td><td>99.5 98.1</td><td>93.2 100.0</td></tr><tr><td>EN→HI</td><td>ALEXA AI UMD</td><td>99.6 99.4</td><td>99.8 98.7</td></tr><tr><td>EN→JA</td><td>ALEXA AI UMD</td><td>88.8 86.3</td><td>98.8 97.5</td></tr><tr><td>EN→IT</td><td>UMD Uos</td><td>32.8 51.2</td><td>97.9 98.6</td></tr><tr><td>EN→RU</td><td>UMD Uos</td><td>100.0 99.5</td><td>1.10 85.8</td></tr></tbody></table></body></html>  \n\nTable 10: Formality control accuracy (M-ACC) reported for Formal (F) and Informal (l) for the unconstrained task.Note that $\\mathrm{EN}{\\rightarrow}\\mathrm{IT}$ ,RU are zero-shot settings.   \nTable 11: Formality control accuracy (M-ACC) reported for Formal (F) and Informal (l) for the constrained task. There was only one system submission by UoS for this track. Note that $\\mathrm{EN}{\\rightarrow}\\mathrm{IT}$ RUarezeroshot settings.   \n\n\n<html><body><table><thead><tr><td><b>Language Pair</b></td><td><b>System</b></td><td><b>F</b></td><td><b>I</b></td></tr></thead><tbody><tr><td>EN→DE</td><td>Uos</td><td>100.0</td><td>88.6</td></tr><tr><td>EN→ES</td><td>Uos</td><td>87.4</td><td>98.0</td></tr><tr><td>EN→IT</td><td>Uos</td><td>29.5</td><td>92.9</td></tr><tr><td>EN→RU</td><td>Uos</td><td>98.1</td><td>15.4</td></tr></tbody></table></body></html>  \n\nFor the supervised and constrained setting, we had one submission from UoS for $\\mathrm{EN{\\rightarrow}D E}$ ,ES. On average over both formality settings, their systems achieved an accuracy of 94.3 on $\\mathrm{EN{\\rightarrow}D E}$ and 92.7 on $\\mathrm{EN}{\\rightarrow}\\mathrm{ES}$ . For $\\mathrm{EN{\\rightarrow}D E}$ , performance was significantly better for formal translations vs. informal translations, while the reverse was true for $\\mathrm{EN}{\\rightarrow}\\mathrm{ES}$  \n\nIn the zero-shot $\\scriptstyle\\mathrm{EN}\\to\\mathrm{IT}$ RU) unconstrained setting, results were more mixed. For the two submissions (from the UMD and UoS teams), there was a clear bias toward one formality level: both systems were better at generating informal Italian and formal Russian translations. This likely refects the inherent bias toward one formality level in the training set. For the zero-shot constrained setting, only the UoS team submitted a system, and results on the two formality levels were similar, with one formality level outperforming the other. In going from the unconstrained to the constrained setting, the UoS system lost an average of 25 points in accuracy for the zero-shot setting, while only losing 6 points in the fully supervised setting.  \n\n<html><body><table><thead><tr><td><b>Language Pair</b></td><td><b>System</b></td><td><b>F</b></td></tr></thead><tbody><tr><td>EN→JA</td><td>ALEXA AI UMD</td><td>92.5 82.7</td></tr><tr><td>EN→IT</td><td>UMD Uos</td><td>78.3 81.0</td></tr><tr><td>EN→RU</td><td>UMD</td><td>0.7 71.3</td></tr></tbody></table></body></html>  \n\nTable 12: Human evaluation of the system level formality accuracy (Formal $\\mathrm{(F)}$ and Informal (I)) for models in the unconstrained setting. Note that EN ${\\mathrm{\\partial}}\\left[\\supset{\\mathrm{I}}\\right]\\left[\\rule{0.3cm}{0cm}\\right]\\left[\\mathrm{\\partial}\\right]$ RU are zero-shot settings.   \nTable 13: Human evaluation of the system level formality accuracy (Formal (F) and Informal (I) for models in the constrained setting. Note that $\\mathrm{EN}{\\rightarrow}\\mathrm{IT}_{\\mathrm{2}}$ RU are zero-shot settings.   \n\n\n<html><body><table><thead><tr><td><b>Language Pair</b></td><td><b>System</b></td><td><b>F</b></td><td><b>I</b></td></tr></thead><tbody><tr><td>EN→IT</td><td>Uos</td><td>0.2</td><td>36.3</td></tr><tr><td>EN→RU</td><td>Uos</td><td>85.3</td><td>12.7</td></tr></tbody></table></body></html>  \n\n# 7.4.2 Human Evaluation  \n\nTo complement the automatic evaluations, we conducted human evaluations of formality accuracy for a subset of the language pairs and settings. We selected $\\mathrm{EN}{\\rightarrow}\\mathrm{JA}$ for the unconstrained supervised task, since Japanese has more complex morphological differences between formal and informal translations than the other target languages. We selected both $\\mathrm{EN}{\\rightarrow}\\mathrm{IT}_{:}$ . RU for the zero-shot tasks (both constrained and unconstrained).  \n\nFor each system, we selected a random sample of 300 source segments and collected the formal and informal outputs of the source segments. Annotators were asked to evaluate the outputs and assess whether the translation was formal, informal, neutral, or other.46 We summarize the results of the human evaluations here, and give full results in Table 34 in the appendix. System-level accuracy was computed as the number of translations matching their desired formality level divided by the total number of outputs for a given formality level. Inter-annotator agreement as measured by the Krippendorf's $\\alpha$ coefficient (Hayes and Krippendorff, 2007) was high, with an average $\\alpha$ of 0.89.  \n\nResults from the human evaluation of $\\mathrm{EN}{\\rightarrow}\\mathrm{JA}$ for the unconstrained supervised setting were in line with those obtained by the automatic metric: the submitted systems were able to control the formality of the output translations with reasonably high accuracy (90.9 for UMD and 82.8 for ALEXA AI on average across formality levels).  \n\nHuman evaluation results also corroborated the automatic evaluations for zero-shot formality transfer. The results underscore how challenging the task of zero-shot formality transfer is, with submitted systems generally performing significantly better on one formality level than the other: informal for $\\mathrm{EN}{\\rightarrow}\\mathrm{IT}$ and formal for $\\mathrm{EN}{\\rightarrow}\\mathrm{RU}$ .A notable exception is the UoS $\\mathrm{EN}{\\rightarrow}\\mathrm{RU}$ unconstrained system, which achieves a reasonable accuracy for both formal (85.0) and informal (71.3) registers (again mirroring the findings of the automatic evaluation). Additionally, human evaluators labeled more systems as “neutral\" or “other\" (i.e., neither formal nor informal) in the zero-shot settings than in the supervised settings.  \n\n# 8 Isometric SLT  \n\nIsometric translation is the task of generating translations similar in length to the source input (Lakew et al., 2021b). As a new research area in machine translation, this is the first time isometric translation is proposed as a shared task.47 We considered 3 translations directions (English - German, English-French and English-Spanish) and 2 training conditions: constrained and unconstrained.  \n\n# 8.1 Challenge  \n\nIsometric MT targets issues that emerge when MT is applied to downstream applications such as dubbing, subtitling, and translation of documents. In particular, dubbing requires that the duration of the target speech to be the same of the source in order to achieve isochrony (Lakew et al., 2021b); subtitle translation requires the output to fit blocks of pre-defined length (Matusov et al., 2019); and, finally, document translation requires sometimes to control the translation length in order to preserve the original layout.  \n\nWe define isometric translations as translations whose length (in characters) is within $\\pm10\\%$ of the length of the source (Lakew et al., 2021a). Subjective evaluations of automatically dubbed videos show that isometric translations generated better dubs than translations without any length control (Lakew et al., 2021a).  \n\nA few works have focused on controlling the output length of neural MT. Lakew et al. (2019) proposed to split the parallel training data based on target to source length ratio and prepend control tokens. Lakew et al. (2019) and Niehues (2020) incorporated length-encoding mechanisms that adapts positional-encoding (Vaswani et al., 2017） to control the length of the output sequence. Post-hoc approaches have been proposed by Saboo and Baumann (2019) and (Lakew et al., 2021a), where MT system generates an N-best list and then each hypothesis is re-ranked based on its length and score. More recently, Schioppa et al. (2021b) proposed to combine embedding representing attributes (such as length and politeness) with the encoder representation, to control for multiple attributes at generation time; whereas Lakew et al. (2021b) applied self-training to let the model incrementally learn how to generate isometric translations from its own output.  \n\nIn this shared task, we proposed isometric MT of spoken language transcripts from $\\mathrm{En}\\rightarrow\\mathrm{De}$ , Fr, Es. These three directions exhibit different targetto-source length ratios in character count. The length-ratios on the $\\mathrm{MuST-C}$ training set is 1.12 for $\\mathrm{En}{\\rightarrow}\\mathrm{De}$ , 1.11 for $\\mathrm{En\\mathrm{{\\rightarrow}\\mathrm{{Fr}}}}$ , and 1.04 for $\\mathrm{En}{\\rightarrow}\\mathrm{Es}$  \n\nShared task participants were invited to work under constrained  or unconstrained   training regimes and to to submit systems for one or multiple translation directions. When submitting their system outputs, participants were asked to score their performance using a script available for the evaluation period.48 Participant were also asked to release their outputs under a MIT license to allow for a human evaluation and further analyses.  \n\n<html><body><table><thead><tr><td><b>En-De</b></td><td><b>En-Fr</b></td><td><b>En-Es</b></td></tr></thead><tbody><tr><td>Test set</td><td>LR LC</td><td>LR RLC</td></tr><tr><td>MuST-C</td><td>1.2 35.2%</td><td>1.0 53.2%</td></tr><tr><td>Blind</td><td>1.1</td><td>1.0</td></tr></tbody></table></body></html>  \n\nTable 14: Target to source sample length ratio (LR), and length compliance (LC) within a $\\pm10\\%$ range, with respect to the source in terms of characters counts, for the MuST-C (t st -COMMON) and blind test sets.  \n\n# 8.2  Data and Metrics  \n\n# 8.2.1 Task Definition  \n\nWe proposed two types of training regimes:  \n\nConstrained task allows the participants to use language pair specific parallel data from the Ted Talks MuST-C v1.2 corpus (Di Gangi et al., 2019). This is an in-domain training data setting for evaluation using the MuST-C test set (tst-COMMON).  \n\nUnconstrained Task allows the participants to leverage WMT data, or any other parallel or monolingual data in addition to the MuST-C data which is available under Constrained task. Participants are also allowed to use any pre-trained models like mBART (Liu et al., 2020).49  \n\n# 8.2.2 Evaluation Sets  \n\nWe evaluated isometric machine translation on two test sets:  \n\n·MuST-C (tst-COMMON): in-domain test ada that is publicly available for participants to optimize their models.  \n\n· Blind Test: a test set of 91 dialogues extracted from 3 YouTube videos.50 Each dialogue is containing 5-17 utterances is segmented into sentences for a total of 200 sentences. During the evaluation period participants had only access to the source sentences (English).51  \n\nTarget to source sample length ratio and length compliance $(\\pm10\\%)$ for these test sets are shown in Table 14.  The blind dataset was manually post-edited for isometric translation condition i.e. the translators were asked to keep the length of the translation possibly within $\\pm10\\%$ ofthe source length.  As a result, it shows a lower length ratio and a higher length compliance than tst-COMMON. Length compliance of the blind set is however not $100\\%$ because translators did not find a way to generate translations for many source sentences (phrases) within the range.  \n\n# 8.2.3 Evaluation Metrics  \n\nSubmissions were evaluated on two dimensions -- translation quality and length compliance with respect to the source input.  \n\nTranslation Quality metrics  for  isometric translation should be robust to length variations in the hypothesis. For this reason we assessed n-gram metrics such as BLEU (Papineni et al., 2002), and recently proposed semantic based metrics like COMET (Rei et al., 2020) and BERTSCore (Zhang et al., 2019). Our analysis shows that BERTScore is more robust to length variations in the hypothesis when compared with BLEU and COMET. The latter two tends to penalize short hypotheses even for cases where the semantics is preserved. As a result, we primarily use BERTScore to assess translation quality.  \n\nLength Compliance (LC) is formulated as the $\\%$ of translations in the test set that meet the $\\pm10\\%$ length criterion. That is, if the source length is 50 characters, a length compliant translation is between 45 to 55 characters. We calculate how many translations fall in this bracket and report the percentage over a test set. In this evaluation, LC is applied only for source samples with length above 10 characters.  \n\n# 8.3  Submissions  \n\nWe have received four submission from APpTEK, HW-TSC, Amazon Prime Video (APV), and NUV teams.52 Below we briefly present submitted systems, followed by the baseline approaches we considered for the evaluation.  \n\nAPPTEK (Wilken and Matusov, 2022) participated in the constrained task for En-De pair. They explored various length controlling approaches with data pre-processing, data augmentation, length tokens as indicators, and multi-pass decoding. For data augmentation, forward and backward translations are applied, together with sample lengthtargeted pre-processing. For modeling, they combine fine-grained length control token on the encoder/decoder (Lakew et al., 2019) and length encoding modifying positional encoding (Takase and Okazaki, 2019). As a post-hoc step after translation, the primary system applies a system combination (denoted as length ROVER) over multiple translations from 7 different length classes, ranging from “extra short\" to “extra long\".  \n\nHW-TSC (Li et al., 2022b) participated in the constrained and unconstrained tasks for En-De, and constrained tasks for En-Fr and En-Es. Their submission investigated  bi-directional  training, R-drop (Wu et al., 2021) (a variant of dropout), data augmentation  in forward and backward translation setting, and model ensemble to improve translation quality. For length control they prepended length tokens to the encoder (Lakew et al., 2019), added length ratio based positional encoding (Takase and Okazaki, 2019), applied length aware beam (LAB) to generate N-best lists, and explored different re-ranking strategies. The primary system for HW-TSC was a combination of length token, decoding with LAB and re-ranking of different system outputs. It shows the highest LC score with, however, a tradeoff on translation quality w.r.t. BERTScore.  \n\nAPV leverages human-in-the-loop mechanism to train an isometric translation model. Their approach builds on top of a multi-source transformer that takes a source and an hypothesis (Tebbifakhr et al., 2018) as input.  The hypothesis comes from human post-editing effort for style variation such as matching translation length with the source input. Differently from previous work on interactive post-editing, their work proposes the isometric translation attribute as a new dimension in the human-in-the-loop translation modeling. APV team participated in the unconstrained task for $\\mathrm{En}\\rightarrow\\mathrm{De}$ , Fr and Es. Their result shows performance gains against the baseline model when utilizing the post-edited reference as addition model input. However, when adding the isometric criterion for the post-editing stage, translation quality degrades with a slight gain in LC.  \n\nNUV (Bhatnagar et al., 2022) participated in the unconstrained task for En-Fr. Their approach is to first translate and then paraphrase. Their MT system is a Marian-NMT system pre-trained on OPUS-MT data (Tiedemann et al., 2020) and finetuned on MuST-C training data with three tokens for “short\", “normal\" and “long\" translations. Paraphrases are generated by a MT5 (Xue et al., 2020) model fine-tuned on the PAWS-X paraphrasing data set (Yang et al., 2019).  \n\nBaselines: based on the task definition two systems are considered as baselines:  \n\n·WEAKBASELINE is a standard neural MT model trained in the constrained data setting, without any isometric translation feature. · STRONGBASELINE is trained in an unconstrained data setting and implements output length control as in Lakew et al. (2021a) by prepending a length token on the input, generating N-best hypotheses, and re-ranking them with a linear combination of model score and length ratio.  \n\n# 8.4 Evaluations  \n\nTo assess the performance of isometric translation systems, we measure translation quality and length compliance via automatic and subjective metrics.  \n\n# 8.4.1  Automatic Evaluation  \n\nAs discussed in Sec. 8.2 we leverage BERTScore and LC metrics to measure isometric translation performance. We take primary system run from each submission and the baseline systems for comparison. Scores are computed against the human post-edited reference of the the blind test set. The automatic evaluation results are given in Table 35.  \n\nTranslation quality in terms of BERTScore shows that STRONGBASELINE is the best performing system for all directions and training conditions.  APPTEK's constrained submission for En-De is the only system performing similarly to STRONGBASELINE. For length compliance, HW-TSC-Constrained shows the best result $(\\mathrm{LC}{>}{=}96\\%)$ ) for all pairs. However, the high LC score comes at the cost of lower translation quality with BERTScore.  \n\nFor the En-De direction, the system from APPTEK-Constrained shows the best trade-off between BERTScore and LC, followed by STRONGBASELINE and HW-TSC-Unconstrained.  On En-Fr, NUV-Unconstrained has the best translation quality among all submitted systems in terms of BERTScore but with a significant tradeoff on length compliance. On En-Es, APVUnconstrained shows the highest translation quality but again with a significant trade-off on length compliance. Over all language pairs, STRONGBASELINE stands out when we look at trade-offs between translation quality and length compliance.  \n\n# 8.4.2 Human Evaluation of Machine Translation Quality  \n\nFor the text-based human evaluation, we employed the Direct Assessment (DA) with document context and extended with Scalar Quality Metric (SQM). The overview of the $\\scriptstyle\\mathrm{DA}+\\mathrm{SQM}$ is provided in Section A.4. In this section we only highlight modifications specific to the task and discuss the results. The original segmentation was preserved when generating annotation tasks for the human evaluation. In contrast to the Dialect Speech Translation Task, annotators were guided to assess both grammar and meaning of the translations, as presented on Figure 7. The total number of assessment scores collected in text-based human evaluation campaigns per language pair is listed in Table 15.  \n\nThe official results of the human evaluation are presented in Table 36. Reference translations (TRANSLATOR-A) are significantly better than participating systems and baselines across all three language pairs. In En-De APPTEKConstrained and the STRONGBASELINE are together in a separate cluster outperforming the rest of the systems. This is also reflected in the automatic metric, where the two systems standout with a higher BERTScore than the other systems. In En-Fr task, a single large cluster includes all systems and baselines. This mean none of the systems were significantly better than the other. In EnEs task, APV-Unconstrained outperformed HWTSC-Constrained and show similar performance with the STRONGBASELINE.  \n\nIn the post-annotation questionnaire, most frequently mentioned common issues found in the translation outputs by annotators were: lack of coherence between segments and inter-sentential translation errors,terminology translation errors and grammatical inconsistencies. Annotators noticed that one source of those issues was splitting source sentences into short utterances, which automatic systems treated and translated as full sentences.  \n\nTable 15: Amount of human assessments collected in the text-based evaluation for the Isometric SLT Task run in Appraise. Counts after removing documents with quality control items.   \n\n\n<html><body><table><thead><tr><td><b>Language pair</b></td><td><b>Sys.</b></td><td><b>Ass.</b></td><td><b>Ass./Sys.</b></td></tr></thead><tbody><tr><td>English→German</td><td>7</td><td>12,996</td><td>1,857</td></tr><tr><td>English→French</td><td>6</td><td>11,286</td><td>1,881</td></tr><tr><td>English→Spanish</td><td>5</td><td>9,692</td><td>1,938</td></tr></tbody></table></body></html>  \n\n# 8.5  Isometric SLT Use case  \n\n# 8.5.1 Automatic Dubbing  \n\nAs noted in Sec. 8.1, Isometric SLT can be useful for Automatic dubbing that requires the dubbed synthetic speech in the target language to fit the duration of the original speech in the source language. In the previous section, $\\scriptstyle\\mathrm{DA+SQM}$ evaluation mainly looked at the translation quality. In this section, using the dubbing architecture of (Federico et al., 2020b) we test the downstream dubbing quality of these translations. To adapt the translations for dubbing, we segment them so as to follow the speech-pause arrangement of the source audio using prosodic alignment (PA) (Virkar et al., 2021, 2022). Using the output from PA module, we produce the dubbed audio utilizing a commercial grade Text-to-Speech system with finegrained duration control (Effendi et al., 2022). We then replace the original audio with the dubbed audio to produce the final dubbed video.  \n\n# 8.5.2 Human evaluation  \n\nWe generate dubbed videos using all MT outputs and (segmented) post-edited references. To reduce cognitive load, each subject is asked to compare only two MT systems at a time. This results in a total of 31 evaluations across the three dubbing directions, i.e., En-De,Fr,Es. Subjects first watch the dubbed video produced using the reference translation and then rate dubbed videos from two MT outputs. We employed subjects native in the target language and asked them to grade each dubbed video on a scale of 0-10 (0 being the worst and 10 being the best). For each MT system, we compute $\\%$ Wins, i.e., $\\%$ subjects preference when comparing two MT systems. For example, if we have 100 clips and according to annotators system A performs better than system B on 60 clips and ties with system B for 10 clips, then $\\%$ Wins is $60\\%$ for system A v/s $30\\%$ for system B. We do not use the absolute grading to avoid the bias of each subject towards dubbing content in general.  \n\nFor our experiments, we selected 60 dialogues from the blind set, to create 15 video clips such that each clip contains 4 continuous dialogues. To achieve statistically significant results, we employed 15 to 20 subjects (depending on the directions) across all the evaluations.  \n\nTable 37shows theresults for $\\%$ Wins for all 31 evaluations. Additionally, in Table 38, we show the ranking of MT systems based on their performance for the dubbing use case. To rank the systems, we use $N_{\\mathrm{Wins}}$ that defines the number of evaluations for which a system was preferred over some other system. In general, similar to human assessment for MT quality, we found STRONGBASELINE to be the best system for all three languages and WEAKBASELINE to be the worst for French and Spanish.  \n\nUnlike MT human evaluation results, we found WEAKBASELINE to be worse compared to HWTSC-Constrained even for English-German. In a similar manner, we find that compared to the rankings from MT evaluation, HW-TSCsystems are ranked either higher or on par to APVUnconstrained and NUV-Constrained. To better understand these differences in the ranking, we computed the Smoothness metric (Federico et al., 2020a) that measures TTS speaking rate stability across contiguous sentences (or phrases) and also consider the LC metric. Note that degraded LC implies that we have either too high or too low speaking rates for the dubbed speech, i.e., LC directly impacts speech fuency (Federico et al., 2020a). Table 39 shows these metrics with systems in a similar order as their ranking. We find that WEAKBASELINE, APV-Unconstrained and NUV-Constrained generally have either a much lower Smoothness or a much lower LC compared to the other systems. This results in poor speaking rate control and impacts $\\%$ Wins resulting in a different ranking from MT evaluation. The main takeaway is that MT evaluations do not show a complete picture for the downstream task of dubbing as we need not only high quality translations but also translations that permit good speaking rate control.  \n\n# 8.6 Conclusion  \n\nThis was the first time a shared task on Isometric MT was organized where we looked at evaluated systems on MT quality and length compliance as well as on a downstream task of automatic dubbing which requires isometric translations. With this shared task, we released a new benchmark of manually transcribed and translated scripts (with length compliance in mind) to evaluate isometry in translation. In the possible extensions of this shared task, we plan to include original video along with the transcribed script at dialogue level so that participants can leverage the duration in the source audio to fit the translation within a given time stamp.  \n\n# Acknowledgements  \n\nWe would like to thank the IWSLT 2022 sponsors and donors Apple, AppTek, AWS, Meta, Microsoft, and Zoom for supporting the human evaluation of the shared tasks and student participants with computing credits. We would like to thank Mary Johnson, Tom Kocmi and Hitokazu Matsushita for their help with conducting parts of the human evaluation and providing useful comments. We are grateful to the many annotators who participated in the human evaluation and provided their feedback. We would like to thank Zhaoheng Ni, Jeff Hwang and the torchaudio team for providing a streaming ASR model for the simultaneous task. We would like to thank Justine Kao and Brian Bui for running the human evaluation for the speech-to-speech task. The creation of the reference interpretations was funded from the EU pr0ject H2020-ICT-2018-2-825460 (ELITR). Ondrej Bojar would like to acknowledge the grant 19-26934X (NEUREM3) of the Czech Science Foundation.", "appendix": "# Appendix A. Human Evaluation  \n\n# A Human Evaluation  \n\nHuman evaluation was carried out for the following tasks: (i) Simultaneous Speech Translation, (i) Offine speech translation, (ii) Speech to speech translation, $(i\\nu)$ Dialect speech translation, $(\\nu)$ Isometric SLT, and $(\\nu i)$ Formality control for SLT.  \n\nDifferent evaluation protocols were adopted, which are described in the following section.  \n\n# A.1 Simultaneous Speech Translation Task  \n\nSimultaneous Speech Translation Task ran two different types of manual evaluation: “continuous rating? for English-to-German and MQM for English-to-Japanese.  \n\n# A.1.1 Human Evaluation for the English-to-German Simultaneous Task  \n\nManual evaluation of English-to-German Simultaneous Task uses a variant of “continuous rating”’ as described by Javorsky et al. (2022).  \n\nDuring the evaluation, bilingual annotators were presented with the source audio and subtitles. The subtitles were displayed in two lines below the audio following the guidelines for video subtitling (BBC, 2019). The annotators were asked to score the quality of the live-presented text output while listening to the input sound. Specifically, the instructions explicitly asked to focus on content preservation, or roughly the adequacy:  \n\n· We ask you to provide your assessment using so-called “continuous rating\", which continuously indicates the quality of the text output given the input utterance you hear in the range from 1 (the worst) to 4 (the best) by clicking the corresponding buttons or pressing the corresponding keys.   \n· The rate of clicking/pressing depends on you. However, we suggest clicking each 5-10 seconds or when your assessment has changed. We encourage you to provide feedback as often as possible even if your assessment has not changed.   \n· The quality scale should refect primarily the meaning preservation (i.e. evaluating primarily the \"content'’ or very approximately the “adequacy\") and the grammaticality and other qualitative aspects like punctuation (i.e. the “form” or extremely roughly the “fuency\") should be the secondary criterion.  \n\nContext-Aware Judgements  One important aspect of the evaluation is that the systems are run independently for each input segment while continuous rating is designed for following the whole speech. Our continuous rating can be thus seen a variant of document-level measure, although the context is (on purpose) available only from the history and not from the future.  \n\nWhen preparing the subtitles from system outputs, we concatenate all sentences into one continuous stream of words.  \n\nTime Shift for Better Simultaneity To ease the memory overload of the evaluators, we reduced the delay by shifting the subtitles ahead in time. The shift was done differently for the systems and for the interpretation:  \n\n· Systems: Each translated sentence was shifted such that its first word was emitted immediately as the source sentence audio began. If there were some words from previous sentence that have not been displayed yet, the emission of the words from the next sentence was delayed. These words were displayed right after all the last word of the previous sentence.  \n\n· Interpreting: Since we did not have the sentence alignment, we shifted the whole interpretation by a constant such that the last word was emitted with the end of the last uttered word in the source speech. This shift constant was chosen empirically.  \n\nTwo Test Sets: Common and Non-Native There were two test sets used for the human evaluation: the common test set (consisting of the TED talks used in the Offline Speech Translation task and serving also in the automatic evaluation of Simultaneous Translation task); and a non-native test set. The non-native test set was already used in IWSLT Non-Native Translation Task in 2020 and it is described in Ansari et al. (2020) Appendix A.6. Specifically, we used the Antrecorp (Machacek et al., 2019; mock business presentations by high-school students) and the auditing presentations (SAO) parts.  \n\nWe show the size of the corpus, as well as the amount of annotation collected in Table 17.  \n\nProcessing of Collected Rankings  Once the results are collected, they are processed as follows. We first inspect the timestamps on the ratings, and remove any that are more than 20 seconds greater than the length of the audio. Because of the natural delay (even with the time-shift) and because the collection process is subject to network and computational constraints, there can be ratings that are timestamped greater than the audio length. If the difference is however too high, we judge it to be an annotation error. We also remove any annotated audio where there is fewer than one rating per 20 seconds, since the annotators were instructed to annotate every 5-10 seconds.  \n\nObtaining Final Scores  To calculate a score for each system, we average the ratings across each annotated audio, then average across all the annotated audios pertaining to each system-latency combination. This type of averaging renders all input speeches equally important and it is not affected by the speech length.  \n\nThe results are shown in Table 18. We observe that, overall, the systems do worse on the non-native audios than they do on the common portion of the test set, whereas the human interpreter performs similarly on both portions.  \n\nIndeed some of the high latency systems are rated slightly higher (on average) than the human interpreter on the common portion.  \n\nThere is a clear effect of latency in almost all systems, with the low-latency subtitles generally rated poorer than the high-latency subtitles by our annotators. This effect is strong in some systems (e.g. FBK) but weaker in others (e.g. NAIST).  \n\n# A.1.2 MQM-based Human Evaluation for English-to-Japanese Simultaneous Task  \n\nFor the English-to-Japanese Simultaneous Translation Task, we conducted a human evaluation using a variant of Multidimensional Quality Metrics (MQM). MQM has been used in recent MT evaluation studies (Freitag et al., 2021a) and WMT Metrics shared task (Freitag et al., 2021b). For the evaluation of Japanese translations, we used JTF Translation Quality Evaluation Guidelines (JTF, 2018), distributed by Japan Translation Federation (JTF). The guidelines are based on MQM but include some modifications in consideration of the property of the Japanese language.  \n\nWe hired a Japanese-native professional translator as the evaluator. The evaluator checked translation hypotheses along with their source speech transcripts and chose the corresponding error category and severity for each translation hypothesis using a spreadsheet. Here, we asked the evaluator to focus only on Accuracy and Fluency errors, because other types of errors in Terminology, Style, and Locale convention would not be so serious in the evaluation of simultaneous translation. Finally, we calculated the cumulative error score for each system based on the error weighting presented by (Freitag et al., 2021a), where Critical and Major errors are not distinguished.  \n\n# A.2  Direct Assessment for Offline Speech Translation Task  \n\nFor the Offline Speech Translation Task (Section 3) we conducted a human evaluation campaign featuring the source-based direct assessment (DA) (Graham et al., 2013; Cettolo et al., 2017; Akhbardeh et al., 2021). In this setting, assessments were performed on a continuous scale between 0 and 100.  \n\nAnnotation Process  We collected segment-level annotations based on the automatic segmentation of the test data. Because we did not want issues from the segmentation to influence scores negatively, we provided translators not only with the source sentence and system translation, but also with the system translation of the previous and following segments. Annotators were then instructed as follows:  \n\n\"'Sentence boundary errors are expected and should not be factored in when judging translation quality. This is when the translation appears to be missing or adding extra words but the source was segmented at a different place. To this end, we have included the translations for the previous and next sentences also. If the source and translation are only different because of sentence boundary issues, do not let this affect your scoring judgement.\" No video or audio context was provided. Segments were shuffled and randomly assigned to annotators to avoid bias related to the presentation order. Annotations were conducted by a trusted vendor, with professional translators fuent in the source language and native in the target language. For English to German, we additionally collected annotations for the references, which received a considerably higher score than the best submitted system as expected (90.8 vs. 88.9).  \n\nComputing rankings System rankings are produced from the average DA scores computed from the average human assessment scores without and with standardization according to each individual annotator's mean and standard deviation, similarly to Akhbardeh et al. (2021). Clusters are identified by grouping together those systems which significantly outperform all others in lower ranking clusters, according to Wilcoxon rank-sum test $p\\,<\\,0.05$ . In Tables 23, 24, and 25 - which show the rankings - clusters are indicated by horizontal lines. Rank ranges giving an indication of the respective system's translation quality within a cluster are based on the same head-to-head statistical significance tests.  \n\nOfficial rankings and details on the evaluation campaign for the Offline Speech Translation Task are presented in Section 3.  \n\n# A.3  Speech to speech translation task  \n\nOutput speech translations were evaluated with respect to translation quality and speech quality.  \n\n· Translation quality: Bilingual annotators were presented with the source audio and the target audio, and gave scores on the translation quality between 1 and 5.   \n· Output speech quality: In addition to translation quality (capturing meaning), the quality of the speech output was also human-evaluated along three dimensions: naturalness (voice and pronunciation), clarity of speech (understandability), and sound quality (noise and other artifacts). These axes are more fine-grained than the traditional overall MOS score.  \n\nThe detailed guidelines for output speech quality were as follows:  \n\n· Naturalness: Recordings that sound human-like, with natural-sounding pauses, stress, and intonation, should be given a high score. Recordings that sound robotic, fat, or otherwise unnatural should be given a low score.   \n· Clarity of speech: Recordings with clear speech and no mumbling and unclear phrases should be given a high score. Recordings with a large amount of mumbling and unclear phrases should be given a low score.   \n· Sound quality: Recordings with clean audio and no noise and static in the background should be given a high score. Recordings with a large amount of noise and static in the background should be given a low score.  \n\n# A.4 Direct Assessment with Scalar Quality Metric for the Dialect and Isometric Speech Translation Tasks  \n\nFor the Dialect Speech Translation Task (Section 6) and Isometric SLT Task (Section 8) we piloted a human evaluation campaign featuring the source-based direct assessment (DA) (Graham et al., 2013; Cettolo et al., 2017; Akhbardeh et al., 2021) with document context extended with Scalar Quality Metric (SQM) (Freitag et al., 2021a). In this setting, assessments were performed on a continuous scale between O and 100 as in traditional DA but with 0-6 markings on the analogue slider and annotator guidelines based on those proposed by Freitag et al. (2021a). SQM helped standardizing scores across annotators.  \n\nTool  We used the Appraise evaluation framework53 (Federmann, 2018) for collecting segment-level judgements within document context. No video or audio context was provided. Annotation guidelines were adapted specifically for each task as described in Sections 6 and 8. Screenshots of an example annotation for the Dialect and Isometric Speech Translation Tasks are presented on Figures 6 and 7.  \n\nTask generation  A single task consisted of 100 segments from around 10 documents. Human references were included as additional system output to provide an estimate of human performance. Each individual annotator completed between 4 and 8 tasks. Whenever possible, we assigned tasks to annotators making sure that one annotator evaluates outputs from all systems on the same subset of the test set. This increased repetitiveness, but potentially improved consistency of assessments across systems.  \n\nAnnotation and quality control All annotators were either professional translators or linguists fuent in the source language and native in the target language or linguists, and the majority of them had previous experience in the evaluation of translation outputs.54 Although our annotators were professonals, we employed a standard quality filtering procedure. Around $10\\%$ of segments in each task were quality control items in the form of bad reference pairs distributed usually across one or two documents. Please refer to (Akhbardeh et al., 2021) for more details on the generation of bad references. Assessments of an annotator who has not demonstrated ability to reliably score degraded translations significantly lower than corresponding original system outputs using a paired significance test with $p<0.05$ wouldbe omitted from the evaluation. As expected, none of our annotators appeared unreliable.  \n\nWe have collected 47,834 assessments. This number already excludes documents with quality control items, which provides almost 2,000 annotations per system, including references.  \n\nComputing rankings System rankings are produced from the average DA scores computed from the average human assessment scores without and with standardization according to each individual annotator's mean and standard deviation, similarly to Akhbardeh et al. (2021). We exclude entire documents with one or more quality control items from ranking computation. Clusters are identified by grouping those systems together which significantly outperform all others in lower ranking clusters, according to Wilcoxonrank-sum test $p<0.05$ . In Tables 31 and 36 - which show the rankings - clusters are indicated by horizontal lines. Rank ranges giving an indication of the respective system’s translation quality within a cluster are based on the same head-to-head statistical significance tests.  \n\nOfficial rankings and details on the evaluation campaign for the Dialect Speech Translation Task and Isometric SLT Task are presented respectively in Sections 6 and 8.  \n\n# A.5 Formality Control  \n\nIn this section, we reproduce the instructions given to the translators for IT, JA and RU for the formality control shared task. Instructions for JA are similar but include some language-specific notes. For brevity, we also remove example translations show to the translators.  \n\nOverview  We would like to annotate multiple system outputs. For each of the 300 sentence ids (sid) there are 4-6 system outputs - please shuffle the order of the systems when showing it to annotators. We would like two annotators per target language.  \n\nGuidelines  You will be shown an English source sentence and a machine translation of the source sentence. Your task will be to label the translation based on the formality level. Note that labels that you generate will be on the sentence level (one label per sentence). For example, given the source sentence “It was nice chatting with you, have a great night! and a translation “Es war schon, mit Ihnen zu plaudern, haben Sie eine tolle Nacht!\"', you would label the example based on the formality level of the translation as one of Formal, Informal, Neutral, Other.  \n\n# Special Cases to Consider  \n\n1. Only label formality level, and ignore other mistakes such as a wrong sense.   \n2. Only label based on the formality level of the translation. Note that we don't want to label whether the formality level is correct in translation, but rather which formality level is marked in the translation.   \n3. If at least one word in the source is not translated at all and some meaning is lost, then label the translation as Other.  \n\n# Label Categories  \n\n1. Formal - The formality level is consistently Formal in the translation.  \n\n2. Informal - The formality level is consistently Informal in the translation.  \n\nutral - The translation is phrased in a way that does not explicitly express a formality leve  \n\n4. Other - Explain the reason in the Notes section.  \n\n- The formality level is inconsistent such as using both formal and informal pronouns.   \n- If at least one word in the source is not translated at all and should have been marked in the target language for formality and some meaning is lost.   \n- If you feel strongly that the translation does not fit into any of the cases listed above, please label it as “other” and explain the reason in the Notes section.  \n\n# Appendix B. Evaluation Results and Details  \n\n# B.1. Simultaneous Speech Translation  \n\n# Automatic Evaluation Results  \n\n· Summary of the results of the simultaneous speech translation for English-German.   \n· Results are reported on the blind test set and systems are grouped by latency regime (set on tst-COMMON v2) · For each entry for latency metric, the upper one is non computation aware, while the lower one is computation aware. · BLEU number in parenthesis indicate that the system does not satisfy the latency constraints.   \n· Raw system logs are also provided on the task web site.s\\* . Summary of the results of the simultaneous speech translation for English-Japanese.   \n$\\cdot$ Results are reported on the blind test set and systems are grouped by latency regime (set on tst-COMMON v2) $\\cdot$ For each entry for latency metric, the upper one is non computation aware, while the lower one is computation aware. Raw system logs are also provided on the task web site.36  \n\n<html><body><table><thead><tr><td></td><td colspan=\"4\"><b>Low Latency</b></td><td colspan=\"4\"><b>Medium Latency</b></td><td colspan=\"4\"><b>High Latency</b></td></tr><tr><td><b>Team</b></td><td><b>BLEU</b></td><td><b>AL</b></td><td><b>AP</b></td><td><b>DAL|E</b></td><td><b>BLEU</b></td><td><b>AL</b></td><td><b>AP</b></td><td><b>DAL </b></td><td><b>BLEU</b></td><td><b>AL</b></td><td><b>AP</b></td><td><b>DAL</b></td></tr></thead><tbody><tr><td colspan=\"10\">tst-COMMON v2</td><td></td><td></td><td></td><td></td></tr><tr><td>CUNI-KIT</td><td>26.82</td><td>0.96 2.94</td><td>0.77 1.52 0.58</td><td>2.07 6.38</td><td>31.47</td><td>31.47</td><td>0.86 1.39</td><td>2.96 5.80</td><td>32.87</td><td>5.54 3.66</td><td>0.96 1.37</td><td>4.45 6.61</td></tr><tr><td>FBK</td><td>13.38</td><td>0.94 1.23</td><td>0.58 0.66</td><td>1.31 1.47</td><td>25.08</td><td>2.48 1.99</td><td>0.80 0.93</td><td>2.36 2.79</td><td>30.07</td><td>4.49 3.92</td><td>0.95 1.09</td><td>4.15 4.70</td></tr><tr><td>HW-TSC</td><td>(18.56)</td><td>1.96 2.39</td><td>0.79 0.92</td><td>2.41 2.82</td><td>23.90</td><td>2.61 3.03</td><td>0.87 1.01</td><td>3.07 3.49</td><td>24.78</td><td>4.02 4.42</td><td>0.96 1.10</td><td>4.31 4.71</td></tr><tr><td>NAIST</td><td>17.54</td><td>0.99 1.58</td><td>0.68 0.87</td><td>1.50 2.43</td><td>19.15</td><td>1.93 2.15</td><td>0.82 0.91</td><td>3.63 3.99</td><td>19.45</td><td>4.23 3.98</td><td>0.94 1.01</td><td>5.17 5.50</td></tr><tr><td>UPV</td><td>20.82</td><td>0.86 2.23</td><td>0.70 1.18</td><td>1.43 3.71</td><td>27.80</td><td>1.93 3.70</td><td>0.83 1.43</td><td>2.34 5.06</td><td>29.78</td><td>3.46 6.23</td><td>0.93 1.71</td><td>3.71 7.53</td></tr><tr><td colspan=\"10\">CUNI-KIT</td><td></td><td></td><td></td><td></td></tr><tr><td>NAIST</td><td>20.56</td><td>10.23</td><td>0.77 0.89</td><td>0.77 0.89</td><td>20.12</td><td>20.12 23.31</td><td>1.37 0.78</td><td>6.27 2.37 3.24</td><td>24.11</td><td>6.12 4.10 4.05</td><td>1.36 0.95 0.96</td><td>4.36 4.93 7.29</td></tr><tr><td>HW-TSC</td><td>(13.97)</td><td>1.18 1.91</td><td>0.61 0.77</td><td>1.42 2.47 2.91</td><td>20.12</td><td>19.10</td><td>0.89 0.86</td><td>2.79 3.18</td><td>23.59</td><td>4.67</td><td>1.07 0.95</td><td>4.93 4.57</td></tr><tr><td>FBK</td><td>13.40</td><td>0.97 1.64</td><td>0.67 0.85</td><td>1.55 2.60</td><td>19.10</td><td>19.10</td><td>0.99 0.82</td><td>3.66 3.96</td><td>19.73</td><td>4.65 4.20</td><td>0.95 1.09</td><td>5.00 5.79</td></tr><tr><td>FBK</td><td>2.39</td><td>0.71 2.18</td><td>0.68 1.13</td><td>1.42 3.78</td><td>19.10</td><td>15.29</td><td>0.89 0.84</td><td>4.35 3.36</td><td>15.47</td><td>5.07 4.80</td><td>0.96 1.02 0.92</td><td>5.79 6.14 3.85</td></tr><tr><td>UPV</td><td>16.09</td><td>1.58</td><td>1.58</td><td>2.14</td><td>19.94</td><td> Segmentation 1</td><td>1.58</td><td>7.76</td><td>23.55</td><td>6.35 3.51</td><td>0.92 1.63</td><td>7.82</td></tr><tr><td>CUNI-KIT</td><td>15.25</td><td>3.59 1.25 1.16</td><td>1.47 0.60 0.75</td><td>7.23 1.95 2.67</td><td>18.15</td><td>18.15</td><td>0.86 1.36</td><td>3.98 6.99</td><td>18.74</td><td>5.00</td><td>0.97</td><td>5.67 8.16</td></tr><tr><td>FBK</td><td>9.20</td><td>2.65 3.10 0.97</td><td>0.79 0.88 0.66 0.65</td><td>3.23 3.59 1.75</td><td>14.58</td><td>15.16</td><td>0.91 0.87 0.80</td><td>3.58 3.94 3.07</td><td>17.71</td><td>4.75 5.41</td><td>0.96 1.07 1.37</td><td>5.08 5.71</td></tr><tr><td colspan=\"10\">NAIST</td><td></td><td></td><td></td></tr><tr><td>CUNI-KIT</td><td>19.51</td><td>4.45</td><td>0.34 0.39</td><td>1.17 1.30</td><td>15.12</td><td>15.12</td><td>0.69 0.75 0.61</td><td>3.17 3.77 2.65</td><td>21.82</td><td>4.81</td><td>0.88</td><td>5.71</td></tr><tr><td>NAIST</td><td>2.87</td><td>0.68 1.07 3.79</td><td>1.43 0.34 0.39</td><td>11.29 1.17 1.30</td><td>4.67</td><td>21.41</td><td>0.74 1.28</td><td>4.10</td><td>17.89</td><td>4.12 7.64</td><td>1.03 0.93 0.98</td><td>6.59 4.51 6.26</td></tr><tr><td>UPV</td><td>12.23</td><td>0.73</td><td>0.66</td><td>2.71</td><td>15.86</td><td> Segmentation 2</td><td>1.35</td><td>5.91</td><td>17.89</td><td>7.64</td><td>0.93 1.67</td><td>4.51 8.86</td></tr><tr><td colspan=\"10\">NAIST</td><td></td><td></td><td></td></tr><tr><td>NAIST</td><td>11.77</td><td>2.11 0.55</td><td>0.83 0.62</td><td>4.32 1.78</td><td>13.49</td><td>3.05</td><td>0.90 0.70</td><td>8.42 2.71</td><td>13.64</td><td>9.26 3.74</td><td>1.00 0.97 0.86</td><td>5.84 6.73 10.62</td></tr><tr><td>FBK</td><td>4.45</td><td>11.77</td><td>0.74 0.60 0.83</td><td>3.58 1.92 4.32</td><td>2.76</td><td>13.49</td><td>0.75 0.88</td><td>3.77 4.75</td><td>18.66</td><td>4.62 7.66 5.56</td><td>1.29 0.85</td><td>11.31 5.50 7.06</td></tr><tr><td>FBK</td><td>(12.53)</td><td>2.66 0.93 1.92</td><td>0.74 0.60 0.63</td><td>2.81 3.58</td><td>17.92</td><td>17.92</td><td>0.69 0.75</td><td>3.17 3.77</td><td>21.82</td><td>20.89</td><td>0.85 0.96</td><td>5.50 6.35</td></tr><tr><td colspan=\"10\">HW-TSC</td><td></td><td></td><td></td></tr><tr><td>NAIST</td><td>UPV</td><td>UPV</td><td>0.62 1.03</td><td>1.78 5.84</td><td>18.32</td><td>18.32</td><td>0.90 0.70 1.17</td><td>8.42 2.71 7.29</td><td>5.68</td><td>9.26 3.74</td><td>0.97 1.03</td><td>6.73 10.62</td></tr><tr><td>NAIST</td><td>11.77</td><td>2.11 0.55</td><td>0.60 0.83 0.62</td><td>1.92 4.32 1.78</td><td>13.49</td><td>13.49</td><td>0.84 0.90 0.70</td><td>7.75 8.42 2.71</td><td>13.64</td><td>8.76 9.26</td><td>0.97 1.03</td><td>6.73 10.62</td></tr><tr><td>UPV</td><td>14.89</td><td>0.55 2.85</td><td>0.62 1.03</td><td>1.78 5.84</td><td>18.32</td><td>18.32</td><td>0.70 1.17</td><td>2.71 7.29</td><td>20.72</td><td>3.74 7.75</td><td>0.82 1.48</td><td>11.23 4.62 11.16</td></tr></tbody></table></body></html>  \n\n<html><body><table><thead><tr><td><b> </b></td><td colspan=\"4\"><b>Low Latency</b></td><td colspan=\"4\"><b>Low Latency</b></td><td colspan=\"4\"><b>Medium Latency</b></td></tr><tr><td><b>High Latency</b></td><td><b>Team</b></td><td><b>BLEU</b></td><td><b>AL</b></td><td><b>AP</b></td><td><b>DAL —</b></td><td><b>BLEU</b></td><td><b>AL</b></td><td><b>AP</b></td><td><b>DAL 一</b></td><td><b>BLEU</b></td><td><b>AL</b></td><td><b>AP</b></td></tr></thead><tbody><tr><td colspan=\"10\">DAL</td></tr><tr><td>tst-COMMON v2</td><td>CUNI-KIT</td><td>16.92</td><td>2.46 3.84</td><td>0.90 1.38</td><td>3.22 5.45</td><td>16.94</td><td>3.77 5.20</td><td>0.97 1.34</td><td>4.29 6.03</td><td>16.91</td><td>4.13 5.61</td><td>0.98 1.34</td></tr><tr><td>6.20 4.53</td><td>HW-TSC</td><td>7.27</td><td>2.28 2.61</td><td>0.81 0.92</td><td>2.68 2.91</td><td>12.17</td><td>2.92 3.30</td><td>0.92 1.06</td><td>3.71 3.38</td><td>11.56</td><td>3.40 3.79</td><td>0.95 1.09</td></tr><tr><td>4.16 3.84</td><td>NAIST</td><td>9.25</td><td>2.24 2.65</td><td>0.88 1.03</td><td>3.04 3.50</td><td>9.90</td><td>3.95 4.26</td><td>0.96 1.07</td><td>4.94 4.59</td><td>10.22</td><td>4.73 5.05</td><td>0.99 1.09</td></tr><tr><td colspan=\"10\">5.30 4.96</td></tr><tr><td>Gold Segmentation</td><td>CUNI-KIT</td><td>16.50</td><td>4.10 2.71 2.44</td><td>0.90 1.37</td><td>3.35 5.79</td><td>16.68</td><td>4.10 5.66</td><td>0.97 1.34</td><td>4.57 6.48</td><td>16.75</td><td>4.42 6.02</td><td>0.98 1.34</td></tr><tr><td>6.67 4.80 3.96</td><td>HW-TSC</td><td>5.62</td><td>2.75 2.28</td><td>0.79 0.89 0.86</td><td>2.92 2.71 2.89</td><td>11.79</td><td>3.11 3.48</td><td>1.04 0.91</td><td>3.80 3.46</td><td>11.48</td><td>4.00 3.63</td><td>1.08 0.95 0.98</td></tr><tr><td>4.30</td><td>NAIST</td><td>8.70</td><td>2.68</td><td>0.99</td><td>3.40</td><td>9.41</td><td>3.41 3.73</td><td>0.94 1.04</td><td>4.87 4.46</td><td>9.83</td><td>4.66 4.98</td><td>1.06</td></tr><tr><td colspan=\"10\">5.44 5.08</td></tr><tr><td>Segmentation 1</td><td>CUNI-KIT</td><td>12.24</td><td>4.99 3.25 3.12</td><td>1.34 0.79 0.87</td><td>7.14 3.75 4.22</td><td>12.38</td><td>7.17 4.05 5.12</td><td>1.33 0.91 0.97</td><td>8.10 4.55 5.79</td><td>12.44</td><td>7.58 4.68 5.54</td><td>1.33 0.95 0.98</td></tr><tr><td>6.03 8.22</td><td>NAIST</td><td>6.67</td><td>2.40 2.87</td><td>0.81 0.92</td><td>3.90 3.35</td><td>7.13</td><td>4.46 4.64</td><td>0.93 1.00</td><td>5.56 5.97</td><td>7.39 8.18</td><td>5.86 6.19 5.09</td><td>0.98 1.04 1.05</td></tr><tr><td>5.14 5.49</td><td>HW-TSC</td><td>4.15</td><td>2.87</td><td>0.92</td><td>3.90</td><td>7.13</td><td>4.98 Segmentation 2</td><td>1.00</td><td>5.97</td><td>7.39</td><td>6.19</td><td>1.04</td></tr><tr><td colspan=\"10\">6.23 6.58</td></tr><tr><td>0.77 4.54</td><td>HW-TSC</td><td>8.10</td><td>2.56</td><td>0.52 0.58 1.27</td><td>2.99 9.80</td><td>14.82</td><td>7.95 3.62</td><td>1.29 0.76 0.90</td><td>11.45 4.38 7.37</td><td>14.71</td><td>6.55 9.06</td><td>0.93 1.30</td></tr><tr><td>8.11</td><td>NAIST</td><td>2.36</td><td>3.05 2.67 3.32</td><td>0.58 0.73 0.85</td><td>3.26 3.81</td><td>10.23</td><td>4.33 5.28 5.71</td><td>0.87 0.91</td><td>4.38 5.01 9.00</td><td>8.70</td><td>4.39 5.17 8.69</td><td>0.82 0.94 0.97</td></tr></tbody></table></body></html>  \n\n· Summary of the results of the simultaneous speech translation for English-Mandarin. $\\cdot$ Results are reported on the blind test set and systems are grouped by latency regime (set on tst-COMMON v2) $\\cdot$ For each entry for latency metric, the upper one is non computation aware, while the lower one is computation aware. · BLEU number in parenthesis indicate that the system does not satisfy the latency constraints. Raw system logs are also provided on the task web site.7  \n\n<html><body><table><thead><tr><td></td><td colspan=\"4\"><b>Low Latency</b></td><td colspan=\"4\"><b>Medium Latency</b></td><td colspan=\"4\"><b>High Latency</b></td></tr><tr><td><b>Team</b></td><td><b>BLEU</b></td><td><b>AL</b></td><td><b>AP</b></td><td><b>DAL </b></td><td><b>BLEU</b></td><td><b>AL</b></td><td><b>AP</b></td><td><b>DAL|</b></td><td><b>BLEU</b></td><td><b>AL</b></td><td><b>AP</b></td><td><b>DAL</b></td></tr></thead><tbody><tr><td colspan=\"10\">tst-COMMON v2</td><td></td> colspan=\"7\"></td></tr><tr><td>AISP-SJTU</td><td>25.87</td><td>1.99 3.39</td><td>0.87 1.81</td><td>3.35 6.53</td><td>26.21</td><td>2.97 5.14</td><td>0.94 1.97</td><td>4.16 7.80</td><td>26.46</td><td>3.97 7.12</td><td>0.98 2.05</td><td>4.62 8.42</td></tr><tr><td>CUNI-KIT</td><td>23.61</td><td>1.75 3.11</td><td>0.85 1.34</td><td>2.56 4.77</td><td>24.37</td><td>2.79 4.16</td><td>0.93 1.34</td><td>3.49 5.32</td><td>24.58</td><td>3.67 5.12</td><td>0.97 1.34</td><td>4.22 5.88</td></tr><tr><td>HW-TSC</td><td>(18.60)</td><td>2.18 2.56</td><td>0.84 0.97</td><td>2.66 2.93</td><td>22.51</td><td>2.88 3.26</td><td>0.92 1.06</td><td>3.33 3.62</td><td>23.60</td><td>3.46 3.82</td><td>0.95 1.09</td><td>3.81 4.10</td></tr><tr><td>Xiaomi</td><td>19.74</td><td>1.97 3.63</td><td>0.83 1.32</td><td>2.64 4.82</td><td>20.18</td><td>2.84 6.46</td><td>0.90 2.18</td><td>3.62 9.68</td><td>20.10</td><td>8.36 3.73</td><td>0.95 2.31</td><td>4.18 10.81</td></tr><tr><td colspan=\"10\">Gold Segmentation</td><td></td><td></td></tr><tr><td>AISP-SJTU</td><td>30.74</td><td>26.71</td><td>0.83 1.32</td><td>6.72 2.65</td><td>31.22</td><td>3.08 5.22</td><td>0.93 1.72</td><td>4.34 8.06</td><td>32.09</td><td>7.34 4.15</td><td>0.97 1.81</td><td>4.83 8.75</td></tr><tr><td>CUNI-KIT</td><td>26.71</td><td>3.29 2.25 1.92</td><td>1.32 0.82</td><td>5.09 2.68</td><td>27.09</td><td>2.93 4.29</td><td>0.92 1.31</td><td>3.62 5.57</td><td>27.22</td><td>3.90 5.39</td><td>0.97 1.32</td><td>4.44 6.23</td></tr><tr><td>HW-TSC</td><td>(19.83)</td><td>2.66 2.04</td><td>0.95 0.82</td><td>2.98 2.62</td><td>26.02</td><td>3.00 3.37</td><td>0.91 1.04</td><td>3.43 3.72</td><td>27.65</td><td>3.62 4.00</td><td>0.95 1.08</td><td>3.97 4.29 4.29</td></tr><tr><td>Xiaomi</td><td>23.75</td><td>2.04 3.61</td><td>1.28</td><td>4.78</td><td>24.34</td><td>2.97 6.48</td><td>0.90 2.11</td><td>3.71 9.86</td><td>24.56</td><td>3.87 8.55</td><td>0.95 2.28</td><td>11.15</td></tr><tr><td colspan=\"10\">AISP-SJTU</td><td></td></tr><tr><td>CUNI-KIT</td><td>20.80</td><td>2.29 (16.09)</td><td>1.27 0.82</td><td>6.30 3.68 3.51</td><td>21.83</td><td>6.56 3.82</td><td>1.60 0.92 0.93</td><td>9.57 4.79 5.30</td><td>26.01</td><td>9.04 5.18</td><td>0.97 1.70</td><td>5.93 10.48</td></tr><tr><td>HW-TSC</td><td>24.90</td><td>3.03 3.47 4.13</td><td>0.82 0.91</td><td>3.68 3.99</td><td>20.29</td><td>5.73 3.90</td><td>1.30 0.91</td><td>7.16 4.50</td><td>21.66</td><td>4.95 6.96</td><td>0.97 1.31</td><td>5.66 7.81</td></tr><tr><td>CUNI-KIT</td><td>19.79</td><td>3.47 2.30 4.03</td><td>0.91 0.79</td><td>3.99 3.20</td><td>20.42</td><td>4.31 3.53</td><td>0.91 1.00 0.89</td><td>4.50 4.80 4.57</td><td>21.52</td><td>4.63 5.04</td><td>1.31 0.95 1.05</td><td>5.11 5.43 5.25</td></tr><tr><td colspan=\"10\">Xiaomi</td><td></td></tr><tr><td>AISP-SJTU</td><td>24.96</td><td>1.97</td><td>1.20 0.59</td><td>8.54 3.00</td><td>25.01</td><td>5.57</td><td>1.21 0.74</td><td>5.19 9.32</td><td>24.81</td><td>10.29 5.11</td><td>0.94 1.70</td><td>9.26 17.68</td></tr><tr><td>AISP-SJTU</td><td>28.36</td><td>4.20 2.26</td><td>1.20 0.59</td><td>3.41 8.54</td><td>25.01</td><td>3.46 5.57 8.33</td><td>1.64 0.80</td><td>16.96 5.19 8.71</td><td>29.03</td><td>5.97 10.29</td><td>0.94 1.70</td><td>9.26</td></tr><tr><td colspan=\"10\">CUNI-KIT</td><td></td></tr><tr><td>HW-TSC</td><td>(13.80)</td><td>22.15</td><td>0.68 0.69</td><td>3.39 3.04</td><td>8.80</td><td>4.00</td><td>0.85 0.77</td><td>4.21 4.70 4.84</td><td>24.77</td><td>7.48 4.21</td><td>1.25 0.82</td><td>10.79 5.21 7.01</td></tr><tr><td>HW-TSC</td><td>(13.80)</td><td>2.26 2.93</td><td>0.59 0.68</td><td>3.00 3.39</td><td>22.27</td><td>3.24 4.00</td><td>0.74 0.85</td><td>4.21 4.70</td><td>24.77</td><td>4.21 5.00</td><td>0.82 0.93</td><td>5.21 5.76</td></tr><tr><td>22.27 4.00</td><td>4.50</td><td>2.93 1.85</td><td>0.68 0.69</td><td>3.39 3.04 8.10</td><td>22.71</td><td>4.00 3.23</td><td>0.85 0.77</td><td>18.63</td><td>23.08</td><td>5.00 4.43</td><td>0.82 0.93 0.83</td><td>5.63</td></tr></tbody></table></body></html>  \n\n: Summary of the results of the simultaneous speech translation for text-to-text track, English-Mandarin . The input of the each system is the output from the provided streaming ASR model, and the latency is evaluated in seconds. · Results are reported on the blind test set and systems are grouped by latency regime (set on tst-COMMON v2) : For each entry for latency metric, the upper one is non computation aware, while the lower one is computation aware. Raw system logs are also provided on the task web site.58  \n\n<html><body><table><thead><tr><td></td><td colspan=\"4\"><b>Low Latency</b></td><td colspan=\"4\"><b>Medium Latency</b></td><td colspan=\"4\"><b>High Latency</b></td></tr></thead><tbody><tr><td>Team</td><td>BLEU</td><td>AL</td><td>AP</td><td>DAL —</td><td>BLEU</td><td>AL</td><td>AP</td><td>DAL|</td><td>BLEU</td><td>AL</td><td>AP</td><td>DAL</td></tr><tr><td colspan=\"10\">tst-COMMON v2</td><td></td><td></td><td></td></tr><tr><td>AISP-SJTU</td><td></td><td></td><td></td><td></td><td>18.36</td><td>2.35 2.89</td><td>0.88 1.05</td><td>4.04 4.83</td><td></td><td></td><td></td><td></td></tr><tr><td>HW-TSC</td><td>14.63</td><td>1.38 1.88</td><td>0.73 0.86</td><td>2.01 2.43</td><td>17.40</td><td>2.31 2.85</td><td>0.86 1.00</td><td>2.90 3.37</td><td>18.19</td><td>3.08 3.65</td><td>0.92 1.07</td><td>3.57 4.08</td></tr><tr><td> Xiaomi</td><td>19.74</td><td>1.97 3.63</td><td>0.83 1.32</td><td>4.82 2.64</td><td>20.18</td><td>2.84 6.46</td><td>0.90 2.18</td><td>3.62 9.68</td><td>20.10</td><td>3.73 8.36</td><td>2.31 0.95</td><td>10.81 4.18</td></tr><tr><td colspan=\"10\">AISP-SJTU</td><td></td><td></td></tr><tr><td>HW-TSC</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Xiaomi</td><td>23.75</td><td>2.04 3.61 1.86</td><td>0.82 1.28 0.81</td><td>2.29 2.62 4.78</td><td>21.03</td><td>2.37 2.85 2.97</td><td>0.97 0.90 0.85</td><td>3.29 3.71 2.89</td><td>24.56 22.56</td><td>3.68 3.87</td><td>1.03 0.95</td><td>4.29 11.15 4.05</td></tr><tr><td>2.97 24.34</td><td>1.28</td><td>1.28</td><td>4.78</td><td>4.78</td><td>19.18</td><td>6.48 Segmentation 1</td><td>2.11</td><td>9.86</td><td>24.56</td><td>8.55</td><td>2.28</td><td>11.15</td></tr><tr><td colspan=\"10\">AISP-SJTU</td><td></td><td></td><td></td></tr><tr><td>Xiaomi</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HW-TSC</td><td>19.79</td><td>1.85</td><td>1.19</td><td>5.43</td><td> Segmentation 2</td><td>3.14 3.53 7.62</td><td>0.82 0.91 0.94</td><td>5.38 3.50</td><td>4.94</td><td>3.66</td><td>0.89 0.99</td><td>4.37</td></tr><tr><td>HW-TSC</td><td>14.44</td><td>1.98 2.30 1.53</td><td>0.76 0.79 0.68</td><td>2.76 3.20 2.42</td><td>17.63 20.29</td><td>3.14 3.53 2.64</td><td>0.91 0.89</td><td>3.92 4.57 11.32</td><td>18.85 20.47</td><td>4.18 4.60</td><td>0.99 0.94 2.09</td><td>5.25 4.84 12.54</td></tr><tr><td colspan=\"10\">4.03</td><td></td><td></td><td></td></tr><tr><td>AISP-SJTU</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HW-TSC</td><td>11.56</td><td>11.56</td><td>1.19</td><td>3.04</td><td>21.61</td><td>3.71 4.08 2.17</td><td>0.88 0.94</td><td>8.70</td><td>9.72</td><td>9.72</td><td>0.86</td><td>0.86</td></tr><tr><td>HW-TSC</td><td>11.56</td><td>1.20 1.77</td><td>0.50 0.57 0.69</td><td>2.05 2.42</td><td>21.61</td><td>4.08 2.17</td><td>0.68 0.76</td><td>3.25 3.76 9.35</td><td>20.37</td><td>3.17 3.99</td><td>0.77</td><td>4.33 4.96</td></tr></tbody></table></body></html>  \n\nHuman Evaluation Results   \n\n\n<html><body><table><thead><tr><td><b>English-Japanese</b></td><td><b>BLEU</b></td><td><b>Error score</b></td><td><b>#Critical</b></td><td><b>#Major</b></td><td><b>#Minor</b></td></tr></thead><tbody><tr><td>CUNI-KIT (high)</td><td>19.43</td><td>219</td><td>0</td><td>31</td><td>64</td></tr><tr><td>CUNI-KIT (low)</td><td>18.29</td><td>225</td><td>0</td><td>31</td><td>70</td></tr><tr><td>HW-TSC (medium)</td><td>15.21</td><td>472</td><td>2</td><td>85</td><td>37</td></tr><tr><td>NAIST (medium)</td><td>11.49</td><td>628</td><td>12</td><td>109</td><td>23</td></tr></tbody></table></body></html>  \n\nTable 16: Human evaluation results on one talk in the English-to-Japanese Simultaneous speech-to-speech translation task. Error weights are 5 for Critical and Major errors and 1 for Minor errors.   \n\n\n<html><body><table><thead><tr><td></td><td><b>Common</b></td><td><b>Non-native</b></td></tr></thead><tbody><tr><td>Number of distinct audios</td><td>17</td><td>43</td></tr><tr><td>Mean length of audio (secs)</td><td>886</td><td>209</td></tr><tr><td>Total of subtitled audios annotated</td><td>439</td><td>1159</td></tr><tr><td> Mean ratings per annotated audio</td><td>164.4</td><td>40.8</td></tr></tbody></table></body></html>  \n\nTable 17: Human evaluation for the English-to-German task on two test sets: the Common one used also in automatic scoring and Non-native one. We show the size of the evaluation corpus, and the number of ratings collected.   \n\n\n<html><body><table><thead><tr><td></td><td></td><td colspan=\"3\"><b>Common</b></td><td colspan=\"3\"><b>Non-native</b></td></tr><tr><td></td><td><b>System</b></td><td><b>Low</b></td><td><b>Medium</b></td><td><b>High</b></td><td><b>Low</b></td><td><b>Medium</b></td><td><b>High</b></td></tr></thead><tbody><tr><td rowspan=\"5\">Human</td><td>CUNI-KIT</td><td>3.13</td><td>3.26</td><td>3.44</td><td>2.46</td><td>2.57</td><td>2.98</td></tr><tr><td>UPV</td><td>2.96</td><td>3.32</td><td>3.40</td><td>2.07</td><td>2.55</td><td>2.72</td></tr><tr><td>FBK</td><td>2.23</td><td>3.02</td><td>3.44</td><td>1.76</td><td>2.20</td><td>2.36</td></tr><tr><td>HW-TSC</td><td>2.34</td><td>2.60</td><td>2.60</td><td>1.58</td><td>1.81</td><td>1.69</td></tr><tr><td>NAIST</td><td>2.28</td><td>2.31</td><td>2.44</td><td>1.77</td><td>1.64</td><td>1.60</td></tr><tr><td></td><td>Average±Std.dev.</td><td>2.59±0.38</td><td>2.90±0.39</td><td>3.06±0.45</td><td>1.93±0.31</td><td>2.15±0.38</td><td>2.27±0.55</td></tr><tr><td></td><td>Interpreting</td><td colspan=\"3\">2.99</td><td colspan=\"3\">3.22</td></tr><tr><td rowspan=\"5\">BLEU</td><td>CUNI-KIT</td><td>20.56</td><td>23.31</td><td>24.11</td><td>16.64</td><td>22.89</td><td>25.65</td></tr><tr><td>UPV</td><td>16.09</td><td>19.94</td><td>23.55</td><td>13.59</td><td>21.16</td><td>22.90</td></tr><tr><td>FBK</td><td>10.23</td><td>20.12</td><td>23.59</td><td>8.40</td><td>16.51</td><td>20.42</td></tr><tr><td>HW-TSC</td><td>13.97</td><td>19.10</td><td>19.73</td><td>10.35</td><td>13.47</td><td>13.55</td></tr><tr><td>NAIST</td><td>13.40</td><td>15.29</td><td>15.47</td><td>6.11</td><td>9.33</td><td>9.25</td></tr></tbody></table></body></html>  \n\nTable 18: Human evaluation results for English-to-German Simultaneous task (upper part), compared with automatic BLEU scores (lower part). We calculate a mean score for each annotated audio file, then take the mean across all annotated audio files, for each system-latency combination. We highlight the best results in bold and report also the average across all submissions of a given latency band. The final row shows the results for human simultaneous interpreting (transcribed). The lower part reports the BLEU scores for the gold segmentation of the Common part of the test set (reported already on page 44) and for the Non-native part of the test set. The BLEU scores correlate very well with the human judgement for each of the test sets parts: Pearson correlation across the systems and latency regimes is .898 for the Common part and .933 for the Non-native part. When considered together, the correlation decreases to .858.  \n\n# B.2. Offline Speech Translation  \n\n# Automatic Evaluation Results  \n\n# Speech Translation: TED English-German tst 2022  \n\n: Systems are ordered according to BLEU_NewRef: BLEU score computed on the NEW reference set (literal translations). $B L E U$ scores are given as percent figures $(\\%)$  \n\nfficial results of the automatic evaluation for the Offline Speech Translation Task, English to Germ.   \n\n\n<html><body><table><thead><tr><td><b>System</b></td><td><b>BLEU_NewRef</b></td><td><b>BLEU_TEDRef</b></td><td><b>BLEU_MultiRef</b></td></tr></thead><tbody><tr><td>USTC-NELSLIP cascade</td><td>26.7</td><td>23.9</td><td>37.6</td></tr><tr><td>Y1 end2end</td><td>25.7</td><td>23.6</td><td>36.5</td></tr><tr><td>Y1 cascade</td><td>25.6</td><td>23.7</td><td>36.4</td></tr><tr><td>USTC-NELSLIP end2end</td><td>25.3</td><td>22.9</td><td>35.7</td></tr><tr><td>NEMO</td><td>24.7</td><td>22.3</td><td>34.8</td></tr><tr><td>HW-TSC</td><td>24.2</td><td>20.8</td><td>33.5</td></tr><tr><td>KIT</td><td>23.9</td><td>22.0</td><td>33.8</td></tr><tr><td>FBK</td><td>23.6</td><td>21.0</td><td>32.9</td></tr><tr><td>UPC</td><td>23.0</td><td>20.8</td><td>32.3</td></tr><tr><td>ALEXA AI</td><td>22.6</td><td>20.1</td><td>31.5</td></tr></tbody></table></body></html>  \n\n# Speech Translation: TED English-German tst 2021  \n\n$\\cdot$ Systems are ordered according to BLEU_TEDRe f: BLEU score computed on the ORIGINAL reference set. $B L E U$ scores are given as percent figures $(\\%)$ · End-to-end systems are indicated by gray background.  \n\nTable 20: Progress test set results of the automatic evaluation for the Offine Speech Translation Task, English to Japanese.   \n\n\n<html><body><table><thead><tr><td><b>System</b></td><td><b>BLEU_NewRef</b></td><td><b>BLEU_TEDRef</b></td><td><b>BLEU_MultiRef</b></td></tr></thead><tbody><tr><td>USTC-NELSLIP cascade</td><td>28.9</td><td>24.1</td><td>40.3</td></tr><tr><td>Y1 cascade</td><td>28.1</td><td>23.2</td><td>39.0</td></tr><tr><td>Y1 end2end</td><td>27.8</td><td>23.1</td><td>38.8</td></tr><tr><td>HW-TSC</td><td>27.5</td><td>21.2</td><td>36.9</td></tr><tr><td>USTC-NELSLIP end2end</td><td>27.2</td><td>23.0</td><td>38.4</td></tr><tr><td>FBK</td><td>25.5</td><td>21.3</td><td>35.6</td></tr><tr><td>KIT</td><td>24.7</td><td>22.4</td><td>36.2</td></tr><tr><td>last Year's best</td><td>24.6</td><td>20.3</td><td>34.0</td></tr><tr><td>UPC</td><td>24.5</td><td>20.9</td><td>34.8</td></tr><tr><td>ALEXA AI</td><td>24.4</td><td>20.6</td><td>34.5</td></tr></tbody></table></body></html>  \n\n# Speech Translation: TED English-Chinese tst 2022  \n\n$\\cdot$ Systems are ordered according to BLEU_TEDRe f: BLEU score computed on the ORIGINAL reference set. $B L E U$ scores are given as percent figures $(\\%)$ · End-to-end systems are indicated by gray background.  \n\nTable 21: Official results of the automatic evaluation for the Offine Speech Translation Task, English to Chinese.   \n\n\n<html><body><table><thead><tr><td><b>System</b></td><td><b>BLEU_NewRef</b></td><td><b>BLEU_TEDRef</b></td><td><b>BLEU_MultiRef</b></td></tr></thead><tbody><tr><td>USTC-NELSLIP cascade</td><td>35.8</td><td>35.7</td><td>44.1</td></tr><tr><td>Y1 cascade</td><td>34.7</td><td>35.0</td><td>42.9</td></tr><tr><td>HW-TSC</td><td>34.6</td><td>33.4</td><td>42.1</td></tr><tr><td>Y1 end2end</td><td>34.1</td><td>34.6</td><td>42.3</td></tr><tr><td>USTC-NELSLIP end2end</td><td>33.8</td><td>34.1</td><td>41.9</td></tr><tr><td>NEMO</td><td>33.3</td><td>33.7</td><td>41.2</td></tr><tr><td>NIUTRANS</td><td>32.3</td><td>33.2</td><td>40.5</td></tr><tr><td>KIT</td><td>31.1</td><td>32.0</td><td>39.0</td></tr><tr><td>ALEXA AI</td><td>30.4</td><td>30.8</td><td>37.9</td></tr><tr><td>UPC</td><td>29.2</td><td>29.9</td><td>36.4</td></tr><tr><td>NEURAL.AI</td><td>22.8</td><td>23.0</td><td>28.2</td></tr></tbody></table></body></html>  \n\n# Speech Translation: TED English-Japanese tst 2022  \n\n<html><body><table><thead><tr><td><b>System</b></td><td><b>BLEU_NewRef</b></td><td><b>BLEU_TEDRef</b></td><td><b>BLEU_MultiRef</b></td></tr></thead><tbody><tr><td>HW-TSC</td><td>22.7</td><td>14.3</td><td>30.8</td></tr><tr><td>USTC-NELSLIP cascade</td><td>21.6</td><td>20.1</td><td>33.4</td></tr><tr><td>USTC-NELSLIP end2end</td><td>20.5</td><td>17.4</td><td>30.5</td></tr><tr><td>Y1 end2end</td><td>18.0</td><td>19.1</td><td>29.8</td></tr><tr><td>Y1 cascade</td><td>18.7</td><td>20.2</td><td>31.3</td></tr><tr><td>KIT</td><td>16.2</td><td>17.2</td><td>26.4</td></tr><tr><td>UPC</td><td>15.1</td><td>15.6</td><td>24.7</td></tr><tr><td>ALEXA AI</td><td>15.3</td><td>16.2</td><td>25.3</td></tr></tbody></table></body></html>  \n\nTable 22: Official results of the automatic evaluation for the Offline Speech Translation Task, English to Japanese.   \nSpeech Translation: TED English-German tst 2022 (subset)   \nHuman Evaluation Results   \nTable 23: Official results of the human evaluation for the Offline Speech Translation Task, English to German. Systems ordered by the standardized DA $z$ -score. Systems within clusters indicated by horizontal lines are considered tied. Scores collected using direct assessment with previous/next-sentence context.   \n\n\n<html><body><table><thead><tr><td><b>Rank</b></td><td><b>Ave.</b></td><td><b>Ave. z</b></td><td><b>System</b></td></tr></thead><tbody><tr><td>1-3</td><td>88.9</td><td>0.142</td><td>USTC-NELSLIP cascade</td></tr><tr><td>1-4</td><td>87.4</td><td>0.075</td><td>USTC-NELSLIP end2end</td></tr><tr><td>1-4</td><td>87.6</td><td>0.063</td><td>Y1 cascade</td></tr><tr><td>4-9</td><td>86.5</td><td>0.008</td><td>KIT</td></tr><tr><td>4-9</td><td>86.1</td><td>-0.004</td><td>FBK</td></tr><tr><td>2-7</td><td>86.3</td><td>-0.011</td><td>Y1 end2end</td></tr><tr><td>4-9</td><td>85.6</td><td>-0.023</td><td>NEMO</td></tr><tr><td>5-9</td><td>85.4</td><td>-0.039</td><td>UPC</td></tr><tr><td>5-9</td><td>84.8</td><td>-0.076</td><td>HW-TSC</td></tr><tr><td>10</td><td>83.9</td><td>-0.133</td><td>ALEXA AI</td></tr></tbody></table></body></html>  \n\nSpeech Translation: TED English-Chinese tst 2022 (subset)   \nTable 24: Official results of the human evaluation for the Offline Speech Translation Task, English to Chinese. Systems ordered by the standardized DA $z$ -score. Systems within clusters indicated by horizontal lines are considered tied. Scores collected using direct assessment with previous/next-sentence context.   \n\n\n<html><body><table><thead><tr><td><b>1</b></td><td><b>85.6</b></td><td><b>0.184</b></td><td><b>USTC-NELSLIP cascade</b></td></tr></thead><tbody><tr><td>2-5</td><td>84.2</td><td>0.121</td><td>Y1 end2end</td></tr><tr><td>2-7</td><td>84.0</td><td>0.097</td><td>Y1 cascade</td></tr><tr><td>2-7</td><td>83.5</td><td>0.086</td><td>USTC-NELSLIP end2end</td></tr><tr><td>3-8</td><td>83.1</td><td>0.061</td><td>NEMO</td></tr><tr><td>3-8</td><td>83.2</td><td>0.057</td><td>KIT</td></tr><tr><td>2-7</td><td>82.8</td><td>0.038</td><td>HW-TSC</td></tr><tr><td>6-9</td><td>82.4</td><td>0.023</td><td>NIUTRANS</td></tr><tr><td>8-10</td><td>81.6</td><td>-0.023</td><td>ALEXA AI</td></tr><tr><td>9-10</td><td>80.8</td><td>-0.055</td><td>UPC</td></tr><tr><td>11</td><td>71.2</td><td>-0.589</td><td>NEURAL.AI</td></tr></tbody></table></body></html>  \n\nTable 25: Official results of the human evaluation for the Offline Speech Translation Task, English to Japanese. Systems ordered by the standardized DA $z$ -score. Systems within clusters indicated by horizontal lines are considered tied. Scores collected using direct assessment with previous/next-sentence context.   \n\n\n<html><body><table><thead><tr><td><b>Speech Translation: TED English-Japanese tst 2022 (subset)</b></td><td><b>78.4</b></td><td><b>0.086</b></td><td><b>Y1 cascade</b></td></tr></thead><tbody><tr><td>1-4</td><td>78.4</td><td>0.086</td><td>Y1 cascade</td></tr><tr><td>1-4</td><td>77.6</td><td>0.065</td><td>USTC-NELSLIP cascade</td></tr><tr><td>1-4</td><td>77.6</td><td>0.061</td><td>Y1 end2end</td></tr><tr><td>1-4</td><td>76.6</td><td>0.005</td><td>HW-TSC</td></tr><tr><td>5-6</td><td>76.3</td><td>-0.009</td><td>USTC-NELSLIP end2end</td></tr><tr><td>5-6</td><td>76.3</td><td>-0.013</td><td>KIT</td></tr><tr><td>7-8</td><td>74.7</td><td>-0.082</td><td>ALEXA AI</td></tr><tr><td>7-8</td><td>73.2</td><td>-0.113</td><td>UPC</td></tr></tbody></table></body></html>  \n\n# B.3. Speech to Speech Translation  \n\nResults for the speech to speech translation task, described in Section 4.  \n\nWhile both automatic metrics and human evaluation are provided, the task ranking was determined by human evaluation of translation quality (Table 28).  \n\n<html><body><table><thead><tr><td><b>System</b></td><td><b>BLEU</b></td><td><b>chrF</b></td></tr></thead><tbody><tr><td>MLLP-VRAIN</td><td>19.70</td><td>53.15</td></tr><tr><td>HW-TSC primary</td><td>19.58</td><td>53.81</td></tr><tr><td>HW-TSC contrastive3</td><td>19.35</td><td>53.75</td></tr><tr><td>HW-TSC contrastivel</td><td>19.22</td><td>53.65</td></tr><tr><td>HW-TSC contrastive2</td><td>18.90</td><td>53.00</td></tr><tr><td>UPC</td><td>16.38</td><td>50.20</td></tr><tr><td>Reference text (+TTS)</td><td>68.46</td><td>88.78</td></tr><tr><td>FBK Offline (+TTS)</td><td>17.37</td><td>51.21</td></tr><tr><td>KIT Offline (+TTS)</td><td>16.63</td><td>50.43</td></tr><tr><td>Reference text (+normalization)</td><td>100.00</td><td>100.00</td></tr><tr><td>FBK Offline (+normalization)</td><td>23.44</td><td>55.84</td></tr><tr><td>KIT Offline (+normalization)</td><td>23.51</td><td>55.18</td></tr></tbody></table></body></html>  \n\nTable 26: S2ST: automatic metrics. Speech output is first transcribed with ASR before scoring against reference text. Text is normalized for scoring (punctuation and case removed, whitespace standardized). The effects of synthesis $+\\operatorname{ASR}$ transcription are shown by synthesizing the reference text and selected Ofine task submissions and scoring after ASR.  \n\n<html><body><table><thead><tr><td><b>System</b></td><td><b>nat.</b></td><td><b>clar.</b></td><td><b>sound.</b></td></tr></thead><tbody><tr><td>MLLP-VRAIN</td><td>4.156 (0.037)</td><td>4.626 (0.028)</td><td>4.562 (0.028)</td></tr><tr><td>HW-TSC primary</td><td>3.135 (0.042)</td><td>3.835 (0.037)</td><td>3.867 (0.034)</td></tr><tr><td>UPC</td><td>3.118 (0.042)</td><td>3.786 (0.037)</td><td>3.862 (0.032)</td></tr><tr><td>Reference</td><td>3.116 (0.043)</td><td>3.678 (0.038)</td><td>3.799 (0.032)</td></tr></tbody></table></body></html>  \n\nTable 27: S2ST: speech quality human evaluation. System outputs were evaluated along 3 dimensions, which are more fine-grained than mean opinion score: speech naturalness (nat.), clarity of speech (clar.) and sound quality (sound.). Numbers in parenthesis indicate a $95\\%$ confidence interval.   \nTable 28: S2ST: translation quality human evaluation. The initial MLLP-VRAIN submission had a misalignment and was later fixed. As a result, the number of samples for MLLP-VRAIN is 1000 instead of 2059. Numbers in parenthesis indicate a $95\\%$ confidence interval.   \n\n\n<html><body><table><thead><tr><td><b>System</b></td><td><b>Translation quality</b></td></tr></thead><tbody><tr><td>HW-TSC primary</td><td>4.606 (0.034)</td></tr><tr><td>MLLP-VRAIN</td><td>4.439 (0.057)</td></tr><tr><td>UPC</td><td>4.374 (0.041)</td></tr><tr><td>Reference</td><td>4.369 (0.038)</td></tr></tbody></table></body></html>  \n\n# B.4. Dialect Speech Translation  \n\nAutomatic Evaluation Results   \nTunisian Arabic $\\rightarrow$ English   \n\n\n<html><body><table><thead><tr><td rowspan=\"2\"><b>Team</b></td><td rowspan=\"2\"><b>Condition</b></td><td rowspan=\"2\"><b>System</b></td><td colspan=\"5\"><b>test2</b></td><td rowspan=\"2\"><b>test1 BLEU</b></td></tr><tr><td><b>BLEU↑</b></td><td><b>BP</b></td><td><b>pr1</b></td><td><b>chrF2</b></td><td><b>TER↓</b></td></tr></thead><tbody><tr><td>CMU</td><td> dialect adapt</td><td>primary (E2)</td><td>20.8 ± 0.7</td><td>0.931</td><td>53.1</td><td>44.3</td><td>64.5</td><td>19.5</td></tr><tr><td>CMU</td><td>dialect adapt</td><td>contrastive</td><td>20.7 ± 0.7</td><td>0.929</td><td>53</td><td>44.1</td><td>64.6</td><td>19.3</td></tr><tr><td>CMU</td><td>basic</td><td>primary (E1)</td><td>20.4 ± 0.7</td><td>0.944</td><td>52.2</td><td>43.8</td><td>65.4</td><td>19.2</td></tr><tr><td>CMU</td><td>basic</td><td>contrastive</td><td>20.1 ± 0.7</td><td>0.936</td><td>52.2</td><td>43.5</td><td>65.3</td><td>19</td></tr><tr><td>CMU</td><td>dialect adapt</td><td>contrastive (D6)</td><td>19.8 ± 0.7</td><td>0.902</td><td>53.2</td><td>43.3</td><td>64.6</td><td>18.9</td></tr><tr><td>CMU</td><td>basic</td><td>contrastive (D3)</td><td>19.7 ± 0.7</td><td>0.916</td><td>52.4</td><td>43</td><td>65.5</td><td>18.7</td></tr><tr><td>CMU</td><td> dialect adapt</td><td>contrastive (D5)</td><td>19.5 ± 0.6</td><td>0.896</td><td>53.2</td><td>42.8</td><td>64.6</td><td>18.3</td></tr><tr><td>CMU</td><td>dialect adapt</td><td>contrastive (C6)</td><td>19.4 ± 0.6</td><td>0.937</td><td>50.7</td><td>43</td><td>67.1</td><td>17.9</td></tr><tr><td>CMU</td><td>basic</td><td>contrastive (D2)</td><td>19.1 ± 0.6</td><td>0.939</td><td>51.3</td><td>42.7</td><td>66.5</td><td>18.1</td></tr><tr><td>JHU</td><td>dialect adapt</td><td> primary</td><td>18.9 ± 0.7</td><td>0.99</td><td>48</td><td>42.1</td><td>70.2</td><td>17.8</td></tr><tr><td>JHU</td><td>unconstrain.</td><td>primary</td><td>18.7 ± 0.7</td><td>0.959</td><td>48.7</td><td>41.6</td><td>69.2</td><td>17.5</td></tr><tr><td>CMU</td><td>basic</td><td>contrastive (C3)</td><td>18.6 ± 0.6</td><td>0.942</td><td>49.4</td><td>41.8</td><td>68.3</td><td>17.5</td></tr><tr><td>JHU</td><td>basic</td><td>primary</td><td>17.1 ± 0.6</td><td>0.973</td><td>46.8</td><td>40.4</td><td>71.4</td><td>16.1</td></tr><tr><td>ON-TRAC</td><td>unconstrain.</td><td> post-evaluation</td><td>14.4 ± 0.6</td><td>1</td><td>42.7</td><td>36.5</td><td>76.7</td><td>-</td></tr><tr><td>ON-TRAC</td><td>unconstrain.</td><td>contrastivel</td><td>13.6 ± 0.6</td><td>1</td><td>41.7</td><td>35.7</td><td>78.3</td><td>—</td></tr><tr><td>ON-TRAC</td><td>basic</td><td>primary</td><td>12.4 ± 0.6</td><td>0.8</td><td>44.3</td><td>32.8</td><td>75.5</td><td>-</td></tr><tr><td>ON-TRAC</td><td>unconstrain.</td><td>contrastive2</td><td>11.3 ± 0.5</td><td>0.95</td><td>38.7</td><td>32.7</td><td>80.6</td><td>-</td></tr><tr><td>Baseline</td><td>basic</td><td>baseline E2E</td><td>11.1 ± 0.5</td><td>0.885</td><td>40</td><td>31.9</td><td>77.8</td><td>10.1</td></tr></tbody></table></body></html>  \n\nTable 29: Automatic evaluation results for the Dialect Speech Translation Task. Systems are ranked in order of the official metric: BLEU on test2 blind evaluation set. We also report chrF2, TER, as well as the brevity penalty (BP) and 1-gram precision (pr1) components of BLEU. We further use bootstrap resampling (1k samples) and report the $95\\%$ confidence interval for BLEU on test2 (Koehn, 2004). For details of each system, refer to the system name in the respective papers.  \n\nTunisian Arabic ASR Automatic Evaluation Results   \n\n\n<html><body><table><thead><tr><td rowspan=\"2\"><b>ASR System</b></td><td colspan=\"2\"><b>WER↓</b></td><td colspan=\"2\"><b>CER↓</b></td></tr><tr><td><b>Orig</b></td><td><b>Norm</b></td><td><b>Orig</b></td><td><b>Norm</b></td></tr></thead><tbody><tr><td>JHU / basic / primary</td><td>70.5</td><td>43.8</td><td>30.5</td><td>22.5</td></tr><tr><td>JHU / dialect adapt / primary</td><td>70.1</td><td>42.9</td><td>30.4</td><td>22.3</td></tr><tr><td> JHU / unconstrained / primary</td><td>69.4</td><td>42.8</td><td>30.6</td><td>22.5</td></tr><tr><td>ON-TRAC / unconstrained / primary</td><td>68.2</td><td>45.1</td><td>28.4</td><td>21.5</td></tr><tr><td>ON-TRAC / unconstrained / post-eval</td><td>65.7</td><td>41.5</td><td>28.1</td><td>21.1</td></tr></tbody></table></body></html>  \n\nTable 30: Word Error Rate (WER) and Character Error Rate (CER) of the ASR component of submitted cascaded systems on test2. This is computed by comparing ASR hypotheses with the Tunisian manual transcripts. The original version (Orig) matches the minimal text pre-processing provided by the organizer's data preparation scripts, and results in relatively high WER. Transcription standards for primarily spoken dialects are challenging, so it may be beneficial as diagnosis to run some additional Arabic-specific normalization (Norm) for e.g. Alif, Ya, Ta-Marbuta on the hypotheses and transcripts before computing WER/CER. We are grateful to Ahmed Ali for assistance on this.  \n\n# Human Evaluation Results  \n\nTable 31: Official results of the human evaluation for the Dialect Speech Translation Task. Systems ordered by the standardized DA $z$ -score. Systems within clusters indicated by horizontal lines are considered tied. Scores collected using the document-level $\\scriptstyle\\mathrm{DA}+\\mathrm{SQM}$ task in Appraise.   \nTunisian Arabic $\\rightarrow$ English   \n\n\n<html><body><table><thead><tr><td><b>Rank</b></td><td><b>Ave.</b></td><td><b>Ave.z</b></td><td><b>Team / Condition / System</b></td></tr></thead><tbody><tr><td>1</td><td>76.6</td><td>0.457</td><td>translator-A</td></tr><tr><td>2-3</td><td>66.5</td><td>0.119</td><td>CMU / dialect adapt / contrastive (D6)</td></tr><tr><td>2-3</td><td>66.5</td><td>0.114</td><td>CMU / dialect adapt / primary (E2)</td></tr><tr><td>4-5</td><td>62.7</td><td>-0.032</td><td>JHU / dialect adapt / primary</td></tr><tr><td>4-5</td><td>60.7</td><td>-0.093</td><td> JHU / basic condition / primary</td></tr><tr><td>6-7</td><td>56.1</td><td>-0.271</td><td>ON-TRAC / unconstrained / primary</td></tr><tr><td>6-7</td><td>55.3</td><td>-0.302</td><td>ON-TRAC / unconstrained / contrastive1</td></tr></tbody></table></body></html>  \n\nBelow you see a document with 10 sentences in Tunisian Arabic (left columns) and their corresponding candidate translations in English (right columns). Score each candidate sentence translation in the document context.You may revisit already scored sentences and update their scores at any time by clickingatasourcetext.  \n\nPlease take into consideration the following aspects when assessing the translation quality:  \n\n· The document is part of a conversation thread between two speakers, and each segment starts with either \"A:\" or \"B:\" to indicate the speaker identity.   \n· Some candidate translations may contain \"%pw\" or \"% pw\", but since they correspond to partial words in the speech they should not be considered aserrorsduringevaluation.   \n· Please ignore the lack of capitalization and punctuation. Also, please ignore “incorrect” grammar and focus more on the meaning: these segments are informal conversations, so grammatical rules are not so strict.   \n· The original source is Tunisian Arabic speech. There may be some variation in the transcription.  \n\nAssess the translationquality ona continuous scaleusing thequality levels described asfollows:  \n\n0: Nonsense/No meaning preserved: Nearly all information is lost between the translation and source.   \n2: Some meaning preserved: The translation preserves some of the meaning of the source but misses significant parts. The narrative is hard to follow   \ndue to fundamental errors.   \n4: Most meaning preserved: The translation retains most of the meaning of the source. It may have some minor mistakes or contextual inconsistencies.   \n6: Perfect meaning: The meaning of the translation is completely consistent with the source and the surrounding context (if applicable).  \n\n![](images/7d2568b18dee8b9c94a2448a0a60c62b8a61fcddbed26d34b2941338965b9f82.jpg)  \n\nPlease score the overall document translation quality (you can score the whole document only after scoring allindividual sentences first).  \n\nAssess the translationqualityon a continuous scaleusing thequalitylevelsdescribed asfollows:  \n\nu. Nulselisenu mealmy pieseiveu. Iveally allmiullllauullislust veiweell uie ulalislauull allu suuiue   \n2: Some meaning preserved: The translation preserves some of the meaning of the source but misses significant parts. The narrative is hard to follow due to fundamental errors.  \n\n4: Most meaning preserved: The translation retains most of the meaning of the source. It may have some minor mistakes or contextual inconsistencies   \n6: Perfect meaning: The meaning of the translation is completely consistent with the source and the surrounding context (if applicable).  \n\n![](images/7712d803d2ba4425d293086e8c8a0991dd0f44c22be502e66c012ee181dfff60.jpg)  \nFigure 6: A screen shot of an example annotation task in Appraise featuring source-based document-level Direct Assessment with SQM for the Dialect Speech Translation Task.  \n\nB.5. Formality Control For Speech Translation   \nAutomatic Evaluation Results   \n\n\n<html><body><table><thead><tr><td colspan=\"2\"></td><td colspan=\"2\"><b>EN→HI</b></td><td colspan=\"2\"><b>EN→JA</b></td><td colspan=\"2\"><b>EN→DE</b></td><td colspan=\"2\"><b>EN→ES</b></td><td colspan=\"2\"><b>EN→IT</b></td><td colspan=\"2\"><b>EN→RU</b></td></tr></thead><tbody><tr><td>Setting</td><td>System baseline ALEXA AI UMD</td><td>BLEU 22.0 38.9 12.1</td><td>COMETE 0.67 0.874</td><td>BLEU 17.9 19.4</td><td>COMETE 0.24</td><td>BLEU 32.6</td><td>0.55 COMET|BLEU</td><td>37.4</td><td>0.70 COMET|BLEU</td><td>32.2</td><td>COMET 0.64</td><td>BLEU 19.5</td><td>COMET 0.32</td></tr><tr><td>unconstrained</td><td>ALEXA AI UMD Uos</td><td>38.9 12.1</td><td>0.874 0.192</td><td>19.4 11.6</td><td>0.378 -0.023</td><td>22.4 32.5</td><td>0.161 0.497</td><td>27.8 37.0</td><td>0.344 0.635</td><td>22.9 33.1</td><td>0.247 0.562</td><td>14.4 21.5</td><td>0.075</td></tr><tr><td>constrained</td><td>Uos</td><td></td><td></td><td></td><td></td><td>31.5</td><td>0.448</td><td>36.5</td><td>0.608</td><td>33.1</td><td>0.553</td><td>21.4</td><td>0.329</td></tr></tbody></table></body></html>  \n\nTable 32: Automatic evaluation using sacrebleu and COMET on generic test sets. For $\\mathrm{EN{\\rightarrow}D E}$ ,ES, IT, RU participants were asked to evaluated their systems on MuST-C dataset. We have also included baseline models trained in the unconstrained setting for comparison. For $\\mathrm{EN{\\rightarrow}H I}$ JA participants were evaluated on WMT Newstest 2014 and 2020 respectively.   \n\n\n<html><body><table><thead><tr><td colspan=\"2\"></td><td colspan=\"8\"><b>Supervised</b></td><td colspan=\"4\"><b>Zero-shot</b></td></tr><tr><td colspan=\"2\"></td><td colspan=\"2\"><b>EN→HI</b></td><td colspan=\"2\"><b>EN→JA</b></td><td colspan=\"2\"><b>EN→DE</b></td><td colspan=\"2\"><b>EN→ES</b></td><td colspan=\"2\"><b>EN→IT</b></td><td colspan=\"2\"><b>EN→RU</b></td></tr><tr><td><b>Setting</b></td><td><b>System</b></td><td><b>F</b></td><td><b>1</b></td><td><b>F</b></td><td><b>1</b></td><td><b>F</b></td><td><b>1</b></td><td><b>F</b></td><td><b>1</b></td><td><b>F</b></td><td><b>1</b></td><td><b>F</b></td><td><b>1</b></td></tr></thead><tbody><tr><td rowspan=\"3\">unconstrained ALEXA AI</td><td>baseline (generic)</td><td>96.3</td><td>3.70</td><td>49.6</td><td>50.3</td><td>45.8</td><td>54.2</td><td>36.6</td><td>63.4</td><td>3.70</td><td>94.5</td><td>93.4</td><td>6.60</td></tr><tr><td>99.6</td><td>99.8</td><td>88.8</td><td>98.8</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>UMD</td><td>99.4</td><td>98.7</td><td>86.3</td><td>97.5</td><td>99.4</td><td>96.5</td><td>99.5</td><td>93.2</td><td>32.8</td><td>97.9</td><td>100.0</td><td>1.10</td></tr><tr><td></td><td>Uos</td><td></td><td></td><td></td><td></td><td>100.0</td><td>100.0</td><td>98.1</td><td>100.0</td><td>51.2</td><td>98.6</td><td>99.5</td><td>85.8</td></tr><tr><td>constrained</td><td>Uos</td><td></td><td></td><td></td><td></td><td>100.0</td><td>88.6</td><td>87.4</td><td>98.0</td><td>29.5</td><td>92.9</td><td>98.1</td></tr></tbody></table></body></html>  \n\nTable 33: Automatic evaluation of formality control accuracy (M-ACC) reported for Formal (F) and Informal (I). For comparison, we have included our baseline generic (uncontrolled) performance on the formality testset. For $\\mathrm{EN}{\\rightarrow}\\mathrm{IT}$ RU participants were given a zero-shot task and asked to train a formality controlled model without labelled training data in Italian or Russian.   \nHuman Evaluation Results   \nTable 34: Percentage of system outputs (with a given formality level (Control) and seting (Setting) labeled by professional translators according to the formality level: formal (F), informal (I), neutral (N), other (O). IAA was computed using the Krippendorff's $\\alpha$ coefficient.   \n\n\n<html><body><table><thead><tr><td><b>Lang.</b></td><td><b>Setting</b></td><td><b>Sys.</b></td><td><b>Control</b></td><td><b>F</b></td><td><b>I</b></td><td><b>N</b></td><td><b>0</b></td><td><b>IAA</b></td></tr></thead><tbody><tr><td rowspan=\"4\">EN→JA</td><td rowspan=\"4\">unconstrained</td><td>UMD</td><td>Formal</td><td>89.3</td><td>0.7</td><td>0.0</td><td>9.7</td><td rowspan=\"4\">0.90</td></tr><tr><td>UMD</td><td>Informal</td><td>2.0</td><td>92.5</td><td>0.0</td><td>5.5</td></tr><tr><td>ALEXA AI</td><td>Formal</td><td>82.8</td><td>1.3</td><td>0.0</td><td>15.5</td></tr><tr><td>ALEXA AI</td><td>Informal</td><td>3.0</td><td>82.7</td><td>0.0</td><td>14.3</td></tr><tr><td rowspan=\"6\">EN→IT</td><td rowspan=\"4\">unconstrained</td><td>UMD</td><td>Formal</td><td>13.7</td><td>25.2</td><td>47.0</td><td>14.2</td><td rowspan=\"6\">0.91</td></tr><tr><td>UMD</td><td> Informal</td><td>1.0</td><td>78.3</td><td>11.5</td><td>9.2</td></tr><tr><td>Uos</td><td>Formal</td><td>6.0</td><td>7.2</td><td>81.3</td><td>5.5</td></tr><tr><td>Uos</td><td> Informal</td><td>0.3</td><td>81.0</td><td>13.2</td><td>5.5</td></tr><tr><td rowspan=\"2\">constrained</td><td>Uos</td><td>Formal</td><td>0.2</td><td>10.2</td><td>87.7</td><td>2.0</td></tr><tr><td>Uos</td><td>Informal</td><td>0.2</td><td>36.3</td><td>58.3</td><td>5.2</td></tr><tr><td rowspan=\"6\">EN→RU</td><td rowspan=\"4\">unconstrained</td><td>UMD</td><td>Formal</td><td>77.2</td><td>0.2</td><td>7.0</td><td>15.7</td><td rowspan=\"6\">0.85</td></tr><tr><td>UMD</td><td>Informal</td><td>74.3</td><td>0.7</td><td>7.8</td><td>17.2</td></tr><tr><td>Uos</td><td>Formal</td><td>85.0</td><td>0.3</td><td>6.0</td><td>8.7</td></tr><tr><td>Uos</td><td> Informal</td><td>10.3</td><td>71.3</td><td>3.2</td><td>15.2</td></tr><tr><td rowspan=\"2\"> constrained</td><td>Uos</td><td>Formal</td><td>85.3</td><td>2.0</td><td>5.7</td><td>7.0</td></tr><tr><td>Uos</td><td>Informal</td><td>65.0</td><td>12.7</td><td>6.3</td><td>16.0</td></tr></tbody></table></body></html>  \n\nB.6. Isometric Spoken Language Translation   \nAutomatic MT Evaluation Results   \n\n\n<html><body><table><thead><tr><td><b>System</b></td><td><b>BERTScore En→De</b></td><td><b>LC</b></td><td><b>BLEU(detok)</b></td></tr></thead><tbody><tr><td>STRONGBASELINE*</td><td>77.44</td><td>68.0</td><td>21.6</td></tr><tr><td>APPTEK-Constrained</td><td>77.32</td><td>86.5</td><td>18.7</td></tr><tr><td>HW-TSC-Unconstrained</td><td>75.79</td><td>96.5</td><td>20.2</td></tr><tr><td>APV-Unconstrained</td><td>73.68</td><td>39.0</td><td>16.5</td></tr><tr><td>WEAKBASELINE</td><td>74.86</td><td>43.0</td><td>15.5</td></tr><tr><td>HW-TSC-Constrained</td><td>74.07</td><td>98.0</td><td>17.9</td></tr><tr><td colspan=\"4\">En→Fr</td></tr><tr><td>System</td><td>BERTScore</td><td>LC</td><td>BLEU(detok)</td></tr><tr><td>STRONGBASELINE*</td><td>81.75</td><td>75.5</td><td>36.2</td></tr><tr><td>NUV-Unconstrained</td><td>79.96</td><td>47.5</td><td>27.1</td></tr><tr><td>APV-Unconstrained</td><td>77.77</td><td>45.0</td><td>32.9</td></tr><tr><td>HW-TSC-Constrained</td><td>76.11</td><td>96.0</td><td>31.5</td></tr><tr><td>WEAKBASELINE</td><td>77.18</td><td>37.0</td><td>25.2</td></tr><tr><td colspan=\"4\">En→Es</td></tr><tr><td>System</td><td>BERTScore</td><td>LC</td><td>BLEU(detok)</td></tr><tr><td>STRONGBASELINE*</td><td>81.86</td><td>80.5</td><td>36</td></tr><tr><td>APV-Unconstrained</td><td>80.87</td><td>49.5</td><td>35.3</td></tr><tr><td>HW-TSC-Constrained</td><td>78.57</td><td>96.5</td><td>29.9</td></tr><tr><td>WEAKBASELINE</td><td>78.32</td><td>51.0</td><td>27.7</td></tr></tbody></table></body></html>  \n\nTable 35: Automatic evaluation results for Isometric SLT task on the blind test set. Metrics are computed using the submissions primary system. System ranking follows the human evaluation ranking in Table 36. If BERTScore is a tie, system with the highest LC wins $(^{*})$ . BERTSCore and LC are the primary metrics for the task, detoknizedBLEU is provided only as a secondary reference. Bold highlights the top score.  \n\n# MT Human Evaluation Results  \n\nTable 36: Official results of the text-based human evaluation for the Isometric SLT Task. Systems ordered by thestandardizedDA $z$ -score. Systems within clusters indicated by horizontal lines are considered tied. Scores collected using the document-level $\\scriptstyle\\mathrm{DA}+\\mathrm{SQM}$ task in Appraise.   \n\n\n<html><body><table><thead><tr><td colspan=\"4\"><b>En→De</b></td></tr><tr><td><b> Rank</b></td><td><b>Ave.</b></td><td><b>Ave.z</b></td><td><b>System</b></td></tr></thead><tbody><tr><td>1</td><td>89.0</td><td>0.755</td><td>translator-A</td></tr><tr><td>2-3</td><td>72.6</td><td>0.189</td><td>STRONGBASELINE</td></tr><tr><td>2-3</td><td>69.9</td><td>0.123</td><td>APPTEK-Constrained</td></tr><tr><td>4-5</td><td>62.6</td><td>-0.153</td><td>HW-TSC-Unconstrained</td></tr><tr><td>4-6</td><td>62.1</td><td>-0.224</td><td>APV-Unconstrained</td></tr><tr><td>5-7</td><td>59.4</td><td>-0.298</td><td>WEAKBASELINE</td></tr><tr><td>6-7</td><td>56.3</td><td>-0.467</td><td>HW-TSC-Constrained</td></tr><tr><td colspan=\"4\">En→Fr</td></tr><tr><td> Rank</td><td>Ave.</td><td>Ave. z</td><td>System</td></tr><tr><td>1</td><td>80.8</td><td>0.624</td><td>translator-A</td></tr><tr><td>2-3</td><td>64.3</td><td>0.009</td><td>STRONGBASELINE</td></tr><tr><td>2-4</td><td>60.2</td><td>-0.152</td><td>NUV-constrained</td></tr><tr><td>3-6</td><td>58.0</td><td>-0.280</td><td>APV-Unconstrained</td></tr><tr><td>4-6</td><td>53.2</td><td>-0.348</td><td>HW-TSC-Constrained</td></tr><tr><td>4-6</td><td>53.6</td><td>-0.389</td><td>WEAKBASELINE</td></tr><tr><td colspan=\"4\">En→Es</td></tr><tr><td> Rank</td><td>Ave.</td><td>Ave.Z</td><td>System</td></tr><tr><td>1</td><td>82.5</td><td>0.601</td><td>translator-A</td></tr><tr><td>2-3</td><td>70.3</td><td>0.020</td><td>STRONGBASELINE</td></tr><tr><td>2-3</td><td>69.9</td><td>-0.031</td><td>APV-Unconstrained</td></tr><tr><td>4-5</td><td>64.0</td><td>-0.283</td><td>HW-TSC-Constrained</td></tr><tr><td>4-5</td><td>59.8</td><td>-0.409</td><td>WEAKBASELINE</td></tr></tbody></table></body></html>  \n\n# Automatic Dubbing Human Evaluation Results  \n\nTable 37: Automatic dubbing human evaluation results on pairwise comparisons of submitted systems for the Isometric SLT task. We report the Wins, i.e, the $\\%$ of times one condition is preferred over the other with statistical significance levels $p<0.01(^{*})$ and $p<0.05(^{+})$   \n\n\n<html><body><table><thead><tr><td colspan=\"2\"><b>En→De</b></td></tr><tr><td><b>Comparison</b></td><td><b>Wins (%)</b></td></tr></thead><tbody><tr><td>WEAKBASELINE vs APPTEK-Constrained</td><td>32.9 vs 49.8*</td></tr><tr><td>WEAKBASELINE vs HW-TSC-Constrained</td><td>29.0 vs 49.4*</td></tr><tr><td>WEAKBASELINE vs HW-TSC-Unconstrained</td><td>41.1 vs 44.2</td></tr><tr><td>WEAKBASELINE vs APV-Unconstrained</td><td>37.9 vs 42.5</td></tr><tr><td>WEAKBASELINEVSSTRONGBASELINE</td><td>29.0 vs 52.3*</td></tr><tr><td>APPTEK-Constrained vs HW-TSC-Constrained</td><td>42.4 vs 38.8</td></tr><tr><td>APPTEK-Constrained vs HW-TSC-Unconstrained</td><td>41.0 vs 38.0</td></tr><tr><td>APPTEK-Constrained vs APV-Unconstrained</td><td>43.9 vs 36.9</td></tr><tr><td>APPTEK-Constrainedvs STRONGBASELINE</td><td>38.0 vs 39.6</td></tr><tr><td>HW-TSC-Constrained vs HW-TSC-Unconstrained</td><td>38.3 vs 36.0</td></tr><tr><td>HW-TSC-Constrained vs APV-Unconstrained</td><td>44.3 vs 37.7</td></tr><tr><td>HW-TSC-Constrained vs STRONGBASELINE</td><td>36.0 vs 42.7</td></tr><tr><td>HW-TSC-Unconstrained vs APV-Unconstrained</td><td>49.3 vs 32.7*</td></tr><tr><td>HW-TSC-Unconstrained vs STRONGBASELINE</td><td>37.2 vs 41.8</td></tr><tr><td>APV-Unconstrained vs STRONGBASELINE</td><td>31.3 vs 49.7*</td></tr><tr><td colspan=\"2\">En-→Fr</td></tr><tr><td>Comparison</td><td>Wins (%)</td></tr><tr><td>WEAKBASELINE vs HW-TSC-Constrained</td><td>31.7 vs 51.7*</td></tr><tr><td>WEAKBASELINE vs NUV-Unconstrained</td><td>32.6 vs 50.9*</td></tr><tr><td>WEAKBASELINE vs APV-Unconstrained</td><td>25.7 vs 55.7*</td></tr><tr><td>WEAKBASELINE VS STRONGBASELINE</td><td>26.7 vs 57.0*</td></tr><tr><td>HW-TSC-Constrained vs NUV-Unconstrained</td><td>40.0 vs 40.0</td></tr><tr><td>HW-TSC-Constrained vs APV-Unconstrained</td><td>46.7 vs 34.7+</td></tr><tr><td>HW-TSC-Constrained vs STRONGBASELINE</td><td>31.9 vs 49.1*</td></tr><tr><td>NUV-Unconstrained vs APV-Unconstrained</td><td>35.6 vs 40.0</td></tr><tr><td>NUV-Unconstrained vs STRONGBASELINE</td><td>29.0 vs 48.6*</td></tr><tr><td>APV-Unconstrained vs STRONGBASELINE</td><td>34.3 vs 44.7</td></tr><tr><td colspan=\"2\">En→Es</td></tr><tr><td>Comparison</td><td>Wins (%)</td></tr><tr><td>WEAKBASELINE vs HW-TSC-Constrained</td><td>21.0 vs 51.0*</td></tr><tr><td>WEAKBASELINE vs APV-Unconstrained</td><td>30.3 vs 46.7*</td></tr><tr><td>WEAKBASELINE VS STRONGBASELINE</td><td>24.3 vs 53.7*</td></tr><tr><td>HW-TSC-Constrained vs APV-Unconstrained</td><td>37.7 vs 35.7</td></tr><tr><td>HW-TSC-Constrained vs STRONGBASELINE</td><td>34.3 vs 40.0</td></tr><tr><td>APV-Unconstrained vs STRONGBASELINE</td><td>30.3 vs 44.7*</td></tr></tbody></table></body></html>  \n\n<html><body><table><thead><tr><td></td><td><b>En→De</b></td><td><b>En→De</b></td></tr><tr><td><b> Rank</b></td><td><b>Nwins</b></td><td><b>System</b></td></tr></thead><tbody><tr><td>1</td><td>5</td><td>STRONGBASELINE</td></tr><tr><td>2</td><td>4</td><td>APPTEK-Constrained</td></tr><tr><td>3</td><td>3</td><td>HW-TSC-Constrained</td></tr><tr><td>4</td><td>2</td><td>HW-TSC-Unconstrained</td></tr><tr><td>5</td><td>1</td><td>APV-Unconstrained</td></tr><tr><td>6</td><td>0</td><td>WEAKBASELINE</td></tr><tr><td></td><td></td><td>En→Fr</td></tr><tr><td>Rank</td><td>Nwins</td><td>System</td></tr><tr><td>1</td><td>4</td><td>STRONGBASELINE</td></tr><tr><td>2</td><td>2</td><td>HW-TSC-Constrained</td></tr><tr><td>3</td><td>2</td><td>APV-Unconstrained</td></tr><tr><td>4</td><td>1</td><td>NUV-Constrained</td></tr><tr><td>5</td><td>0</td><td>WEAKBASELINE</td></tr><tr><td></td><td></td><td>En→Es</td></tr><tr><td> Rank</td><td>Nwins</td><td>System</td></tr><tr><td>1</td><td>3</td><td>STRONGBASELINE</td></tr><tr><td>2</td><td>2</td><td>APV-Unconstrained</td></tr><tr><td>3</td><td>1</td><td>HW-TSC-Constrained</td></tr><tr><td>4</td><td>0</td><td>WEAKBASELINE</td></tr></tbody></table></body></html>  \n\nTable 38: Results of human evaluation of dubbed videos. Systems are ranked using $N_{\\mathrm{Wins}}$ , i.e., the number of evaluations for which that systems was preferred over some other system.   \nTable 39: Results of automatic evaluation for subset of 60 dialogues used for dubbing evaluation using smoothness (Federico et al., 2020a) that measures the stability of speaking rate across contiguous phrases and length compliance (LC).   \n\n\n<html><body><table><thead><tr><td colspan=\"3\"><b>En→De</b></td></tr><tr><td><b>Systems</b></td><td><b>Smoothness</b></td><td><b>LC</b></td></tr></thead><tbody><tr><td>STRONGBASELINE</td><td>88.55</td><td>68</td></tr><tr><td>APPTEK-Constrained</td><td>86.22</td><td>86.5</td></tr><tr><td>HW-TSC-Constrained</td><td>88.45</td><td>98</td></tr><tr><td>HW-TSC-Unconstrained</td><td>88.92</td><td>96.5</td></tr><tr><td>APV-Unconstrained</td><td>82.53</td><td>39</td></tr><tr><td>WEAKBASELINE</td><td>84.22</td><td>43</td></tr><tr><td colspan=\"3\">En→Fr</td></tr><tr><td>Systems</td><td>Smoothness</td><td>LC</td></tr><tr><td>STRONGBASELINE</td><td>80.66</td><td>75.5</td></tr><tr><td>HW-TSC-Constrained</td><td>77.93</td><td>96</td></tr><tr><td>APV-Unconstrained</td><td>78.31</td><td>45</td></tr><tr><td>NUV-Constrained</td><td>75.52</td><td>47.5</td></tr><tr><td>WEAKBASELINE</td><td>66.84</td><td>37</td></tr><tr><td colspan=\"3\">En→Es</td></tr><tr><td>Systems</td><td>Smoothness</td><td>LC</td></tr><tr><td>STRONGBASELINE</td><td>92.01</td><td>80.5</td></tr><tr><td>HW-TSC-Constrained</td><td>92.65</td><td>96.5</td></tr><tr><td>APV-Unconstrained</td><td>92.02</td><td>49.5</td></tr><tr><td>WEAKBASELINE</td><td>85.21</td><td>51</td></tr></tbody></table></body></html>  \n\nBelow you see a document with 10 sentences in English (left columns) and their corresponding candidate translations in German (deutsch) (right columns). Score each candidate sentence translation in the document context. You may revisit already scored sentences and update their scores at any timebyclicking atasourcetext.  \n\nPlease take into consideration the following aspects when assessing the translationquality:  \n\n·The source texts come from transcribed video content published onYouTube.   \n· Transcribed sentences have been split into segments based on pauses in the audio. It may happen that a single source sentence is split into multiple segments.   \n· Please score each segment (including very short segments) individually with regard to the source segment and the surrounding context.   \n·Take into account both grammar and meaning when scoring the segments.   \n·Please pay attention to issues like repeated or new content in the candidate translation, which is not present in the source text.  \n\nAssess the translation qualityon a continuous scaleusing thequalitylevels described as follows:  \n\n0: Nonsense/No meaning preserved: Nearly allinformation is lost between the translation and source. Grammar is irrelevant.   \n2: Some meaning preserved: The translation preserves some of the meaning of the source but misses significant parts. The narrative is hard to follow due to fundamental errors.Grammar may bepoor.   \n4: Most meaning preserved and few grammar mistakes: The translation retains most of the meaning of the source. It may have some grammar mistakesorminorcontextual inconsistencies.   \n6: Perfect meaning and grammar: The meaning of the translation is completely consistent with the source and the surrounding context (if applicable). Thegrammarisalsocorrect.  \n\n![](images/33b7f745deccfd79d987fe1083d0f2b56e37781965d4925f0ca1e923f5046292.jpg)  \n\nPlease score the overalldocument translationquality(you can score the whole documentonly after scoring allindividual sentencesfirst)  \n\nAssess the translationquality on a continuous scale using the quality levels described as follows:  \n\n0: Nonsense/No meaning preserved: Nearly all information is lost between the translation and source. Grammar is irrelevant. 2: Some meaning preserved: The translation preserves some of the meaning of the source but misses significant parts. The narrative is hard to follow due to fundamental errors. Grammar may be poor.  \n\n4: Most meaning preserved and few grammar mistakes: The translation retains most of the meaning of the source. It may have some grammar mistakes orminor contextual inconsistencies.  \n\n6: Perfect meaning and grammar: The meaning of the translation is completely consistent with the source and the surrounding context (if applicable) Thegrammar isalsocorrect.  \n\n![](images/17ab473d4042270e83f2df5f2a45e0d5912a8fd2688b548b0091dfca2f2c45e3.jpg)  \nFigure 7: A screen shot of an example annotation task in Appraise featuring source-based document-level Direct Assessment with SQM for the Isometric SLT Task.   \n①Thisisthe GitHub version #iwslt22dev of the Appraise evaluation system. Some rights reserved.  Developed and maintained by Christian Federmann and the Appraise Dev team."}, {"title": "Pre-training on high-resource speech recognition improves low-resource  speech-to-text translation", "authors": "Sameer Bansal, Herman Kamper, Karen Livescu, Adam Lopez, Sharon Goldwater", "bibkey": "pre_training_on_high_resource_speech_recognition_improves_low_resource_speech_to_text_translation", "bibitem": "@article{GoldwaterPOHSRILST,\n  url = {http://arxiv.org/abs/1809.01431v2},\n  title = {Pre-training on high-resource speech recognition improves low-resource  speech-to-text translation},\n  authors = {Sameer Bansal, Herman Kamper, Karen Livescu, Adam Lopez, Sharon Goldwater},\n  abstract = {  We present a simple approach to improve direct speech-to-text translation (ST) when the source language is low-resource: we pre-train the model on a high-resource automatic speech recognition (ASR) task, and then fine-tune its parameters for ST. We demonstrate that our approach is effective by pre-training on 300 hours of English ASR data to improve Spanish-English ST from 10.8 to 20.2 BLEU when only 20 hours of Spanish-English ST training data are available. Through an ablation study, we find that the pre-trained encoder (acoustic model) accounts for most of the improvement, despite the fact that the shared language in these tasks is the target language text, not the source language audio. Applying this insight, we show that pre-training on ASR helps ST even when the ASR language differs from both source and target ST languages: pre-training on French ASR also improves Spanish-English ST. Finally, we show that the approach improves performance on a true low-resource task: pre-training on a combination of English ASR and French ASR improves Mboshi-French ST, where only 4 hours of data are available, from 3.5 to 7.1 BLEU. },\n  bibkey = {GoldwaterPOHSRILST},\n  arxiv_id = {1809.01431v2},\n  subject = {cs.CL},\n  submission_date = {2018-09-05T10:56:30Z}\n}", "url": "http://arxiv.org/abs/1809.01431v2", "latex_url": "http://arxiv.org/src/1809.01431v2", "latex_path": "output/download_papers/1809.01431v2/1809.01431v2", "pdf_url": "http://arxiv.org/pdf/1809.01431v2", "pdf_path": "output/download_papers/1809.01431v2/1809.01431v2.pdf", "md_url": null, "latex_length": 0, "latex": null, "abstract": "  We present a simple approach to improve direct speech-to-text translation (ST) when the source language is low-resource: we pre-train the model on a high-resource automatic speech recognition (ASR) task, and then fine-tune its parameters for ST. We demonstrate that our approach is effective by pre-training on 300 hours of English ASR data to improve Spanish-English ST from 10.8 to 20.2 BLEU when only 20 hours of Spanish-English ST training data are available. Through an ablation study, we find that the pre-trained encoder (acoustic model) accounts for most of the improvement, despite the fact that the shared language in these tasks is the target language text, not the source language audio. Applying this insight, we show that pre-training on ASR helps ST even when the ASR language differs from both source and target ST languages: pre-training on French ASR also improves Spanish-English ST. Finally, we show that the approach improves performance on a true low-resource task: pre-training on a combination of English ASR and French ASR improves Mboshi-French ST, where only 4 hours of data are available, from 3.5 to 7.1 BLEU. ", "abstract_length": 1141, "abstract_token": 254, "introduction": "Speech-to-text Translation (ST) has many potential applications for low-resource languages: for example in language documentation, where the source language is often unwritten or endangered (Besacier et al., 2006; Martin et al., 2015; Adams et al., 2016a,b; Anastasopoulos and Chiang, 2017); or in crisis relief, where emergency workers might need to respond to calls or requests in a foreign language (Munro, 2010). Traditional ST is a pipeline of automatic speech recognition (ASR) and machine translation (MT), and thus requires transcribed source audio to train ASR and parallel text to train MT. These resources are often unavailable for low-resource languages, but for our potential applications, there may be some source language audio paired with target language text translations. In these scenarios, end-to-end ST is appealing. Recently, Weiss et al. (2017) showed that endto-end ST can be very effective, achieving an impressive BLEU score of 47.3 on Spanish-English ST. But this result required over 150 hours of translated audio for training, still a substantial resource requirement. By comparison, a similar system trained on only 20 hours of data for the same task achieved a BLEU score of 5.3 (Bansal et al., 2018). Other low-resource systems have similarly low accuracies (Anastasopoulos and Chiang, 2018; B´erard et al., 2018). To improve end-to-end ST in low-resource settings, we can try to leverage other data resources. For example, if we have transcribed audio in the source language, we can use multi-task learning to improve ST (Anastasopoulos and Chiang, 2018; Weiss et al., 2017; Be´rard et al., 2018). But source language transcriptions are unlikely to be available in our scenarios of interest. Could we improve low-resource ST by leveraging data from a high-resource language? For ASR, training a single model on multiple languages can be effective for all of them (Toshniwal et al., 2018b; Deng et al., 2013). For MT, transfer learning (Thrun, 1995) has been very effective: pretraining a model for a high-resource language pair and transferring its parameters to a low-resource language pair when the target language is shared (Zoph et al., 2016; Johnson et al., 2017). Inspired by these successes, we show that low-resource ST can leverage transcribed audio in a high-resource target language, or even a different language altogether, simply by pre-training a model for the high-resource ASR task, and then transferring and fine-tuning some or all of the model’s parameters for low-resource ST. ![](images/522a8ccdf88ed4cdf2194ac3edb8bfc14064ddc58b14e3f805c05dcc8e41bb46.jpg) Figure 1: Encoder-decoder with attention model architecture for both ASR and ST. The encoder input is the Spanish speech utterance claro, translated as clearly, represented as BPE (subword) units. We first test our approach using Spanish as the source language and English as the target. After training an ASR system on 300 hours of English, fine-tuning on 20 hours of Spanish-English yields a BLEU score of 20.2, compared to only 10.8 for an ST model without ASR pre-training. Analyzing this result, we discover that the main benefti of pre-training arises from the transfer of the encoder parameters, which model the input acoustic signal. In fact, this effect is so strong that we also obtain improvements by pre-training on a language that differs from either the source or target: pre-training on French and then fine-tuning on Spanish-English. We hypothesize that pre-training the encoder parameters, even on a different language, allows the model to better normalize over acoustic variability (such as speaker and channel differences), and conclude that this variability, rather than translation itself, is one of the main difficulties in low-resource ST. A final set of experiments confirm that ASR pretraining also helps on another language pair where the input is truly low-resource: Mboshi-French.", "introduction_length": 3918, "introduction_token": 892, "reference": "# References  \n\nOliver Adams, Graham Neubig, Trevor Cohn, and Steven Bird. 2016a. Learning a translation model from word lattices. In Proc. Interspeech.   \nOliver Adams, Graham Neubig, Trevor Cohn, Steven Bird, Quoc Truong Do, and Satoshi Nakamura. 2016b. Learning a lexicon and translation model from phoneme lattices. In Proc. EMNLP.   \nTanel Aluma¨e, Stavros Tsakalidis, and Richard M Schwartz. 2016. Improved multilingual training of stacked neural network acoustic models for low resource languages. In Proc. Interspeech.   \nAntonios Anastasopoulos and David Chiang. 2017. A case study on using speech-to-translation alignments for language documentation. In Proc. ACL.   \nAntonios Anastasopoulos and David Chiang. 2018. Tied multitask learning for neural speech translation. In Proc. NAACL HLT.   \nSameer Bansal, Herman Kamper, Karen Livescu, Adam Lopez, and Sharon Goldwater. 2018. Lowresource speech-to-text translation. In Proc. Interspeech.   \nAlexandre Be´rard, Laurent Besacier, Ali Can Kocabiyikoglu, and Olivier Pietquin. 2018. End-to-end automatic speech translation of audiobooks. In Proc. ICASSP.   \nLaurent Besacier, Bowen Zhou, and Yuqing Gao. 2006. Towards speech translation of non written languages. In Proc. SLT.   \nMichael A Carlin, Samuel Thomas, Aren Jansen, and Hynek Hermansky. 2011. Rapid evaluation of speech representations for spoken term discovery. In Proc. Interspeech.   \nJia Cui, Brian Kingsbury, Bhuvana Ramabhadran, Abhinav Sethy, Kartik Audhkhasi, Xiaodong Cui, Ellen Kislal, Lidia Mangu, Markus Nussbaum-Thom, Michael Picheny, et al. 2015. Multilingual representations for low resource speech recognition and keyword search. In Proc. ASRU.   \nLi Deng, Jinyu Li, Jui-Ting Huang, Kaisheng Yao, Dong Yu, Frank Seide, Mike Seltzer, Geoff Zweig, Xiaodong He, Jason Williams, Yifan Gong, and Alex Acero. 2013. Recent advances in deep learning for speech research at Microsoft. In Proc. ICASSP.   \nYarin Gal. 2016. A theoretically grounded application of dropout in recurrent neural networks. In Proc. NIPS.   \nPierre Godard, Gilles Adda, Martine Adda-Decker, Juan Benjumea, Laurent Besacier, Jamison CooperLeavitt, Guy-No”el Kouarata, Lori Lamel, H’el‘ene Maynard, Markus M”uller, Annie Rialland, Sebastian St”uker, Fran¸cois Yvon, and Marcely Zanon Boito. 2018. A very low resource language speech corpus for computational language documentation experiments. In Proc. LREC.   \nJohn Godfrey and Edward Holliman. 1993. Switchboard-1 Release 2 (LDC97S62). https: //catalog.ldc.upenn.edu/ldc97s62.   \nDavid Graff, Shudong Huang, Ingrid Cartagena, Kevin Walker, and Christopher Cieri. 2010. Fisher Spanish Speech (LDC2010S01). https://catalog. ldc.upenn.edu/ldc2010s01.   \nCaglar Gu¨l¸cehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Loıc Barrault, Huei-Chi Lin, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2015. On using monolingual corpora in neural machine translation. CoRR, abs/1503.03535.   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectifiers: Surpassing human-level performance on Imagenet classification. In Proc. ICCV.   \nEnno Hermann and Sharon Goldwater. 2018. Multilingual bottleneck features for subword modeling in zero-resource languages. In Proc. Interspeech.   \nSepp Hochreiter and Ju¨rgen Schmidhuber. 1997. Long short-term memory. Neural Comput.   \nSergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proc. ICML.   \nAren Jansen, Kenneth Church, and Hynek Hermansky. 2010. Towards spoken term discovery at scale with zero resources. In Proc. Interspeech.   \nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda B. Vie´gas, Martin Wattenberg, Gregory S. Corrado, Macduff Hughes, and Jeffrey Dean. 2017. Google’s multilingual neural machine translation system: Enabling zero-shot translation. Trans. ACL, 5:339–351.   \nHerman Kamper, Micha Elsner, Aren Jansen, and Sharon Goldwater. 2015. Unsupervised neural network based feature extraction using weak top-down constraints. In Proc. ICASSP.   \nHerman Kamper, Aren Jansen, and Sharon Goldwater. 2017. A segmental framework for fullyunsupervised large-vocabulary speech recognition. Comput. Speech Lang., 46:154–174.   \nDiederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proc. ICLR.   \nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL.   \nAlon Lavie and Abhaya Agarwal. 2007. Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments. In Proc. WMT.  \n\nMinh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Effective approaches to attentionbased neural machine translation. In Proc. EMNLP.  \n\nLara J Martin, Andrew Wilkinson, Sai Sumanth Miryala, Vivian Robison, and Alan W Black. 2015. Utterance classification in speech-to-speech translation for zero-resource languages in the hospital administration domain. In Proc. ASRU.  \n\nRobert Munro. 2010. Crowdsourced translation for emergency response in Haiti: The global collaboration of local knowledge. In AMTA Workshop Collaborative Crowdsourcing Transl.  \n\nVinod Nair and Geoffrey E. Hinton. 2010. Rectified linear units improve restricted Boltzmann machines. In Proc. ICML.  \n\nKishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. ACL.  \n\nMatt Post, Gaurav Kumar, Adam Lopez, Damianos Karakos, Chris Callison-Burch, and Sanjeev Khudanpur. 2013. Improved speech-to-text translation with the Fisher and Callhome Spanish-English speech translation corpus. In Proc. IWSLT.  \n\nMatt Post, Gaurav Kumar, Adam Lopez, Damianos Karakos, Chris Callison-Burch, and Sanjeev Khudanpur. 2014. Fisher and CALLHOME Spanish– English Speech Translation. https://catalog. ldc.upenn.edu/ldc2014t23.  \n\nDaniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, Jan Silovsky, Georg Stemmer, and Karel Vesely. 2011. The Kaldi Speech Recognition Toolkit. In Proc. ASRU.  \n\nPrajit Ramachandran, Peter J Liu, and Quoc V Le. 2017. Unsupervised pretraining for sequence to sequence learning. In Proc. EMNLP.  \n\nDaniel Renshaw, Herman Kamper, Aren Jansen, and Sharon Goldwater. 2015. A comparison of neural network methods for unsupervised representation learning on the zero resource speech challenge. In Proc. Interspeech.  \n\nTanja Schultz. 2002. Globalphone: a multilingual speech and text database developed at karlsruhe university. In Seventh International Conference on Spoken Language Processing.  \n\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proc. ACL.  \n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. J. Mach. Learn. Res.  \n\nSamuel Thomas, Sriram Ganapathy, and Hynek Hermansky. 2012. Multilingual mlp features for lowresource LVCSR systems. In Proc. ICASSP.  \n\nSebastian Thrun. 1995. Is learning the n-th thing any easier than learning the first? In Proc. NIPS.  \n\nSeiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. 2015. Chainer: A next-generation open source framework for deep learning. In Proc. LearningSys.  \n\nShubham Toshniwal, Anjuli Kannan, Chung-Cheng Chiu, Yonghui Wu, Tara N Sainath, and Karen Livescu. 2018a. A Comparison of Techniques for Language Model Integration in Encoder-Decoder Speech Recognition. In Proc. SLT.  \n\nShubham Toshniwal, Tara N. Sainath, Ron J. Weiss, Bo Li, Pedro Moreno, Eugene Weinstein, and Kanishka Rao. 2018b. Multilingual Speech Recognition with A Single End-To-End Model. In Proc. ICASSP.  \n\nNgoc Thang Vu, Wojtek Breiter, Florian Metze, and Tanja Schultz. 2012. An investigation on initialization schemes for multilayer perceptron training using multilingual data and their effect on ASR performance. In Proc. Interspeech.  \n\nRon J Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, and Zhifeng Chen. 2017. Sequence-tosequence models can directly transcribe foreign speech. In Proc. Interspeech.  \n\nRonald J. Williams and David Zipser. 1989. A learning algorithm for continually running fully recurrent neural networks. Neural Comput.  \n\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Gregory S. Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google’s neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144.  \n\nYougen Yuan, Cheung-Chi Leung, Lei Xie, Bin Ma, and Haizhou Li. 2016. Learning neural network representations using cross-lingual bottleneck features with word-pair information. In Proc. Interspeech.  \n\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. 2016. Transfer learning for low-resource neural machine translation. In Proc. EMNLP.", "reference_length": 9475, "reference_token": 2640, "txt_length": 33823, "txt_token": 8436, "txt": "# Pre-training on high-resource speech recognition improves low-resource speech-to-text translation  \n\nSameer Bansal1, Herman Kamper2, Karen Livescu3, Adam Lopez1, Sharon Goldwater1 1School of Informatics, University of Edinburgh 2E&E Engineering, Stellenbosch University, South Africa 3Toyota Technological Institute at Chicago, USA  \n\n{sameer.bansal, sgwater, alopez}@inf.ed.ac.uk, kamperh@sun.ac.za, klivescu@ttic.edu  \n\n# Abstract  \n\nWe present a simple approach to improve direct speech-to-text translation (ST) when the source language is low-resource: we pre-train the model on a high-resource automatic speech recognition (ASR) task, and then fine-tune its parameters for ST. We demonstrate that our approach is effective by pre-training on 300 hours of English ASR data to improve SpanishEnglish ST from 10.8 to 20.2 BLEU when only 20 hours of Spanish-English ST training data are available. Through an ablation study, we find that the pre-trained encoder (acoustic model) accounts for most of the improvement, despite the fact that the shared language in these tasks is the target language text, not the source language audio. Applying this insight, we show that pre-training on ASR helps ST even when the ASR language differs from both source and target ST languages: pre-training on French ASR also improves Spanish-English ST. Finally, we show that the approach improves performance on a true low-resource task: pre-training on a combination of English ASR and French ASR improves Mboshi-French ST, where only 4 hours of data are available, from 3.5 to 7.1 BLEU.  \n\n# 1 Introduction  \n\nSpeech-to-text Translation (ST) has many potential applications for low-resource languages: for example in language documentation, where the source language is often unwritten or endangered (Besacier et al., 2006; Martin et al., 2015; Adams et al., 2016a,b; Anastasopoulos and Chiang, 2017); or in crisis relief, where emergency workers might need to respond to calls or requests in a foreign language (Munro, 2010). Traditional ST is a pipeline of automatic speech recognition (ASR) and machine translation (MT), and thus requires transcribed source audio to train ASR and parallel text to train MT. These resources are often unavailable for low-resource languages, but for our potential applications, there may be some source language audio paired with target language text translations. In these scenarios, end-to-end ST is appealing.  \n\nRecently, Weiss et al. (2017) showed that endto-end ST can be very effective, achieving an impressive BLEU score of 47.3 on Spanish-English ST. But this result required over 150 hours of translated audio for training, still a substantial resource requirement. By comparison, a similar system trained on only 20 hours of data for the same task achieved a BLEU score of 5.3 (Bansal et al., 2018). Other low-resource systems have similarly low accuracies (Anastasopoulos and Chiang, 2018; B´erard et al., 2018).  \n\nTo improve end-to-end ST in low-resource settings, we can try to leverage other data resources. For example, if we have transcribed audio in the source language, we can use multi-task learning to improve ST (Anastasopoulos and Chiang, 2018; Weiss et al., 2017; Be´rard et al., 2018). But source language transcriptions are unlikely to be available in our scenarios of interest.  \n\nCould we improve low-resource ST by leveraging data from a high-resource language? For ASR, training a single model on multiple languages can be effective for all of them (Toshniwal et al., 2018b; Deng et al., 2013). For MT, transfer learning (Thrun, 1995) has been very effective: pretraining a model for a high-resource language pair and transferring its parameters to a low-resource language pair when the target language is shared (Zoph et al., 2016; Johnson et al., 2017). Inspired by these successes, we show that low-resource ST can leverage transcribed audio in a high-resource target language, or even a different language altogether, simply by pre-training a model for the high-resource ASR task, and then transferring and fine-tuning some or all of the model’s parameters for low-resource ST.  \n\n![](images/522a8ccdf88ed4cdf2194ac3edb8bfc14064ddc58b14e3f805c05dcc8e41bb46.jpg)  \nFigure 1: Encoder-decoder with attention model architecture for both ASR and ST. The encoder input is the Spanish speech utterance claro, translated as clearly, represented as BPE (subword) units.  \n\nWe first test our approach using Spanish as the source language and English as the target. After training an ASR system on 300 hours of English, fine-tuning on 20 hours of Spanish-English yields a BLEU score of 20.2, compared to only 10.8 for an ST model without ASR pre-training. Analyzing this result, we discover that the main benefti of pre-training arises from the transfer of the encoder parameters, which model the input acoustic signal. In fact, this effect is so strong that we also obtain improvements by pre-training on a language that differs from either the source or target: pre-training on French and then fine-tuning on Spanish-English. We hypothesize that pre-training the encoder parameters, even on a different language, allows the model to better normalize over acoustic variability (such as speaker and channel differences), and conclude that this variability, rather than translation itself, is one of the main difficulties in low-resource ST. A final set of experiments confirm that ASR pretraining also helps on another language pair where the input is truly low-resource: Mboshi-French.  \n\n# 2 Method  \n\nFor both ASR and ST, we use an encoder-decoder model with attention adapted from Weiss et al. (2017), Be´rard et al. (2018) and Bansal et al. (2018), as shown in Figure 1. We use the same model architecture for all our models, allowing us to conveniently transfer parameters between them. We also constrain the hyper-parameter search to fit a model into a single Titan X GPU, allowing us to maximize available compute resources.  \n\nWe use a pre-trained English ASR model to initialize training of Spanish-English ST models, and a pre-trained French ASR model to initialize training of Mboshi-French ST models. In these configurations, the decoder shares the same vocabulary across the ASR and ST tasks. This is practical for settings where the target text language is highresource with ASR data available.  \n\nIn settings where both ST languages are lowresource, ASR data may only be available in a third language. To test whether transfer learning will help in this setting, we use a pre-trained French ASR model to train Spanish-English ST models; and English ASR for Mboshi-French models. In these cases, the ST languages are different from the ASR language, so we can only transfer the encoder parameters of the ASR model, since the dimensions of the decoder’s output softmax layer are indexed by the vocabulary, which is not shared.1 Sharing only the speech encoder parameters is much easier, since the speech input can be preprocessed in the same manner for all languages. This form of transfer learning is more flexible, as there are no constraints on the ASR language used.  \n\n# 3 Experimental Setup  \n\n# 3.1 Data sets  \n\nEnglish ASR. We use the Switchboard Telephone speech corpus (Godfrey and Holliman, 1993), which consists of around 300 hours of English speech and transcripts, split into $260\\mathbf{k}$ utterances. The development set consists of 5 hours that we removed from the training set, split into 4k utterances.  \n\nFrench ASR. We use the French speech corpus from the GlobalPhone collection (Schultz, 2002), which consists of around 20 hours of high quality read speech and transcripts, split into 9k utterances. The development set consists of 2 hours, split into 800 utterances.  \n\nSpanish-English ST. We use the Fisher Spanish speech corpus (Graff et al., 2010), which consists of 160 hours of telephone speech in a variety of Spanish dialects, split into 140K utterances. To simulate low-resource conditions, we construct smaller training corpora consisting of 50, 20, 10, 5, or 2.5 hours of data, selected at random from the full training data. The development and test sets each consist of around 4.5 hours of speech, split into 4K utterances. We do not use the corresponding Spanish transcripts; our target text consists of English translations that were collected through crowdsourcing (Post et al., 2013, 2014).  \n\nMboshi-French ST. Mboshi is a Bantu language spoken in the Republic of Congo, with around 160,000 speakers.2 We use the Mboshi-French parallel corpus (Godard et al., 2018), which consists of around 4 hours of Mboshi speech, split into a training set of 5K utterances and a development set of 500 utterances. Since this corpus does not include a designated test set, we randomly sampled and removed 200 utterances from training to use as a development set, and use the designated development data as a test set.  \n\n# 3.2 Preprocessing  \n\nSpeech. We convert raw speech input to 13- dimensional MFCCs using Kaldi (Povey et al., 2011).3 We also perform speaker-level mean and variance normalization.  \n\nText. The target text of the Spanish-English data set contains 1.5M word tokens and 17K word types. If we model text as sequences of words, our model cannot produce any of the unseen word types in the test data and is penalized for this, but it can be trained very quickly (Bansal et al., 2018). If we instead model text as sequences of characters as in (Weiss et al., 2017), we would have 7M tokens and 100 types, resulting in a model that is open-vocabulary, but very slow to train (Bansal et al., 2018). As an effective middle ground, we use byte pair encoding (BPE; Sennrich et al., 2016) to segment each word into subwords, each of which is a character or a high-frequency sequence of characters—we use 1000 of these high-frequency sequences. Since the set of subwords includes the full set of characters, the model is still open vocabulary; but it results in a text with only 1.9M tokens and just over 1K types, which can be trained almost as fast as the word-level model.  \n\nThe vocabulary for BPE depends on the frequency of character sequences, so it must be computed with respect to a specific corpus. For English, we use the full 160-hour Spanish-English  \n\nST target training text. For French, we use the Mboshi-French ST target training text.  \n\n# 3.3 Model architecture for ASR and ST  \n\nSpeech encoder. As shown schematically in Figure 1, MFCC feature vectors are fed into a stack of two CNN layers, with 128 and 512 fliters with a fliter width of 9 frames each. In each CNN layer we stride with a factor of 2 along time, apply a ReLU activation (Nair and Hinton, 2010), and apply batch normalization (Ioffe and Szegedy, 2015). The output of the CNN layers is fed into a three-layer bidirectional LSTM (Hochreiter and Schmidhuber, 1997); each hidden layer has 512 dimensions.  \n\nText decoder. At each time step, the decoder chooses the most probable token from the output of a softmax layer produced by a fully-connected layer, which in turn receives the current state of a recurrent layer computed from previous time steps and an attention vector computed over the input. Attention is computed using the global attentional model with general score function and inputfeeding, as described in Luong et al. (2015). The predicted token is then fed into a 128-dimensional embedding layer followed by a three-layer LSTM to update the recurrent state; each hidden state has 256 dimensions. While training, we use the predicted token $20\\%$ of the time as input to the next decoder step and the training token for the remaining $80\\%$ of the time (Williams and Zipser, 1989). At test time we use beam decoding with a beam size of 5 and length normalization (Wu et al., 2016) with a weight of 0.6.  \n\nTraining and implementation. Parameters for the CNN and RNN layers are initialized using the scheme from (He et al., 2015). For the embedding and fully-connected layers, we use Chainer’s (Tokui et al., 2015) default initialition.  \n\nWe regularize using dropout (Srivastava et al., 2014), with a ratio of 0.3 over the embedding and LSTM layers (Gal, 2016), and a weight decay rate of 0.0001. The parameters are optimized using Adam (Kingma and Ba, 2015), with a starting alpha of 0.001.  \n\nFollowing some preliminary experimentation on our development set, we add Gaussian noise with standard deviation of 0.25 to the MFCC features during training, and drop frames with a probability of 0.10. After 20 epochs, we corrupt the true decoder labels by sampling a random output label  \n\nwith a probability of 0.3.  \n\nOur code is implemented in Chainer (Tokui et al., 2015) and we plan to make it freely available.  \n\n# 3.4 Evaluation  \n\nMetrics. We report BLEU (Papineni et al., 2002) for all our models.4 In low-resource settings, BLEU scores tend to be low, difficult to interpret, and poorly correlated with model performance. This is because BLEU requires exact four-gram matches only, but low four-gram accuracy may obscure a high unigram accuracy and inexact translations that partially capture the semantics of an utterance, and these can still be very useful in situations like language documentation and crisis response. Therefore, we also report word-level unigram precision and recall, taking into account stem, synonym, and paraphrase matches. To compute these scores, we use METEOR (Lavie and Agarwal, 2007) with default settings for English and French.5 For example, METEOR assigns “eat” a recall of 1 against reference “eat” and a recall of 0.8 against reference “feed”, which it considers a synonym match.  \n\nNaive baselines. We also include evaluation scores for a naive baseline model that predicts the $K$ most frequent words of the training set as a bag of words for each test utterance. We set $K$ to be the value at which precision/recall are most similar, which is always between 5 and 20 words. This provides an empirical lower bound on precision and recall, since we would expect any usable model to outperform a system that does not even depend on the input utterance. We do not compute BLEU for these baselines, since they do not predict sequences, only bags of words.  \n\n# 4 ASR results  \n\nUsing the experimental setup of Section 3, we pretrained ASR models in English and French, and report their word error rates (WER) on development data in Table 1.6 We denote each ASR model by $L{-}N h$ , where $L$ is a language code and $N$ is the size of the training set in hours. For example, en$300h$ denotes an English ASR model trained on 300 hours of data.  \n\nTraining ASR models for state-of-the-art performance requires substantial hyper-parameter tuning and long training times. Since our goal is simply to see whether pre-training is useful, we stopped pretraining our models after around 30 epochs (3 days) to focus on transfer experiments. As a consequence, our ASR results are far from state-of-the-art: current end-to-end Kaldi systems obtain $16\\%$ WER on Switchboard train-dev, and $22.7\\%$ WER on the French Globalphone dev set.7 We believe that better ASR pre-training may produce better ST results, but we leave this for future work.  \n\nTable 1: Word Error Rate (WER, in $\\%$ ) for the ASR models used as pretraining, computed on Switchboard train-dev for English and Globalphone dev for French.   \n\n\n<html><body><table><thead><tr><td></td><td><b>en-100h</b></td><td><b>en-300h</b></td><td><b>fr-20h</b></td></tr></thead><tbody><tr><td>WER</td><td>35.4</td><td>27.3</td><td>29.6</td></tr></tbody></table></body></html>  \n\n# 5 Spanish-English ST  \n\nIn the following, we denote an ST model by S-T$N\\!h$ , where $S$ and $T$ are source and target language codes, and $N$ is the size of the training set in hours. For example, sp-en-20h denotes a Spanish-English ST model trained using 20 hours of data. We use the code mb for Mboshi and $f\\boldsymbol r$ for French.  \n\n# 5.1 Using English ASR to improve ST  \n\nFigure 2 shows the BLEU and unigram precision/recall scores on the development set for baseline Spanish-English ST models and those trained after initializing with the en-300h model. Corresponding results on the test set (Table 2) reveal very similar patterns. The remainder of our analysis is confined to the development set. The naive baseline, which predicts the 15 most frequent English words in the training set, achieves a precision/recall of around $20\\%$ , setting a performance lower bound.  \n\nLow-resource: 20-50 hours of ST training data. Our baseline ST models substantially improve over previous results (Bansal et al., 2018) using the same train/test splits, primarily due to better regularization and modeling of subwords rather than words. Yet transfer learning still substantially improves over these strong baselines. For sp-en- $2O h$ , transfer learning improves dev set BLEU from 10.8 to 19.9, precision from $41\\%$ to $51\\%$ , and recall from $38\\%$ to $49\\%$ . For sp-en- $.5O h$ , transfer learning improves  \n\n![](images/f6a25ca4d80ec98ce0023a3054aebb093c0f2d97a7137eec8db6f01f9e130ef8.jpg)  \nFigure 2: (top) BLEU and (bottom) Unigram precision/recall for Spanish-English ST models computed on Fisher dev set. base indicates no transfer learning; $\\mathbf{+asr}$ are models trained by fine-tuning $e n{-30}O h$ model parameters. naive baseline indicates the score when we predict the 15 most frequent English words in the training set.   \nTable 3: Example translations on selected sentences from the Fisher development set, with stem-level $n$ - gram matches to the reference sentence underlined. 20h and 50h are Spanish-English models without pretraining; $\\mathbf{20h+asr}$ and $\\mathbf{50h+asr}$ are pre-trained on 300 hours of English ASR.  \n\nTable 2: BLEU scores for Spanish-English ST on the Fisher test set, using $N$ hours of training data. base: no transfer learning. $\\mathbf{+asr}$ : using model parameters from English ASR (en-300h).   \n\n\n<html><body><table><thead><tr><td><b>N=</b></td><td><b>0</b></td><td><b>2.5</b></td><td><b>5</b></td><td><b>10</b></td><td><b>20</b></td><td><b>50</b></td></tr></thead><tbody><tr><td>base</td><td>0</td><td>2.1</td><td>1.8</td><td>2.1</td><td>10.8</td><td>22.7</td></tr><tr><td>+asr</td><td>0.5</td><td>5.7</td><td>9.1</td><td>14.5</td><td>20.2</td><td>28.2</td></tr></tbody></table></body></html>  \n\nBLEU from 23.3 to 27.8, precision from $54\\%$ to $58\\%$ , and recall from $51\\%$ to $56\\%$ .  \n\nVery low-resource: 10 hours or less of ST training data. Figure 2 shows that without transfer learning, ST models trained on less than 10 hours of data struggle to learn, with precision/recall scores close to or below that of the naive baseline. But with transfer learning, we see gains in precision and recall of between 10 and 20 points.  \n\nWe also see that with transfer learning, a model trained on only 5 hours of ST data achieves a BLEU of 9.1, nearly as good as the 10.8 of a model trained on 20 hours of ST data without transfer learning. In other words, fine-tuning an English ASR model— which is relatively easy to obtain—produces similar results to training an ST model on four times as much data, which may be difficult to obtain.  \n\n<html><body><table><thead><tr><td><b>Spanish  super caliente pero muy bonito</b></td></tr></thead><tbody><tr><td>English  super hot but very nice</td></tr><tr><td>20h</td><td> you support it but it was very nice</td></tr><tr><td> 20h+asr you can get alright but it's very nice</td></tr><tr><td>50h</td><td>super expensive but very nice</td></tr><tr><td>50h+asr super hot but it's very nice</td></tr><tr><td>Spanish si y usted hace mucho tiempo que que vive aqui</td></tr><tr><td>English  yes and have you been living here a long time</td></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></body></html>  \n\nWe even find that in the very low-resource setting of just 2.5 hours of ST data, with transfer learning the model achieves a precision/recall of around $30\\%$ and improves by more than 10 points over the naive baseline. In very low-resource scenarios with time constraints—such as in disaster relief—it is possible that even this level of performance may be useful, since it can be used to spot keywords in speech and can be trained in just three hours.  \n\nSample translations. Table 3 shows example translations for models sp-en-20h and sp-en-50h with and without transfer learning using en-300h.  \n\nFigure 3 shows the attention weights for the last sample utterance in Table 3. For this utterance, the Spanish and English text have a different word order: mucho tiempo occurs in the middle of the speech utterance, and its translation, long time, is at the end of the English reference. Similarly, vive aqu´ı occurs at the end of the speech utterance, while the translation, living here, is in the middle of the English reference. The baseline sp-en-50h model translates the words correctly but doesn’t get the English word order right. With transfer learning, the model produces a shorter but still accurate translation in the correct word order.  \n\n# 5.2 Analysis  \n\nTo understand the source of these improvements, we carried out a set of ablation experiments. For most of these experiments, we focus on SpanishEnglish ST with 20 hours of training data, with and without transfer learning.  \n\n![](images/52263867925f045540b5ba1aa88490c93daeee8fbc9777470e3f8a01d8a3fb8e.jpg)  \nFigure 3: Attention plots for the final example in Table 3, using 50h models with and without pre-training. The $x$ -axis shows the reference Spanish word positions in the input; the $y$ -axis shows the predicted English subwords. In the reference, mucho tiempo is translated to long time, and vive aqu´ı to living here, but their order is reversed, and this is reflected in (b).  \n\nTransfer learning with selected parameters. In our first set of experiments, we transferred all parameters of the en-300h model, including the speech encoder CNN and LSTM; the text decoder embedding, LSTM and output layer parameters; and attention parameters. To see which set of parameters has the most impact, we train the sp-en$2O h$ model by transferring only selected parameters from en-300h, and randomly initializing the rest.  \n\nThe results (Figure 4) show that transferring all parameters is most effective. But they also show that the speech encoder parameters account for most of the gains. We hypothesize that the encoder learns transferable low-level acoustic features that normalize across variability like speaker and channel differences, and that much of this learning is language-independent. This hypothesis is supported by other work showing the beneftis of crosslingual and multilingual training for speech technology in low-resource target languages (Carlin et al., 2011; Jansen et al., 2010; Deng et al., 2013; Vu et al., 2012; Thomas et al., 2012; Cui et al., 2015; Alum¨ae et al., 2016; Yuan et al., 2016; Renshaw et al., 2015). Indeed, there is evidence that speech features trained on multiple languages transfer better than those trained on the same amount of data from a single language (Hermann and Goldwater, 2018).  \n\n![](images/ca17de27eb95a1adb9743d5d7d4a7ee28e53f132e79da0774318b31a962d10d3.jpg)  \nFigure 4: Fisher development set training curves (reported using BLEU) for sp-en-20h using selected parameters from $e n{-}3O O h$ : none (base); encoder CNN only $(\\mathbf{+asr:}\\mathbf{cnm})$ ; encoder CNN and LSTM only (+asr:enc); decoder only (+asr:dec); and all: encoder, attention, and decoder (+asr:all). These scores do not use beam search and are therefore lower than the best scores reported in Figure 2.  \n\nBy contrast, transferring only decoder parameters does not improve accuracy. Since decoder parameters help when used in tandem with encoder parameters, we suspect that the dependency in parameter training order might explain this: the transferred decoder parameters have been trained to expect particular input representations from the encoder, so transferring only the decoder parameters without the encoder might not be useful.  \n\nFigure 4 also suggests that models make strong gains early on in the training when using transfer learning. The sp-en-20h model initialized with all model parameters $(+a s r;a l l)$ from en-300h reaches a higher BLEU score after just 5 epochs (2 hours) of training than the model without transfer learning trained for 60 epochs/20 hours. This again can be useful in disaster-recovery scenarios, where the time to deploy a working system must be minimized.  \n\nAmount of ASR data required. Figure 5 shows the impact of increasing the amount of English ASR data used on Spanish-English ST performance for two models: sp-en-20h and sp-en-50h.  \n\n![](images/eea6cf657d62b1cb51477a2fbfced61f5824008ef201e3e5e07b9f9c652e4189.jpg)  \nFigure 5: Spanish-to-English BLEU scores on Fisher dev set, with 0h (no transfer learning), 100h and $300\\mathrm{h}$ of English ASR data used.  \n\nFor $s p{-e n-20h}$ , we see that using en-100h improves performance by almost 6 BLEU points. By using more English ASR training data (en-300h) model, the BLEU score increases by almost 9 points. However, for sp-en-50h, we only see improvements when using $e n{-30}O h$ . This implies that transfer learning is most useful in true low-resource scenarios, when only a few tens of hours of training data are available for ST. As the amount of ST training data increases, the beneftis of transfer learning tail off, although it’s possible that using even more monolingual data, or improving the training at the ASR step, could extend the benefits to larger ST data sets.  \n\nImpact of code-switching. We also tried using the en-300h ASR model without any fine-tuning to translate Spanish audio to English text. This model achieved a BLEU score of 1.1, with a precision of 15 and recall of 21. The non-zero BLEU score indicates that the model is matching some 4-grams in the reference. This seems to be due to code-switching in the Fisher-Spanish speech data set. Looking at the dev set utterances, we find several examples where the Spanish transcriptions match the English translations, indicating that the speaker switched into English. For example, there is an utterance whose Spanish transcription and English translation are both “right yeah”, and this English expression is indeed present in the source audio. The English ASR model correctly translates this utterance, which is unsurprising since the phrase “right yeah” occurs nearly 500 times in Switchboard.  \n\nOverall, we find that nearly 500 of the 4,000 development set utterances $(14\\%)$ likely contain code-switching, since the Spanish transcription and  \n\nTable 4: Fisher dev set BLEU scores for $s p{-e n-20h}$ . baseline: model without transfer learning. Last two columns: Using encoder parameters from French ASR $\\left(+\\mathbf{fr}{-}20\\mathbf{h}\\right)$ , and English ASR $\\left(+\\mathbf{en}{-20}\\mathbf{h}\\right)$ .   \n\n\n<html><body><table><thead><tr><td></td><td><b>baseline</b></td><td><b>+fr-20h</b></td><td><b>+en-20h</b></td></tr></thead><tbody><tr><td>sp-en-20h</td><td>10.8</td><td>12.5</td><td>13.2</td></tr></tbody></table></body></html>  \n\nEnglish translations share more than half of their tokens. This suggests that transfer learning from English ASR models might help more than from other languages. To isolate this effect from transfer learning of language-independent speech features, we carried out a further experiment.  \n\n# 5.3 Using French ASR to improve Spanish-English ST  \n\nIn this experiment, we pre-train using French ASR data for a Spanish-English translation task. Here, we can only transfer the speech encoder parameters, and there should be little if any benefti due to codeswitching.  \n\nBecause our French data set (20 hours) is much smaller than our English one (300 hours), for a fair comparison we used a 20 hour subset of the English data for pre-training in this experiment. For both the English and French models, we transferred only the encoder parameters.  \n\nTable 4 shows that both the English and French 20-hour pre-trained models improve performance on Spanish-English ST. The English model works slightly better, as would be predicted given our discussion of code-switching, but the French model is also useful, improving BLEU from 10.8 to 12.5. This result strengthens the claim that ASR pretraining on a completely distinct third language can help low-resource ST. Presumably benefits would be much greater if we used a larger ASR data set, as we did with English above.  \n\n# 6 Mboshi-French ST  \n\nOur final set of experiments test our transfer method on ST for the low-resource language Mboshi, where we have only 4 hours of ST training data: Mboshi speech input paired with French text output.  \n\nTable 5 shows the ST model scores for MboshiFrench with and without using transfer learning. The first two rows $f\\scriptstyle{r}$ -top- $.\\vartheta w$ , $f\\boldsymbol r$ -top- $I O w$ , show precision and recall scores for the naive baselines where we predict the top 8 or $10\\ \\mathrm{most}$ frequent French words in the Mboshi-French training set. These show that a precision/recall in the low 20s is easy to achieve, although with no n-gram matches (0 BLEU). The pre-trained ASR models by themselves (next two lines) are much worse.  \n\nTable 5: Mboshi-to-French translation scores, with and without ASR pre-training. Pr. is the precision, and Rec. the recall score. fr-top-8w and fr-top-10w are naive baselines that, respectively, predict the 8 or 10 most frequent training words. For $\\mathbf{en}+\\mathbf{fr}$ , we use encoder parameters from en-300h and attention+decoder parameters from fr-20h   \n\n\n<html><body><table><thead><tr><td><b>model</b></td><td><b>pretrain</b></td><td><b>BLEU</b></td><td><b>Pr.</b></td></tr></thead><tbody><tr><td>fr-top-8w</td><td>0</td><td>23.5</td><td>22.2</td></tr><tr><td>fr-top-10w</td><td>0</td><td>20.6</td><td>24.5</td></tr><tr><td>en-300h</td><td>0</td><td>0.2</td><td>5.7</td></tr><tr><td>fr-20h</td><td>0</td><td>4.1</td><td>3.2</td></tr><tr><td rowspan=\"5\">mb-fr-4h</td><td>3.5</td><td>18.6</td><td>19.4</td></tr><tr><td>fr-20h</td><td>5.9 23.6</td><td>20.9</td></tr><tr><td>en-300h</td><td>23.5</td><td>22.6</td></tr><tr><td>en + fr</td><td>26.7</td><td>23.1</td></tr></tbody></table></body></html>  \n\nThe baseline model trained only on ST data actually has lower precision/recall than the naive baseline, although its non-zero BLEU score indicates that it is able to correctly predict some n-grams. We see comparable precision/recall to the naive baseline with improvements in BLEU by transferring either French ASR parameters (both encoder and decoder, fr-20h) or English ASR parameters (encoder only, en-300h).  \n\nFinally, to achieve the beneftis of both the larger training set size for the encoder and the matching language of the decoder, we tried transferring the encoding parameters from the en-300h model and the decoding parameters from the $f\\!r{-}2O h$ model. This configuration $(e n\\!+\\!f r)$ gives us the best evaluation scores on all metrics, and highlights the flexibility of our framework. Nevertheless, the 4-hour scenario is clearly a very challenging one.  \n\nEnglish ST model with 20 hours of parallel data and 300 hours of English ASR data. Moreover, the pre-trained model trains faster than the baseline, achieving higher BLEU in only a couple of hours, while the baseline trains for more than a day.  \n\nWe also showed that these methods can be used effectively on a real low-resource language, Mboshi, with only 4 hours of parallel data. The very small size of the data set makes the task challenging, but by combining parameters from an English encoder and French decoder, we outperformed baseline models to obtain a BLEU score of 7.1 and precision/recall of about $25\\%$ . We believe ours is the first paper to report word-level BLEU scores on this data set.  \n\nOur analysis indicated that, other things being equal, transferring both encoder and decoder parameters works better than just transferring one or the other. However, transferring the encoder parameters is where most of the benefit comes from. Pre-training using a large ASR corpus from a mismatched language will therefore probably work better than using a smaller ASR corpus that matches the output language.  \n\nOur analysis suggests several avenues for further exploration. On the speech side, it might be even more effective to use multilingual training; or to replace the MFCC input features with pre-trained multilingual features, or features that are targeted to low-resource multispeaker settings (Kamper et al., 2015, 2017; Thomas et al., 2012; Cui et al., 2015; Yuan et al., 2016; Renshaw et al., 2015). On the language modeling side, simply transferring decoder parameters from an ASR model did not work; but an alternative, and perhaps better, method would be to use pre-trained decoder parameters from a language model, as proposed by Ramachandran et al. (2017), or shallow fusion (G¨ul¸cehre et al., 2015; Toshniwal et al., 2018a), which interpolates a pre-trained language model during beam search. In these methods, the decoder parameters are independent, and can therefore be used on their own. We plan to explore these strategies in future work.  \n\n# 7 Conclusion  \n\nThis paper introduced the idea of pre-training an end-to-end speech translation system involving a low-resource language using ASR training data from a higher-resource language. We showed that large gains are possible: for example, we achieved an improvement of 9 BLEU points for a Spanish  \n\n# Acknowledgments  \n\nThis work was supported in part by a James S McDonnell Foundation Scholar Award, a Google faculty research award, and NSF grant 1816627. We thank Ida Szubert and Clara Vania for helpful comments on previous drafts of this paper and Antonios Anastasopoulos for tips on experimental setup", "appendix": "."}, {"title": "End-to-End Automatic Speech Translation of Audiobooks", "authors": "Alexandre Bérard, Laurent Besacier, Ali Can Kocabiyikoglu, Olivier Pietquin", "bibkey": "end_to_end_automatic_speech_translation_of_audiobooks", "bibitem": "@article{Berard_IEEE2018,\n  url = {http://arxiv.org/abs/1802.04200v1},\n  title = {End-to-End Automatic Speech Translation of Audiobooks},\n  authors = {Alexandre Bérard, Laurent Besacier, Ali Can Kocabiyikoglu, Olivier Pietquin},\n  abstract = {We investigate end-to-end speech-to-text translation on a corpus of audiobooks specifically augmented for this task. Previous works investigated the extreme case where source language transcription is not available during learning nor decoding, but we also study a midway case where source language transcription is available at training time only. In this case, a single model is trained to decode source speech into target text in a single pass. Experimental results show that it is possible to train compact and efficient end-to-end speech translation models in this setup. We also distribute the corpus and hope that our speech translation baseline on this corpus will be challenged in the future.},\n  arxiv_id = {1802.04200v1},\n  subject = {cs.CL},\n  submission_date = {2018-02-12T17:37:11Z}\n}", "url": "http://arxiv.org/abs/1802.04200v1", "latex_url": "http://arxiv.org/src/1802.04200v1", "latex_path": "output/download_papers/1802.04200v1/1802.04200v1", "pdf_url": "http://arxiv.org/pdf/1802.04200v1", "pdf_path": "output/download_papers/1802.04200v1/1802.04200v1.pdf", "md_url": null, "latex_length": 0, "latex": "", "abstract": "We investigate end-to-end speech-to-text translation on a corpus of audiobooks specifically augmented for this task. Previous works investigated the extreme case where source language transcription is not available during learning nor decoding, but we also study a midway case where source language transcription is available at training time only. In this case, a single model is trained to decode source speech into target text in a single pass. Experimental results show that it is possible to train compact and efficient end-to-end speech translation models in this setup. We also distribute the corpus and hope that our speech translation baseline on this corpus will be challenged in the future.", "abstract_length": 701, "abstract_token": 123, "introduction": "Most spoken language translation (SLT) systems integrate (loosely or closely) two main modules: source language speech recognition (ASR) and source-to-target text translation (MT). In these approaches, a symbolic sequence of words (or characters) in the source language is used as an intermediary representation during the speech translation process. However, recent works have attempted to build end-to-end speech-to-text translation without using source language transcription during learning or decoding. One attempt to translate directly a source speech signal into target language text is that of [1]. However, the authors focus on the alignment between source speech utterances and their text translation without proposing a complete end-to-end translation system. The first attempt to build an end-to-end speech-to-text translation system (which does not use source language) is our own work [2] but it was applied to a synthetic (TTS) speech corpus. A similar approach was then proposed and evaluated on a real speech corpus by [3]. This paper is a follow-up of our previous work [2]. We now investigate end-to-end speech-to-text translation on a corpus of audiobooks - LibriSpeech [4] - specifically augmented to perform end-to-end speech translation [5]. While previous works [2, 3] investigated the extreme case where source language transcription is not available during learning nor decoding (unwritten language scenario defined in [6, 7]), we also investigate, in this paper, a midway case where a certain amount of source language transcription is available during training. In this intermediate scenario, a unique (endto-end) model is trained to decode source speech into target text through a single pass (which can be interesting if compact speech translation models are needed). This paper is organized as follows: after presenting our corpus in section 2, we present our end-to-end models in section 3. Section 4 describes our evaluation on two datasets: the synthetic dataset used in [2] and the audiobook dataset described in section 2. Finally, section 5 concludes this work.", "introduction_length": 2098, "introduction_token": 414, "reference": "#  \n\n6. REFERENCES   \n[1] Long Duong, Antonios Anastasopoulos, David Chiang, Steven Bird, and Trevor Cohn, “An Attentional Model for Speech Translation Without Transcription,” in NAACL-HLT, 2016.   \n[2] Alexandre Be´rard, Olivier Pietquin, Laurent Besacier, and Christophe Servan, “Listen and Translate: A Proof of Concept for End-to-End Speech-to-Text Translation,” NIPS 2016 End-to-end Learning for Speech and Audio Processing Workshop, 2016.   \n[3] Ron J. Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, and Zhifeng Chen, “Sequence-to-Sequence Models Can Directly Transcribe Foreign Speech,” in Interspeech, 2017.   \n[4] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, “Librispeech: an ASR corpus based on public domain audio books,” in ICASSP, 2015.   \n[5] Ali Can Kocabiyikoglu, Laurent Besacier, and Olivier Kraif, “Augmenting Librispeech with French Translations: A Multimodal Corpus for Direct Speech Translation Evaluation,” in LREC, 2018.   \n[6] Gilles Adda, Sebastian Stu¨cker, Martine Adda-Decker, Odette Ambouroue, Laurent Besacier, David Blachon, He´le\\`ne Bonneau-Maynard, Pierre Godard, Fatima Hamlaoui, Dmitri Idiatov, Guy-Noe¨l Kouarata, Lori Lamel, Emmanuel-Moselly Makasso, Annie Rialland, Mark Van de Velde, Franc¸ois Yvon, and Sabine Zerbian, “Breaking the Unwritten Language Barrier: The Bulb Project,” in Proceedings of SLTU (Spoken Language Technologies for Under-Resourced Languages), 2016.   \n[7] Antonios Anastasopoulos and David Chiang, “A case study on using speech-to-translation alignments for language documentation,” in Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages, 2017.   \n[8] Matt Post, Gaurav Kumar, Adam Lopez, Damianos Karakos, Chris Callison-Burch, and Sanjeev Khudanpur, “Improved Speech-to-Text Translation with the Fisher and Callhome Spanish-English Speech Translation Corpus,” in IWSLT, 2013.   \n[9] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, “Neural Machine Translation by Jointly Learning to Align and Translate,” in ICLR, 2015.   \n[10] Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, “Attention-Based Models for Speech Recognition,” in NIPS, 2015.   \n[11] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and Yoshua Bengio, “End-to-End Attention-based Large Vocabulary Speech Recognition,” in ICASSP, 2016.   \n[12] Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexandra Birch, Barry Haddow, Julian Hitschler, Marcin JunczysDowmunt, Samuel Laeubli, Antonio Valerio, Antonio Valerio Miceli Barone, Jozef Mokry, and Maria Nadejde, “Nematus: a Toolkit for Neural Machine Translation,” in EACL, 2017.   \n[13] Benoit Mathieu, Slim Essid, Thomas Fillon, Jacques Prado, and Gae¨l Richard, “YAAFE, an Easy to Use and Efficient Audio Feature Extraction Software,” in ISMIR (International Society of Music Information Retrieval), 2010.   \n[14] William Chan, Navdeep Jaitly, Quoc V. Le, and Oriol Vinyals, “Listen, Attend and Spell,” in ICASSP, 2016.   \n[15] Rico Sennrich, Barry Haddow, and Alexandra Birch, “Neural Machine Translation of Rare Words with Subword Units,” in ACL, 2016.   \n[16] Diederik Kingma and Jimmy Ba, “Adam: A Method for Stochastic Optimization,” in ICLR, 2015.   \n[17] Diederik P Kingma, Tim Salimans, and Max Welling, “Variational dropout and the local reparameterization trick,” in NIPS, 2015.   \n[18] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals, “Recurrent Neural Network Regularization,” in ICLR, 2014.   \n[19] Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man, Rajat Monga, Sherry Moore, Derek Murray, Jon Shlens, Benoit Steiner, Ilya Sutskever, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Oriol Vinyals, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng, “TensorFlow: LargeScale Machine Learning on Heterogeneous Distributed Systems,” arXiv, 2015.   \n[20] Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals, and Łukasz Kaiser, “Multi-task Sequence to Sequence Learning,” in ICLR, 2016.", "reference_length": 4262, "reference_token": 1288, "txt_length": 19446, "txt_token": 5450, "txt": "# D-TO-END AUTOMATIC SPEECH TRANSLATION OF AUDIOBOOK  \n\nAlexandre B´erard†⋆ Laurent Besacier⋆ Ali Can Kocabiyikoglu⋆ Olivier Pietquin†  \n\n⋆LIG - Univ. Grenoble-Alpes (France) †CRIStAL - Univ. Lille (France)  \n\n# ABSTRACT  \n\nWe investigate end-to-end speech-to-text translation on a corpus of audiobooks specifically augmented for this task. Previous works investigated the extreme case where source language transcription is not available during learning nor decoding, but we also study a midway case where source language transcription is available at training time only. In this case, a single model is trained to decode source speech into target text in a single pass. Experimental results show that it is possible to train compact and efficient end-to-end speech translation models in this setup. We also distribute the corpus and hope that our speech translation baseline on this corpus will be challenged in the future.  \n\nIndex Terms— End-to-end models, Speech Translation, LibriSpeech.  \n\n# 1. INTRODUCTION  \n\nMost spoken language translation (SLT) systems integrate (loosely or closely) two main modules: source language speech recognition (ASR) and source-to-target text translation (MT). In these approaches, a symbolic sequence of words (or characters) in the source language is used as an intermediary representation during the speech translation process. However, recent works have attempted to build end-to-end speech-to-text translation without using source language transcription during learning or decoding. One attempt to translate directly a source speech signal into target language text is that of [1]. However, the authors focus on the alignment between source speech utterances and their text translation without proposing a complete end-to-end translation system. The first attempt to build an end-to-end speech-to-text translation system (which does not use source language) is our own work [2] but it was applied to a synthetic (TTS) speech corpus. A similar approach was then proposed and evaluated on a real speech corpus by [3].  \n\nThis paper is a follow-up of our previous work [2]. We now investigate end-to-end speech-to-text translation on a corpus of audiobooks - LibriSpeech [4] - specifically augmented to perform end-to-end speech translation [5]. While previous works [2, 3] investigated the extreme case where source language transcription is not available during learning nor decoding (unwritten language scenario defined in [6, 7]), we also investigate, in this paper, a midway case where a certain amount of source language transcription is available during training. In this intermediate scenario, a unique (endto-end) model is trained to decode source speech into target text through a single pass (which can be interesting if compact speech translation models are needed).  \n\nThis paper is organized as follows: after presenting our corpus in section 2, we present our end-to-end models in section 3. Section 4 describes our evaluation on two datasets: the synthetic dataset used in [2] and the audiobook dataset described in section 2. Finally, section 5 concludes this work.  \n\n# 2. AUDIOBOOK CORPUS FOR END-TO-END SPEECH TRANSLATION  \n\n# 2.1. Augmented LibriSpeech  \n\nLarge quantities of parallel texts (e.g. Europarl or OpenSubtitles) are available for training text machine translation systems, but there are no large $(>\\!100\\mathrm{h})$ and publicly available parallel corpora that include speech in a source language aligned to text in a target language. The Fisher/Callhome Spanish-English corpora [8] are only medium size (38h), contain low-bandwidth recordings, and are not available for free.  \n\nWe very recently built a large English to French corpus for direct speech translation training and evaluation $[5]^{1}$ , which is much larger than the existing corpora described above. We started from the LibriSpeech corpus used for Automatic Speech Recognition (ASR), which has 1000 hours of speech aligned with their transcriptions [4].  \n\nThe read audiobook recordings derive from a project based on a collaborative effort: LibriVox. The speech recordings are based on public domain books available on Gutenberg Project2 which are distributed in LibriSpeech along with the recordings.  \n\nOur augmentation of LibriSpeech is straightforward: we automatically aligned e-books in a foreign language (French) with English utterances of LibriSpeech. This lead to 236 hours of English speech aligned to French translations at utterance level (more details can be found in [5]). Since  \n\nEnglish (source) transcriptions are initially available for $L i b$ - riSpeech, we also translated them using Google Translate. To summarize, for each utterance of our 236h corpus, the following quadruplet is available: English speech signal, English transcription, French text translation 1 (from alignment of e-books) and translation 2 (from MT of English transcripts).  \n\n# 2.2. MT and AST tasks  \n\nThis paper focuses on the speech translation (AST) task of audiobooks from English to French, using the Augmented LibriSpeech corpus. We compare a direct (end-to-end) approach, with a cascaded approach that combines a neural speech transcription (ASR) model with a neural machine translation model (MT). The ASR and MT results are also reported as baselines for future uses of this corpus.  \n\nAugmented LibriSpeech contains 236 hours of speech in total, which is split into 4 parts: a test set of 4 hours, a dev set of 2 hours, a clean train set of 100 hours, and an extended train set with the remaining 130 hours. Table 1 gives detailed information about the size of each corpus. All segments in the corpus were sorted according to their alignment confidence scores, as produced by the alignment software used by the authors of the corpus [5]. The test, dev and train sets correspond to the highest rated alignments. The remaining data (extended train) is more noisy, as it contains more incorrect alignments. The test set was manually checked, and incorrect alignments were removed. We perform all our experiments using train only (without extended train). Furthermore, we double the training size by concatenating the aligned references with the Google Translate references. We also mirror our experiments on the BTEC synthetic speech corpus, as a follow-up to [2].  \n\n# 3. END-TO-END MODELS  \n\nFor the three tasks, we use encoder-decoder models with attention [9, 10, 11, 2, 3]. Because we want to share some parts of the model between tasks (multi-task training), the ASR and AST models use the same encoder architecture, and the AST and MT models use the same decoder architecture.  \n\n# 3.1. Speech Encoder  \n\nThe speech encoder is a mix between the convolutional encoder presented in [3] and our previously proposed encoder [2]. It takes as input a sequence of audio features: $\\mathbf{x}\\,=\\,(x_{1},\\hdots,x_{T_{x}})\\,\\in\\,\\mathbb{R}^{T_{x}\\times n}$ . Like [2], these features are given as input to two non-linear $(t a n h)$ layers, which output new features of size $n^{\\prime}$ . Like [3], this new set of features is then passed to a stack of two convolutional layers. Each layer applies 16 convolution filters of shape $(3,3,d e p t h)$ with a stride of $(2,2)$ w.r.t. time and feature dimensions; depth is 1 for the first layer, and 16 for the second layer. We get features of shape $(T_{x}/2,n^{\\prime}/2,16)$ after the $1^{\\mathrm{st}}$ layer, and $(T_{x}/4,n^{\\prime}/4,16)$ after the $2^{\\mathrm{nd}}$ layer. This latter tensor is flattened with shape $(T_{x}^{\\prime}\\,=\\,T_{x}/4,4n^{\\prime})$ before being passed to a stack of three bidirectional LSTMs. This set of features has $1/4t h$ the time length of the initial features, which speeds up training, as the complexity of the model is quadratic with respect to the source length. In our models, we use $n^{\\prime}=128$ , which gives features of size 512.  \n\nThe last bidirectional LSTM layer computes a sequence of annotations $\\mathbf{h}=h_{1},\\cdot\\cdot\\cdot,h_{T_{x}^{\\prime}}$ , where each annotation $h_{i}$ is a concatenation of the corresponding forward and backward states: $h_{i}=(\\vec{h_{i}}\\oplus\\overleftarrow{h}_{i})\\in\\mathbb{R}^{2m}$ , with $m$ the encoder cell size.  \n\nThis model differs from [2], which did not use convolutions, but time pooling between each LSTM layer, resulting in a shorter sequence (pyramidal encoder).  \n\n# 3.2. Character-level decoder  \n\nWe use a character-level decoder composed of a conditional LSTM [12], followed by a dense layer.  \n\n$$\n\\begin{array}{r}{s_{t},o_{t}=u p d a t e^{1}(s_{t-1}^{\\prime},E(y_{t-1}))}\\\\ {c_{t}=l o o k(o_{t},\\mathbf{h})}\\\\ {s_{t}^{\\prime},o_{t}^{\\prime}=u p d a t e^{2}(s_{t-1},c_{t})}\\\\ {y_{t}=g e n e r a t e(o^{\\prime}t\\oplus c_{t}\\oplus E(y_{t-1}))}\\end{array}\n$$  \n\nwhere update1 and update2 are two LSTMs with cell size $m^{\\prime}$ . look is a vanilla global attention mechanism [9], which uses a feed-forward network with one hidden layer of size $m^{\\prime}$ . $E^{k\\times|V|}$ is the target embedding matrix, with $k$ the embedding size and $|V|$ the vocabulary size. $c_{t}\\in\\mathbb{R}^{2m}$ is a context vector which summarizes the input states to help the decoder generate a new symbol and update its state. generate uses a nonlinear layer followed by a linear projection to compute a score for each symbol in target vocabulary $V$ . It then picks target symbol $z_{t}$ with the highest score:  \n\n$$\n\\begin{array}{r}{g e n e r a t e(x)=\\arg\\underset{i=1}{\\operatorname*{max}}\\,z_{i}}\\\\ {z=W_{p r o j}\\operatorname{tanh}(W_{o u t}^{T}x+b_{o u t})+b_{p r o j}}\\end{array}\n$$  \n\nwith $W_{p r o j}\\in\\mathbb{R}^{|V|\\times l},b_{p r o j}\\in\\mathbb{R}^{|V|}$ , $W_{o u t}\\in\\mathbb{R}^{l\\times(m^{\\prime}+2m+k)}$ , $b_{o u t}\\in\\mathbb{R}^{l}$ , where $l$ is the output layer size.  \n\n# 4. EXPERIMENTS  \n\n# 4.1. Model Settings  \n\nSpeech files were preprocessed using Yaafe [13], to extract 40 MFCC features and frame energy for each frame with a step size of $10~\\mathrm{{ms}}$ and window size of $40~\\mathrm{{ms}}$ , following [14, 2]. We tokenize and lowercase all the text, and normalize the punctuation, with the Moses scripts3. For BTEC, the same preprocessing as [2] is applied. Character-level vocabularies for LibriSpeech are of size 46 for English (transcriptions) and 167 for French (translation). The decoder outputs are always at the character-level (for AST, MT and ASR). For the MT task, the LibriSpeech English (source) side is preprocessed into subword units [15]. We limit the number of merge operations to $30k$ , which gives a vocabulary of size $27k$ . The MT encoder for BTEC takes entire words as input.  \n\n<html><body><table><thead><tr><td rowspan=\"2\" colspan=\"2\"><b></b></td><td colspan=\"2\"><b>Total</b></td><td colspan=\"3\"><b>Source (per segment)</b></td><td><b>Target (per segment)</b></td></tr><tr><td><b>segments</b></td><td><b>hours</b></td><td><b>frames</b></td><td><b>chars</b></td><td><b>(sub)words</b></td><td><b>chars</b></td></tr></thead><tbody><tr><td rowspan=\"2\">LibriSpeech (Real)</td><td>train 1 train 2</td><td>47271</td><td>100:00</td><td>762</td><td>111</td><td>20.7</td><td>143 126</td></tr><tr><td>dev test</td><td>1071 2048</td><td>2:00 3:44</td><td>673 657</td><td>93 95</td><td>17.9 18.3</td><td>110 112</td></tr><tr><td rowspan=\"2\">BTEC (Synthetic)</td><td>train dev</td><td>19972</td><td>15:51</td><td>276</td><td>50</td><td>10</td><td>42</td></tr><tr><td>dev test</td><td>1512 933</td><td>0:59 0:36</td><td>236</td><td>40</td><td>8.1</td><td>33</td></tr></tbody></table></body></html>  \n\nOur BTEC models use an LSTM size of $m=m^{\\prime}=256$ , while the LibriSpeech models use a cell size of 512, except for the speech encoder layers which use a cell size of $m=$ 256 in each direction. We use character embeddings of size $k\\,=\\,64$ for BTEC, and $k\\,=\\,128$ for LibriSpeech. The MT encoders are more shallow, with a single bidirectional layer. The source embedding sizes for words (BTEC) and subwords (LibriSpeech) are respectively 128 and 256.  \n\nThe input layers in the speech encoders have a size of 256 for the first layer and $n^{\\prime}\\;=\\;128$ for the second. The LibriSpeech decoders use an output layer size of $l\\,=\\,512$ . For BTEC, we do not use any non-linear output layer, as we found that this led to overfitting.  \n\n# 4.2. Training settings  \n\nWe train our models with Adam [16], with a learning rate of 0.001, and a mini-batch size of 64 for BTEC, and 32 for LibriSpeech (because of memory constraints). We use variational dropout [17], i.e., the same dropout mask is applied to all elements in a batch at all time steps, with a rate of 0.2 for LibriSpeech and 0.4 for BTEC. In the MT tasks, we also drop source and target symbols at random, with probability 0.2. Dropout is not applied on recurrent connections [18].  \n\nWe train all our models on LibriSpeech train augmented with the Google Translate references, i.e., the source side of the corpus (speech) is duplicated, and the target side (translations) is a concatenation of the aligned references with the Google Translate references. Because of GPU memory limits, we set the maximum length to 1400 frames for LibriSpeech input, and 300 characters for its output. This covers about  \n\n$90\\%$ of the training corpus. Longer sequences are kept but truncated to the maximum size. We evaluate our models on the dev set every 1000 mini-batch updates using BLEU for AST and MT, and WER for ASR, and keep the best performing checkpoint for final evaluation on the test set.  \n\nOur models are implemented with TensorFlow [19] as part of the LIG-CRIStAL NMT toolkit4.  \n\n# 4.3. Results  \n\nTable 2 presents the results for the ASR and MT tasks on BTEC and LibriSpeech. The MT task (and by extension the AST task) on LibriSpeech (translating novels) looks particularly challenging, as we observe BLEU scores around $20\\%^{5}$ .  \n\nTable 2: MT and ASR results on test set for BTEC and Augmented LibriSpeech. We use a beam size of 8, and ensembles of 2 models trained from scratch.   \n\n\n<html><body><table><thead><tr><td></td><td><b>Model</b></td><td><b>ASR (WER ↓)</b></td><td><b>MT (BLEU ↑)</b></td></tr></thead><tbody><tr><td></td><td>greedy beam-search</td><td>14.9</td><td>47.4</td></tr><tr><td>ensemble</td><td>13.8</td><td>13.8</td><td>49.2</td></tr><tr><td>ensemble</td><td>11.3</td><td>11.3</td><td>50.7</td></tr><tr><td>greedy</td><td>greedy</td><td>19.9</td><td>19.2</td></tr><tr><td>beam-search</td><td>17.9</td><td>17.9</td><td>18.8</td></tr><tr><td>ensemble</td><td>15.1</td><td>15.1</td><td>19.3</td></tr><tr><td>Google Translate</td><td></td><td></td><td>19.3 22.2</td></tr></tbody></table></body></html>  \n\nFor Automatic Speech Translation (AST), we try four settings. The cascaded model combines both the ASR and MT models (as a pipeline). The end-to-end model (described in section 3) does not make any use of source language transcripts. The pre-trained model is identical to end-to-end, but its encoder and decoder are initialized with our ASR and MT models. The multi-task model is also pre-trained, but continues training for all tasks, by alternating updates like [20], with $60\\%$ of updates for AST and $20\\%$ for ASR and MT.  \n\n<html><body><table><thead><tr><td rowspan=\"2\"></td><td><b>greedy</b></td><td><b>beam</b></td><td><b>ensemble</b></td><td rowspan=\"2\"><b>params (million)</b></td></tr><tr><td colspan=\"3\"><b>Test BLEU</b></td></tr></thead><tbody><tr><td>Baseline [2]</td><td>29.1</td><td>31.3</td><td>37.9t</td><td>10.4</td></tr><tr><td>Cascaded</td><td>38.9</td><td>40.7</td><td>43.8</td><td>7.9 + 3.4</td></tr><tr><td>End-to-End</td><td>31.3</td><td>33.7</td><td></td><td rowspan=\"3\">6.7</td></tr><tr><td>Pre-trained</td><td>33.7</td><td>36.3</td><td rowspan=\"2\">40.4</td></tr><tr><td>Multi-task</td><td>35.1</td><td>37.6</td></tr></tbody></table></body></html>  \n\n![](images/78fe06d7063ebcded0e1f979de7f76a283a81e2a8fdb1fbe54318cec86eb0c4c.jpg)  \nTable 3: Results of the AST task on BTEC test. $\\dagger$ was obtained with an ensemble of 5 models, while we use ensembles of 2 models. The non-cascaded ensemble combines the pre-trained and multi-task models. Contrary to [2], we only present mono-reference results.   \nFig. 1: Augmented LibriSpeech Dev BLEU scores for the MT task, and WER scores for the ASR task, with the initial (mono-task) models, and when multi-task training picks up.  \n\nTable 3 and 4 present the results for the end-to-end AST task on BTEC and LibriSpeech. On both corpora, we show that: (1) it is possible to train compact end-to-end AST models with a performance close to cascaded models; (2) pretraining and multi-task learning6 improve AST performance; (3) contrary to [3], in both BTEC and LibriSpeech settings, best AST performance is observed when a symbolic sequence of symbols in the source language is used as an intermediary representation during the speech translation process (cascaded system); (4) finally, the AST results presented on LibriSpeech demonstrate that our augmented corpus is useful, although challenging, to benchmark end-to-end AST systems on real speech at a large scale. We hope that the baseline we established on Augmented LibriSpeech will be challenged in the future.  \n\nThe large improvements on MT and AST on the BTEC corpus, compared to [2] are mostly due to our use of a better decoder, which outputs characters instead of words.  \n\nTable 4: AST results on Augmented LibriSpeech test. $\\dagger$ combines the end-to-end, pre-trained and multi-task models.   \n\n\n<html><body><table><thead><tr><td rowspan=\"2\"></td><td><b>greedy</b></td><td><b>beam</b></td><td><b>ensemble</b></td><td rowspan=\"2\"><b> params (million)</b></td></tr><tr><td colspan=\"3\"><b>Test BLEU</b></td></tr></thead><tbody><tr><td>Cascaded</td><td>14.6</td><td>14.6</td><td>15.8</td><td>6.3 + 15.9</td></tr><tr><td>End-to-End</td><td>12.3</td><td>12.9</td><td rowspan=\"3\">15.5</td><td rowspan=\"3\">9.4</td></tr><tr><td>Pre-trained</td><td>12.6</td><td>13.3</td></tr><tr><td>Multi-task</td><td>12.6</td><td>13.4</td></tr></tbody></table></body></html>  \n\n![](images/99061b0eee154a4c33bb0fb5ad2f5b47a366986625619c8e213e5b0b728c8827.jpg)  \nFig. 2: Dev BLEU scores on 3 models for end-to-end AST of audiobooks. Best scores on the dev set for the end-to-end (mono-task), pre-train and multi-task models were achieved at steps $369\\mathrm{k}$ , 129k and $95\\mathrm{k}$ .  \n\n# 4.4. Analysis  \n\nFigure 1 shows the evolution of BLEU and WER scores for MT and ASR tasks with single models, and when we continue training them as part of a multi-task model. The multi-task procedure does more updates on AST, which explains the degraded results, but we observe that the speech encoder and text decoder are still able to generalize well to other tasks.  \n\nFigure 2 shows the evolution of dev BLEU scores for our three AST models on LibriSpeech. We see that pre-training helps the model converge much faster. Eventually, the End-toEnd system reaches a similarly good solution, but after three times as many updates. Multi-Task training does not seem to be helpful when combined with pre-training.  \n\n# 5. CONCLUSION  \n\nWe present baseline results on End-to-End Automatic Speech Translation on a new speech translation corpus of audiobooks, and on a synthetic corpus extracted from BTEC (follow-up to [2]). We show that, while cascading two neural models for ASR and MT gives the best results, end-to-end methods that incorporate the source language transcript come close in performance.", "appendix": ""}, {"title": "MAM: Masked Acoustic Modeling for End-to-End Speech-to-Text Translation", "authors": "Junkun Chen and\n               Mingbo Ma and\n               Renjie Zheng and\n               Liang Huang", "bibkey": "mam_masked_acoustic_modeling_for_end_to_end_speech_to_text_translation", "bibitem": "@article{ChenMMAMFEST,\n  url = {http://arxiv.org/abs/2010.11445},\n  title = {MAM: Masked Acoustic Modeling for End-to-End Speech-to-Text Translation},\n  authors = {Junkun Chen and\n               Mingbo Ma and\n               Renjie Zheng and\n               Liang Huang},\n  bibkey = {ChenMMAMFEST}\n}", "url": "http://arxiv.org/abs/2010.11445", "latex_url": "http://arxiv.org/src/2010.11445", "latex_path": "output/download_papers/2010.11445v2/2010.11445v2", "pdf_url": "http://arxiv.org/pdf/2010.11445", "pdf_path": "output/download_papers/2010.11445v2/2010.11445v2.pdf", "md_url": null, "latex_length": 0, "latex": null, "abstract": "End-to-end Speech-to-text Translation (E2E-ST), which directly translates source language speech to target language text, is widely useful in practice, but traditional cascaded approaches $(\\mathrm{ASR+MT})$ often suffer from error propagation in the pipeline. On the other hand, existing end-to-end solutions heavily depend on the source language transcriptions for pre-training or multi-task training with Automatic Speech Recognition (ASR). We instead propose a simple technique to learn a robust speech encoder in a self-supervised fashion only on the speech side, which can utilize speech data without transcription. This technique termed Masked Acoustic Modeling (MAM), not only provides an alternative solution to improving E2E-ST, but also can perform pre-training on any acoustic signals (including non-speech ones) without annotation. We conduct our experiments over 8 different translation directions. In the setting without using any transcriptions, our technique achieves an average improvement of $+1.1$ BLEU, and $+2.3$ BLEU with MAM pre-training. Pre-training of MAM with arbitrary acoustic signals also has an average improvement with $+1.6$ BLEU for those languages. Compared with ASR multi-task learning solution, which replies on transcription during training, our pre-trained MAM model, which does not use transcription, achieves similar accuracy.", "abstract_length": 1368, "abstract_token": 275, "introduction": "Speech-to-text translation (ST), which translates the source language speech to target language text, is useful in many scenarios such as international conferences, travels, foreignlanguage video subtitling, etc. Conventional cascaded approaches to ST (Ney, 1999; Matusov et al., 2005; Mathias & Byrne, 2006; Berard et al., 2016) first transcribe the speech audio into source language text (ASR) and then perform text-to-text machine translation (MT), which inevitably suffers from error propagation in the pipeline. To alleviate this problem, recent efforts explore end-to-end approaches (E2E-ST) (Weiss et al., 2017; Berard et al., 2018; Vila et al., 2018; Gangi et al., 2019), which are computationally more efficient at inference time and mitigate the risk of error propagation from imperfect ASR. ![](images/d473b7a17b4632d943267027a44c8a5f23013db72041cbf49c21bff9ce22de69.jpg) Figure 1. Comparisons with different existing solutions and our proposed Masked Acoustic Modeling (MAM). To improve the translation accuracy of E2E-ST models, researchers either initialize the encoder of ST with a pretrained ASR encoder (Berard et al., 2018; Bansal et al., 2019; Wang et al., 2020b) to get better representations of the speech signal, or perform Multi-Task Learning (MTL) with ASR to bring more training and supervision signals to the shared encoder (Anastasopoulos et al., 2016; Anastasopoulos & Chiang, 2018; Sperber et al., 2019; Liu et al., 2019b) (see Fig. 1). These methods improve the translation quality by providing more training signals to the encoder to learn better phonetic information and hidden representation correspondence (Stoian et al., 2020). However, both above solutions assume the existence of substantial speech transcriptions of the source language. Unfortunately, this assumption is problematic. On the one hand, for certain low-resource languages, especially endangered ones (Bird, 2010; Bird et al., 2014), the source speech transcriptions are expensive to collect. Moreover, according to the report from Ethnologue, there are more than 3000 languages that have no written form or no standard orthography, making phonetic transcription impossible (Duong et al., 2016). On the other hand, the amount of speech audios with transcriptions are limited (as they are expensive to collect), and there exist far more audios without any annotations. It will be much more straightforward and cheaper to leverage these raw audios to train a robust encoder directly. To relieve from the dependency on source language transcriptions, we present a straightforward yet effective solution, Masked Acoustic Modeling (MAM), to utilize the speech data in a self-supervised fashion without using any source language transcription, unlike other speech pretraining models (Chuang et al., 2019; Wang et al., 2020c). Aside from the regular training of E2E-ST (without ASR as MTL or pre-training), MAM masks certain portions of the speech input randomly and aims to recover the masked speech signals with their context on the encoder side. MAM not merely provides an alternative solution to improving E2E-ST, but also is a general technique that can be used as a pre-training module on arbitrary acoustic signals, e.g., multilingual speech, music, animal sounds. The contributions of our paper are as follows: • We demonstrate the importance of a self-supervising module for E2E-ST. Unlike all previous attempts, which heavily depend on transcription, MAM improves the capacity of the encoder by recovering masked speech signals merely based on their context. MAM also can be used together with transcriptions in ASR pre-training and MTL settings to further boost the translation accuracy. • MAM also can be used as a pre-training module solely by itself. During pre-training, MAM is capable to utilize arbitrary acoustic signal (e.g., music, animal sound) other than regular speech audio. Considering there are much more acoustic data than human speech, MAM has better potential to be used for pre-training. To the best of our knowledge, MAM is the first technique that is able to perform pre-training with any form of the audio signal. • For 8 different translation directions, when we do not use any transcription, MAM demonstrates an average BLEU improvements of 1.09 in the basic setting and 2.26 with pre-training. • We show that the success of MAM does not rely on intensive or expensive computation. MAM only has $6.5\\%$ more parameters than the baseline model.", "introduction_length": 4472, "introduction_token": 1012, "reference": "# References  \n\nAnastasopoulos, A. and Chiang, D. Tied multitask learning for neural speech translation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 2018.   \nAnastasopoulos, A., Chiang, D., and Duong, L. An unsupervised probability model for speech-to-translation alignment of low-resource languages. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 2016.   \nArdila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M., and Weber, G. Common voice: A massively-multilingual speech corpus. 2020.   \nBaevski, A., Zhou, H., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. NeurIPS 2020, 2020.   \nBahar, P., Zeyer, A., Schl¨uter, R., and Ney, H. On using specaugment for end-to-end speech translation. 2019.   \nBansal, S., Kamper, H., Livescu, K., Lopez, A., and Goldwater, S. Pre-training on high-resource speech recognition improves low-resource speech-to-text translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.   \nBerard, A., Pietquin, O., Servan, C., and Besacier, L. Listen and translate: A proof of concept for end-to-end speechto-text translation. CoRR, abs/1612.01744, 2016. URL http://arxiv.org/abs/1612.01744.   \nBerard, A., Besacier, L., Kocabiyikoglu, A., and Pietquin, O. End-to-end automatic speech translation of audiobooks. 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6224–6228, 2018.   \nBird, S. A scalable method for preserving oral literature from small languages. In Chowdhury, G., Koo, C., and Hunter, J. (eds.), The Role of Digital Libraries in a Time of Global Change. Springer Berlin Heidelberg, 2010.   \nBird, S., Gawne, L., Gelbart, K., and McAlister, I. Collecting bilingual audio in remote indigenous communities. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, 2014.   \nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. 2020.   \nChuang, Y.-S., Liu, C.-L., Lee, H.-Y., and Lee, L.-s. SpeechBERT: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering. arXiv e-prints, 2019.   \nDefferrard, M., Benzi, K., Vandergheynst, P., and Bresson, X. Fma: A dataset for music analysis. arXiv preprint arXiv:1612.01840, 2016.   \nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2019.   \nDi Gangi, M. A., Cattoni, R., Bentivogli, L., Negri, M., and Turchi, M. MuST-C: a Multilingual Speech Translation Corpus. In NAACL, 2019.   \nDuong, L., Anastasopoulos, A., Chiang, D., Bird, S., and Cohn, T. An attentional model for speech translation without transcription. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2016.   \nEthnologue. Ethnologue (21st edition). URL https: //www.ethnologue.com/enterprise-faq/ how-many-languages-world-are-unwritten  \nGangi, M. A. D., Negri, M., and Turchi, M. Adapting Transformer to End-to-End Spoken Language Translation. In Proc. Interspeech 2019, 2019.   \nGemmeke, J. F., Ellis, D. P. W., Freedman, D., Jansen, A., Lawrence, W., Moore, R. C., Plakal, M., and Ritter, M. Audio set: An ontology and human-labeled dataset for audio events. In Proc. IEEE ICASSP 2017, 2017.   \nGraves, A., Fern´andez, S., Gomez, F., and Schmidhuber, J. Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd International Conference on Machine Learning, 2006.   \nInaguma, H., Kiyono, S., Duh, K., Karita, S., Soplin, N. E. Y., Hayashi, T., and Watanabe, S. Espnet-st: All-in-one speech translation toolkit. arXiv preprint arXiv:2004.10234, 2020.   \nJoshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, L., and Levy, O. SpanBERT: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics, 8, 2020a.   \nJoshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, L., and Levy, O. Spanbert: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics, 8:64–77, 2020b.   \nKahn, J., Rivi\\`ere, M., Zheng, W., Kharitonov, E., Xu, Q., Mazar´e, P. E., Karadayi, J., Liptchinsky, V., Collobert, R., Fuegen, C., Likhomanenko, T., Synnaeve, G., Joulin, A., Mohamed, A., and Dupoux, E. Librilight: A benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7669–7673, 2020. https://github. com/facebookresearch/libri-light.   \nKudo, T. and Richardson, J. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Brussels, Belgium, November 2018. Association for Computational Linguistics.   \nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019a. URL http://arxiv.org/ abs/1907.11692.   \nLiu, Y., Xiong, H., Zhang, J., He, Z., Wu, H., Wang, H., and Zong, C. End-to-End Speech Translation with Knowledge Distillation. In Proc. Interspeech 2019, 2019b.   \nMathias, L. and Byrne, W. Statistical phrase-based speech translation. In 2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings, 2006.   \nMatusov, E., Kanthak, S., and Ney, H. On the integration of speech recognition and statistical machine translation. In INTERSPEECH, 2005.   \nMcAuliffe, M., Socolof, M., Mihuc, S., Wagner, M., and Sonderegger, M. Montreal forced aligner: Trainable text-speech alignment using kaldi. In Proc. Interspeech 2017, pp. 498–502, 2017. doi: 10.21437/ Interspeech.2017-1386. URL http://dx.doi.org/ 10.21437/Interspeech.2017-1386.   \nNey, H. Speech translation: coupling of recognition and translation. In 1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258), volume 1, pp. 517–520 vol.1, 1999.   \nPanayotov, V., Chen, G., Povey, D., and Khudanpur, S. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5206–5210. IEEE, 2015.   \nPark, D. S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E. D., and Le, Q. V. Specaugment: A simple data augmentation method for automatic speech recognition. In Interspeech 2019, 2019.   \nPovey, D., Ghoshal, A., Boulianne, G., Goel, N., Hannemann, M., Qian, Y., Schwarz, P., and Stemmer, G. The kaldi speech recognition toolkit. In In IEEE 2011 workshop, 2011.   \nSperber, M., Neubig, G., Niehues, J., and Waibel, A. Attention-Passing Models for Robust and Data-Efficient End-to-End Speech Translation. Transactions of the Association for Computational Linguistics (TACL), 2019. URL https://arxiv.org/abs/1904.07209.   \nStoian, M., Bansal, S., and Goldwater, S. Analyzing asr pretraining for low-resource speech-to-text translation. In ICASSP, 2020.   \nVila, L. C., Escolano, C., Fonollosa, J. A. R., and Costajussa\\`, M. R. End-to-end speech translation with the transformer. In IberSPEECH, 2018.   \nWang, C., Wu, Y., Du, Y., Li, J., Liu, S., Lu, L., Ren, S., Ye, G., Zhao, S., and Zhou, M. Semantic mask for transformer based end-to-end speech recognition. In Interspeech, 2020a.   \nWang, C., Wu, Y., Liu, S., Yang, Z., and Zhou, M. Bridging the gap between pre-training and fine-tuning for endto-end speech translation. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, 2020b.   \nWang, C., Wu, Y., Liu, S., Zhou, M., and Yang, Z. Curriculum pre-training for end-to-end speech translation. In ACL, 2020c.   \nWatanabe, S., Hori, T., Kim, S., Hershey, J. R., and Hayashi, T. Hybrid ctc/attention architecture for end-to-end speech recognition. 2017.   \nWeiss, R. J., Chorowski, J., Jaitly, N., Wu, Y., and Chen, Z. Sequence-to-sequence models can directly translate foreign speech. In Proc. Interspeech 2017, 2017.  \n\nWu, A., Wang, C., Pino, J., and Gu, J. Self-supervised representations improve end-to-end speech translation. In Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020, 2020.  \n\nZhang, Z., Han, X., Liu, Z., Jiang, X., Sun, M., and Liu, Q. ERNIE: enhanced language representation with informative entities. 2019.", "reference_length": 9582, "reference_token": 2941, "txt_length": 40905, "txt_token": 11263, "txt": "# MAM: Masked Acoustic Modeling for End-to-End Speech-to-Text Translation  \n\nunkun Chen \\* 1 2 Mingbo Ma \\* 1 Renjie Zheng 1 Kaibo Liu 1 Liang  \n\n# Abstract  \n\nEnd-to-end Speech-to-text Translation (E2E-ST), which directly translates source language speech to target language text, is widely useful in practice, but traditional cascaded approaches $(\\mathrm{ASR+MT})$ often suffer from error propagation in the pipeline. On the other hand, existing end-to-end solutions heavily depend on the source language transcriptions for pre-training or multi-task training with Automatic Speech Recognition (ASR). We instead propose a simple technique to learn a robust speech encoder in a self-supervised fashion only on the speech side, which can utilize speech data without transcription. This technique termed Masked Acoustic Modeling (MAM), not only provides an alternative solution to improving E2E-ST, but also can perform pre-training on any acoustic signals (including non-speech ones) without annotation. We conduct our experiments over 8 different translation directions. In the setting without using any transcriptions, our technique achieves an average improvement of $+1.1$ BLEU, and $+2.3$ BLEU with MAM pre-training. Pre-training of MAM with arbitrary acoustic signals also has an average improvement with $+1.6$ BLEU for those languages. Compared with ASR multi-task learning solution, which replies on transcription during training, our pre-trained MAM model, which does not use transcription, achieves similar accuracy.  \n\n# 1 Introduction  \n\nSpeech-to-text translation (ST), which translates the source language speech to target language text, is useful in many scenarios such as international conferences, travels, foreignlanguage video subtitling, etc. Conventional cascaded approaches to ST (Ney, 1999; Matusov et al., 2005; Mathias & Byrne, 2006; Berard et al., 2016) first transcribe the speech audio into source language text (ASR) and then perform text-to-text machine translation (MT), which inevitably suffers from error propagation in the pipeline. To alleviate this problem, recent efforts explore end-to-end approaches (E2E-ST) (Weiss et al., 2017; Berard et al., 2018; Vila et al., 2018; Gangi et al., 2019), which are computationally more efficient at inference time and mitigate the risk of error propagation from imperfect ASR.  \n\n![](images/d473b7a17b4632d943267027a44c8a5f23013db72041cbf49c21bff9ce22de69.jpg)  \nFigure 1. Comparisons with different existing solutions and our proposed Masked Acoustic Modeling (MAM).  \n\nTo improve the translation accuracy of E2E-ST models, researchers either initialize the encoder of ST with a pretrained ASR encoder (Berard et al., 2018; Bansal et al., 2019; Wang et al., 2020b) to get better representations of the speech signal, or perform Multi-Task Learning (MTL) with ASR to bring more training and supervision signals to the shared encoder (Anastasopoulos et al., 2016; Anastasopoulos & Chiang, 2018; Sperber et al., 2019; Liu et al., 2019b) (see Fig. 1). These methods improve the translation quality by providing more training signals to the encoder to learn better phonetic information and hidden representation correspondence (Stoian et al., 2020).  \n\nHowever, both above solutions assume the existence of substantial speech transcriptions of the source language. Unfortunately, this assumption is problematic. On the one hand, for certain low-resource languages, especially endangered ones (Bird, 2010; Bird et al., 2014), the source speech transcriptions are expensive to collect. Moreover, according to the report from Ethnologue, there are more than 3000 languages that have no written form or no standard orthography, making phonetic transcription impossible (Duong et al., 2016). On the other hand, the amount of speech audios with transcriptions are limited (as they are expensive to collect), and there exist far more audios without any annotations. It will be much more straightforward and cheaper to leverage these raw audios to train a robust encoder directly.  \n\nTo relieve from the dependency on source language transcriptions, we present a straightforward yet effective solution, Masked Acoustic Modeling (MAM), to utilize the speech data in a self-supervised fashion without using any source language transcription, unlike other speech pretraining models (Chuang et al., 2019; Wang et al., 2020c). Aside from the regular training of E2E-ST (without ASR as MTL or pre-training), MAM masks certain portions of the speech input randomly and aims to recover the masked speech signals with their context on the encoder side. MAM not merely provides an alternative solution to improving E2E-ST, but also is a general technique that can be used as a pre-training module on arbitrary acoustic signals, e.g., multilingual speech, music, animal sounds. The contributions of our paper are as follows:  \n\n• We demonstrate the importance of a self-supervising module for E2E-ST. Unlike all previous attempts, which heavily depend on transcription, MAM improves the capacity of the encoder by recovering masked speech signals merely based on their context. MAM also can be used together with transcriptions in ASR pre-training and MTL settings to further boost the translation accuracy.   \n• MAM also can be used as a pre-training module solely by itself. During pre-training, MAM is capable to utilize arbitrary acoustic signal (e.g., music, animal sound) other than regular speech audio. Considering there are much more acoustic data than human speech, MAM has better potential to be used for pre-training. To the best of our knowledge, MAM is the first technique that is able to perform pre-training with any form of the audio signal.   \n• For 8 different translation directions, when we do not use any transcription, MAM demonstrates an average BLEU improvements of 1.09 in the basic setting and 2.26 with pre-training.   \n• We show that the success of MAM does not rely on intensive or expensive computation. MAM only has $6.5\\%$ more parameters than the baseline model.  \n\n# 2 Preliminaries: ASR and ST  \n\nWe first briefly review the standard E2E-ST and E2E-ST with ASR MTL to set up the notations.  \n\n# 2.1 Vanilla E2E-ST Training with Seq2Seq  \n\nRegardless of particular design of Seq2Seq models for different tasks, the encoder always takes the source input sequence $\\pmb{x}=(x_{1},...,x_{n})$ of $n$ elements where each $x_{i}~\\in~\\mathbb{R}^{d_{x}}$ is a $d_{x}$ -dimension vector and produces a sequence of hidden representations $\\pmb{h}=f(\\pmb{x})=(h_{1},...,h_{n})$ where $h_{i}\\;=\\;f({\\pmb x})$ . The encoding function $f$ can be implemented by a mixture between Convolution, RNN and Transformer. More specifically, $\\textbf{\\em x}$ can be the spectrogram or mel-spectrogram of the source speech, and each $x_{i}$ represents the frame-level speech feature with certain duration.  \n\nOn the other hand, the decoder greedily predicts a new output word $y_{t}$ given both the source sequence $\\textbf{\\em x}$ and the prefix of decoded tokens, denoted $\\pmb{y}_{<t}~=~(y_{1},...,y_{t-1})$ The decoder continues the generation until it emits <eos> and finishes the entire decoding process. Finally, we obtain the hypothesis $\\pmb{{y}}\\,=\\,(y_{1},...,..\\mathrm{eos})$ with the model score which defined as following:  \n\n$$\n\\begin{array}{r}{p(\\pmb{y}\\mid\\pmb{x})=\\prod_{t=1}^{|\\pmb{y}|}p(y_{t}\\mid\\pmb{x},\\pmb{y}_{<t})}\\end{array}\n$$  \n\nDuring the training time, the entire model aims to maximize the conditional probability of each ground-truth target sentence $\\boldsymbol{y}^{\\star}$ given input $\\textbf{\\em x}$ over the entire training corpus $D_{x,y^{\\star}}$ , or equivalently minimizing the following loss:  \n\n$$\n\\begin{array}{r}{\\ell_{\\mathrm{ST}}(D_{\\mathbf{x},\\mathbf{y}^{\\star}})=-\\sum_{(\\mathbf{x},\\mathbf{y}^{\\star})\\in D_{\\mathbf{x},\\mathbf{y}^{\\star}}}\\log p(\\pmb{y}^{\\star}\\mid\\pmb{x})}\\end{array}\n$$  \n\n# 2.2 Multi-task Learning with ASR  \n\nTo further boost the performance of E2E-ST, researchers proposed to either use pre-trained ASR encoder to initialize ST encoder, or to perform ASR MTL together with ST training. We only discuss the MTL since pre-training does not require significant change to Seq2Seq model.  \n\nDuring multi-task training, there are two decoders sharing one encoder. Besides the MT decoder, there is also another decoder for generating transcriptions. With the help of ASR training, the encoder is able to learn more accurate speech segmentations (similar to forced alignment) making the global reordering of those segments for MT relatively easier. We defined the following training loss for ASR:  \n\n$$\n\\begin{array}{r}{\\ell_{\\mathrm{ASR}}(D_{\\mathbf{x},\\mathbf{z}^{\\star}})=-\\sum_{(\\mathbf{x},\\mathbf{z}^{\\star})\\in D_{\\mathbf{x},\\mathbf{z}^{\\star}}}\\log p(\\mathbf{z}^{\\star}\\mid\\mathbf{x})}\\end{array}\n$$  \n\nwhere $\\mathbf{z}^{\\star}$ represents the annotated, ground-truth transcription for speech audio $\\textbf{\\em x}$ . In our baseline setting, we also hybrid CTC/Attention framework (Watanabe et al., 2017) on the encoder side. In the case of multi-task training with ASR for ST, the total loss is defined as  \n\n$$\n\\ell_{\\mathrm{MTL}}(D_{x,y^{\\star},\\mathbf{z}^{\\star}})=\\ell_{\\mathrm{ST}}(D_{x,y^{\\star}})+\\ell_{\\mathrm{ASR}}(D_{x,\\mathbf{z}^{\\star}})\n$$  \n\nwhere $D_{x,y^{\\star},\\mathbf{z}^{\\star}}$ is the training dataset which contains speech, translation and transcription triplets.  \n\n# 3 Masked Acoustic Modeling  \n\nAll the existing solutions to boost the current E2E-ST performance heavily depend on the availability of the transcription of the source language. Those solutions are not able to take advantage of large amount of speeches without any annotations. They also become inapplicable when the source language is low-resource or even does not have a standard orthography system. Therefore, the ideal solution should not be constrained by source language transcription and still achieves similar translation quality. Thus, we introduce MAM in this section.  \n\n![](images/c8721839a2de2356856af82c8a148d51c89b4f07861212d63d3a07374025b5ad.jpg)  \nFigure 2. MAM (in blue box) can be treated as one extra module besides standard Transformer encoder-decoder and convolution layers for processing speech signals.  \n\n# 3.1 MAM as Part of Training Objective  \n\nWe propose to perform self-supervised training on the encoder side by reconstructing sabotaged speech signals from the input. Note that MAM is totally different from another self-supervised training (Chuang et al., 2019; Wang et al., 2020a;c) which rely on transcription to segment the speech audio with forced alignment tools(Povey et al., 2011; McAuliffe et al., 2017). We directly apply random masks with different widths over speech audio, eliminating the dependency of transcription. Therefore, MAM can be easily applied to speech audio without transcription and even to any non-human speech audio, e.g., music and animal sound.  \n\nFormally, we define a random replacement function over the original speech input $\\textbf{\\em x}$ :  \n\n$$\n\\begin{array}{r}{\\hat{\\pmb{x}}\\sim\\mathrm{Mask_{frame}}(\\pmb{x},\\lambda),}\\end{array}\n$$  \n\nwhere $\\mathrm{Mask(\\cdot)_{frame}}$ randomly replaces some certain frames in $\\textbf{\\em x}$ with the same random initialized vector, $\\epsilon\\in\\mathbb{R}^{d_{x}}$ , with a probability of $\\lambda$ ( $30\\%$ in our experiments). Note that we use the same vector $\\epsilon$ to represent all the corrupted frames (see one example in Fig.4b). Then we obtain a corrupted input $\\hat{\\pmb{x}}$ and its corresponding latent representation $\\hat{h}$ .  \n\nFor MAM module, we have the following training objective to reconstruct the original speech signal with the surrounding context information with self-supervised fashion:  \n\n$$\n\\begin{array}{r}{\\ell_{\\mathrm{Rec}}(D_{\\pmb{x}})=\\sum_{\\pmb{x}\\in D_{\\pmb{x}}}||\\pmb{x}-\\phi(f(\\pmb{\\hat{x}}))||_{2}^{2}}\\end{array}\n$$  \n\nwhere $\\phi$ is a reconstruction function which tries to recover the original signal from the hidden representation $f(\\hat{\\pmb x})$ with corrupted inputs. For simplicity, we use regular 2D deconvolution as $\\phi$ , and mean squared error for measuring the difference between original input and recovered signal. Finally, we have the following total loss of our model  \n\n$$\n\\ell_{\\mathrm{MAM}}(D_{x,y^{\\star}})=\\ell_{\\mathrm{ST}}(D_{x,y^{\\star}})+\\ell_{\\mathrm{Rec}}(D_{x})\n$$  \n\nTo further boost the performance of E2E-ST, we can train MAM with ASR MTL when transcription is available:  \n\n$$\n\\ell_{\\mathrm{MAM}+\\mathrm{MTL}}(D_{x,y^{\\star},\\mathbf{z}^{\\star}})=\\ell_{\\mathrm{MTL}}(D_{x,y^{\\star},\\mathbf{z}^{\\star}})+\\ell_{\\mathrm{Rec}}(D_{x})\n$$  \n\n# 3.2 Different Masking Strategies  \n\nMAM aims at much harder tasks than pure textual pretraining models, e.g., BERT or ERINE, which only perform semantic learning over missing tokens. In our case, we not only try to recover semantic meaning, but also acoustic characteristics of given audio. MAM simultaneously predicts the missing words and generates spectrograms like speech synthesis tasks.  \n\nTo ensure the masked segments contain different levels of granularity of speech semantic, we propose the following masking methods.  \n\nSingle Frame Masking Uniformly mask $\\lambda\\%$ frames out of $\\textbf{\\em x}$ to construct $\\hat{\\pmb x}$ . Note that we might have continuous frames that were masked.  \n\nSpan Masking Similar with SpanBERT (Joshi et al., 2020b), we first sample a serial of span widths and then apply those spans randomly to different positions of the input signal. Note that we do not allow overlap in this case. Our span masking is defined as $\\hat{\\pmb{x}}\\sim\\mathrm{Mask}_{\\mathrm{span}}(\\pmb{x},\\lambda)$ .  \n\n# 3.3 Pre-training MAM  \n\nMAM is a powerful technique that is not only beneficial to the conventional training procedure, but also can be used as a pre-training framework that does not need any annotation.  \n\nThe bottleneck of current speech-related tasks, e.g., ASR, ST, is lacking of the annotated training corpus. For some languages that do not even have a standard orthography system, these annotations are even impossible to obtain. Although current speech-related, pre-training frameworks (Chuang et al., 2019; Wang et al., 2020c) indeed relieve certain needs of large scale parallel training corpus for E2E-ST, all of these pre-training methods still need intense transcription annotation for the source speech.  \n\nDuring pre-training time, we only use the encoder part of MAM. Thanks to our flexible masking techniques, MAM is able to perform pre-training with any kind of audio signal. This allows us to perform pre-training with MAM with three different settings, pre-training with source language speech, with multilingual speech, and arbitrary audios. To the best of our knowledge, MAM is the first pre-training technique that can be applied to arbitrary audios. Considering about the vast arbitrary acoustic signals existing on the Internet (e.g., youtube), MAM has great potential to further boost the downstream tasks. MAM that pre-trained with arbitrary acoustic signals does not differentiate languages and provides the unified pre-trained model for any downstream, fine-tuning task. This is different from the multilingual pre-training setting since the downstream task’s source speech language is not necessary to be included in the pre-training stage, which is essential to the low-resource and zero-resource languages.  \n\n# 4 Experiments  \n\nIn this section, we conducted MAM pre-training experiment on three corpora, Libri-Light (only English speech, medium version) (Kahn et al., 2020), Common Voice 1(We select 14 languages, which contains ca, de, en, es, fr, it, kab, nl, pl, pt, ro, ru, zh-CN, zh-TW) (Ardila et al., 2020), and Audioset (arbitrary acoustic data) (Gemmeke et al., 2017). The statistical results of the dataset are shown in Table. 2. Note that Audioset includes a wide range of arbitrary sounds, from human and animal sounds, to natural and environmental sounds, to musical and miscellaneous sounds.  \n\nThen, we analyze the performance of MAM in E2E-ST with 8 different language translation directions using English as the source speech on $\\mathrm{MuST-C}$ dataset (Di Gangi et al., 2019). All raw audio files are processed by Kaldi (Povey et al., 2011) to extract 80-dimensional log-Mel filterbanks stacked with 3-dimensional pitch features using a window size of $25~\\mathrm{ms}$ and step size of $10~\\mathrm{{ms}}$ . We train sentencepiece (Kudo & Richardson, 2018) models with a joint vocabulary size of 8K for each dataset. We remove samples that have more than 3000 frames for GPU efficiency. Our basic Transformer based E2E-ST framework has similar  \n\n1https://commonvoice.mozilla.org/en/datasets settings with ESPnet-ST(Inaguma et al., 2020). We first downsample the speech input with 2 layers of 2D convolution of size 3 with stride size of 2. Then there is a standard 12-layers Transformer with 2048 hidden size to bridge the source and target side. We only use 4 attention heads on each side of the transformer and each of them has a dimensionality of 256. For MAM module, we simply linearly project the outputs of the Transformer encoder to another latent space, then upsample the latent representation with 2-layers deconvolution to match the size of the original input signal. For the random masking ratio $\\lambda$ , we choose $30\\%$ across all the experiments including pre-training. During inference, we do not perform any masking over the speech input. We average the last 5 checkpoints for testing. For decoding, we use a beam search with setting beam size and length penalty to 5 and 0.6, respectively.  \n\n<html><body><table><thead><tr><td></td><td><b>ST</b></td><td><b>ST+ASR</b></td><td><b>ST+MAM</b></td></tr></thead><tbody><tr><td># of parameters</td><td>31M</td><td>47M</td><td>33M</td></tr></tbody></table></body></html>  \n\nTable 1. MAM only has $6.5\\%$ more parameters than the baseline model while ASR multi-tasking needs to use $51.6\\%$ more parameters.   \n\n\n<html><body><table><thead><tr><td></td><td><b>MuST-C</b></td><td><b>Libri-SpeechLibri-Light</b></td><td><b>tCommonVoice</b></td><td><b>eAudioset</b></td></tr></thead><tbody><tr><td>Type</td><td>★</td><td>?</td><td></td><td>?</td><td></td></tr><tr><td>Hours</td><td>408h</td><td>960h</td><td>3748h</td><td>4421h</td></tr></tbody></table></body></html>\n\nTable 2. The statistical results of corpora. $\\spadesuit$ and $\\star$ denote the corpus has transcripts and translations, respectively. Note that although Common Voice has transcripts, we do not use them.  \n\nOur MAM is very easy to replicate as we do not perform any parameters and architecture search upon the baseline system. Due to the simple, but effective design of MAM, MAM does not rely on intensive computation. It converges within 2 days of training with 8 1080Ti GPUs for the basic model. We showcase the comparison of parameters between different solutions to E2E-ST in Table. 1 This makes a big difference with current popular intensive computations frameworks such as BERT(Devlin et al., 2019) (340M parameters) and GPT3(Brown et al., 2020) (175B parameters), making this technique is accessible to regular users.  \n\n# 4.1 Analyzing ASR and MAM  \n\nAside from the extra training signal that is introduced by transcriptions, there is a deeper reason why ASR and MAM are beneficial to E2E-ST. In this section, we first discuss the difficulties and challenges in E2E-ST. Then we analyze the reasons why ASR MTL and MAM are helpful for E2E-ST by visualizing the self-attention over source side encoder.  \n\nCompared with other tasks, e.g., MT or ASR, which also employ Seq2Seq framework for E2E training, E2E-ST is a more difficult and challenging task in many ways. Firstly, data modalities are different on the source and target sides. For ST, the encoder deals with speech signals and tries to learn word presentations on the decoder side, while MT has text format on both sides. Secondly, due to the nature of the high sampling rate of speech signals, speech inputs are generally multiple (e.g. 4 to 7) times longer than the target sequence, which increases the difficulties of learning the correspondence between source and target. Thirdly, compared with the monotonicity natural of the alignment of ASR, ST usually needs to learn the global reordering between speech signal and translation, and this raises the difficulties to another level. Especially in ST, since source and target are in different languages, it is very challenging to obtain the corresponding phoneme or syllable segments given the training signal from a different language.  \n\n![](images/d7929f37aac206d4bb763d5d5061d3becf666b1f7c79ada80ab550b151513b1d.jpg)  \nFigure 3. One head of the last layer self-attention comparison between different models. ASR MTL and MAM help the encoder learns similar self-attentions. See detailed discussion in 4.1.  \n\nFig. 3 tries to explain and analyze the difference between E2E-ST (a) and E2E-ST with ASR MTL (b). We extract the most top layer from the encoder for comparison. We notice that E2E-ST (a) tends to get more meaningful selfattention on the encoder with the training signal from ASR. With help from ASR, the source input spectrogram is chunked into segments that contain phoneme-level information. During training, the monotonicity natural of the ASR alignment functions as a forced alignment to group a set of adjacent frames to represent certain phonemes or syllables from source speech. With a larger scale of segmented spectrograms, the target side decoder only needs to perform reordering on those segments instead of frames. Our observations also align with the analysis from Stoian et al. (2020).  \n\nWe also visualize the self-attention on encoder for E2E-ST with MAM (without pre-training) in (c) of Fig. 3. We find that MAM has the similar ability with ASR to segment the source speech into chunks. As it is shown in (d) of Fig. 3, when we only perform pre-training on the English speech (Libri-Light dataset), without E2E-ST training, selfattentions that are generated by pre-trained MAM are mostly monotonic on source side. Recovering local frames usually needs the information from surrounding context, especially for the speaker and environment-related characteristic. But we still observe that self-attention sometimes focuses on longer distance frames as well. This type of attention is very similar with low to mid layer self-attention of ASR. When there is a down streaming task (e.g., ASR or ST) is used for fine tuning, the top layer’s self-attention will get chunked attention which is similar to (a) and (c).  \n\nTo conclude, we observe that MAM functions very similar to ASR on the encoder side. Hence, MAM is a reliable framework that can be used as an alternative solution when there is no transcription available. Especially, with the help of a large scale acoustic dataset, which does not have transcription annotation, MAM provides the E2E-ST a much better encoder initialization.  \n\n# 4.2 Visualizing Reconstruction  \n\nTo demonstrate what MAM has learned from pre-training step, we first showcase the reconstruction ability of MAM by visualizing the differences of spectrograms between the original and recovered inputs. This experiment was conducted on two corpora, Libri-Light and the Free Music Archive (FMA) (Defferrard et al., 2016) dataset. We use (a) The original speech spectrogram. Note that though we annotate the transcription underneath, we do not use transcription information at all during pre-training.  \n\n![](images/bc26349aab1640966abd38ae5ca48e32dc27573d934d267ada2cccc1437d324d.jpg)  \n\n![](images/30a77f8210550dd779a6c6e4b6c4700cb50508a3a0daf09117df5ca9dada0282.jpg)  \n\n(b) We mask the selected frames (underlined with blue lines) with the same random initialized vector.  \n\n![](images/09f2569eebb08f10c993a75d10d8b66a70b0254a4056c60f6086c3036a4ec2ab.jpg)  \n\n(c) Recovered spectrogram with MAM, pre-trained with Libri-Light corpus.  \n\n![](images/c8edcbd7b52661c6fa04106103d805dbf87a8514ac200f2ee6585b8f1d9fb49b.jpg)  \n\n(d) MAM that pre-trains with FMA music corpus still have the ability to reconstruct corrupted speech signal.  \n\nFigure 4. One speech example to showcase the reconstruction ability of pre-trained MAM. We notice that MAM reconstructs the corrupted audio signal in both pre-training with ordinary speech and music dataset.  \n\nthe “fma-medium” setting 2 which contains about 25,000 tracks of 30 seconds music within 16 unbalanced genres. The total music length is about 208 hours. We use FMA dataset for reconstruction visualization since FMA only contains music data and the characteristic of the music signal is very different from pure human speech. Note that our reconstructed spectrograms are a little blur compared with the original input since there are some downsampling steps in the E2E-ST baseline framework.  \n\nTo verify the pre-trained results of MAM, we demonstrate the reconstruction ability of MAM by visualizing the results in Fig. 4. We show the original spectrogram of input speech in Fig. 4a. Then we corrupted the original spectrogram by replacing the selected mask frames with $\\epsilon$ , which is a random initialized vector, to form $\\hat{\\pmb x}$ (see Fig. 4b). In Fig. 4c, we show that our proposed MAM is able to recover the missing segments of input speech by pre-training over the  \n\nLibri-Light dataset. More interestingly, since MAM does not need any transcription to perform pre-training, we also pre-train MAM with FMA dataset. Surprisingly, as shown in Fig. 4d, MAM performs very similar reconstruction ability compared with the one that is pre-trained with speech dataset considering the corrupted audio is only about pure speech. This might be because some music tracks include human singing voices and MAM learns human speech characteristics from those samples though human singing voice can be quite different from speech. We also conduct reconstruction with speech pre-trained MAM for corrupted FMA data (see Fig. A1 in Appendix).  \n\n# 4.3 Translation Accuracy Comparisons  \n\nWe showcase the translation accuracy of MAM comparing against to 6 baselines from Table 3 to Table 5:  \n\n• Cascade: cascade framework first transcribes the speech into transcription then passes the results to later machines translation system.   \n• MT with ASR annotation: an MT system which directly generates the target translation from the humanannotated transcription.   \n• E2E-ST: this is the vanilla translation system which does not use transcriptions in MuST-C.   \n• E2E-ST $^+$ ASR MTL: ST trained with ASR MTL using the transcription in MuST-C.   \n• ST $^+$ SpecAugment: a data augmentation method (Park et al., 2019; Bahar et al., 2019) by performing random masking over input speech.   \n• E2E-ST $^+$ ASR PT: the encoder of ST is initialized by a pre-trained ASR encoder which is trained from the speech and transcription pairs in Libri-Speech (Panayotov et al., 2015).  \n\nTo better make a conclusion of our results from Table 3 to Table 5, we organize the comparisons as follows.  \n\n# 4.3.1 Comparison in the settings without transcription and pre-training  \n\nIn Table 3, we first compare MAM against E2E-ST where there is no transcription and pre-training. Both MAM with single and span masking methods achieve averagely $+0.44$ (single) and $+1.09$ (span) improvements in BLEU score correspondingly against to E2E-ST in 8 different translation directions. Span masking consistently outperforms single frame masking as it is a more difficult self-supervised task.  \n\n# 4.3.2 Comparison in the pre-training settings without transcription  \n\nIn Table 4, we have three different pre-training settings for MAM, which are pre-training with English speech (same with the source language) from Libri-Light, multilingual speech data from Common Voice, and arbitrary acoustic data from Audioset corpus. Among those methods, MAM pre-trained with Libri-Light achieves the best results as it consistently outperforms the baseline. Averagely speaking, there is $+2.26$ improvements compared with E2E-ST. When we compare to “E2E-ST $^+$ ASR PT”, there are about $+0.79$ improvements in BLEU score across 8 target languages.  \n\nMAM: Masked Acoustic Modeling for End-to-End Speech-to-Text Translation   \n\n\n<html><body><table><thead><tr><td></td><td><b>Models</b></td><td><b>De</b></td><td><b>Es</b></td><td><b>Fr</b></td><td><b>It</b></td><td><b>N1</b></td><td><b>Pt</b></td><td><b>Ro</b></td><td><b>Ru</b></td><td><b>Avg.</b></td></tr></thead><tbody><tr><td></td><td>MT with ASR annotation (Di Gangi et al., 2019)</td><td>28.09</td><td>34.16</td><td>42.23</td><td>30.40</td><td>33.43</td><td>32.44</td><td>28.16</td><td>18.30</td><td></td></tr><tr><td></td><td>Cascaded methods (Inaguma et al., 2020)</td><td>23.65</td><td>28.68</td><td>33.84</td><td>24.04</td><td>27.91</td><td>29.04</td><td>22.68</td><td>16.39</td><td></td></tr><tr><td></td><td>E2E-ST</td><td>19.64</td><td>23.68</td><td>28.91</td><td>19.95</td><td>23.01</td><td>24.00</td><td>21.06</td><td>12.05</td><td>-</td></tr><tr><td></td><td>ST + SpecAug</td><td>20.06</td><td>24.51</td><td>29.26</td><td>20.27</td><td>23.73</td><td>24.40</td><td>21.21</td><td>12.84</td><td>+0.49</td></tr><tr><td></td><td>MAM (single)</td><td>20.34</td><td>24.46</td><td>29.18</td><td>19.52</td><td>23.81</td><td>24.56</td><td>21.37</td><td>12.57</td><td>+0.44</td></tr><tr><td></td><td>MAM (span)</td><td>20.78</td><td>25.34</td><td>30.26</td><td>20.51</td><td>24.46</td><td>24.90</td><td>21.62</td><td>13.14</td><td>+1.09</td></tr></tbody></table></body></html>  \n\nTable 3. Comparisons between MAM and other baselines over 8 languages on MuST-C. In this setting, we use MAM as an extra training module for E2E-ST, and there is no pre-training involved. We notice that MAM with span masking achieves better performance and there is 1.09 BLEU score improvements upon E2E-ST. The column starts with “Avg. $\\Delta^{\\bullet}$ summarizes the average improvements upon baseline method, E2E-ST. See more discussions in Sec. 4.3.1.   \n\n\n<html><body><table><thead><tr><td><b>Models</b></td><td><b>De</b></td><td><b>Es</b></td><td><b>Fr</b></td><td><b>It</b></td><td><b>NI</b></td><td><b>Pt</b></td><td><b>Ro</b></td><td><b>Ru</b></td><td><b>Avg. </b></td></tr></thead><tbody><tr><td>E2E-ST</td><td>19.64</td><td>23.68</td><td>28.91</td><td>19.95</td><td>23.01</td><td>24.00</td><td>21.06</td><td>12.05</td><td>-</td></tr><tr><td>E2E-ST+ASR PT*</td><td>20.75</td><td>25.57</td><td>30.75</td><td>20.62</td><td>24.31</td><td>25.33</td><td>22.50</td><td>14.24</td><td>++1.47</td></tr><tr><td>E2E-ST+ASR MTL</td><td>21.70</td><td>26.83</td><td>31.36</td><td>21.45</td><td>25.44</td><td>26.52</td><td>23.71</td><td>14.54</td><td>++2.41</td></tr><tr><td>MAM w/ English PT</td><td>21.44</td><td>26.48</td><td>31.21</td><td>21.28</td><td>25.22</td><td>26.41</td><td>23.83</td><td>14.53</td><td>+2.26</td></tr><tr><td>MAM w/ multilingual PT</td><td>21.02</td><td>25.93</td><td>30.62</td><td>21.05</td><td>24.87</td><td>25.64</td><td>22.94</td><td>13.90</td><td>+1.71</td></tr><tr><td>MAM w/ acoustic PT</td><td>20.81</td><td>25.85</td><td>30.48</td><td>20.52</td><td>24.81</td><td>25.46</td><td>22.90</td><td>13.83</td><td>+1.55</td></tr></tbody></table></body></html>  \n\nSurprisingly, MAM trained with acoustic data still achieves about $+1.55$ improvements upon E2E-ST. Considering acoustic data does not need any annotation and this kind of dataset is much easier to collect, the results are very encouraging. With the help of vast acoustic data on the website (e.g., youtube), MAM trained with arbitrary acoustic data has great potential to further boost the performance. To the best of our knowledge, MAM is the first technique that performs pre-training with any form of the audio signal.  \n\nMAM trained with Common Voice does not have significant improvements with two following reasons: firstly, speech audios in Common Voice sometimes are very short (about 2 to 3 seconds) while MuST-C usually contains much longer speech (above 10 seconds) leading to very limited options for random masking; secondly there are much fewer English speech in this corpus.  \n\n# 4.3.3 Comparison in the settings using transcription  \n\nIn this setting, we use “E2E-ST $^+$ ASR MTL” as the baseline. MAM MTL with pre-training over Libri-Light achieves $+0.9$ average improvements over 8 languages.  \n\n# 4.3.4 Comparison to Wav2vec  \n\nWe also compare MAM against to other wav2vec-based methods (Wu et al., 2020) in Table 6. Due to the differences in baseline methods, to make a fair comparison, we only compare the relative improvements upon our own baseline on the same test data. MAM still achieves much larger improvements upon a much stronger baseline. Especially our baseline is already about 4 BLEU points better than the baseline in wav2vec, MAM still achieves $+1.6$ more BLEU points improvements compared with wav2vec-based pre-training methods making our performance on En-Ro about 5.6 BLEU points better than wav2vec-based methods.  \n\n# 4.4 Comparisons in Low and Mid-resource Settings  \n\nIn Table 7, we reduce the size of MuST-C from 408 hours to 50 hours and 200 hours to mimic the low and mid-resource language speech translation.  \n\nIn the scenario when the source language is extremely lowresource (no transcribed pre-training and fine-tuning data), we have “E2E-ST” as the baseline. MAM in both multi  \n\nMAM: Masked Acoustic Modeling for End-to-End Speech-to-Text Translation   \n\n\n<html><body><table><thead><tr><td><b>Models</b></td><td><b>De</b></td><td><b>Es</b></td><td><b>Fr</b></td><td><b>It</b></td><td><b>N1</b></td><td><b>Pt</b></td><td><b>Ro</b></td><td><b>Ru</b></td><td><b>Avg.</b></td></tr></thead><tbody><tr><td>E2E-ST+ASR MTL</td><td>21.70</td><td>26.83</td><td>31.36</td><td>21.45</td><td>25.44</td><td>26.52</td><td>23.71</td><td>14.54</td><td>-</td></tr><tr><td>MAM+ASR MTL MAM w/ English PT</td><td>22.41</td><td>26.89</td><td>32.55</td><td>22.12</td><td>26.49</td><td>27.22</td><td>24.45</td><td>14.90</td><td>+0.69</td></tr><tr><td>MAM w/ English PT + ASR MTL</td><td>22.87</td><td>26.86</td><td>32.80</td><td>22.12</td><td>26.81</td><td>27.43</td><td>24.65</td><td>15.21</td><td>+0.90</td></tr></tbody></table></body></html>  \n\nTable 5. Comparisons between MAM with ASR MTL and E2E-ST with ASR MTL. MAM still achieves an improvement about $+0.9$ BLEU. The column starts with “Avg. $\\Delta^{\\bullet}$ summarizes the average improvements upon baseline method.   \n\n\n<html><body><table><thead><tr><td colspan=\"5\"><b>Wav2vec-based Method (Wu et al., 2020)</b></td></tr></thead><tbody><tr><td></td><td>En-Fr</td><td>△</td><td>En-Ro</td><td></td></tr><tr><td> their baselinet</td><td>27.8</td><td>-</td><td>17.1</td><td>-</td></tr><tr><td>+ wav2vec PTt</td><td>29.8</td><td>+2.0</td><td>18.2</td><td>+1.1</td></tr><tr><td>+ vq-wav2vec PTt</td><td>28.6</td><td>+0.8</td><td>17.4</td><td>+0.3</td></tr><tr><td colspan=\"5\">MAM-basedMethod</td></tr><tr><td>ourbaseline</td><td>28.9</td><td>-</td><td>21.1</td></tr><tr><td>+ MAM w/ English PT</td><td>31.2</td><td>+2.3</td><td>23.8</td></tr></tbody></table></body></html>  \n\nTable 6. Comparisons between wav2vec-based pre-train method for E2E-ST. Results that are decorated with † are from Wu et al. (2020). Our relative improvements over baseline methods are much larger than wav2vec-based pre-training methods. See more discussions in Sec. 4.3.4.  \n\nlingual and acoustic pre-training boosts the performance significantly.  \n\nWhen the transcription is only available at the fine-tuning stage (compare with $\\mathbf{\\dot{\\omega}}+\\mathbf{A}\\mathbf{S}\\mathbf{R}$ MTL”), MAM pre-trained with Libri-Light achieves similar performance without using transcription in fine-tuning.  \n\nIn the cases when there is no transcription in the fine-tuning stage, but there exist a large scale annotated pre-training corpus, MAM still achieves similar performance in 200 hours training setting without using any transcription.  \n\n<html><body><table><thead><tr><td rowspan=\"2\"><b>Models</b></td><td colspan=\"2\"><b>Fr</b></td><td colspan=\"2\"><b>Es</b></td></tr><tr><td><b>50h</b></td><td><b>200h</b></td><td><b>50h</b></td><td><b>200h</b></td></tr></thead><tbody><tr><td>E2E-ST</td><td>0.52</td><td>19.83</td><td>0.4</td><td>16.13</td></tr><tr><td>E2E-ST+ASR MTL</td><td>8.84</td><td>25.64</td><td>7.67</td><td>20.21</td></tr><tr><td>E2E-ST+ASR PT*</td><td>12.50</td><td>24.59</td><td>11.80</td><td>19.35</td></tr><tr><td>MAM</td><td>0.6</td><td>20.54</td><td>0.4</td><td>16.85</td></tr><tr><td>MAM w/ English PT</td><td>6.84</td><td>24.86</td><td>6.53</td><td>19.17</td></tr><tr><td>MAM w/ acoustic PT</td><td>3.29</td><td>22.22</td><td>2.46</td><td>17.98</td></tr></tbody></table></body></html>  \n\nTable 7. Experimental comparisons with difference training resource. ∗denotes pretrained with Librispeech corpus. See Section 4.4 for detailed discussion.  \n\ntain portions randomly over the input signals. But different from BERT-style pre-training, MAM tries to recover the missing semantic information (e.g., words, subword units) and learns the capabilities to restore the missing speech characteristics and generate the original speech.  \n\nSpecAugment (Park et al., 2019) was originally proposed for ASR as a data augmentation method by applying a mask over input speech, then it is adapted to ST by Bahar et al. (2019). However, there is no recovering step in SpecAugment, and it can not be used as a pre-training framework.  \n\nFor the self-supervised training in speech domain, Chuang et al. (2019); Wang et al. (2020c;a) proposed to use forcedalignment to segment speech audio into pieces at word level and masked some certain words during fine-tuning. Obviously the forced-alignment based approaches rely on the transcriptions of source speech, and can not be applied to low or zero resource source speech while MAM will relief the needs of large-scale, annotated speech and translation pairs during pre-training.  \n\nBaevski et al. (2020) proposed wav2vec 2.0 pre-training model for ASR task which masks the speech input in the latent space and pre-trains the model via contrastive task defined over a quantization of the latent representations. In contrast, as the objective of MAM is much simpler and straightforward, we don’t need much extra fine-tuning efforts given an E2E-ST baseline and massive computational resource. Furthermore, wav2vec 2.0 is build upon discretized, fix-size, quantized codebooks, and it is not easy to be adapted to arbitrary acoustic signal pre-training. Lastly, wav2vec 2.0 is designed to have ASR as the downstream task, and the fine-tuning stage relies on CTC loss (Graves et al., 2006) which is not straightforward to be adapted in translation task since translation usually involves with many reordering between target and source side while CTC depends on monotonic transition function on source side.  \n\n# 5 Related Work  \n\nText-based BERT-style (Devlin et al., 2019; Liu et al., $2019\\mathrm{a}$ ; Joshi et al., 2020a; Zhang et al., 2019) frameworks are extremely popular in recent years due to the remarkable improvement that they bring to the downstream tasks at fine-tuning stages. Inspired by techniques mentioned above, MAM also performs self-supervised training that masks cer  \n\n# 6 Conclusions  \n\nWe have presented a novel acoustic modeling framework MAM in this paper. MAM not only can be used as an extra component during training time, but also can be used as a separate pre-training framework with arbitrary acoustic signal. We demonstrate the effectiveness of MAM with multiple different experiment settings in 8 languages. Especially, for the first time, we show that pre-training with arbitrary acoustic data with MAM boosts the performance of speech translation.", "appendix": "# Appendix  \n\nIn the other way around, we also try to use Libri-Light pretrained MAM to recover the corrupted music in Fig. A1. MAM that pre-trained with human speech data does not show good reconstruction in Fig. A1c since there are many different musical instruments’ sounds that are unseen in speech data.  \n\n![](images/82ce2ae9bbc4d45503ac25d4d7bf1c60a2c701cdf175ee87be748029cc6284e6.jpg)  \n\n(a) The original musical spectrogram that is mixed with different instruments’ sound.  \n\n![](images/32c32de07b3e27af5e82371e7c1bfdec620849e7159e56d497e0c8e780c632b4.jpg)  \n\n(b) We mask the selected frames (underlined with blue lines) with the same random initialized vector.  \n\n![](images/b7481c1ad9e7ed1c3bf65caed3a03b9d7a6b06b5378a990c8a38b2053ac0da94.jpg)  \n\n(c) Recovered spectrogram with MAM, pre-trained with LibriLight corpus.  \n\nFigure A1. One speech example to showcase the reconstruction ability of pre-trained MAM. Pre-trained MAM with Libri-Light corpus (only human speech data) can not reconstruct the original music spectrogram accurately since there are many different musical instruments’ sound that is unseen in speech data."}, {"title": "MuST-C: a Multilingual Speech Translation Corpus", "authors": "Di Gangi, Mattia A. and Cattoni, Roldano and Bentivogli, Luisa and Negri, Matteo and Turchi, Marco", "bibkey": "must_c_a_multilingual_speech_translation_corpus", "bibitem": "@article{Gangi_NAACL2019,\n  url = {https://aclanthology.org/N19-1202/},\n  title = {MuST-C: a Multilingual Speech Translation Corpus},\n  authors = {Di Gangi, Mattia A. and Cattoni, Roldano and Bentivogli, Luisa and Negri, Matteo and Turchi, Marco},\n  abstract = {Current research on spoken language translation (SLT) has to confront with the scarcity of sizeable and publicly available training corpora. This problem hinders the adoption of neural end-to-end approaches, which represent the state of the art in the two parent tasks of SLT: automatic speech recognition and machine translation. To fill this gap, we created MuST-C, a multilingual speech translation corpus whose size and quality will facilitate the training of end-to-end systems for SLT from English into 8 languages. For each target language, MuST-C comprises at least 385 hours of audio recordings from English TED Talks, which are automatically aligned at the sentence level with their manual transcriptions and translations. Together with a description of the corpus creation methodology (scalable to add new data and cover new languages), we provide an empirical verification of its quality and SLT results computed with a state-of-the-art approach on each language direction.},\n  year = {2019},\n  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},\n  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},\n  address = {Minneapolis, Minnesota},\n  publisher = {Association for Computational Linguistics},\n  doi = {10.18653/v1/N19-1202},\n  pages = {2012--2017},\n  month = {jun},\n  entrytype = {inproceedings}\n}", "url": "https://aclanthology.org/N19-1202/", "latex_url": null, "latex_path": null, "pdf_url": "https://aclanthology.org/N19-1202.pdf", "pdf_path": "output/download_papers/N19-1202/N19-1202.pdf", "md_url": null, "latex_length": 0, "latex": "", "abstract": "Current research on spoken language translation (SLT) has to confront with the scarcity of sizeable and publicly available training corpora. This problem hinders the adoption of neural end-to-end approaches, which represent the state of the art in the two parent tasks of SLT: automatic speech recognition and machine translation. To fill this gap, we created MuST-C, a multilingual speech translation corpus whose size and quality will facilitate the training of end-to-end systems for SLT from English into 8 languages. For each target language, MuST-C comprises at least 385 hours of audio recordings from English TED Talks, which are automatically aligned at the sentence level with their manual transcriptions and translations. Together with a description of the corpus creation methodology (scalable to add new data and cover new languages), we provide an empirical verification of its quality and SLT results computed with a state-of-the-art approach on each language direction.", "abstract_length": 985, "abstract_token": 186, "introduction": "Besides the increased computing power, the recent surge of neural end-to-end approaches to natural language processing tasks has been stoked by the increased availability of data. For instance, when supported by sizeable training corpora, the robustness and the strong generalization capabilities of neural networks led to their dominance over previous paradigms both in automatic speech recognition (ASR (Chiu et al., 2018)) and machine translation (MT (Bojar et al., 2018)). Compared to its two parent research areas, spoken language translation (SLT) has not shown such a steady progress yet. Despite recent claims by big industry players about the effectiveness of end-toend learning (Weiss et al., 2017; Jia et al., 2018), its adoption does not yet represent the mainstream solution to the SLT task. One of the main obstacles <html><body><table><thead><tr><td><b>Corpus</b></td><td><b>Languages</b></td><td><b>Hours</b></td></tr></thead><tbody><tr><td>Niehues et al. (2018)</td><td>En→De</td><td>273</td></tr><tr><td>Kocabiyikoglu et al. (2018)</td><td>En-→Fr</td><td>236</td></tr><tr><td>Tohyama et al. (2005)</td><td>En→Jp</td><td>182</td></tr><tr><td>Paulik and Waibel (2009)</td><td>En→Es Es→En</td><td>105 111</td></tr><tr><td>Post et al. (2013)</td><td>En→Es</td><td>38</td></tr><tr><td>Stiker et al. (2012)</td><td>De→→En</td><td>37</td></tr><tr><td>Shimizu et al. (2014)</td><td>En→Jp</td><td>22</td></tr><tr><td>Federmann and Lewis (2017)</td><td>En<→Jp/Zh</td><td>22</td></tr><tr><td>Bendazzoli and Sandrelli (2005)</td><td>En<→It/Es It>Es</td><td>18</td></tr><tr><td>Bérard et al. (2016)</td><td>Fr-→En En←>Fr/De</td><td>17</td></tr><tr><td>Federmann and Lewis (2016)</td><td>En←>Fr/De</td><td>8</td></tr><tr><td>Woldeyohannis et al. (2017)</td><td>Am→En</td><td>7</td></tr><tr><td>Godard et al. (2017)</td><td>Mboshi—→Fr</td><td>4</td></tr></tbody></table></body></html> Table 1: Publicly available SLT corpora. The two most recent resources (also known as IWSLT18 and Augmented LibriSpeech) are also the largest ones. Though considerably smaller, the Fisher and Callhome corpus described in (Post et al., 2013) is among the most widely used ones in previous research. to a stable dominance of the end-to-end paradigm also in this area is the scarcity of training corpora. While cascade $\\mathrm{ASR+MT}$ solutions can exploit the wealth of task-specific data available for each of the two tasks,1 the situation for end-to-end model training is much less favourable. As shown in Table 1, few publicly available corpora exist, their language coverage is rather limited and, most importantly, their size is often too small (less than 100 hours of translated audio) for training datahungry neural models.2 To circumvent the problem, neural SLT approaches currently rely on: i) large proprietary corpora (Jia et al., 2018), ii) multitask learning (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Be´rard et al., 2018), iii) encoder/decoder pre-training (Bansal et al., 2018; Be´rard et al., 2018), iv) synthesized speech data (B´erard et al., 2016), or v) machine-translated target text data (B´erard et al., 2018). Though effective, solutions ii) and iii) assume the availability of ASR and MT data, which is not always guaranteed (especially in low-resource language settings). Solutions $i\\nu$ ) and $\\nu_{,}$ ), instead, rely on training material derived from sub-optimal automatic data creation/augmentation procedures. This situation calls for initiatives towards the creation of large, high-quality multilingual corpora suitable to explore end-to-end SLT in more favorable conditions similar to condition $i,$ ). Along this direction, our contributions are: • A large ${\\sim}400$ hours of speech per language) multilingual corpus for SLT from English into 8 languages (German, Spanish, French, Italian, Dutch, Portuguese, Romanian and Russian); • An empirical verification of its quality; • ASR, MT and SLT results computed with strong baseline systems on each language direction. MuST-C is released under a Creative Commons license, Attribution - Non Commercial - No Derivatives (CC BY NC ND 4.0 International), and is freely downloadable at mustc.fbk.eu", "introduction_length": 4180, "introduction_token": 1211, "reference": "# References  \n\nAntonios Anastasopoulos and David Chiang. 2018. Tied Multitask Learning for Neural Speech Translation. In Proceedings of NAACL-HLT 2018, pages 82–91, New Orleans, Louisiana.  \n\nSameer Bansal, Herman Kamper, Karen Livescu, Adam Lopez, and Sharon Goldwater. 2018. Pre  \n\nTraining on High-Resource Speech Recognition Improves Low-Resource Speech-to-Text Translation. arXiv preprint arXiv:1809.01431.  \n\nClaudio Bendazzoli and Annalisa Sandrelli. 2005. An Approach to Corpus-based Interpreting Studies: Developing EPIC (European Parliament Interpreting Corpus). In Proceedings of the EU-High-Level Scientific Conference Series MuTra 2005Challenges of Multidimensional Translation, pages 149–160, Saarbru¨cken, Germany.  \n\nAlexandre Be´rard, Laurent Besacier, Ali Can Kocabiyikoglu, and Olivier Pietquin. 2018. End-toEnd Automatic Speech Translation of Audiobooks. In Proceedings of ICASSP 2018, pages 6224–6228, Calgary, Alberta, Canada.  \n\nAlexandre B´erard, Olivier Pietquin, Laurent Besacier, and Christophe Servan. 2016. Listen and Translate: A Proof of Concept for End-to-End Speech-to-Text Translation. In Proceedings of the NIPS Workshop on end-to-end learning for speech and audio processing, Barcelona, Spain.  \n\nMarcely Zanon Boito, Antonios Anastasopoulos, Marika Lekakou, Aline Villavicencio, and Laurent Besacier. 2018. A Small Griko-Italian Speech Translation Corpus. CoRR, abs/1807.10740.  \n\nOndrej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, and Christof Monz. 2018. Findings of the 2018 Conference on Machine Translation (WMT18). In Proceedings of the Third Conference on Machine Translation, Volume 2: Shared Task Papers, pages 272–307, Belgium, Brussels.  \n\nFabienne Braune and Alexander Fraser. 2010. Improved Unsupervised Sentence Alignment for Symmetrical and Asymmetrical Parallel Corpora. In Proceedings of COLING 2010, pages 81–89, Beijing, China.  \n\nChung-Cheng Chiu, Tara Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, et al. 2018. Stateof-the-art Speech Recognition With Sequence-toSequence Models. In Proceedings of ICASSP 2018, pages 4774–4778, Calgary, Alberta, Canada.  \n\nMattia Antonino Di Gangi, Roberto Dess\\`ı, Roldano Cattoni, Matteo Negri, and Marco Turchi. 2018. Fine-tuning on Clean Data for End-to-End Speech Translation: FBK $@$ IWSLT 2018. In Proceedings of IWSLT 2018, Bruges, Belgium.  \n\nChristian Federmann and William D Lewis. 2016. Microsoft Speech Language Translation (MSLT) Corpus: The IWSLT 2016 Release for English, French and German. In Proceedings of IWSLT 2016, Seattle, USA.  \n\nChristian Federmann and William D Lewis. 2017. The Microsoft Speech Language Translation (MSLT) Corpus for Chinese and Japanese: Conversational Test data for Machine Translation and Speech  \n\nRecognition. In Proceedings of the 16th Machine Translation Summit, Nagoya, Japan.  \n\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. 2017. Convolutional Sequence to Sequence Learning. In Proceedings of ICML 2017, pages 1243–1252, Sydney, Australia.  \n\nPierre Godard, Gilles Adda, Martine Adda-Decker, Juan Benjumea, Laurent Besacier, et al. 2017. A Very Low Resource Language Speech Corpus for Computational Language Documentation Experiments. arXiv preprint arXiv:1710.03501.  \n\nAlex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. 2013. Speech Recognition with Deep Recurrent Neural Networks. In Proceedings of ICASSP 2018, pages 6645–6649, Vancouver, BC, Canada.  \n\nSepp Hochreiter and Ju¨rgen Schmidhuber. 1997. Long Short-Term Memory. Neural computation, 9(8).  \n\nYe Jia, Melvin Johnson, Wolfgang Macherey, RonJ. Weiss, Yuan Cao, Chung-Cheng Chiu, StellaLaurenzo Ari, and Yonghui Wu. 2018. Leveraging Weakly Supervised Data to Improve End-toEnd Speech-to-Text Translation. ArXiv e-prints arXiv:1811.02050.  \n\nAli Can Kocabiyikoglu, Laurent Besacier, and Olivier Kraif. 2018. Augmenting Librispeech with French Translations: A Multimodal Corpus for Direct Speech Translation Evaluation. In Proceedings of LREC 2018, Miyazaki, Japan.  \n\nDan Liu, Junhua Liu, Wu Guo, Shifu Xiong, Zhiqiang Ma, Rui Song, Chongliang Wu, and Quan Liu. 2018. The USTC-NEL Speech Translation system at IWSLT 2018. In Proceedings of IWSLT 2018, Bruges, Belgium.  \n\nThang Luong, Hieu Pham, and Christopher D Manning. 2015. Effective Approaches to Attentionbased Neural Machine Translation. In Proceedings of EMNLP 2015, pages 1412–1421, Lisbon, Portugal.  \n\nGraham Neubig, Matthias Sperber, Xinyi Wang, Matthieu Felix, Austin Matthews, et al. 2018. XNMT: The eXtensible Neural Machine Translation Toolkit. In Proceedings of AMTA 2018, pages 185– 192, Boston, MA.  \n\nJan Niehues, Roldano Cattoni, Sebastian Stu¨ker, Mauro Cettolo, Marco Turchi, and Marcello Federico. 2018. The IWSLT 2018 Evaluation Campaign. In Proceedings of IWSLT 2018, Bruges, Belgium.  \n\nKishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of ACL 2002, Philadelphia, PA, USA.  \n\nRazvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. 2014. How to Construct Deep Recurrent Neural Networks. In Proceedings of ICLR 2014, Banff, Canada.  \n\nMatthias Paulik and Alex Waibel. 2009. Automatic Translation from Parallel Speech: Simultaneous Interpretation as MT Training Data. In Proceedings of ASRU 2009, pages 496–501, Merano, Italy.  \n\nMatt Post, Gaurav Kumar, Adam Lopez, Damianos Karakos, Chris Callison-Burch, and Sanjeev Khudanpur. 2013. Improved Speech-to-Text Translation with the Fisher and Callhome Spanish–English Speech Translation Corpus. In Proceedings of IWSLT 2013, Heidelberg, Germany.  \n\nDaniel Povey, Arnab Ghoshal, Gilles Boulianne, Luk´asˇ Burget, Ondˇrej Glembek, K. Nagendra Goel, Mirko Hannemann, Petr Motl´ıcˇek, Yanmin Qian, Petr Schwarz, Jan Silovsky´, Georg Stemmer, and Karel Vesel´y. 2011. The Kaldi Speech Recognition Toolkit. In Proceedings of ASRU 2011, pages 1–4, Big Island, Hawaii, USA.  \n\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural Machine Translation of Rare Words with Subword Units. arXiv preprint arXiv:1508.07909.  \n\nHiroaki Shimizu, Graham Neubig, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2014. Collection of a Simultaneous Translation Corpus for Comparative Analysis. In Proceedings of LREC 2014, Reykjavik, Iceland.  \n\nSebastian Stu¨ker, Florian Kraft, Christian Mohr, Teresa Herrmann, Eunah Cho, and Alex Waibel. 2012. The KIT Lecture Corpus for Speech Translation. In Proceedings of LREC-2012, Istanbul, Turkey.  \n\nHitomi Tohyama, Shigeki Matsubara, Nobuo Kawaguchi, and Yasuyoshi Inagaki. 2005. Construction and utilization of bilingual speech corpus for simultaneous machine interpretation research. In Proceedings of INTERSPEECH, pages 1585–1588, Lisbon, Portugal.  \n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Proceedings of NIPS 2017, pages 5998–6008, Long Beach, CA, USA.  \n\nRon J. Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, and Zhifeng Chen. 2017. Sequence-toSequence Models Can Directly Translate Foreign Speech. In Proceedings of Interspeech 2017, pages 2625–2629, Stockholm, Sweden.  \n\nMichael Melese Woldeyohannis, Laurent Besacier, and Million Meshesha. 2017. A Corpus for AmharicEnglish Speech Translation: the Case of Tourism Domain. In Proceedings of ICT4DA 2017, pages 129–139, Bahir Dar, Ethiopia.", "reference_length": 7505, "reference_token": 2161, "txt_length": 20326, "txt_token": 5442, "txt": "# MuST-C: a Multilingual Speech Translation Corpus  \n\nMattia Antonino Di Gangi1,2, Roldano Cattoni1, Luisa Bentivogli1, Matteo Negri1 and Marco Turchi1 1Fondazione Bruno Kessler 2University of Trento Trento, Italy {digangi,cattoni,bentivo,negri,turchi}@fbk.eu  \n\n# Abstract  \n\nCurrent research on spoken language translation (SLT) has to confront with the scarcity of sizeable and publicly available training corpora. This problem hinders the adoption of neural end-to-end approaches, which represent the state of the art in the two parent tasks of SLT: automatic speech recognition and machine translation. To fill this gap, we created MuST-C, a multilingual speech translation corpus whose size and quality will facilitate the training of end-to-end systems for SLT from English into 8 languages. For each target language, MuST-C comprises at least 385 hours of audio recordings from English TED Talks, which are automatically aligned at the sentence level with their manual transcriptions and translations. Together with a description of the corpus creation methodology (scalable to add new data and cover new languages), we provide an empirical verification of its quality and SLT results computed with strong baseline system on each language direction.  \n\n# 1 Introduction  \n\nBesides the increased computing power, the recent surge of neural end-to-end approaches to natural language processing tasks has been stoked by the increased availability of data. For instance, when supported by sizeable training corpora, the robustness and the strong generalization capabilities of neural networks led to their dominance over previous paradigms both in automatic speech recognition (ASR (Chiu et al., 2018)) and machine translation (MT (Bojar et al., 2018)).  \n\nCompared to its two parent research areas, spoken language translation (SLT) has not shown such a steady progress yet. Despite recent claims by big industry players about the effectiveness of end-toend learning (Weiss et al., 2017; Jia et al., 2018), its adoption does not yet represent the mainstream solution to the SLT task. One of the main obstacles  \n\n<html><body><table><thead><tr><td><b>Corpus</b></td><td><b>Languages</b></td><td><b>Hours</b></td></tr></thead><tbody><tr><td>Niehues et al. (2018)</td><td>En→De</td><td>273</td></tr><tr><td>Kocabiyikoglu et al. (2018)</td><td>En-→Fr</td><td>236</td></tr><tr><td>Tohyama et al. (2005)</td><td>En→Jp</td><td>182</td></tr><tr><td>Paulik and Waibel (2009)</td><td>En→Es Es→En</td><td>105 111</td></tr><tr><td>Post et al. (2013)</td><td>En→Es</td><td>38</td></tr><tr><td>Stiker et al. (2012)</td><td>De→→En</td><td>37</td></tr><tr><td>Shimizu et al. (2014)</td><td>En→Jp</td><td>22</td></tr><tr><td>Federmann and Lewis (2017)</td><td>En<→Jp/Zh</td><td>22</td></tr><tr><td>Bendazzoli and Sandrelli (2005)</td><td>En<→It/Es It>Es</td><td>18</td></tr><tr><td>Bérard et al. (2016)</td><td>Fr-→En En←>Fr/De</td><td>17</td></tr><tr><td>Federmann and Lewis (2016)</td><td>En←>Fr/De</td><td>8</td></tr><tr><td>Woldeyohannis et al. (2017)</td><td>Am→En</td><td>7</td></tr><tr><td>Godard et al. (2017)</td><td>Mboshi—→Fr</td><td>4</td></tr></tbody></table></body></html>  \n\nTable 1: Publicly available SLT corpora. The two most recent resources (also known as IWSLT18 and Augmented LibriSpeech) are also the largest ones. Though considerably smaller, the Fisher and Callhome corpus described in (Post et al., 2013) is among the most widely used ones in previous research.  \n\nto a stable dominance of the end-to-end paradigm also in this area is the scarcity of training corpora. While cascade $\\mathrm{ASR+MT}$ solutions can exploit the wealth of task-specific data available for each of the two tasks,1 the situation for end-to-end model training is much less favourable. As shown in Table 1, few publicly available corpora exist, their language coverage is rather limited and, most importantly, their size is often too small (less than 100 hours of translated audio) for training datahungry neural models.2  \n\nTo circumvent the problem, neural SLT approaches currently rely on: i) large proprietary corpora (Jia et al., 2018), ii) multitask learning (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Be´rard et al., 2018), iii) encoder/decoder pre-training (Bansal et al., 2018; Be´rard et al., 2018), iv) synthesized speech data (B´erard et al., 2016), or v) machine-translated target text data (B´erard et al., 2018). Though effective, solutions ii) and iii) assume the availability of ASR and MT data, which is not always guaranteed (especially in low-resource language settings). Solutions $i\\nu$ ) and $\\nu_{,}$ ), instead, rely on training material derived from sub-optimal automatic data creation/augmentation procedures. This situation calls for initiatives towards the creation of large, high-quality multilingual corpora suitable to explore end-to-end SLT in more favorable conditions similar to condition $i,$ ). Along this direction, our contributions are:  \n\n• A large ${\\sim}400$ hours of speech per language) multilingual corpus for SLT from English into 8 languages (German, Spanish, French, Italian, Dutch, Portuguese, Romanian and Russian);  \n\n• An empirical verification of its quality;  \n\n• ASR, MT and SLT results computed with strong baseline systems on each language direction.  \n\nMuST-C is released under a Creative Commons license, Attribution - Non Commercial - No Derivatives (CC BY NC ND 4.0 International), and is freely downloadable at mustc.fbk.eu  \n\n# 2 Corpus Creation Methodology  \n\nMust-C was created pursuing high quality as well as large size, speaker variety (male/female, native/non-native) and coverage in terms of topics and languages. To achieve these objectives, similar to (Niehues et al., 2018), we started from English TED Talks, in which a variety of speakers discuss topics spanning from business to science and entertainment. Most importantly, the fact that TED talks are often manually transcribed and translated sets ideal conditions for creating an SLT corpus from high-quality text material. Although the initial data are similar to those used to build the IWSLT18 corpus, our methodology is different. Inspired by Kocabiyikoglu et al. (2018), it exploits automatic alignment procedures, first at the text level (between transcriptions and translations) and then with the corresponding audio segments.  \n\nMore in detail, for each target language $L_{i}$ , the (English- $.L_{i}$ ) section of MuST-C is created as follows. First, for all the English talks available from the TED website,3 we download the videos and the HTML files containing the manual transcriptions and their translation into $L_{i}$ .4  \n\nThen, the plain text transcription and the translation of each talk are split at the sentence level based on strong punctuation marks and aligned using the Gargantua sentence alignment tool (Braune and Fraser, 2010). This step produces a bilingual text corpus aligned at the sentence level.  \n\nIn the third step, the English side of this bilingual corpus is aligned to the corresponding audio track extracted from the video. This is done using Gentle,5 an off-the-shelf English forced-aligner built on the Kaldi ASR toolkit (Povey et al., 2011).  \n\nNext, the audio-text alignments are processed to create a YAML file containing time information (i.e. start and duration) for each sentence. In this processing step, two filters are applied to weed out potentially noisy segments, or entire talks, based on the number of words that were not aligned by Gentle. First, entire talks are discarded if the proportion of unrecognized words is equal or greater than $15\\%$ of the total. This threshold was determined after a manual analysis of 73 talks (those with the highest percentage of unrecognized words). The analysis showed that these cases are representative of different types of noise like: i) non-English speech, ii) long silences, iii) music, non-transcribed songs and videos played during the talk, and iv) wrong transcriptions (e.g. captions from other talks in the material downloaded from the TED website). The second rule applies to the single sentences of the talks that passed the first filter, and removes those in which none of the words was aligned by Gentle.6  \n\nIn the last step, the log Mel 40-dimensional filter-bank features – commonly used as input representation for ASR (Graves et al., 2013) and SLT (Weiss et al., 2017) – are extracted from the aligned audio using the XNMT tool (Neubig et al., 2018).7  \n\n<html><body><table><thead><tr><td><b>Tgt</b></td><td><b>#Talk</b></td><td><b>#Sent</b></td><td><b>Hours</b></td><td><b>src w</b></td><td><b>tgt w</b></td></tr></thead><tbody><tr><td>De</td><td>2,093</td><td>234K</td><td>408</td><td>4.3M</td><td>4.0M</td></tr><tr><td>Es</td><td>2,564</td><td>270K</td><td>504</td><td>5.3M</td><td>5.1M</td></tr><tr><td>Fr</td><td>2,510</td><td>280K</td><td>492</td><td>5.2M</td><td>5.4M</td></tr><tr><td>It</td><td>2,374</td><td>258K</td><td>465</td><td>4.9M</td><td>4.6M</td></tr><tr><td>NI</td><td>2,267</td><td>253K</td><td>442</td><td>4.7M</td><td>4.3M</td></tr><tr><td>Pt</td><td>2,050</td><td>211K</td><td>385</td><td>4.0M</td><td>3.8M</td></tr><tr><td>Ro</td><td>2,216</td><td>240K</td><td>432</td><td>4.6M</td><td>4.3M</td></tr><tr><td>Ru</td><td>2,498</td><td>270K</td><td>489</td><td>5.1M</td><td>4.3M</td></tr></tbody></table></body></html>\n\nTable 2: Statistics for each section of MuST-C.  \n\nTable 2 provides basic statistics for the 8 sections of the MuST-C corpus. Comparing the $4^{t h}$ column with the numbers reported in Table 1, it is worth noting that, in terms of hours of transcribed/translated speech, each section is larger than any existing publicly available SLT resource.  \n\n# 3 Experiments  \n\nIn this section we present two sets of experiments, which are respectively aimed to: i) empirically assess the quality of the MuST-C corpus (Section 3.3) and $i i$ ) compute baseline ASR, MT, and SLT results for future comparisons (Section 3.4).  \n\nIn these experiments, the audio-transcription alignments of MuST-C are used to train and evaluate ASR models, transcription-translation alignments are used for the MT models, and audiotranslation alignments are used for the SLT models.  \n\n# 3.1 ASR, MT and SLT Models  \n\nASR and SLT. For our experiments in ASR and SLT we use the same neural architecture. This setting allows us to use the encoder of the ASR models to initialize the weights of the SLT encoders and achieve a faster convergence (Bansal et al., 2018). Our SLT architecture is a variant of the system proposed by Be´rard et al. (2018), which we re-implemented in the fairseq toolkit (Gehring et al., 2017). The system relies on an attentional encoder-decoder model that takes in input sequences of audio features and outputs the target sequence at the character level. The encoder processes the input with two consecutive fullyconnected layers to expand the size of the representation, followed by two 2D strided convolutional layers that reduce the sequence length. The output of the convolutions is then processed by three stacked LSTMs (Hochreiter and Schmidhuber, 1997). The decoder consists of a two-layered deep transition (Pascanu et al., 2014) LSTM with an attention network based on the general soft attention score (Luong et al., 2015). The final output of the decoder is a function of the concatenation of the LSTM output, the context vector and the previous-character embedding.  \n\nMT. For the MT experiments we use the open source version of ModernMT.8 The system is based on the Transformer (Vaswani et al., 2017) architecture, which represents the state of the art in NMT (Bojar et al., 2018). The encoder consists of a stack of 6 layers, each containing a sequence of two sub-layers, a self-attention network based on multi-head attention, and a position-wise feedforward layer. The decoder layers have an additional sub-layer: between the self attention and the position-wise feed-forward layer they have an encoder-decoder multi-head attention. All the sublayers in both the encoder and decoder are preceded by layer normalization and are followed by residual connections.  \n\n# 3.2 Data Processing and Evaluation Metrics  \n\nIn our experiments, texts are tokenized and punctuation is normalized. Furthermore, the English texts are lowercased, while the target language texts are split into characters still preserving the word boundaries. For MT, we segment the English words with the BPE algorithm (Sennrich et al., 2015) using a maximum of $30K$ merge operations. The output generation of all models is performed using beam search with a beam size of 5.  \n\nASR performance is measured with word error rate (WER) computed on lower-cased, tokenized texts without punctuation. MT and SLT results are computed with BLEU (Papineni et al., 2002).  \n\n# 3.3 Experiment 1: Corpus Quality  \n\nAs observed in Section 2, each section of MuSTC is larger than any other existing publicly available SLT corpus. The usefulness of a resource, however, is not only a matter of size but also of quality (in this case, the quality of the audio-transcription-translation alignments). For an empirical verification of this aspect, we experimented with two comparable datasets. One is the TED-derived English-German IWSLT18 corpus (Niehues et al., 2018), which is built following a pipeline that performs segment extraction and alignment based on time information (i.e. start and end position of each segment in the SubRip Text (SRT) files) instead of text-level alignments. The other is the English-German subset of MuSTC derived from the same TED Talks used to build the IWSLT18 corpus. On one side (MuST-C), the number of segments, their length, and the overall corpus quality depend on text-level alignments. On the other side (IWSLT18), they depend on matching time stamps. This strategy, however, has some drawbacks. First, as pointed out by (Niehues et al., 2018; Liu et al., 2018; Di Gangi et al., 2018), the use of time information brings some noise in the corpus. Second, it often results in utterancelevel alignment (based on speakers’ pauses in the original audio). Compared to sentence-level alignment, this level of granularity can be sub-optimal during model training (e.g. for MT and SLT, learning from complete sentences is easier than learning from phrases). Finally, time information about the recorded speech is not always available: bypassing this need would make the method replicable on other data (not only TED-like).  \n\nThough initialized with the same set of 1, 619 talks, the two pipelines produce different corpora. As shown in Table 3, our approach filters out 58 entire talks ${\\sim}3.6\\%$ of the total) but the final number of segments, their corresponding audio duration and their average length (in words) are larger.  \n\n<html><body><table><thead><tr><td><b>Training set</b></td><td><b>ASR (↓)</b></td><td><b>MT (↑)</b></td><td><b>SLT (↑)</b></td></tr></thead><tbody><tr><td>IWSLT18</td><td>42.15</td><td>24.90</td><td>8.94</td></tr><tr><td>MuST-C</td><td>32.05</td><td>25.46</td><td>12.25</td></tr></tbody></table></body></html>  \n\nTable 4: Performance of ASR, MT and SLT systems trained with En-De IWSLT18 and MuST-C data.   \nTable 5: Baseline ASR, MT and SLT results for each language direction.   \n\n\n<html><body><table><thead><tr><td><b>Tgt</b></td><td><b>ASR (↓)</b></td><td><b>MT (↑)</b></td><td><b>SLT (↑)</b></td></tr></thead><tbody><tr><td>De</td><td>27.00</td><td>28.09</td><td>12.93</td></tr><tr><td>Es</td><td>26.61</td><td>34.16</td><td>18.20</td></tr><tr><td>Fr</td><td>25.81</td><td>42.23</td><td>22.29</td></tr><tr><td>It</td><td>26.38</td><td>30.40</td><td>14.95</td></tr><tr><td>NI</td><td>26.55</td><td>33.43</td><td>18.20</td></tr><tr><td>Pt</td><td>28.00</td><td>32.44</td><td>17.10</td></tr><tr><td>Ro</td><td>27.61</td><td>28.16</td><td>13.35</td></tr><tr><td>Ru</td><td>26.97</td><td>18.30</td><td>7.22</td></tr></tbody></table></body></html>  \n\nper corpus). All the systems are evaluated on the common test set.  \n\nTable 4 shows that the models trained on MuSTC data achieve better results on the balanced test set in all the three tasks. In particular: i) a reduction of 10.1 WER points in ASR indicates a higher quality of audio-transcription alignments, ii) a BLEU increase of 0.56 points in MT indicates a similar quality for transcription-translation alignments, and iii) a BLEU increase of 3.31 points in SLT indicates a higher quality of audio-translation alignments. We consider these results as evidence of the reliability of our corpus creation methodology. Being the same for all the language pairs, we expect this procedure to end up in comparable quality for all the 8 sections of MuST-C.  \n\n<html><body><table><thead><tr><td><b>Corpus</b></td><td><b>#Talk</b></td><td><b>#Sent</b></td><td><b>Hours</b></td><td><b>src w</b></td><td><b>tgt w</b></td></tr></thead><tbody><tr><td>IWSLT18</td><td>1,619</td><td>176K</td><td>280</td><td>2.7M</td><td>2.5M</td></tr><tr><td>MuST-C</td><td>1,561</td><td>179K</td><td>313</td><td>3.3M</td><td>3.1M</td></tr></tbody></table></body></html>  \n\nEach corpus was divided into training, development and test. Development and test contain segments from randomly selected common talks (i.e. those preserved by the MuST-C pipeline). Their size is respectively $2.3K$ (from 28 talks) and $2.1K$ segments (from 26 talks). The test portions were concatenated to create a balanced test set $(4.2K$ segments) containing half of the instances from the IWSLT18 corpus and half from MuST-C. The remaining material was used to separately train ASR, MT and SLT models on homogeneous data from either of the two corpora (i.e. three systems  \n\n# 3.4 Experiment 2: Baseline Results  \n\nWe finally present baseline results computed, for all the three tasks, on each section of MuST-C. Also for these experiments, development and test data are created with segments from talks that are common to all the languages. Their size is respectively $1.4K$ (from 11 talks) and $2.5K$ segments (from 27 talks). The remaining data (of variable size depending on the language pairs) are used for training. For the sake of replicability, these splits are preserved in the released version of MuST-C.  \n\nThe results in Table 5 lead to the following observations. First, though not directly comparable since they are computed on different test sets, English-German results are in line (actually higher, since they are produced by models built on larger training data) with those presented in Section 3.3. This indicates that the level of quality observed in the previous experiments with a subset of the training data is preserved by the whole material released for this language pair. Second, looking at the other language pairs, ASR, MT and SLT results are comparable with the English-German scores. Besides normal fluctuations in the optimization of the neural models, performance differences are coherent with: i) the relative difficulty of each target language (e.g. Russian is more difficult due to high inflection) and $i i$ ) the variable quantity of training data available (e.g. French has the largest training set, see Table 2). Overall, these explainable differences suggest that our corpus creation methodology yields homogeneous quality for all the languages covered by MuST-C.  \n\n# 4 Conclusion and Future Work  \n\nWe presented MuST-C, a Multilingual Speech Translation Corpus built to address the need of resources for training data-hungry neural SLT models. To the best of our knowledge, to date MuSTC is the largest publicly available corpus of this kind. In its current version, it comprises the English transcription and the translations into 8 target languages of at least 385 hours of speech (up to 504) per language. Thanks to a scalable corpus creation procedure initialized with constantly expanding TED talks data, future extensions will increase the coverage of the already present target languages and introduce new ones.  \n\nMuST-C is released under a Creative Commons license, Attribution - Non Commercial - No Derivatives (CC BY NC ND 4.0 International), and is freely downloadable at mustc.fbk.eu  \n\n# Acknowledgments  \n\nThe authors gratefully acknowledge NVIDIA Corporation for the donation of the Tesla K80 and GeForce GTX 1080 Ti GPUs used for this research", "appendix": "."}, {"title": "ESPnet-ST: All-in-One Speech Translation Toolkit", "authors": "Inaguma, Hirofumi and Kiyono, Shun and Duh, Kevin and Karita, Shigeki and Yalta, Nelson and Hayashi, Tomoki and Watanabe, Shinji", "bibkey": "espnet_st_all_in_one_speech_translation_toolkit", "bibitem": "@article{inaguma-etal-2020-espnet,\n  url = {https://aclanthology.org/2020.acl-demos.34/},\n  title = {ESPnet-ST: All-in-One Speech Translation Toolkit},\n  authors = {Inaguma, Hirofumi and Kiyono, Shun and Duh, Kevin and Karita, Shigeki and Yalta, Nelson and Hayashi, Tomoki and Watanabe, Shinji},\n  abstract = {We present ESPnet-ST, which is designed for the quick development of speech-to-speech translation systems in a single framework. ESPnet-ST is a new project inside end-to-end speech processing toolkit, ESPnet, which integrates or newly implements automatic speech recognition, machine translation, and text-to-speech functions for speech translation. We provide all-in-one recipes including data pre-processing, feature extraction, training, and decoding pipelines for a wide range of benchmark datasets. Our reproducible results can match or even outperform the current state-of-the-art performances; these pre-trained models are downloadable. The toolkit is publicly available at https://github.com/espnet/espnet.},\n  year = {2020},\n  bibkey = {inaguma-etal-2020-espnet},\n  editor = {Celikyilmaz, Asli and Wen, Tsung-Hsien},\n  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},\n  address = {Online},\n  publisher = {Association for Computational Linguistics},\n  doi = {10.18653/v1/2020.acl-demos.34},\n  pages = {302--311},\n  month = {jul},\n  entrytype = {inproceedings}\n}", "url": "https://aclanthology.org/2020.acl-demos.34/", "latex_url": null, "latex_path": null, "pdf_url": "https://aclanthology.org/2020.acl-demos.34.pdf", "pdf_path": "output/download_papers/2020.acl-demos.34/2020.acl-demos.34.pdf", "md_url": null, "latex_length": 0, "latex": null, "abstract": "We present ESPnet-ST, which is designed for the quick development of speech-to-speech translation systems in a single framework. ESPnet-ST is a new project inside end-to-end speech processing toolkit, ESPnet, which integrates or newly implements automatic speech recognition, machine translation, and text-to-speech functions for speech translation. We provide all-in-one recipes including data pre-processing, feature extraction, training, and decoding pipelines for a wide range of benchmark datasets. Our reproducible results can match or even outperform the current state-of-the-art performances; these pre-trained models are downloadable. The toolkit is publicly available at https://github.com/espnet/espnet.", "abstract_length": 714, "abstract_token": 134, "introduction": "Speech translation (ST), where converting speech signals in a language to text in another language, is a key technique to break the language barrier for human communication. Traditional ST systems involve cascading automatic speech recognition (ASR), text normalization (e.g., punctuation insertion, case restoration), and machine translation (MT) modules; we call this Cascade-ST (Ney, 1999; Casacuberta et al., 2008; Kumar et al., 2014). Recently, sequence-to-sequence (S2S) models have become the method of choice in implementing both the ASR and MT modules (c.f. (Chan et al., 2016; Bahdanau et al., 2015)). This convergence of models has opened up the possibility of designing end-to-end speech translation (E2E-ST) systems, where a single S2S directly maps speech in a source language to its translation in the target language (B´erard et al., 2016; Weiss et al., 2017). E2E-ST has several advantages over the cascaded approach: (1) a single E2E-ST model can reduce latency at inference time, which is useful for time-critical use cases like simultaneous interpretation. (2) A single model enables back-propagation training in an end-to-end fashion, which mitigates the risk of error propagation by cascaded modules. (3) In certain use cases such as endangered language documentation (Bird et al., 2014), source speech and target text translation (without the intermediate source text transcript) might be easier to obtain, necessitating the adoption of E2E-ST models (Anastasopoulos and Chiang, 2018). Nevertheless, the verdict is still out on the comparison of translation quality between E2E-ST and CascadeST. Some empirical results favor E2E (Weiss et al., 2017) while others favor Cascade (Niehues et al., 2019); the conclusion also depends on the nuances of the training data condition (Sperber et al., 2019). We believe the time is ripe to develop a unified toolkit that facilitates research in both E2E and cascaded approaches. We present ESPnet-ST, a toolkit that implements many of the recent models for E2E-ST, as well as the ASR and MT modules for Cascade-ST. Our goal is to provide a toolkit where researchers can easily incorporate and test new ideas under different approaches. Recent research suggests that pre-training, multi-task learning, and transfer learning are important techniques for achieving improved results for E2E-ST (Be´rard et al., 2018; Anastasopoulos and Chiang, 2018; Bansal et al., 2019; Inaguma et al., 2019). Thus, a unified toolkit that enables researchers to seamlessly mix-and-match different ASR and MT models in training both E2E-ST and Cascade-ST systems would facilitate research in the field.1 ESPnet-ST is especially designed to target the ST task. ESPnet was originally developed for the <html><body><table><thead><tr><td rowspan=\"2\"><b>Toolkit</b></td><td colspan=\"6\"><b>Supported task</b></td><td colspan=\"6\"><b>Example (w/ corpus pre-processing)</b></td><td rowspan=\"2\"><b> Pre-trained model</b></td></tr><tr><td><b>ASR</b></td><td><b>LM</b></td><td><b>E2E- ST</b></td><td><b>Cascade- ST</b></td><td><b>MT</b></td><td><b>TTS</b></td><td><b>ASR</b></td><td><b>LM</b></td><td><b>E2E- ST</b></td><td><b>Cascade- ST</b></td><td><b>MT</b></td><td><b>TTS</b></td></tr></thead><tbody><tr><td>ESPnet-ST (ours)</td><td>√</td><td></td><td>√</td><td>√</td><td><</td><td>√</td><td>√</td><td>√</td><td>V</td><td>V</td><td><</td><td>√</td><td>√</td></tr><tr><td>Lingvo1</td><td>√</td><td>√</td><td><</td><td>V%</td><td>√</td><td><%</td><td>√</td><td>√</td><td>一</td><td>一</td><td>√</td><td>一</td></tr><tr><td> OpenSeq2seq2</td><td>√</td><td>√</td><td>一</td><td>√</td><td>√</td><td>√</td><td>√</td><td>一</td><td>一</td><td>√</td><td>√</td></tr><tr><td>NeMo3</td><td>√</td><td>√</td><td>-</td><td>√</td><td>√</td><td>√</td><td>√</td><td>一</td><td>一</td><td>√</td><td>√</td></tr><tr><td>RETURNN4</td><td>√</td><td>√</td><td>√</td><td>√</td><td>√</td></tr><tr><td>SLT.KIT5</td><td>√</td><td><</td><td>√</td><td>√</td><td>√</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Fairseq</td><td>√</td></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></body></html> Table 1: Framework comparison on supported tasks in January, 2020. ♣Not publicly available. ♦Available only in Google Cloud storage. 1(Shen et al., 2019) 2(Kuchaiev et al., 2018) 3(Kuchaiev et al., 2019) 4(Zeyer et al., 2018) 5(Zenkel et al., 2018) 6(Ott et al., 2019) 7(Vaswani et al., 2018) 8(Klein et al., 2017) 9(Povey et al., 2011) 10(Pratap et al., 2019) ASR task (Watanabe et al., 2018), and recently extended to the text-to-speech (TTS) task (Hayashi et al., 2020). Here, we extend ESPnet to ST tasks, providing code for building translation systems and recipes (i.e., scripts that encapsulate the entire training/inference procedure for reproducibility purposes) for a wide range of ST benchmarks. This is a non-trivial extension: with a unified codebase for ASR/MT/ST and a wide range of recipes, we believe ESPnet-ST is an all-in-one toolkit that should make it easier for both ASR and MT researchers to get started in ST research. The contributions of ESPnet-ST are as follows: • To the best of our knowledge, this is the first toolkit to include ASR, MT, TTS, and ST recipes and models in the same codebase. Since our codebase is based on the unified framework with a common stage-by-stage processing (Povey et al., 2011), it is very easy to customize training data and models. • We provide recipes for ST corpora such as Fisher-CallHome (Post et al., 2013), Libri-trans (Kocabiyikoglu et al., 2018), How2 (Sanabria et al., 2018), and MustC (Di Gangi et al., $2019\\mathrm{a})^{2}$ . Each recipe contains a single script (run.sh), which covers all experimental processes, such as corpus preparation, data augmentations, and transfer learning. • We provide the open-sourced toolkit and the pre-trained models whose hyper-parameters are intensively tuned. Moreover, we provide interactive demo of speech-to-speech translation hosted by Google Colab.3", "introduction_length": 5962, "introduction_token": 1873, "reference": "# References  \n\nAntonios Anastasopoulos and David Chiang. 2018. Tied multitask learning for neural speech translation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018), pages 82–91.  \n\nParnia Bahar, Tobias Bieschke, and Hermann Ney. 2019a. A comparative study on end-to-end speech to text translation. In Proceedings of 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2019), pages 792–799.  \n\nParnia Bahar, Albert Zeyer, Ralf Schlu¨ter, and Hermann Ney. 2019b. On using SpecAugment for endto-end speech translation. In Proceedings of 16th International Workshop on Spoken Language Translation 2019 (IWSLT 2019).  \n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of the 3rd International Conference on Learning Representations (ICLR 2015).  \n\nSameer Bansal, Herman Kamper, Karen Livescu, Adam Lopez, and Sharon Goldwater. 2019. Pretraining on high-resource speech recognition improves low-resource speech-to-text translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019), pages 58–68.  \n\nAlexandre Be´rard, Laurent Besacier, Ali Can Kocabiyikoglu, and Olivier Pietquin. 2018. End-to-end automatic speech translation of audiobooks. In Proceedings of 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2018), pages 6224–6228.  \n\nAlexandre Be´rard, Olivier Pietquin, Christophe Servan, and Laurent Besacier. 2016. Listen and translate: A proof of concept for end-to-end speech-to-text translation. In Proceedings of NIPS 2016 End-to-end Learning for Speech and Audio Processing Workshop.  \n\nSteven Bird, Lauren Gawne, Katie Gelbart, and Isaac McAlister. 2014. Collecting bilingual audio in remote indigenous communities. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics (COLING 2014), pages 1015–1024.  \n\nF. Casacuberta, M. Federico, H. Ney, and E. Vidal. 2008. Recent efforts in spoken language translation. IEEE Signal Processing Magazine, 25(3):80–88.  \n\nMauro Cettolo, Christian Girardi, and Marcello Federico. 2012. Wit3: Web inventory of transcribed and translated talks. In Conference of european association for machine translation, pages 261–268.  \n\nWilliam Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. 2016. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. In Proceedings of 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2016), pages 4960–4964.  \n\nMattia A. Di Gangi, Roldano Cattoni, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2019a. MuSTC: a Multilingual Speech Translation Corpus. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019), pages 2012–2017.  \n\nMattia A Di Gangi, Matteo Negri, and Marco Turchi. 2019b. Adapting transformer to end-to-end spoken language translation. In Proceedings of 20th Annual Conference of the International Speech Communication Association (INTERSPEECH 2019), pages 1133–1137.  \n\nMattia Antonino Di Gangi, Matteo Negri, and Marco Turchi. 2019c. One-to-many multilingual end-toend speech translation. In Proceedings of 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2019), pages 585–592.  \n\nPierre Godard, Gilles Adda, Martine Adda-Decker, Juan Benjumea, Laurent Besacier, Jamison CooperLeavitt, Guy-Noel Kouarata, Lori Lamel, He´le\\`ne Maynard, Markus Mueller, Annie Rialland, Sebastian Stueker, Franc¸ois Yvon, and Marcely ZanonBoito. 2018. A very low resource language speech corpus for computational language documentation experiments. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).  \n\nTomoki Hayashi, Ryuichi Yamamoto, Katsuki Inoue, Takenori Yoshimura, Shinji Watanabe, Tomoki Toda,  \n\nKazuya Takeda, Yu Zhang, and Xu Tan. 2020. ESPnet-TTS: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit. In Proceedings of 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2020).  \n\nHirofumi Inaguma, Kevin Duh, Tatsuya Kawahara, and Shinji Watanabe. 2019. Multilingual end-to-end speech translation. In Proceedings of 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2019), pages 570–577.  \n\nNiehues Jan, Roldano Cattoni, Stu¨ker Sebastian, Mauro Cettolo, Marco Turchi, and Marcello Federico. 2018. The IWSLT 2018 evaluation campaign. In Proceedings of 15th International Workshop on Spoken Language Translation 2018 (IWSLT 2018), pages 2–6.  \n\nAnjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara N Sainath, Zhifeng Chen, and Rohit Prabhavalkar. 2017. An analysis of incorporating an external language model into a sequence-to-sequence model. In Proceedings of 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2017), pages 5824–5828.  \n\nShigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hirofumi Inaguma, Ziyan Jiang, Masao Someki, Nelson Enrique Yalta Soplin, Ryuichi Yamamoto, Xiaofei Wang, et al. 2019. A comparative study on Transformer vs RNN in speech applications. In Proceedings of 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2019), pages 499–456.  \n\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. 2017. OpenNMT: Opensource toolkit for neural machine translation. In Proceedings of ACL 2017, System Demonstrations, pages 67–72.  \n\nAli Can Kocabiyikoglu, Laurent Besacier, and Olivier Kraif. 2018. Augmenting Librispeech with French translations: A multimodal corpus for direct speech translation evaluation. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).  \n\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180.  \n\nOleksii Kuchaiev, Boris Ginsburg, Igor Gitman, Vitaly Lavrukhin, Carl Case, and Paulius Micikevicius. 2018. OpenSeq2Seq: Extensible toolkit for distributed and mixed precision training of sequenceto-sequence models. In Proceedings of Workshop for NLP Open Source Software (NLP-OSS), pages 41–46.  \n\nOleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii Hrinchuk, Ryan Leary, Boris Ginsburg, Samuel Kriman, Stanislav Beliaev, Vitaly Lavrukhin, Jack Cook, et al. 2019. NeMo: a toolkit for building AI applications using Neural Modules. arXiv preprint arXiv:1909.09577.  \n\nTaku Kudo. 2018. Subword regularization: Improving neural network translation models with multiple subword candidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018), pages 66–75.  \n\nGaurav Kumar, Matt Post, Daniel Povey, and Sanjeev Khudanpur. 2014. Some insights from translating conversational telephone speech. In Proceedings of 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2014), pages 3231–3235.  \n\nNaihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. 2019. Neural speech synthesis with transformer network. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6706–6713.  \n\nYuchen Liu, Hao Xiong, Zhongjun He, Jiajun Zhang, Hua Wu, Haifeng Wang, and Chengqing Zong. 2019. End-to-end speech translation with knowledge distillation. In Proceedings of 20th Annual Conference of the International Speech Communication Association (INTERSPEECH 2019), pages 1128–1132.  \n\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. 2018. Mixed precision training. In Proceedings of the 6th International Conference on Learning Representations (ICLR 2018).  \n\nHermann Ney. 1999. Speech translation: Coupling of recognition and translation. In Proceedings of 1999 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 1999), pages 517–520.  \n\nJ. Niehues, R. Cattoni, S. Stu¨ker, M. Negri, M. Turchi, E. Salesky, R. Sanabria, L. Barrault, L. Specia, and M Federico. 2019. The IWSLT 2019 evaluation campaign. In Proceedings of 16th International Workshop on Spoken Language Translation 2019 (IWSLT 2019).  \n\nAaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. 2016. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499.  \n\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. Fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48–53.  \n\nKishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 311–318.  \n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. PyTorch: An imperative style, high-performance deep learning library. In Proceedings of Advances in Neural Information Processing Systems 32 (NeurIPS 2019), pages 8024–8035.  \n\nStephan Peitz, Markus Freitag, Arne Mauser, and Hermann Ney. 2011. Modeling punctuation prediction as machine translation. In Proceedings of 8th International Workshop on Spoken Language Translation 2011 (IWSLT 2011), pages 238–245.  \n\nMartin Popel and Ondˇrej Bojar. 2018. Training Tips for the Transformer Model. The Prague Bulletin of Mathematical Linguistics, 110(1):43–70.  \n\nMatt Post, Gaurav Kumar, Adam Lopez, Damianos Karakos, Chris Callison-Burch, and Sanjeev Khudanpur. 2013. Improved speech-to-text translation with the Fisher and Callhome Spanish–English speech translation corpus. In Proceedings of 10th International Workshop on Spoken Language Translation 2013 (IWSLT 2013).  \n\nDaniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al. 2011. The kaldi speech recognition toolkit. In Proceedings of 2011 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2011).  \n\nVineel Pratap, Awni Hannun, Qiantong Xu, Jeff Cai, Jacob Kahn, Gabriel Synnaeve, Vitaliy Liptchinsky, and Ronan Collobert. 2019. Wav2Letter++: A fast open-source speech recognition system. In Proceedings of 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2019), pages 6460–6464.  \n\nYi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. 2019. Fastspeech: Fast, robust and controllable text to speech. In Advances in Neural Information Processing Systems 32 (NeurIPS 2019), pages 3165–3174.  \n\nRamon Sanabria, Ozan Caglayan, Shruti Palaskar, Desmond Elliott, Lo¨ıc Barrault, Lucia Specia, and Florian Metze. 2018. How2: A large-scale dataset for multimodal language understanding. In Proceedings of the Workshop on Visually Grounded Interaction and Language (ViGIL).  \n\nHiroshi Seki, Takaaki Hori, Shinji Watanabe, Niko Moritz, and Jonathan Le Roux. 2019. Vectorized Beam Search for CTC-Attention-Based Speech Recognition. In Proceedings of 20th Annual Conference of the International Speech Communication Association (INTERSPEECH 2019), pages 3825– 3829.  \n\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016), pages 1715–1725.  \n\nJonathan Shen, Patrick Nguyen, Yonghui Wu, Zhifeng Chen, Mia X Chen, Ye Jia, Anjuli Kannan, Tara Sainath, Yuan Cao, Chung-Cheng Chiu, et al. 2019. Lingvo: a modular and scalable framework for sequence-to-sequence modeling. arXiv preprint arXiv:1902.08295.  \n\nJonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, R. J. Skerry-Ryan, Rif A. Saurous, Yannis Agiomyrgiannakis, and Yonghui Wu. 2018. Natural TTS synthesis by conditioning WaveNet on Mel spectrogram predictions. In Proceedings of 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2017), pages 4779–4783.  \n\nMatthias Sperber, Graham Neubig, Jan Niehues, and Alex Waibel. 2019. Attention-passing models for robust and data-efficient end-to-end speech translation. Transactions of the Association for Computational Linguistics, 7:313–325.  \n\nSeiya Tokui, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani, Toru Ogawa, Shunta Saito, Shuji Suzuki, Kota Uenishi, Brian Vogel, and Hiroyuki Yamazaki Vincent. 2019. Chainer: A deep learning framework for accelerating the research cycle. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD 2019), pages 2002–2011.  \n\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan Gomez, Stephan Gouws, Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit. 2018. Tensor2Tensor for neural machine translation. In Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers), pages 193–199, Boston, MA. Association for Machine Translation in the Americas.  \n\nChengyi Wang, Yu Wu, Shujie Liu, Zhenglu Yang, and Ming Zhou. 2020. Bridging the gap between pretraining and fine-tuning for end-to-end speech translation. In Proceedings of the AAAI conference on artificial intelligence 2020 (AAAI 2020).  \n\nShinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, et al. 2018. ESPnet: Endto-end speech processing toolkit. In Proceedings of 19th Annual Conference of the International Speech Communication Association (INTERSPEECH 2018), pages 2207–2211.  \n\nShinji Watanabe, Takaaki Hori, Suyoun Kim, John R Hershey, and Tomoki Hayashi. 2017. Hybrid CTC/attention architecture for end-to-end speech recognition. IEEE Journal of Selected Topics in Signal Processing, 11(8):1240–1253.  \n\nRon J Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, and Zhifeng Chen. 2017. Sequence-tosequence models can directly translate foreign speech. In Proceedings of 18th Annual Conference of the International Speech Communication Association (INTERSPEECH 2017), pages 2625–2629.  \n\nRyuichi Yamamoto, Eunwoo Song, and Jae-Min Kim. 2020. Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. In Proceedings of 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2020).  \n\nThomas Zenkel, Matthias Sperber, Jan Niehues, Markus Mu¨ller, Ngoc-Quan Pham, Sebastian Stu¨ker, and Alex Waibel. 2018. Open source toolkit for speech to text translation. Prague Bull. Math. Linguistics, 111:125–135.  \n\nAlbert Zeyer, Tamer Alkhouli, and Hermann Ney. 2018. RETURNN as a generic flexible neural toolkit with application to translation and speech recognition. In Proceedings of ACL 2018, System Demonstrations, pages 128–133.", "reference_length": 16038, "reference_token": 4375, "txt_length": 26752, "txt_token": 8029, "txt": "# ESPnet-ST: All-in-One Speech Translation Toolkit  \n\nHirofumi Inaguma1 Shun Kiyono2 Kevin Duh3 Shigeki Karita4 Nelson Yalta5 Tomoki Hayashi6,7 Shinji Watanabe3 1 Kyoto University 2 RIKEN AIP 3 Johns Hopkins University 4 NTT Communication Science Laboratories 5 Waseda University 6 Nagoya University 7 Human Dataware Lab. Co., Ltd. inaguma@sap.ist.i.kyoto-u.ac.jp  \n\n# Abstract  \n\nWe present ESPnet-ST, which is designed for the quick development of speech-to-speech translation systems in a single framework. ESPnet-ST is a new project inside end-toend speech processing toolkit, ESPnet, which integrates or newly implements automatic speech recognition, machine translation, and text-to-speech functions for speech translation. We provide all-in-one recipes including data pre-processing, feature extraction, training, and decoding pipelines for a wide range of benchmark datasets. Our reproducible results can match or even outperform the current state-of-the-art performances; these pretrained models are downloadable. The toolkit is publicly available at https://github. com/espnet/espnet.  \n\n# 1 Introduction  \n\nSpeech translation (ST), where converting speech signals in a language to text in another language, is a key technique to break the language barrier for human communication. Traditional ST systems involve cascading automatic speech recognition (ASR), text normalization (e.g., punctuation insertion, case restoration), and machine translation (MT) modules; we call this Cascade-ST (Ney, 1999; Casacuberta et al., 2008; Kumar et al., 2014). Recently, sequence-to-sequence (S2S) models have become the method of choice in implementing both the ASR and MT modules (c.f. (Chan et al., 2016; Bahdanau et al., 2015)). This convergence of models has opened up the possibility of designing end-to-end speech translation (E2E-ST) systems, where a single S2S directly maps speech in a source language to its translation in the target language (B´erard et al., 2016; Weiss et al., 2017).  \n\nE2E-ST has several advantages over the cascaded approach: (1) a single E2E-ST model can reduce latency at inference time, which is useful for time-critical use cases like simultaneous interpretation. (2) A single model enables back-propagation training in an end-to-end fashion, which mitigates the risk of error propagation by cascaded modules. (3) In certain use cases such as endangered language documentation (Bird et al., 2014), source speech and target text translation (without the intermediate source text transcript) might be easier to obtain, necessitating the adoption of E2E-ST models (Anastasopoulos and Chiang, 2018). Nevertheless, the verdict is still out on the comparison of translation quality between E2E-ST and CascadeST. Some empirical results favor E2E (Weiss et al., 2017) while others favor Cascade (Niehues et al., 2019); the conclusion also depends on the nuances of the training data condition (Sperber et al., 2019).  \n\nWe believe the time is ripe to develop a unified toolkit that facilitates research in both E2E and cascaded approaches. We present ESPnet-ST, a toolkit that implements many of the recent models for E2E-ST, as well as the ASR and MT modules for Cascade-ST. Our goal is to provide a toolkit where researchers can easily incorporate and test new ideas under different approaches. Recent research suggests that pre-training, multi-task learning, and transfer learning are important techniques for achieving improved results for E2E-ST (Be´rard et al., 2018; Anastasopoulos and Chiang, 2018; Bansal et al., 2019; Inaguma et al., 2019). Thus, a unified toolkit that enables researchers to seamlessly mix-and-match different ASR and MT models in training both E2E-ST and Cascade-ST systems would facilitate research in the field.1  \n\nESPnet-ST is especially designed to target the ST task. ESPnet was originally developed for the  \n\n<html><body><table><thead><tr><td rowspan=\"2\"><b>Toolkit</b></td><td colspan=\"6\"><b>Supported task</b></td><td colspan=\"6\"><b>Example (w/ corpus pre-processing)</b></td><td rowspan=\"2\"><b> Pre-trained model</b></td></tr><tr><td><b>ASR</b></td><td><b>LM</b></td><td><b>E2E- ST</b></td><td><b>Cascade- ST</b></td><td><b>MT</b></td><td><b>TTS</b></td><td><b>ASR</b></td><td><b>LM</b></td><td><b>E2E- ST</b></td><td><b>Cascade- ST</b></td><td><b>MT</b></td><td><b>TTS</b></td></tr></thead><tbody><tr><td>ESPnet-ST (ours)</td><td>√</td><td></td><td>√</td><td>√</td><td><</td><td>√</td><td>√</td><td>√</td><td>V</td><td>V</td><td><</td><td>√</td><td>√</td></tr><tr><td>Lingvo1</td><td>√</td><td>√</td><td><</td><td>V%</td><td>√</td><td><%</td><td>√</td><td>√</td><td>一</td><td>一</td><td>√</td><td>一</td></tr><tr><td> OpenSeq2seq2</td><td>√</td><td>√</td><td>一</td><td>√</td><td>√</td><td>√</td><td>√</td><td>一</td><td>一</td><td>√</td><td>√</td></tr><tr><td>NeMo3</td><td>√</td><td>√</td><td>-</td><td>√</td><td>√</td><td>√</td><td>√</td><td>一</td><td>一</td><td>√</td><td>√</td></tr><tr><td>RETURNN4</td><td>√</td><td>√</td><td>√</td><td>√</td><td>√</td></tr><tr><td>SLT.KIT5</td><td>√</td><td><</td><td>√</td><td>√</td><td>√</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Fairseq</td><td>√</td></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></body></html>\n\nTable 1: Framework comparison on supported tasks in January, 2020. ♣Not publicly available. ♦Available only in Google Cloud storage. 1(Shen et al., 2019) 2(Kuchaiev et al., 2018) 3(Kuchaiev et al., 2019) 4(Zeyer et al., 2018) 5(Zenkel et al., 2018) 6(Ott et al., 2019) 7(Vaswani et al., 2018) 8(Klein et al., 2017) 9(Povey et al., 2011) 10(Pratap et al., 2019)  \n\nASR task (Watanabe et al., 2018), and recently extended to the text-to-speech (TTS) task (Hayashi et al., 2020). Here, we extend ESPnet to ST tasks, providing code for building translation systems and recipes (i.e., scripts that encapsulate the entire training/inference procedure for reproducibility purposes) for a wide range of ST benchmarks. This is a non-trivial extension: with a unified codebase for ASR/MT/ST and a wide range of recipes, we believe ESPnet-ST is an all-in-one toolkit that should make it easier for both ASR and MT researchers to get started in ST research.  \n\nThe contributions of ESPnet-ST are as follows:  \n\n• To the best of our knowledge, this is the first toolkit to include ASR, MT, TTS, and ST recipes and models in the same codebase. Since our codebase is based on the unified framework with a common stage-by-stage processing (Povey et al., 2011), it is very easy to customize training data and models.  \n\n• We provide recipes for ST corpora such as Fisher-CallHome (Post et al., 2013), Libri-trans (Kocabiyikoglu et al., 2018), How2 (Sanabria et al., 2018), and MustC (Di Gangi et al., $2019\\mathrm{a})^{2}$ . Each recipe contains a single script (run.sh), which covers all experimental processes, such as corpus preparation, data augmentations, and transfer learning.  \n\n• We provide the open-sourced toolkit and the pre-trained models whose hyper-parameters are intensively tuned. Moreover, we provide interactive demo of speech-to-speech translation hosted by Google Colab.3  \n\n# 2 Design  \n\n# 2.1 Installation  \n\nAll required tools are automatically downloaded and built under tools (see Figure 1) by a make command. The tools include (1) neural network libraries such as PyTorch (Paszke et al., 2019), (2) ASR-related toolkits such as Kaldi (Povey et al., 2011), and (3) MT-related toolkits such as Moses (Koehn et al., 2007) and sentencepiece (Kudo, 2018). ESPnet-ST is implemented with Pytorch backend.  \n\n# 2.2 Recipes for reproducible experiments  \n\nWe provide various recipes for all tasks in order to quickly and easily reproduce the strong baseline systems with a single script. The directory structure is depicted as in Figure 1. egs contains corpus directories, in which the corresponding task directories (e.g., st1) are included. To run experiments, we simply execute run.sh under the desired task directory. Configuration yaml files for feature extraction, data augmentation, model training, and decoding etc. are included in conf. Model directories including checkpoints are saved under exp. More details are described in Section 2.4.  \n\n![](images/7e5f14f5a107ac99e1f270f5619e0b5f7add48e07f697a0381f614d5540aadff.jpg)  \nFigure 1: Directory structure of ESPnet-ST  \n\n![](images/0e0a725d15590dd03d96bd1d00ae1f94fcc132682428942a86b92fdcefc171cd.jpg)  \nFigure 2: All-in-one process pipelines in ESPnet-ST  \n\n# 2.3 Tasks  \n\nWe support language modeling (LM), neural textto-speech (TTS) in addition to ASR, ST, and MT tasks. To the best of our knowledge, none of frameworks support all these tasks in a single toolkit. A comparison with other frameworks are summarized in Table 1. Conceptually, it is possible to combine ASR and MT modules for Cascade-ST, but few frameworks provide such examples. Moreover, though some toolkits indeed support speechto-text tasks, it is not trivial to switch ASR and E2E-ST tasks since E2E-ST requires the auxiliary tasks (ASR/MT objectives) to achieve reasonable performance.  \n\n# 2.4 Stage-by-stage processing  \n\nESPnet-ST is based on a stage-by-stage processing including corpus-dependent pre-processing, feature extraction, training, and decoding stages. We follow Kaldi-style data preparation, which makes it easy to augment speech data by leveraging other data resources prepared in egs.  \n\nOnce run.sh is executed, the following processes are started.  \n\nStage 0: Corpus-dependent pre-processing is conducted using scripts under local and the resulting text data is automatically saved under data. Both transcriptions and the corresponding translations with three different treatments of casing and punctuation marks (hereafter, punct.) are generated after text normalization and tokenization with tokenizer.perl in Moses; (a) tc: truecased text with punct., (b) lc: lowercased text with punct., and (3) lc.rm: lowercased text without punct. except for apostrophe. lc.rm is designed for the ASR task since the conventional ASR system does not generate punctuation marks. However, it is possible to train ASR models so as to generate truecased text using tc.4  \n\nStage 1: Speech feature extraction based on Kaldi and our own implementations is performed.  \n\nStage 2: Dataset JSON files in a format ingestable by ESPnet’s Pytorch back-end (containing token/utterance/speaker/language IDs, input and output sequence lengths, transcriptions, and translations) are dumped under dump.  \n\nStage 3: (ASR recipe only) LM is trained.  \n\nStage 4: Model training (RNN/Transformer) is performed.  \n\nStage 5: Model averaging, beam search decoding, and score calculation are conducted.  \n\nStage 6: (Cascade-ST recipe only) The system is evaluated by feeding ASR outputs to the MT model.  \n\n# 2.5 Multi-task learning and transfer learning  \n\nIn ST literature, it is acknowledged that the optimization of E2E-ST is more difficult than individually training ASR and MT models. Multitask training (MTL) and transfer learning from ASR and MT tasks are promising approaches for this problem (Weiss et al., 2017; B´erard et al., 2018; Sperber et al., 2019; Bansal et al., 2019). Thus, in Stage 4 of the E2E-ST recipe, we allow options to add auxiliary ASR and MT objectives. We also support options to initialize the parameters of the ST encoder with a pre-trained ASR encoder in asr1, and to initialize the parameters of the ST decoder with a pre-trained MT decoder in mt1.  \n\n# 2.6 Speech data augmentation  \n\nWe implement techniques that have shown to give improved robustness in the ASR component.  \n\nSpeed perturbation We augmented speech data by changing the speed with factors of 0.9, 1.0, and 1.1, which results in 3-fold data augmentation. We found this is important to stabilize E2E-ST training.  \n\nSpecAugment Time and frequency masking blocks are randomly applied to log mel-filterbank features. This has been originally proposed to improve the ASR performance and shown to be effective for E2E-ST as well (Bahar et al., 2019b).  \n\n# 2.7 Multilingual training  \n\nMultilingual training, where datasets from different language pairs are combined to train a single model, is a potential way to improve performance of E2E-ST models (Inaguma et al., 2019; Di Gangi et al., 2019c). Multilingual E2E-ST/MT models are supported in several recipes.  \n\n# 2.8 Additional features  \n\nExperiment manager We customize the data loader, trainer, and evaluator by overriding Chainer (Tokui et al., 2019) modules. The common processes are shared among all tasks.  \n\nLarge-scale training/decoding We support job schedulers (e.g., SLURM, Grid Engine), multiple GPUs and half/mixed-precision training/decoding with apex (Micikevicius et al., 2018).5 Our beam search implementation vectorizes hypotheses for faster decoding (Seki et al., 2019).  \n\nPerformance monitoring Attention weights and all kinds of training/validation scores and losses for ASR, MT, and ST tasks can be collectively monitored through TensorBoard.  \n\nEnsemble decoding Averaging posterior probabilities from multiple models during beam search decoding is supported.  \n\n# 3 Example Models  \n\nTo give a flavor of the models that are supported with ESPnet-ST, we describe in detail the construction of an example E2E-ST model, which is used later in the Experiments section. Note that there are many customizable options not mentioned here.  \n\nAutomatic speech recognition (ASR) We build ASR components with the Transformer-based hybrid CTC/attention framework (Watanabe et al., 2017), which has been shown to be more effective than RNN-based models on various speech corpora (Karita et al., 2019). Decoding with the external LSTM-based LM trained in the Stage 3 is also conducted (Kannan et al., 2017). The transformer uses 12 self-attention blocks stacked on the two VGG blocks in the speech encoder and 6 self-attention blocks in the transcription decoder; see (Karita et al., 2019) for implementation details.  \n\nMachine translation (MT) The MT model consists of the source text encoder and translation decoder, implemented as a transformer with 6 selfattention blocks. For simplicity, we train the MT model by feeding lowercased source sentences without punctuation marks (lc.rm) (Peitz et al., 2011). There are options to explore characters and different subword units in the MT component.  \n\nEnd-to-end speech translation (E2E-ST) Our E2E-ST model is composed of the speech encoder and translation decoder. Since the definition of parameter names is exactly same as in the ASR and MT components, it is quite easy to copy parameters from the pre-trained models for transfer learning. After ASR and MT models are trained as described above, their parameters are extracted and used to initialize the E2E-ST model. The model is then trained on ST data, with the option of incorporating multi-task objectives as well.  \n\nText-to-speech (TTS) We also support end-toend text-to-speech (E2E-TTS), which can be applied after ST outputs a translation. The E2ETTS model consists of the feature generation network converting an input text to acoustic features (e.g., log-mel filterbank coefficients) and the vocoder network converting the features to a waveform. Tacotron 2 (Shen et al., 2018), TransformerTTS (Li et al., 2019), FastSpeech (Ren et al., 2019), and their variants such as a multi-speaker model are supported as the feature generation network. WaveNet (van den Oord et al., 2016) and Parallel WaveGAN (Yamamoto et al., 2020) are available as the vocoder network. See Hayashi et al. (2020) for more details.  \n\n# 4 Experiments  \n\nIn this section, we demonstrate how models from our ESPnet recipes perform on benchmark speech translation corpora: Fisher-CallHome Spanish $\\mathrm{En{\\rightarrow}E s}$ , Libri-trans $\\mathrm{En{\\rightarrow}F r}$ , How2 $\\mathrm{En}{\\rightarrow}\\mathrm{Pt}$ , and Must-C En $\\to\\!8$ languages. Moreover, we also performed experiments on IWSLT16 En-De to validate the performance of our MT modules.  \n\n<html><body><table><thead><tr><td rowspan=\"3\"></td><td rowspan=\"3\"><b>Model</b></td><td colspan=\"5\"><b>Es → En</b></td></tr><tr><td colspan=\"3\"><b>Fisher</b></td><td colspan=\"2\"><b>CallHome</b></td></tr><tr><td><b>dev</b></td><td><b>dev2</b></td><td><b>test</b></td><td><b>devtest</b></td><td><b>evltest</b></td></tr></thead><tbody><tr><td rowspan=\"8\">E2E</td><td>Char RNN + ASR-MTL (Weiss et al., 2017)</td><td>48.30</td><td>49.10</td><td>48.70</td><td>16.80</td><td>17.40</td></tr><tr><td>ESPnet-ST (Transformer)</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ASR-MTL (multi-task w/ ASR)</td><td>46.64</td><td>47.64</td><td>46.45</td><td>16.80</td><td>16.80</td></tr><tr><td>+ MT-MTL (multi-task w/ MT)</td><td>47.17</td><td>48.20</td><td>46.99</td><td>17.51</td><td>17.64</td></tr><tr><td>ASR encoder init. (①)</td><td>46.25</td><td>47.11</td><td>46.21</td><td>17.35</td><td>16.94</td></tr><tr><td>+ MT decoder init. (②)</td><td>46.25</td><td>47.60</td><td>46.72</td><td>17.62</td><td>17.50</td></tr><tr><td>+ SpecAugment (③)</td><td>48.94</td><td>49.32</td><td>48.39</td><td>18.83</td><td>18.67</td></tr><tr><td>+ Ensemble 3 models (① + ② + ③)</td><td>50.76</td><td>52.02</td><td>50.85</td><td>19.91</td><td>19.36</td></tr><tr><td rowspan=\"3\">Cascade</td><td>Char RNN ASR →→ Char RNN MT (Weiss et al., 2017)</td><td>45.10</td><td>46.10</td><td>45.50</td><td>16.20</td><td>16.60</td></tr><tr><td> Char RNN ASR → Char RNN MT (Inaguma et al., 2019)*</td><td>37.3</td><td>39.6</td><td>38.6</td><td>16.8</td><td>16.5</td></tr><tr><td>ESPnet-ST</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>Transformer ASR→ Transformer MT</td><td>41.96 43.46</td><td>642.16</td><td>19.56</td></tr></tbody></table></body></html>  \n\nTable 3: BLEU of ST systems on Libri-trans corpus. ♣Implemented w/ ESPnet. △Pre-training. ♦w/ SpecAugment. 1(Liu et al., 2019) 2(Bahar et al., 2019a) 3(Bahar et al., 2019b) 4(Wang et al., 2020)   \n\n\n<html><body><table><thead><tr><td colspan=\"2\"><b>Model</b></td><td><b>[En → Fr</b></td></tr></thead><tbody><tr><td rowspan=\"10\">E2E</td><td>Transformer + ASR/MT-trans + KDI</td><td>17.02</td></tr><tr><td>+ Ensemble 3 models</td><td>17.8</td></tr><tr><td>Transformer + PT+ adaptor2</td><td>16.80</td></tr><tr><td>Transformer + PT + SpecAugment?3</td><td>17.0</td></tr><tr><td>RNN + TCEN4,*</td><td>17.05</td></tr><tr><td>ESPnet-ST (Transformer) ASR-MTL</td><td>15.30 15.47</td></tr><tr><td>+ SpecAugment (③) + Ensemble 3 models (① + ② + ③)</td><td>17.85 16.96</td></tr><tr><td>+ Ensemble 3 models (① + ② + ③)</td><td>16.70 16.22</td></tr><tr><td>Transformer ASR →→ Transformer MTI</td><td>16.70</td></tr><tr><td>+ Ensemble 3 models (① + ② + ③)</td><td>17.40</td></tr><tr><td colspan=\"2\">Transformer ASR→→ Transformer MT</td><td>17.85</td></tr></tbody></table></body></html>  \n\nAll sentences were tokenized with the tokenizer.perl script in the Moses toolkit (Koehn et al., 2007). We used the joint source and target vocabularies based on byte pair encoding (BPE) (Sennrich et al., 2016) units. ASR vocabularies were created with English sentences only with lc.rm. We report 4-gram BLEU (Papineni et al., 2002) scores with the multi-bleu.perl script in Moses. For speech features, we extracted 80-channel log-mel filterbank coefficients with 3-dimensional pitch features using Kaldi, resulting 83-dimensional features per frame. Detailed training and decoding configurations are available in conf/train.yaml and conf/decode.yaml, respectively.  \n\nTable 4: BLEU of ST systems on How2 corpus   \n\n\n<html><body><table><thead><tr><td colspan=\"2\"><b>Model</b></td><td><b>[En → Pt</b></td></tr></thead><tbody><tr><td rowspan=\"9\">E2E</td><td>RNN (Sanabria et al., 2018)</td><td>36.0</td></tr><tr><td>ESPnet-ST Transformer</td><td>40.59 44.90 45.10</td></tr><tr><td>+ MT decoder init. (②) + MT-MLT Transformer + ASR encoder init. (①)</td><td>45.03 45.63 45.68</td></tr><tr><td>+ MT decoder init. (②) + SpecAugment (③)</td><td>45.63 45.68</td></tr><tr><td>+ SpecAugment (③) + Ensemble 3 models (① + ② + ③)</td><td>48.04</td></tr><tr><td>ESPnet-ST Transformer ASR —→ Transformer MT</td><td>44.90</td></tr><tr><td>Transformer ASR —→ Transformer MT</td><td>+ ASR-MTL</td></tr><tr><td>Cascade</td><td>44.90</td></tr><tr><td>Cascade</td><td>+ ASR-MTL</td></tr></tbody></table></body></html>  \n\n# 4.1 Fisher-CallHome Spanish $(\\mathbf{Es}\\mathrm{\\mathrm{\\rightarrow}}\\mathbf{En})$  \n\nFisher-CallHome Spanish corpus contains 170- hours of Spanish conversational telephone speech, the corresponding transcription, as well as the English translations (Post et al., 2013). All punctuation marks except for apostrophe were removed (Post et al., 2013; Kumar et al., 2014; Weiss et al., 2017). We report case-insensitive BLEU on Fisher- $\\{d e\\nu,d e\\nu2,t e s t\\}$ (with four references), and CallHome-{devtest, evltest} (with a single reference). We used 1k vocabulary for all tasks.  \n\nResults are shown in Table 2. It is worth noting that we did not use any additional data resource. Both MTL and transfer learning improved the performance of vanilla Transformer. Our best system with SpecAugment matches the current state-ofthe-art performance (Weiss et al., 2017). Moreover, the total training/inference time is much shorter since our E2E-ST models are based on the BPE1k unit rather than characters.6  \n\n<html><body><table><thead><tr><td></td><td><b>Model</b></td><td><b>De</b></td><td><b>Pt</b></td><td><b>Fr</b></td><td><b>Es</b></td><td><b>Ro</b></td><td><b>Ru</b></td><td><b>N1</b></td><td><b>It</b></td></tr></thead><tbody><tr><td rowspan=\"3\">E2E</td><td>Transformer + ASR encoder init.*</td><td>17.30</td><td>20.10</td><td>26.90</td><td>20.80</td><td>16.50</td><td>10.50</td><td>18.80</td><td>16.80</td></tr><tr><td>ESPnet-ST (Transformer) ASR encoder/MT decoder init.</td><td>22.33</td><td>27.26</td><td>31.54</td><td>27.84</td><td>20.91</td><td>15.32</td><td>26.86</td><td>22.81</td></tr><tr><td> + SpecAugment</td><td>22.91</td><td>28.01</td><td>32.69</td><td>27.96</td><td>21.90</td><td>15.75</td><td>27.43</td><td>23.75</td></tr><tr><td rowspan=\"2\">Cascade</td><td>ESPnet-ST Transformer →→ Transformer ASR</td><td>18.5</td><td>21.5</td><td>27.9</td><td>22.5</td><td>16.8</td><td>11.1</td><td>22.2</td><td>18.9</td></tr><tr><td>ESPnet-ST Transformer ASR →→ Transformer MT</td><td>23.65</td><td>29.04</td><td>33.84</td><td>28.68</td><td>22.68</td><td>16.39</td><td>27.91</td><td>24.04</td></tr></tbody></table></body></html>  \n\nTable 5: BLEU of ST systems on Must-C corpus. ♣Implemented w/ Fairseq. 1(Di Gangi et al., 2019b)   \nTable 6: BLEU of MT systems on IWSLT 2016 corpus   \n\n\n<html><body><table><thead><tr><td></td><td colspan=\"3\"><b>En→De</b></td><td colspan=\"3\"><b>De→En</b></td></tr><tr><td><b>Framework</b></td><td><b>test2012</b></td><td><b>test2013</b></td><td><b>test2014</b></td><td><b>test2012</b></td><td><b>test2013</b></td><td><b>test2014</b></td></tr></thead><tbody><tr><td>Fairseq</td><td>27.73</td><td>29.45</td><td>25.14</td><td>32.25</td><td>34.23</td><td>29.49</td></tr><tr><td>ESPnet-ST</td><td>26.92</td><td>28.88</td><td>24.70</td><td>32.19</td><td>33.46</td><td>29.22</td></tr></tbody></table></body></html>  \n\n# 4.2 Libri-trans $(\\mathbf{En}{\\rightarrow}\\,\\mathbf{Fr})$  \n\nLibri-trans corpus contains 236-hours of English read speech, the corresponding transcription, and the French translations (Kocabiyikoglu et al., 2018). We used the clean 100-hours of speech data and augmented translation references with Google Translate for the training set (B´erard et al., 2018; Liu et al., 2019; Bahar et al., 2019a,b). We report case-insensitive BLEU on the test set. We used 1k vocabulary for all tasks.  \n\nResults are shown in Table 3. Note that all models used the same data resource and are competitive to previous work.  \n\n# 4.3 How2 $(\\bf E n\\mathrm{{\\rightarrow}\\,P t)}$  \n\nHow2 corpus contains English speech extracted from YouTube videos, the corresponding transcription, as well as the Portuguese translation (Sanabria et al., 2018). We used the official 300-hour subset for training. Since speech features in the How2 corpus is pre-processed as 40-channel log-mel filterbank coefficients with 3-dimensional pitch features with Kaldi in advance, we used them without speed perturbation. We used $5\\mathrm{k}$ and 8k vocabularies for ASR and E2E-ST/MT models, respectively. We report case-sensitive BLEU on the dev5 set.  \n\nResults are shown in Table 4. Our systems significantly outperform the previous RNN-based model (Sanabria et al., 2018). We believe that our systems can be regarded as the reliable baselines for future research.  \n\n# 4.4 Must-C $\\mathbf{En}{\\rightarrow}\\,\\mathbf{8}$ langs)  \n\nMust-C corpus contains English speech extracted from TED talks, the corresponding transcription, and the target translations in 8 language directions (De, Pt, Fr, Es, Ro, Ru, Nl, and It) (Di Gangi et al., 2019a). We conducted experiments in all 8 directions. We used $5\\mathrm{k}$ and $8\\mathbf{k}$ vocabularies for ASR and E2E-ST/MT models, respectively. We report case-sensitive BLEU on the tst-COMMON set.  \n\nResults are shown in Table 5. Our systems outperformed the previous work (Di Gangi et al., 2019b) implemented with the custermized Fairseq7 with a large margin.  \n\n# 4.5 MT experiment: IWSLT16 ${\\bf E n}\\leftrightarrow{\\bf D e}$  \n\nIWSLT evaluation campaign dataset (Cettolo et al., 2012) is the origin of the dataset for our MT experiments. We used En-De language pair. Specifically, IWSLT 2016 training set for training data, test2012 as the development data, and test2013 and test2014 sets as our test data respectively.  \n\nWe compare the performance of Transformer model in ESPnet-ST with that of Fairseq in Table 6. ESPnet-ST achieves the performance almost comparable to the Fairseq. We assume that the performance gap is due to the minor difference in the implementation of two frameworks. Also, we carefully tuned the hyper-parameters for the MT task in the small ST corpora, which is confirmed from the reasonable performances of our Cascaded-ST systems. It is acknowledged that Transformer model is extremely sensitive to the hyper-parameters such as the learning rate and the number of warmup steps (Popel and Bojar, 2018). Thus, it is possible that the suitable sets of hyper-parameters are different across frameworks.  \n\n# 5 Conclusion  \n\nWe presented ESPnet-ST for the fast development of end-to-end and cascaded ST systems. We provide various all-in-one example scripts containing corpus-dependent pre-processing, feature extraction, training, and inference. In the future, we will support more corpora and implement novel techniques to bridge the gap between end-to-end and cascaded approaches.  \n\n# Acknowledgment  \n\nWe thank Jun Suzuki for providing helpful feedback for the paper.", "appendix": ""}, {"title": "Bridging the Modality Gap for Speech-to-Text Translation", "authors": "Yuchen Liu and\n               Junnan Zhu and\n               Jiajun Zhang and\n               Chengqing Zong", "bibkey": "bridging_the_modality_gap_for_speech_to_text_translation", "bibitem": "@article{LiuBTMGFST,\n  url = {http://arxiv.org/abs/2010.14920},\n  title = {Bridging the Modality Gap for Speech-to-Text Translation},\n  authors = {Yuchen Liu and\n               Junnan Zhu and\n               Jiajun Zhang and\n               Chengqing Zong},\n  bibkey = {LiuBTMGFST}\n}", "url": "http://arxiv.org/abs/2010.14920", "latex_url": "http://arxiv.org/src/2010.14920", "latex_path": "output/download_papers/2010.14920v1/2010.14920v1", "pdf_url": "http://arxiv.org/pdf/2010.14920", "pdf_path": "output/download_papers/2010.14920v1/2010.14920v1.pdf", "md_url": null, "latex_length": 0, "latex": null, "abstract": "End-to-end speech translation aims to translate speech in one language into text in another language via an end-to-end way. Most existing methods employ an encoder-decoder structure with a single encoder to learn acoustic representation and semantic information simultaneously, which ignores the speech-andtext modality differences and makes the encoder overloaded, leading to great difficulty in learning such a model. To address these issues, we propose a Speech-to-Text Adaptation for Speech Translation (STAST) model which aims to improve the end-to-end model performance by bridging the modality gap between speech and text. Specifically, we decouple the speech translation encoder into three parts and introduce a shrink mechanism to match the length of speech representation with that of the corresponding text transcription. To obtain better semantic representation, we completely integrate a text-based translation model into the STAST so that two tasks can be trained in the same latent space. Furthermore, we introduce a cross-modal adaptation method to close the distance between speech and text representation. Experimental results on English-French and English-German speech translation corpora have shown that our model significantly outperforms strong baselines, and achieves the new state-of-the-art performance.", "abstract_length": 1329, "abstract_token": 234, "introduction": "Speech-to-Text translation (ST) aims at translating speech in one language into text in another language, which can be widely applied to conference speech, cross-border service, international business talk, academic forum, etc. Most existing approaches to speech-to-text translation are based on the pipeline paradigm, which first transforms the speech into text via an automatic speech recognition (ASR) system and then translates the transcribed text into the target language by a text-based machine translation model (Ney, 1999; Kanthak et al., 2005; Mathias and Byrne, 2006). Recently, the end-to-end speech-to-text translation model has attracted more attention due to its advantages over the pipeline paradigm, such as low latency, alleviation of error propagation, and fewer parameters (Weiss et al., 2017; B´erard et al., 2018; Bansal et al., 2018; Jia et al., 2019; Sperber et al., 2019a). Previous studies employ an encoder-decoder structure to directly learn the mapping relationship between the speech input and text sequences in the target language. However, modality differences between speech and text result in the difficulty of training such a model, making its performance usually much inferior to the corresponding text-based neural machine translation (NMT) model. We observe that there are three main modality differences between speech and text which affect the speech representational capacity. First, the length of frame-level speech features is much longer than that of the corresponding text data, which hinders the model from learning alignments between the input and the output sequence. Second, text data are generally digitalized into trainable embeddings, while speech features are calculated by hand-crafted fliters and fixed during training, resulting in a lack of semantic information. Third, text data contain less noise with low uncertainty, while the speech is variational and easily affected by factors like the speech rate of the different speaker, silence, and noise, which leads to the poor robustness of the model. To make matter worse, the encoder in the ST model is overloaded since it requires to both learn acoustic information and extract semantic knowledge from the speech input simultaneously. In order to release the burden of speech translation encoder and help it learn better representation, the text representation learned by the NMT model can be utilized as a guidance for speech representation learning. To achieve that, we propose a Speechto-Text Adaptation for Speech Translation (STAST) model which aims to improve the end-to-end model performance by bridging the modality gap between speech and text. Specifically, the representations of two modalities need to be consistent in terms of length and share in the same latent space. Therefore, we decouple speech translation encoder into three parts, including an acoustic encoder, a shrink mechanism, and a semantic encoder. The shrink mechanism can fliter redundant hidden states based on the spike-like label posterior probabilities generated by the CTC module, which has the ability to solve length inconsistency problems. To ensure the representation of two modalities being in the same semantic latent space, we apply the multitask learning method and completely integrate the MT model into the ST model by sharing the semantic encoder and decoder. To further close the distance of two representations, we apply a crossmodal adaptation method, which can afford speech representation more semantic information. Experimental results on two speech translation corpora have illustrated the effectiveness of our proposed model. The contributions of this paper are as follows: • We propose the STAST model which can improve speech translation performance by bridging the modality gap between speech and text. • Experimental results have demonstrated that our proposed method outperforms existing methods and achieves the new state-of-the-art performance. • Our method can easily leverage extra data and has shown especially superiority in lowresource scenarios.", "introduction_length": 4068, "introduction_token": 770, "reference": "# References  \n\nHanan Aldarmaki and Mona Diab. 2019. Contextaware cross-lingual mapping. In Proceedings of NAACL, pages 3906–3911.  \n\nAntonios Anastasopoulos, David Chiang, and Long Duong. 2016. An unsupervised probability model for speech-to-translation alignment of low-resource languages. In Proceedings of EMNLP.  \n\nRohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E Dahl, and Geoffrey E Hinton. 2018. Large scale distributed neural network training through online distillation. arXiv preprint arXiv:1804.03235.  \n\nParnia Bahar, Albert Zeyer, Ralf Schlu¨ter, and Hermann Ney. 2019. On using specaugment for end-to-end speech translation. arXiv preprint arXiv:1911.08876.   \nSameer Bansal, Herman Kamper, Karen Livescu, Adam Lopez, and Sharon Goldwater. 2018. Pretraining on high-resource speech recognition improves low-resource speech-to-text translation. arXiv preprint arXiv:1809.01431.   \nAlexandre Be´rard, Laurent Besacier, Ali Can Kocabiyikoglu, and Olivier Pietquin. 2018. End-to-end automatic speech translation of audiobooks. In Proceedings of ICASSP.   \nAlexandre Be´rard, Olivier Pietquin, Christophe Servan, and Laurent Besacier. 2016. Listen and translate: A proof of concept for end-to-end speech-to-text translation. In Proceedings of NeurIPS.   \nWon-Ik Cho, Donghyun Kwak, Ji Won Yoon, and Nam Soo Kim. 2020. Speech to text adaptation: Towards an efficient cross-modal distillation. In Proceedings of Interspeech.   \nPavel Denisov and Ngoc Thang Vu. 2020. Pretrained semantic speech embeddings for end-to-end spoken language understanding via cross-modal teacherstudent learning. In Proceedings of Interspeech.   \nLong Duong, Antonios Anastasopoulos, David Chiang, Steven Bird, and Trevor Cohn. 2016. An attentional model for speech translation without transcription. In Proceedings of NAACL.   \nMarkus Freitag, Yaser Al-Onaizan, and Baskaran Sankaran. 2017. Ensemble distillation for neural machine translation. arXiv preprint arXiv:1702.01802.   \nMattia Antonino Di Gangi, Matteo Negri, Marco Turchi, and Mattia Antonino Di Gangi. 2019a. Adapting transformer to end-to-end spoken language translation. In Proceedings of Interspeech, pages 1133–1137.   \nMattia Di Gangi, Roldano Cattoni, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2019b. Must-c: a multilingual speech translation corpus. In In Proceedings of NAACL, pages 2012–2017.   \nAlex Graves, Santiago Ferna´ndez, Faustino Gomez, and Ju¨rgen Schmidhuber. 2006. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of ICML, pages 369–376.   \nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.   \nHirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Enrique Yalta Soplin, Tomoki Hayashi, and Shinji Watanabe. 2020. Espnet-st: Allin-one speech translation toolkit. In In Proceedings of ACL (demo), pages 302–311.   \nSathish Indurthi, Houjeung Han, Nikhil Kumar Lakumarapu, Beomseok Lee, Insoo Chung, Sangha Kim, and Chanwoo Kim. 2020. End-end speech-to-text translation with modality agnostic meta-learning. In In Proceedings of ICASSP.   \nYe Jia, Melvin Johnson, Wolfgang Macherey, Ron J. Weiss, Yuan Cao, Chung-Cheng Chiu, Naveen Ari, Stella Marie Laurenzo, and Yonghui Wu. 2019. Leveraging weakly supervised data to improve endto-end speech-to-text translation. In Proceddings of ICASSP, pages 7180–7184.   \nStephan Kanthak, David Vilar, Evgeny Matusov, Richard Zens, and Hermann Ney. 2005. Novel reordering approaches in phrase-based statistical machine translation. In Proceedings of ACL Workshop on Building and Using Parallel Texts, pages 167– 174.   \nSuyoun Kim, Takaaki Hori, and Shinji Watanabe. 2017. Joint ctc-attention based end-to-end speech recognition using multi-task learning. In Proceedings of ICASSP, pages 4835–4839.   \nYoon Kim and Alexander M Rush. 2016. Sequencelevel knowledge distillation. In Proceedings of EMNLP.   \nDiederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of ICLR.   \nAli Can Kocabiyikoglu, Laurent Besacier, and Olivier Kraif. 2018. Augmenting librispeech with french translations: A multimodal corpus for direct speech translation evaluation. In Language Resources and Evaluation.   \nAlon Lavie, Donna Gates, Marsal Gavalda\\`, Laura Mayfield, Alex Waibel, and Lori Levin. 1996. Multilingual translation of spontaneously spoken language in a limited domain. In Proceeding of COLING, 1:442–447.   \nYuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia Li. 2017. Learning from noisy labels with distillation. In Proceedings of ICCV, pages 1910–1918.   \nYuchen Liu, Hao Xiong, Jiajun Zhang, Zhongjun He, Hua Wu, Haifeng Wang, and Chengqing Zong. 2019. End-to-end speech translation with knowledge distillation. In Proceedings of Interspeech, pages 1128– 1132.   \nYuchen Liu, Jiajun Zhang, Hao Xiong, Long Zhou, Zhongjun He, Hua Wu, Haifeng Wang, and Chengqing Zong. 2020. Synchronous speech recognition and speech-to-text translation with interactive decoding. In Proceedings of AAAI, 34(5):8417– 8424.   \nL. Mathias and W. Byrne. 2006. Statistical phrasebased speech translation. In Proceedings of ICASSP, 1:561–564.   \nH. Ney. 1999. Speech translation: coupling of recognition and translation. In Proceedings of ICASSP, 1:517–520.   \nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An asr corpus based on public domain audio books. In Proceedings of ICASSP.   \nDaniel S. Park, William Chan, Yu Zhang, ChungCheng Chiu, Barret Zoph, Ekin Dogus Cubuk, and Quoc V. Le. 2019. Specaugment: A simple data augmentation method for automatic speech recognition. In Proceedings of Interspeech, pages 2613–2617.   \nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. 2015. Fitnets: Hints for thin deep nets. In Proceedings of ICLR.   \nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of ACL.   \nMatthias Sperber, Graham Neubig, Jan Niehues, and Alex Waibel. 2017. Neural lattice-to-sequence models for uncertain inputs. In Proceedings of EMNLP, pages 1380–1389.   \nMatthias Sperber, Graham Neubig, Jan Niehues, and Alex Waibel. 2019a. Attention-passing models for robust and data-efficient end-to-end speech translation. Transactions of the Association for Computational Linguistics, 7:313–325.   \nMatthias Sperber, Graham Neubig, Ngoc-Quan Pham, and Alex Waibel. 2019b. Self-attentional models for lattice inputs. In Proceedings of ACL 2019, pages 1185–1197.   \nXu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and TieYan Liu. 2019. Multilingual neural machine translation with knowledge distillation. In Proceedings of ICLR.   \nZhengkun Tian, Jiangyan Yi, Jianhua Tao, Ye Bai, Shuai Zhang, and Zhengqi Wen. 2020. Spiketriggered non-autoregressive transformer for end-toend speech recognition. In Proceedings of Interspeech.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of NeurIPS, pages 5998– 6008.   \nA. Waibel and C. Fugen. 2008. Spoken language translation. IEEE Signal Processing Magazine, 25(3):70– 79.   \nChengyi Wang, Yu Wu, Shujie Liu, Zhenglu Yang, and Ming Zhou. 2020a. Bridging the gap between pretraining and fine-tuning for end-to-end speech translation. In Proceedings of AAAI, 34(5):9161–9168.   \nChengyi Wang, Yu Wu, Shujie Liu, Ming Zhou, and Zhenglu Yang. 2020b. Curriculum pre-training for end-to-end speech translation. In Proceedings of ACL, pages 3728–3738.   \nRon J Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, and Zhifeng Chen. 2017. Sequence-tosequence models can directly translate foreign speech. In Proceedings of Interspeech.   \nChenglin Yang, Lingxi Xie, Siyuan Qiao, and Alan Yuille. 2018. Knowledge distillation in generations: More tolerant teachers educate better students. In Proceedings of CVPR.   \nCheng Yi, Feng Wang, and Bo Xu. 2019. Ectc-docd: An end-to-end structure with ctc encoder and ocd decoder for speech recognition. In Proceedings of Interspeech, pages 4420–4424.   \nJunho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. 2017. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In Proceedings of CVPR, pages 4133–4141.   \nChengqing Zong, Taiyi Huang, and Bo Xu. 1999. The technical analysis on automatic spoken language translation systems (in chinese). In Journal of Chinese Information Processing.", "reference_length": 8661, "reference_token": 2470, "txt_length": 38776, "txt_token": 9468, "txt": "# Bridging the Modality Gap for Speech-to-Text Translation  \n\nYuchen Liu1,2, Junnan ${\\mathbf{Z}}{\\mathbf{h}}{\\mathbf{u}}^{1,2}$ , Jiajun Zhang1,2, and Chengqing Zong1,2,3 1 National Laboratory of Pattern Recognition, Institute of Automation, CAS 2School of Artificial Intelligence, University of Chinese Academy of Sciences 3CAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai, China {yuchen.liu, junnan.zhu, jjzhang, cqzong}@nlpr.ia.ac.cn  \n\n# Abstract  \n\nEnd-to-end speech translation aims to translate speech in one language into text in another language via an end-to-end way. Most existing methods employ an encoder-decoder structure with a single encoder to learn acoustic representation and semantic information simultaneously, which ignores the speech-andtext modality differences and makes the encoder overloaded, leading to great difficulty in learning such a model. To address these issues, we propose a Speech-to-Text Adaptation for Speech Translation (STAST) model which aims to improve the end-to-end model performance by bridging the modality gap between speech and text. Specifically, we decouple the speech translation encoder into three parts and introduce a shrink mechanism to match the length of speech representation with that of the corresponding text transcription. To obtain better semantic representation, we completely integrate a text-based translation model into the STAST so that two tasks can be trained in the same latent space. Furthermore, we introduce a cross-modal adaptation method to close the distance between speech and text representation. Experimental results on English-French and English-German speech translation corpora have shown that our model significantly outperforms strong baselines, and achieves the new state-of-the-art performance.  \n\n# 1 Introduction  \n\nSpeech-to-Text translation (ST) aims at translating speech in one language into text in another language, which can be widely applied to conference speech, cross-border service, international business talk, academic forum, etc. Most existing approaches to speech-to-text translation are based on the pipeline paradigm, which first transforms the speech into text via an automatic speech recognition (ASR) system and then translates the transcribed text into the target language by a text-based machine translation model (Ney, 1999; Kanthak et al., 2005; Mathias and Byrne, 2006).  \n\nRecently, the end-to-end speech-to-text translation model has attracted more attention due to its advantages over the pipeline paradigm, such as low latency, alleviation of error propagation, and fewer parameters (Weiss et al., 2017; B´erard et al., 2018; Bansal et al., 2018; Jia et al., 2019; Sperber et al., 2019a). Previous studies employ an encoder-decoder structure to directly learn the mapping relationship between the speech input and text sequences in the target language. However, modality differences between speech and text result in the difficulty of training such a model, making its performance usually much inferior to the corresponding text-based neural machine translation (NMT) model.  \n\nWe observe that there are three main modality differences between speech and text which affect the speech representational capacity. First, the length of frame-level speech features is much longer than that of the corresponding text data, which hinders the model from learning alignments between the input and the output sequence. Second, text data are generally digitalized into trainable embeddings, while speech features are calculated by hand-crafted fliters and fixed during training, resulting in a lack of semantic information. Third, text data contain less noise with low uncertainty, while the speech is variational and easily affected by factors like the speech rate of the different speaker, silence, and noise, which leads to the poor robustness of the model. To make matter worse, the encoder in the ST model is overloaded since it requires to both learn acoustic information and extract semantic knowledge from the speech input simultaneously.  \n\nIn order to release the burden of speech translation encoder and help it learn better representation, the text representation learned by the NMT model can be utilized as a guidance for speech representation learning. To achieve that, we propose a Speechto-Text Adaptation for Speech Translation (STAST) model which aims to improve the end-to-end model performance by bridging the modality gap between speech and text. Specifically, the representations of two modalities need to be consistent in terms of length and share in the same latent space. Therefore, we decouple speech translation encoder into three parts, including an acoustic encoder, a shrink mechanism, and a semantic encoder. The shrink mechanism can fliter redundant hidden states based on the spike-like label posterior probabilities generated by the CTC module, which has the ability to solve length inconsistency problems. To ensure the representation of two modalities being in the same semantic latent space, we apply the multitask learning method and completely integrate the MT model into the ST model by sharing the semantic encoder and decoder. To further close the distance of two representations, we apply a crossmodal adaptation method, which can afford speech representation more semantic information. Experimental results on two speech translation corpora have illustrated the effectiveness of our proposed model.  \n\nThe contributions of this paper are as follows:  \n\n• We propose the STAST model which can improve speech translation performance by bridging the modality gap between speech and text.   \n• Experimental results have demonstrated that our proposed method outperforms existing methods and achieves the new state-of-the-art performance.   \n• Our method can easily leverage extra data and has shown especially superiority in lowresource scenarios.  \n\n# 2 Related Work  \n\nSpeech Translation. Traditional studies on speech translation are based on the pipeline paradigm which consists of an ASR model and an MT model (Ney, 1999; Kanthak et al., 2005; Mathias and Byrne, 2006). Focusing on how to improve the translation robustness on spoken language domain and combine the separate ASR and MT models, previous studies propose lattice-to-sequence models, synthetic data augment, and domain adaptation techniques (Lavie et al., 1996; Waibel and Fugen, 2008; Sperber et al., 2017, 2019b).  \n\nRecently, studies based on the end-to-end paradigm have emerged rapidly due to its advantages, such as lower latency, alleviation of error propagation, and fewer parameters. Zong et al. (1999) presume that it is possible to implement such end-to-end speech translation with the development of memory, computation speed, and representation methods. Then B´erard et al. (2016) give the first proof of the potential for an end-to-end ST model. Since then, pre-training, multitask learning, attention-passing, and knowledge distillation have been applied to improve the end-to-end model performance (Anastasopoulos et al., 2016; Duong et al., 2016; Weiss et al., 2017; Be´rard et al., 2018; Sperber et al., 2019a; Liu et al., 2019; Jia et al., 2019; Liu et al., 2020).  \n\nConsidering the difficulty of modeling this crossmodal cross-lingual task in a single model, recent studies propose novel model structures or auxiliary tasks to enhance the model capability. Wang et al. (2020a) propose TCEN model which connects speech encoder and NMT encoder in tandem. This model aims to bridge the gap between pre-training and fine-tuning by reusing every subnet. Considering the length inconsistency between speech encoder outputs and word embeddings, they lengthen the source text sentence by adding word repetitions and blank tokens to mimic the CTC output sequence. However, this process needs to train an extra sequence-to-sequence model and introduces much noise to the NMT model. They later propose a curriculum pre-training method which integrates two elementary courses to enable the encoder to understand the meaning of a sentence and map words in different languages (Wang et al., 2020b). However, it conducts the force-alignment between the speech and the source word as well as the source-to-target word alignment which needs to train an extra ASR model and may introduce alignment errors. In total, these studies do not well solve the modality gap between speech and text. Consequently, much valuable semantic information learned by text-based MT model cannot be applied to the ST model, which limits the latter performance.  \n\nCross-modal Adaptation. The paradigm of minimizing the difference between two models can be adopted in the transfer learning, where the knowledge embedded in one model can be transferred to another model, including output probabilities (Hinton et al., 2015; Freitag et al., 2017), hidden representations (Yim et al., 2017; Romero et al., 2015), and generated sequences (Kim and Rush, 2016). Such scheme has been applied in a variety of tasks, such as image classification (Hinton et al., 2015; Li et al., 2017; Yang et al., 2018; Anil et al., 2018), speech recognition (Hinton et al., 2015) and natural language processing (Freitag et al., 2017; Kim and Rush, 2016; Tan et al., 2019). Some related studies have attempted to transfer knowledge from the text model to the speech model by crossmodal adaptation. Cho et al. (2020) and Denisov and Vu (2020) adopts this method on spoken language understanding task, while Liu et al. (2019) apply it to speech translation task. However, they only transfer the knowledge from the output of the last model layer, ignoring the representation gap between speech and text modality.  \n\n# 3 Method  \n\nIn this section, we first introduce the architecture of our proposed STAST model. Focusing on bridging the representation gap between speech and text modality, it decouples the encoder into three parts to transcribe the speech and extract semantic representation separately. To further close the semantic gap between two modalities, we apply a crossmodal adaptation method. Finally, we will give the training strategy for this task.  \n\n# 3.1 Problem Formulation  \n\nThe speech translation corpus usually includes triplets of speech, transcription, and translation, denotes as $\\begin{array}{c c l}{{\\mathcal D_{S T}}}&{{=}}&{{\\{(s,x,y)\\}}}\\end{array}$ , where $s=$ $\\left[s_{1},\\cdots,s_{T_{s}}\\right]$ is a sequence of speech features which are converted from the speech signals, $\\textbf{\\em x}=$ $\\left[x_{1},\\cdots\\,,x_{T_{x}}\\right]$ is the corresponding transcription in source language, and $\\pmb{{y}}\\,=\\,[y_{1},\\cdots\\,,y_{T_{y}}]$ denotes the translation in target language. $T_{s},T_{x},T_{y}$ are the length of speech features, transcription and translation, respectively, where $T_{s}\\gg T_{x}$ . An extra ASR dataset $\\mathcal{D}_{A S R}=\\{(s^{\\prime},x^{\\prime})\\}$ can be leveraged for pre-training the ST model.  \n\n# 3.2 Model Architecture  \n\nSTAST model adopts the encoder-decoder framework, as shown in Figure 1. Both the encoder and decoder adopt Transformer (Vaswani et al., 2017) as the basic model structure since its superior property. Compared with the encoder in the ASR model which only needs to learn acoustic knowledge, the encoder in the end-to-end speech translation models is overloaded which requires to learn both acoustic and semantic knowledge of the source speech. To release its burden, we decouple the ST encoder into three parts, i.e. an acoustic encoder concatenated by a shrink mechanism, and a semantic encoder. Specifically, the acoustic encoder adopts a CTC module to learn speech representation and to predict the source transcription, which plays the role of an ASR model. To ensure the length of speech representation and text representations being consistent, a shrink mechanism is applied on the output of the acoustic encoder to fliter redundant states based on the spike-like label posterior probabilities which are generated by the CTC module. This process can significantly reduce the length of speech input, as well as reserve most of the meaningful information in the source speech. Then the semantic encoder encodes the hidden states corresponding to the non-redundant positions to obtain better semantic representation, based on which decoder with an attention module generates the final translation. We introduce each part in detail as follows.  \n\nAcoustic Encoder. Since we decouple the ST encoder to executive different functions, the acoustic encoder here is mainly used to learn acoustic knowledge. It takes as input the sequence of speech features $\\pmb{s}$ . For speech inputs, we first employ a speech pre-net to extract speech features, which is a linear layer here. The feature dimension is converted into the model hidden size $d_{\\mathrm{model}}$ . Then the acoustic representation is extracted by multiple stacked self-attention layers. The above process can be formalized as :  \n\n$$\n\\begin{array}{r l}&{\\tilde{\\pmb{s}}=\\mathrm{Pre}.\\mathrm{Net}(\\pmb{s})}\\\\ &{\\pmb{h}=\\mathrm{Acoustic.Encoder}(\\tilde{\\pmb{s}})}\\\\ &{~~~=\\mathrm{FFN}(\\mathrm{Self}.\\mathrm{Attention}(\\tilde{\\pmb{s}}))}\\end{array}\n$$  \n\nCTC Module. To predict source transcriptions, we adopt the Connectionist Temporal Classification (CTC) (Graves et al., 2006) module on the output of the acoustic encoder. Given the hidden states $^h$ generated by the acoustic encoder, a softmax classification layer is applied to predict a CTC path $\\pi=\\left[\\pi_{1},\\pi_{2},\\cdot\\cdot\\cdot\\,,\\pi_{T_{s}}\\right]$ , where $\\pi_{t}\\in\\mathbb{V}\\cup\\mathbf{\\hat{\\mu}}_{-}^{\\star\\star}$ denotes the label predicted by softmax layer at each step $t,\\,\\mathbb{V}$ is the vocabulary and “-” is a blank label. Then the distribution over a CTC path $\\pi$ can be calculated as the probability of a sequence of conditional independent outputs:  \n\n![](images/5ab70ce3c05f3cfc724c737c363ebf4b8c25c4784cff782800f4441042dcf16e.jpg)  \nFigure 1: Overview of our proposed STAST model. The speech translation encoder is decoupled into three parts, including (a) acoustic encoder, (b) shrink mechanism, and (c) semantic encoder, where (c) semantic encoder and (d) decoder form the integrated NMT model. Black dotted lines denote that process is only used during training. For the sake of clarity, residual connection and layer normalization are not shown.  \n\n$$\np(\\pi|s)=\\prod_{t=1}^{\\mathrm{T}_{s}}p(\\pi_{t}|s)=\\prod_{t=1}^{\\mathrm{T}_{s}}\\mathrm{softmax}(W_{\\mathrm{ctc}}^{T}\\times h_{t})\n$$  \n\n$$\n\\mathcal{L}_{\\mathrm{CTC}}=-\\sum_{(s,\\pmb{x})\\in\\mathcal{D}}\\log p_{\\mathrm{ctc}}(\\pmb{x}|\\pmb{s})\n$$  \n\nwhere $W_{\\mathrm{ctc}}\\in\\mathbb{R}^{d\\times(|\\mathbb{V}|+1)}$ is a trainable matrix in the classification layer.  \n\nNote that the CTC path $\\pi$ is a many-to-one mapping of the source transcription $\\textbf{\\em x}$ by allowing occurrences of blank or consecutively repeated label. For example, let $\\mathbb{B}$ denotes the mapping from CTC paths to the transcription sequence, then $\\mathbb{B}(a a\\mathrm{~-~}a b-)\\;=\\;B(a\\mathrm{~-~}a b b-)\\;=\\;a a b$ , where $\\pi_{1}\\;=\\;a a\\mathrm{~-~}a b-$ and $\\pi_{2}\\;=\\;a\\mathrm{~-~}a b b-$ . There exists many legal CTC paths for each transcription $\\textbf{\\em x}$ . Therefore, the conditional probability of each transcription $\\textbf{\\em x}$ can be modeled by summing all the paths corresponding to it.  \n\n$$\np_{c t c}(\\pmb{x}|\\pmb{s})=\\sum_{\\pmb{\\pi}\\in\\mathbb{B}^{-1}(\\pmb{x})}p(\\pmb{\\pi}|\\pmb{s})\n$$  \n\nwhere $\\mathbb{B}^{-1}(\\pmb{x})$ denotes the set of all legal CTC paths corresponding to a transcription $\\textbf{\\em x}$ . Finally, the objective training function of CTC loss is defined as,  \n\nShrink Mechanism. Note that the length of CTC output is the same as the input speech feature, which is still much larger than that of the corresponding source text. To bridge the length gap between speech and text representation, we apply a shrink mechanism.  \n\nAs mentioned above, CTC paths are variation of the source transcription by allowing occurrences of blank tokens and repetitions. However, blank and repeated tokens do not contain any useful information and hinder the linguistics modeling. We assume that the triggered encode state sequence contains prior information of original speech input. Therefore, we only extract the encoded states $^h$ in the acoustic encoder which corresponds to the CTC spike, as shown in the middle part of Figure 1. Specifically, we treat the label which has the largest value after the softmax layer in each state $h_{t}$ as the predicted label. Then, we only extract the hidden state whose label does not correspond to the blank label or consecutively repeated label and mask other states, which is similar with Yi et al. (2019) and Tian et al. (2020). Note that this process does not affect the back propagation of gradient. Then the shrunk hidden state can be formalized as follows, where Idx denotes the corresponding index of token in the vocabulary.  \n\n$$\n\\tilde{\\boldsymbol{h}}=[h_{i}\\in\\boldsymbol{h}|\\operatorname*{arg\\,max}_{\\mathrm{Idx}}\\;\\mathrm{softmax}(\\boldsymbol{W}_{\\mathrm{ctc}}^{T}\\times\\boldsymbol{h}_{i})\\neq\\mathrm{Idx}(\\boldsymbol{^{\\alpha}}-\\boldsymbol{^{\\circ}})]\n$$  \n\nSemantic Encoder and Decoder. The shrunk hidden state contains acoustic information but still lacks semantic knowledge. To obtain better semantic representation, we apply semantic encoder to encode the shrunk hidden states by another multiple stacked self-attention layers.  \n\nthe source transcription $\\textbf{\\em x}$ is first embedded into word representation by looking up the embedding weight $W_{s}$ . Then semantic encoder extracts highlevel semantic text representation $h_{x}$ , based on which the decoder performs translation task. To map speech representation and text representation into the same semantic space, we share the parameters of semantic encoder and decoder. Meanwhile, we also share the weight $W_{c t c}$ of softmax layer in acoustic encoder with the source word embedding weight and the weight $W_{t}$ of the decoder softmax layer to constrain the space gap, which means $W_{c t c}\\,=\\,W_{s}\\,=\\,W_{t}$ . Then the objective function of the MT task can be calculated by the cross-entropy loss as follow.  \n\n$$\nh_{s}=\\mathrm{FFN}(\\mathrm{Self.Attention}(\\tilde{h}))\n$$  \n\nThe decoder also follow the basic network structure of Transformer, it first adopts self-attention layers on target embeddings and then attends to the output of semantic encoder by cross-attention layers. Followed by feed-forward layer, the target token is predicted through a softmax layer based on the output of the decoder $h_{d}$ . The above process can be formalized as follows:  \n\n$$\n\\begin{array}{r l}&{h_{y}=\\!\\!\\mathrm{Embedding}(y)}\\\\ &{h_{d}=\\!\\!\\mathrm{FFN}(\\mathrm{Cross.Attention}(}\\\\ &{\\qquad\\mathrm{Self.Attention}(h_{y}),h_{s}))}\\end{array}\n$$  \n\nFinally, the distribution probability over a sequence of target tokens is calculated.  \n\n$$\np(\\pmb{y}|\\pmb{s})=\\mathrm{softmax}(\\pmb{W}_{t}^{T}\\times\\pmb{h}_{d})\n$$  \n\nwhere $W_{t}\\,\\in\\,\\mathbb{R}^{d\\times(|\\mathbb{V}|+1)}$ is the trainable weight matrix in the softmax layer which is shared with $W_{c t c}$ in CTC module. Then objective function of ST task can be calculated by the cross-entropy loss as follow.  \n\n$$\n\\mathcal{L}_{S T}=-\\sum_{(s,y)\\in\\mathcal{D}}\\log p(y|s)\n$$  \n\nIntegrated NMT Model. Since the NMT model has the same structure with the combination of semantic encoder and decoder, we integrate the whole NMT model into STAST. In NMT model,  \n\n$$\n\\mathcal{L}_{M T}=-\\sum_{(\\pmb{x},\\pmb{y})\\in\\mathcal{D}}\\log p(\\pmb{y}|\\pmb{x})\n$$  \n\n# 3.3 Cross-Modal Adaption  \n\nTo further make the representation of speech and text modality closer, we propose a cross-modal adaptation method. Specifically, the cross-modal adaptation method is applied to transfer the semantic knowledge from text representation to speech representation. For each utterance, the semantic encoder encodes the shrunk output of acoustic encoder into semantic speech representation $h_{s}$ and encodes word embeddings into text representation $h_{x}$ , respectively. We take advantages of text representation $h_{x}$ as a regulation to constrain the space of speech representation $h_{s}$ during training. The supervision is implemented by minimizing the distance between two representations. We propose two adaptation methods, including sentence-level adaptation and word-level adaptation. The loss function can be calculated as:  \n\nsequence-level  \n\n$$\n\\mathcal{L}_{\\mathrm{AD}}=\\left\\{\\begin{array}{l l}{\\sum_{(s,x)\\in\\mathcal{D}}\\mathrm{MSE}(\\overline{{h}}_{s},\\overline{{h}}_{x})}\\\\ {\\sum_{(s,x)\\in\\mathcal{D}}\\mathrm{MSE}(h_{s},h_{x})}\\end{array}\\right.\n$$  \n\nwhere MSE is mean-squared error loss function used to evaluate the difference between the representation of speech and text, $\\overline{{h}}_{s}$ and $\\overline{{h}}_{x}$ are the average of two contextual representations.  \n\n# 3.4 Training Process  \n\nThe final training objective function is the sum of four parts, including the CTC loss $L_{c t c}$ , the crossentropy loss for ST task $L_{S T}$ , the cross-entropy loss  \n\nfor MT task $L_{M T}$ , and the cross-modal adaptation loss $L_{A D}$ :  \n\n$$\n\\mathcal{L}_{S T}=\\alpha\\mathcal{L}_{C T C}+\\beta\\mathcal{L}_{S T}+\\gamma\\mathcal{L}_{M T}+\\eta\\mathcal{L}_{A D}\n$$  \n\nwhere $\\alpha,\\,\\beta,\\,\\gamma$ , and $\\eta$ are hyper-parameters, which denote the weight of each loss.  \n\nFor training strategy, we first train the acoustic encoder by speech-transcription pairs $(s,x)$ in the ST corpus $\\mathcal{D}_{S T}$ . Then we use speech-transcriptiontranslation triplets $(s,x,y)$ to train the ST model and the NMT model by a multi-task learning framework, where transcriptions are only used during training. The module in our model is very flexible, where the CTC module and the integrated NMT model can perform auxiliary task to obtain better optimized parameters. Therefore, it can be easily trained by extra data, such as the part of acoustic encoder can be trained by extra ASR corpus to obtain better acoustic representation.  \n\n# 4 Experiments  \n\n# 4.1 Datasets  \n\nWe conduct experiments on two public speech translation datasets, including Augmented LibriSpeech English-French Corpus (Kocabiyikoglu et al., 2018) and the MuST C English-German TED Corpus (Gangi et al., 2019b).  \n\nAugmented LibriSpeech En-Fr. This corpus is a subset of the LibriSpeech ASR corpus (Panayotov et al., 2015), which is automatically aligned with e-books in French. This corpus is from clean audiobooks, which contains quadruplets, including English audios, manual transcriptions, French translations, and the translations obtained by Google Translate. The total audio contains 236 hours of speech. Following previous works (Wang et al., 2020a), we only use the 100-hour clean training set and concatenate the aligned references with the provided Google translations, resulting in 90K utterances. We validate on the development set (1,071 utterances) and report the model performance on the test set (2,048 utterances). For pre-training, we use the total LibriSpeech ASR corpus as extra data, which includes 960 hours of speech.  \n\nMuST C En-De. The MuST C corpus is collected from TED talks1, which includes the English speech, the corresponding transcription, and the target translations in different languages. We conduct experiments on English-German language direction. The speech in this corpus is recorded from the live presentation which contains more noise. The corpus contains a total of 408-hour speech with 234K translation pairs, which is divided into training set (400 hours with 229,703 utterances), development set, and test set. We report case-sensitive BLEU on the dev set (1,423 utterances) and tstCOMMON set (2,641 utterances).  \n\n# 4.2 Experimental Settings  \n\nThe speech features are 80-dimensional log-Mel filterbanks extracted with a step size of 10ms and window size of $25\\mathrm{ms}$ , which are extended with mean subtraction and variance normalization. We adopt dimensionality reduction to downsample one frame every three frames.  \n\nFor text data, we apply lowercase, punctuation normalization, and tokenization by Moses scripts2. Punctuations in English transcriptions are removed. We apply the BPE method (Sennrich et al., 2016) on the combination of source and target text to obtain shared subword vocabulary. The number of merge operations in BPE is set to 8K for both tasks. In order to be comparable with other works, we employ case-insensitive BLEU computed using multi-bleu.pl script3 as the evaluation of the translation task.  \n\nWe use the base configuration of original Transformer (Vaswani et al., 2017), where the number of self-attention layers in acoustic encoder, semantic encoder, and decoder is 6 with 512-dimensional hidden sizes4, the filter size in feed-forward layer $d_{f f}\\,=\\,2,048$ , the residual dropout and attention dropout are 0.1. We set $\\alpha$ , $\\beta,\\gamma,$ and $\\eta$ in Equation 14 to 1.0, 1.0, 1.0, and 1.0 respectively. Samples are batched by approximate sequence length of 10,000- frame features. The STAST model is trained by Adam optimizer (Kingma and Ba, 2015) on one GPU. We save checkpoints every 1,000 steps and conduct the model average on the last 5 checkpoints as the final model. For inference, we perform beam search with a beam size of 4. Our code will be released after reviews.  \n\n# 5 Experimental Results  \n\n# 5.1 Main Results  \n\nResults on Augmented LibriSpeech. Following previous work (Wang et al., 2020b), we have two settings in this experiment. In the Base setting, we only use the data in the Augmented LibriSpeech corpus. Regarding the Expended setting, we use the 960-hour LibriSpeech ASR corpus as extra data to pre-train the acoustic encoder. Table 1 presents the results of MT models, pipeline systems, and end-to-end ST models in both settings.  \n\nComparison with pipeline ST systems. We compare our end-to-end STAST model with text-based MT models and pipeline ST systems. The textbased MT model takes manual transcriptions as input, so its result can be regarded as the upper bound of the speech translation task. For the pipeline system, the ASR model and the NMT model are trained only by data in the ST corpus. The final translation is generated based on the output of the ASR model. As shown in Table 1, in the base setting our STAST model can achieve comparable or even better results than pipeline systems under the same data scale. However, STAST only uses a single model to combine ASR and MT tasks, which has much fewer parameters and computational cost than pipeline systems. When more ASR data is available, our method outperforms the best pipeline system by 0.9 BLEU and is slightly worse than text-based MT models. It indicates that with the use of extra ASR corpus, our method can obtain more acoustic information, which has beneftis of extracting semantic knowledge and achieving better performances.  \n\nComparison with end-to-end models. As shown in Table 1, our model outperforms all the previous end-to-end models and achieves the new state-ofthe-art performance. Specifically, in the base setting our proposed model is much better than the LSMT-based ST model (B´erard et al., 2018) and outperforms it by $4\\sim5$ BLEU scores. Our model is also better than a widely used toolkit ESPnet by 1.1 BLEU, whose encoder and decoder are both pre-trained. Compared with Liu et al. (2019) who utilize an NMT model to teach an ST model on the probability of decoder output, our method achieves 0.8 better BLEU scores. We believe this is because their method only adopts knowledge distillation on the last output layer of model, which is hard to propagate the gradient back to the preceding networks. However, our method adopts cross-modal adaptation on speech and text representation, giving the model more guidance to bridge the gap between two modalities. STAST is also better than TCEN-LSTM proposed by Wang et al. (2020a). Compared with their method which requires to transform normal source sentences to noisy sentences in CTC path format, our method solves length inconsistency problem more effectively. Besides, the speech representation can obtain better semantic information with the cross-modal adaptation. Our method is also superior to the best previous model proposed by Wang et al. (2020b). In the expanded setting, with extra ASR data, our method improves the translation performance by 0.9 BLEU compared with that in the base setting. It also outperforms other previous end-to-end models by a large margin, which indicates the effectiveness of our method.  \n\nTable 1: BLEU results on the Augmented LibriSpeech En-Fr corpus.   \n\n\n<html><body><table><thead><tr><td><b>Model</b></td><td><b>Method</b></td><td><b>BLEU</b></td></tr></thead><tbody><tr><td>MT</td><td>LSTM(Bérard et al., 2018) ESPnet(Inaguma et al., 2020) Transformer(Liu et al., 2019)</td><td>19.30 18.30 22.05</td></tr><tr><td> Pipeline</td><td>Transformer(Liu et al., 2019) LSTM(Bérard et al., 2018) ESPnet (Inaguma et al., 2020) Transformer(Liu et al., 2019) LSTM(Bérard et al., 2018)</td><td>14.60 16.96 17.85</td></tr><tr><td>E2E Base</td><td>+pre-train+multitask ESPnet (Inaguma et al., 2020) Transformer (Liu et al., 2019) +decoder KD TCEN-LSTM (Wang et al., 2020a) Transformer (Wang et al., 2020b) +curriculum STAST (ours)</td><td>16.70 14.30 17.02 17.05 15.97 17.66 13.40</td></tr><tr><td>E2E Expanded</td><td>STAST (ours) LSTM(Bahar et al., 2019) Multilingual(Inaguma et al., 2020) Transformer(Wang et al., 2020b) +curriculum learning STAST (ours)</td><td>17.00 17.60 16.90 18.01 18.74 17.81</td></tr></tbody></table></body></html>  \n\nResults on MuST C English-German. The results on MuST C English-German corpus are listed in Table 2. Compared with the Augmented LibriSpeech corpus, the performance gap between textbased MT models and pipeline systems is much larger. This can be attributed to that the speech in this corpus is recorded from live presentations, which contains more noise, such as laughter, applause, and cheers, besides that the segment of this corpus is also noisy. As seen in Table 2, STAST model outperforms all of the previous end-to-end models in the base setting. We also find that the STAST model is slightly better than the pipeline system we implement but worse than the best system, which indicates the end-to-end ST model still lacks robustness to the noisy input. It is challenging to handle the speech under noisy circumstances and needs to design more powerful ST models.  \n\n<html><body><table><thead><tr><td><b>Model</b></td><td><b>Method</b></td><td><b>BLEU</b></td></tr></thead><tbody><tr><td rowspan=\"2\">MT</td><td>Transformer(Gangi et al., 2019a)</td><td>25.30</td></tr><tr><td>ESPnet (Inaguma et al., 2020)</td><td>30.16</td></tr><tr><td rowspan=\"4\">Pipeline</td><td>Transformer(Gangi et al., 2019a)</td><td>18.50</td></tr><tr><td>Transformer (Indurthi et al., 2020)</td><td>20.86</td></tr><tr><td>ESPnet (Inaguma et al., 2020)</td><td>23.65</td></tr><tr><td>Transformer (ours)</td><td>23.00</td></tr><tr><td rowspan=\"8\">E2E Base</td><td>Transformer(Gangi et al., 2019a)</td><td>17.30</td></tr><tr><td>Transformer (Indurthi et al., 2020)</td><td>15.60</td></tr><tr><td>+ Meta-Learning</td><td>22.11</td></tr><tr><td>ESPnet (Inaguma et al., 2020)</td><td>22.33</td></tr><tr><td>+ SpecAugment</td><td>22.91</td></tr><tr><td>STAST (ours)</td><td>22.55</td></tr><tr><td>+ SpecAugment?</td><td>23.06</td></tr></tbody></table></body></html>  \n\nTable 2: BLEU results on MuST C En-De.   \n\n\n<html><body><table><thead><tr><td><b>Methods</b></td><td><b>LibriSpeech</b></td><td><b>MuST.C</b></td></tr></thead><tbody><tr><td>STAST</td><td>18.74</td><td>22.55</td></tr><tr><td>- Cross-modal Adaptation</td><td>18.33</td><td>21.88</td></tr><tr><td>- Multi-task</td><td>18.17</td><td>21.25</td></tr><tr><td>- Semantic Encoder</td><td>16.91</td><td>19.94</td></tr><tr><td>- Shrink Mechanism</td><td>16.55</td><td>19.10</td></tr><tr><td>- CTC Loss</td><td>16.19</td><td>18.84</td></tr></tbody></table></body></html>\n\nTable 3: Ablation Studies on the test set of Augmented LibriSpeech En-Fr and MuST C En-De.  \n\n# 5.2 Ablation Studies  \n\nTo better evaluate the contribution of different proposed methods, we perform ablation studies on both corpora. The results in Table 3 show that all of the proposed methods have positive effects, and the benefits of these methods are accumulative. The performance drops if we remove the cross-modal adaption method. Without multi-task learning, the performance further drops 0.2 and 0.6 BLEU scores. It indicates that both cross-modal adaption and multi-task learning have the ability to transform the latent space of speech representation to text representation closely. With the guidance from the text-based NMT model, the ST model can learn more semantic knowledge and obtain better performance. We find if the shrunk output from the acoustic encoder is directly fed into decoder without the semantic encoder, the translation performance drops significantly. This means decoupling the ST encoder is necessary and the semantic encoder supplements the model with more capability to learn semantic information. If we further remove the shrink mechanism, the performance decreases by 0.4 and 0.8 BLEU scores on two corpora, which proves that blank and repeated tokens hinder the alignment learning between output sequence and input sequence. The shrink mechanism has the ability to bridge the length gap between speech and text representation and is indispensable for crossmodal adaptation. The discussion in detail will be depicted in Section 5.3. Finally, the CTC loss as an auxiliary loss is also beneficial, which is consistent with previous work in the ASR fields that CTC loss has the benefti of accelerating convergence and achieving better performance (Kim et al., 2017).  \n\n![](images/60ba2ad84f50a85c507933e87c1101d4648fc864f11d2488c26df2609d63193d.jpg)  \nFigure 2: The analysis of the length after shrink mechanism, which shows the difference between the length of CTC output after shrink mechanism and that of the source transcription on the training set of Augmented LibriSpeech corpus.  \n\nTable 4: BLEU results on the development and test set of Augmented LibriSpeech En-Fr with sequence-level adaptation and word-level adaptation.   \n\n\n<html><body><table><thead><tr><td><b>KD Method</b></td><td><b>Dev</b></td><td><b>Test</b></td></tr></thead><tbody><tr><td>Sequence-level</td><td>19.86</td><td>18.74</td></tr><tr><td>Word-level</td><td>19.48</td><td>18.47</td></tr></tbody></table></body></html>  \n\n# 5.3 Analyses  \n\nEffect of Shrink Mechanism on the CTC output. As shown in Figure 2, the histogram counts the difference between the length of the CTC output after shrink mechanism and that of the corresponding transcription on the training set of the Augmented LibriSpeech corpus. When the value is below zero, it means the length of shrunk hidden states is longer than that of transcription and vice versa. We find that the length of shrunk hidden states equals to that of the corresponding text transcription for $84.0\\%$ of data, and the difference is less than two for over $93.7\\%$ of data. Therefore, we conclude the shrink mechanism has the ability to solve length inconsistency problem. With more extra ASR data, we believe the accuracy of the shrunk length can be further improved.  \n\n![](images/50cc8ac1b7bb13b59d3acbbfcb9e3c32608380a3489fb2639419812dd7e87661.jpg)  \nFigure 3: The BLEU results of the baseline model, multi-task model and our proposed STAST model on MuST C En-De under different scales of training data, including 10 hours, 50 hours, 100 hours, and 400 hours.  \n\nSequence-Level Adaptation v.s. Word-Level Adaptation. We compare two different adaptation methods as mentioned in Section 3.3, i.e. sequence-level adaptation and word-level adaptation. We conduct experiments on the Augmented LibriSpeech corpus and the results are reported in Table 4. We find that both adaptation methods have improvements compared with baselines. However, the performance of sequence-level adaptation is slightly better. This is consistent with Aldarmaki and Diab (2019), where they find learning the aggregate mapping can yield a more optimal solution compared to word-level mapping.  \n\nEffect of Extra Text Data. The module in our proposed STAST model is flexible, which can easily leverage extra data. In Section 5.1, we have shown that extra ASR data can boost the model performance, here we analyze the effect of extra text data. To simulate lower-resource scenarios, we randomly select 10-hour, 50-hour, and 100-hour of speech training data (speechtranscription-translation triplets) from the MuST C En-De dataset, which originally contains 400-hour speech. Then, we train the end-to-end ST baseline model (a simple encoder-decoder model), multitask baseline model (the decoder of ST and MT is shared with separated encoders), and our STAST model under different scales of training data. The multi-task baseline model and our STAST model can have access to extra text data (transcriptiontranslation pairs in the original corpus). Figure 3 shows the BLEU scores of three models under different scales of training data. We find that with the increase of the data size, all of the three models can obtain improvements. However, the endto-end model is significantly inferior to the multitask model and the STAST model. With only 10 hours of training data, our proposed model can achieve comparable performance with the end-toend model trained on 100-hour data. The multitask model is better than the end-to-end model but worse than our proposed model. The reason is that the multi-task model only shares part of parameters for different tasks but leaves the valuable semantic information learned by the NMT encoder unexploited by the ST model. While our method integrates the NMT model into the ST model and transfers speech representation to text representation more closely by the cross-modal adaptation method, which can obtain better performance.  \n\n# 6 Conclusions  \n\nIn this paper, we propose the STAST model to improve end-to-end ST model. Considering the modality differences between speech and text, we propose ST encoder decoupling, length shrink mechanism, the NMT model integration, and crossmodal adaptation methods. Empirical studies have demonstrated that each proposed method has positive effects and the combination of them can achieve the new state-of-the-art result. In the future, we will explore how to effectively transfer more knowledge from the NMT model and the ST model. We also expect that the idea of bridging the representation gap between different modalities can be adopted on other tasks.", "appendix": ""}, {"title": "Statistical Phrase-Based Speech Translation", "authors": null, "bibkey": "statistical_phrase_based_speech_translation", "bibitem": "@article{Mathias_ICASSP2006,\n  url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ec418938314fb29156a9a0bc6db49038a19abb37},\n  title = {Statistical Phrase-Based Speech Translation}\n}", "url": "https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ec418938314fb29156a9a0bc6db49038a19abb37", "latex_url": null, "latex_path": null, "pdf_url": "https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ec418938314fb29156a9a0bc6db49038a19abb37", "pdf_path": "output/download_papers/document?repid=rep1&type=pdf&doi=ec418938314fb29156a9a0bc6db49038a19abb37/document?repid=rep1&type=pdf&doi=ec418938314fb29156a9a0bc6db49038a19abb37.pdf", "md_url": null, "latex_length": 0, "latex": "", "abstract": "A generative statistical model of speech-to-text translation is developed as an extension of existing models of phrase-based text translation. Speech is translated by mapping ASR word lattices to lattices of phrase sequences which are then translated using operations developed for text translation. Performance is reported on Chinese to English translation of Mandarin Broadcast News.", "abstract_length": 385, "abstract_token": 66, "introduction": "Statistical speech translation systems vary in the degree to which the Statistical Machine Translation (SMT) system and the automatic speech recognition (ASR) component are integrated within the overall translation process. In the ‘pipeline’ approach to speech translation, the transcription produced by ASR is translated as if it were any fluent, written sentence in the foreign language. This is a reasonable first approach to speech translation, and if ASR systems performed flawlessly, it would be perfectly adequate. However ASR systems are imperfect, and in imperfect statistical information processing systems it is generally desirable that initial processing procedures should pass on as much information as possible for use by subsequent stages. With this motivation, speech translation architectures have been developed within which the ASR and SMT systems are tightly coupled (e.g. [1, 2, 3]). The objective is to allow the SMT system to search among many likely ASR hypotheses and hopefully produce a better translation than if it had been restricted to the single, best ASR hypothesis. In practice, the close coupling of ASR and MT can be realized by translating ASR N-Best lists [4, 5] or word lattices [6, 7]. N-Best translation is straightforward: a text-based SMT system can be used without modification to translate each entry, and the resulting translations can be sorted by some combination of ASR and SMT scores. Although it is a complicated modeling and implementation problem, lattice-based translation offers potential advantages over translation of N-Best lists. Lattices provide larger search spaces, as well as detailed, sub-sentential information, such as word-level acoustic and language model scores, that can be passed directly to the SMT system. However it is not trivial to obtain gains in latticebased translation relative to simply translating the ASR transcription. Initial attempts at incorporating word lattice information in translation did not yield consistent improvements in translation performance [6]. However approaches were subsequently developed by which lattices and confusion networks can be translated with improvements in translation quality [7, 8]. Motivated by this prior work, we present a novel approach to statistical phrase-based speech translation. This approach is based on a generative, source-channel model of translation, similar in spirit to the modeling approaches that underly HMM-based ASR systems - in fact, our model of speech-to-text translation contains the acoustic models of a large vocabulary ASR system as one of its components. We develop this model of speech-to-text translation as a direct extension of the phrase-based models used in our text translation systems, and we will show how lattice-based speechto-text translation can be carried out easily, using the existing textbased translation systems essentially without modification. We begin with a review of the underlying phrase-based translation model, and then extend it to speech translation by incorporating the acoustic models from a target language ASR system.", "introduction_length": 3098, "introduction_token": 580, "reference": "# 5. REFERENCES  \n\n[1] E. Vidal, “Finite-state speech-to-speech translation,” in Proc. ICASSP, 1997.  \n\n[2] H. Ney, “Speech translation: Coupling of recognition and translation,” in Proc. ICASSP, 1999.   \n[3] Casacuberta et al., “Architectures for speech-to-speech translation using finite-state models,” in Proc. Workshop on Speech-to-Speech Translation, 2002.   \n[4] R. Zhang et al, “A unified approach in speech-to-speech translation: integrating features of speech recognition and machine translation,” in Proc. COLING, 2004.   \n[5] V.H. Quan et al., “Integrated N-best re-ranking for spoken language translation,” in In EuroSpeech, 2005.   \n[6] S.Saleem, S. C. Jou, S. Vogel, and T. Schultz, “Using word lattice information for a tighter coupling in speech translation systems,” in Proc. ICSLP, 2004.   \n[7] E. Matusov, S.Kanthak, and H. Ney, “On the integration of speech recognition and statistical machine translation,” in Proc. InterSpeech, 2005.   \n[8] N. Bertoldi and M. Federico, “A new decoder for spoken language translation based on confusion networks,” in IEEE ASRU Workshop, 2005.   \n[9] S. Kumar, Y. Deng, and W. Byrne, “A weighted finite state transducer translation template model for statistical machine translation,” J. Natural Language Engineering, vol. 11, no. 3, 2005.   \n[10] S. Kumar and W. Byrne, “Local phrase reordering models for statistical machine translation,” in Proc. of HLT-EMNLP, 2005.   \n[11] M. Mohri, F. Pereira, and M. Riley, “Weighted automata in text and speech processing,” in European Conference on Artificial Intelligence, 1996.   \n[12] M. Mohri, F. Pereira, and M. Riley, ATT Generalpurpose finite-state machine software tools, 2001, http://www.research.att.com/sw/tools/fsm/.   \n[13] F. Och, Statistical Machine Translation: From Single Word Models to Alignment Templates, Ph.D. thesis, RWTH Aachen, Germany, 2002.   \n[14] Y. Deng and W. Byrne, “HMM word and phrase alignment for statistical machine translation,” in Proc. of HLT-EMNLP, 2005.   \n[15] Y. Liu et al., “Structural metadata research in the EARS program,” in Proc. ICASSP, 2005.   \n[16] C. Allauzen, M. Mohri, and B. Roark, “Generalized algorithms for constructing statistical language models,” in 41st Meeting of the ACL, July 2003.   \n[17] S. Young et al., The HTK Book, Version 3.1, Dec. 2001.   \n[18] L. Chen, L. Lamel, and J.-L. Gauvain, “Transcribing Mandarin Broadcast News,” in IEEE ASRU Workshop, 2003.   \n[19] NIST, The NIST Machine Translation Evaluations, 2004, http://www.nist.gov/speech/tests/mt/.   \n[20] Y. Deng, S. Kumar, and W. Byrne, “Bitext chunk alignment for statistical machine translation,” J. Natural Language Engineering, Submitted.   \n[21] K. Papineni, S. Roukos, T. Ward, and W. Zhu, “BLEU: a method for automatic evaluation of machine translation,” Tech. Rep. RC22176(W0109-022), IBM Research, 2001.   \n[22] A. Stolcke, “SRILM – an extensible language modeling toolkit,” in Proc. ICSLP, 2002.", "reference_length": 2932, "reference_token": 858, "txt_length": 23163, "txt_token": 5767, "txt": "# STATISTICAL PHRASE-BASED SPEECH TRANSLATION  \n\nLambert Mathias 1 and William Byrne 1,2  \n\nCenter for Language and Speech Processing, The Johns Hopkins University, 3400 N. Charles Street, Baltimore, MD 21218, U.S.A. 1 Department of Engineering, Cambridge University Trumpington Street, Cambridge, CB2 1PZ, U.K. lambert@jhu.edu , wjb31@cam.ac.uk  \n\n# ABSTRACT  \n\nA generative statistical model of speech-to-text translation is developed as an extension of existing models of phrase-based text translation. Speech is translated by mapping ASR word lattices to lattices of phrase sequences which are then translated using operations developed for text translation. Performance is reported on Chinese to English translation of Mandarin Broadcast News.  \n\n# 1. INTRODUCTION  \n\nStatistical speech translation systems vary in the degree to which the Statistical Machine Translation (SMT) system and the automatic speech recognition (ASR) component are integrated within the overall translation process. In the ‘pipeline’ approach to speech translation, the transcription produced by ASR is translated as if it were any fluent, written sentence in the foreign language. This is a reasonable first approach to speech translation, and if ASR systems performed flawlessly, it would be perfectly adequate. However ASR systems are imperfect, and in imperfect statistical information processing systems it is generally desirable that initial processing procedures should pass on as much information as possible for use by subsequent stages.  \n\nWith this motivation, speech translation architectures have been developed within which the ASR and SMT systems are tightly coupled (e.g. [1, 2, 3]). The objective is to allow the SMT system to search among many likely ASR hypotheses and hopefully produce a better translation than if it had been restricted to the single, best ASR hypothesis. In practice, the close coupling of ASR and MT can be realized by translating ASR N-Best lists [4, 5] or word lattices [6, 7]. N-Best translation is straightforward: a text-based SMT system can be used without modification to translate each entry, and the resulting translations can be sorted by some combination of ASR and SMT scores.  \n\nAlthough it is a complicated modeling and implementation problem, lattice-based translation offers potential advantages over translation of N-Best lists. Lattices provide larger search spaces, as well as detailed, sub-sentential information, such as word-level acoustic and language model scores, that can be passed directly to the SMT system. However it is not trivial to obtain gains in latticebased translation relative to simply translating the ASR transcription. Initial attempts at incorporating word lattice information in translation did not yield consistent improvements in translation performance [6]. However approaches were subsequently developed by which lattices and confusion networks can be translated with improvements in translation quality [7, 8].  \n\nMotivated by this prior work, we present a novel approach to statistical phrase-based speech translation. This approach is based on a generative, source-channel model of translation, similar in spirit to the modeling approaches that underly HMM-based ASR systems - in fact, our model of speech-to-text translation contains the acoustic models of a large vocabulary ASR system as one of its components. We develop this model of speech-to-text translation as a direct extension of the phrase-based models used in our text translation systems, and we will show how lattice-based speechto-text translation can be carried out easily, using the existing textbased translation systems essentially without modification.  \n\nWe begin with a review of the underlying phrase-based translation model, and then extend it to speech translation by incorporating the acoustic models from a target language ASR system.  \n\n# 2. PHRASE-BASED GENERATIVE MODELS OF SPEECH TRANSLATION  \n\nThe Translation Template Model (TTM) [9, 10] is a generative model of translation that consists of a series of transformative operations specified by conditional probability distributions. A (simplified) description of the generative process has the following steps.  \n\nStep 1 The source language sentence $s_{1},\\ldots,s_{I}$ is generated by the Source Language Model, $P(s_{1}^{I})$ .   \nStep 2 The source language sentence is segmented into a series of source language phrases, $u_{1}^{K}$ . There are many possible sequences of phrases that can be derived from a single sentence, as defined by the Source Phrase Segmentation distribution, $P(u_{1}^{K},K|\\dot{s}_{1}^{I})$ .   \nStep 3 The sequences of source language phrases are translated into target language phrase sequences, $x_{1}^{K}$ . The target language phrases are generated in source language phrase order, and each was generated by a single source phrase. Since source language phrases can generate multiple target phrases, the generation of target phrase sequences is specified by the Phrase Translation distribution, $\\hat{P}(x_{1}^{K}|u_{1}^{K})$ .   \nStep 4 New target language phrases are allowed to insert themselves into the target language sequences which are then (optionally) reordered; the tendency towards phrase insertion is controlled by a single parameter, the Phrase Exclusion Probability. This generates modified target language  \n\nphrase sequences, $v_{1}^{R}$ , under the Phrase Movement and Insertion distribution, $P(v_{1}^{R}|x_{1}^{K},u_{1}^{K})$ .  \n\nStep 5 The target language phrase sequences are transformed to target language word sequences, $t_{1},\\ldots,t_{J}$ , under the Target Phrase Segmentation distribution, $P(t_{1}^{J}|v_{1}^{R})$ . In practice, this is a degenerate transformation which maps every target phrase sequence to its unique word sequence.  \n\nTaken together, these distributions form a joint probability distribution over the source and target language sentences, and over the possible intermediate source and target phrase sequences. Moreover, the component distributions are formulated so that each can be implemented as a Weighted Finite State Machine (WFSM) [11, 12]. The component distributions form $P(t_{1}^{J},v_{1}^{R},x_{1}^{\\dot{K}},u_{1}^{K},s_{1}^{\\dot{I}})$ as  \n\n$$\n\\begin{array}{r l r}&{}&{P(t_{1}^{J}|v_{1}^{R})\\ P(v_{1}^{R}|x_{1}^{K},u_{1}^{K})\\ P(x_{1}^{K}|u_{1}^{K})\\ P(u_{1}^{K}|s_{1}^{I})\\ P(s_{1}^{I})}\\\\ &{}&{\\Phi\\qquad\\qquad\\qquad R\\qquad\\qquad G}\\end{array}\n$$  \n\nwhere the symbol beneath each distribution denotes its FSM.  \n\nTo translate a given target language sentence $t_{1}^{J}$ into the source language, we construct an acceptor $T$ for the target sentence. In theory, we could then create a lattice of translations via the following sequence of FSM compositions  \n\n$$\n\\mathcal{T}=G\\circ W\\circ R\\circ\\Phi\\circ\\Omega\\circ T\n$$  \n\nand extract the translation $\\widehat{s_{1}^{I}}$ as the path in the translation lattice $\\tau$ with the least cost (negative log likelihood), to approximate $\\widehat{s_{1}^{I}}=$ $\\mathrm{argmax}_{s_{1}^{I}}\\,P(t_{1}^{J}|s_{1}^{I})\\,P(s_{1}^{I})$ as  \n\n$$\n\\widehat{s_{1}^{I}}=\\operatorname*{argmax}_{s_{1}^{I}}\\{\\operatorname*{max}_{v_{1}^{R},x_{1}^{K},u_{1}^{K},K}P(t_{1}^{J},v_{1}^{R},x_{1}^{K},u_{1}^{K},s_{1}^{I})\\}\\;.\n$$  \n\nIn practice, we perform translation in distinct steps. We first generate the target phrase lattice, $Q$ , which is a WFSM acceptor for all phrase sequences in the target sentence. $Q$ is found by composition $\\Omega\\circ T$ followed by projection onto the input side of the resulting transducer. We next list all the unique target phrases in $Q$ ; these are the phrases for which source language translations are needed, and candidate source language phrases are extracted for them from bilingual training text [13, 14]. This collection of source and target translation pairs is the phrase pair inventory.  \n\nAt this point, we have the statistics to construct a compact Phrase Translation transducer $R$ for the sentence to be translated, as well as the Source Phrase Segmentation transducer $W$ and the reordering and insertion transducer, $\\Phi$ [9]. The translation lattice is then generated as  \n\n<html><body><table><thead><tr><td><b>o</b></td><td><b>WoRoΦ</b></td><td><b>o</b></td><td><b>二</b></td></tr></thead><tbody><tr><td>Source</td><td>Source Wordto</td><td>Target</td><td>Target</td></tr><tr><td>Language</td><td>Target Phrase</td><td>二</td><td>Phrase</td></tr><tr><td>Model</td><td>Translation</td><td>二</td><td>Acceptor</td></tr></tbody></table></body></html>  \n\nThe point to stress here is that translation is actually carried out through a series of FSM compositions acting on a phrase lattice, i.e. an acceptor of target language phrases. In text translation, this accepts all the phrase sequences that can be derived from the single sentence to be translated. To translate speech, we can simply use an acceptor for all the target language phrase sequences in an ASR word lattice. We now extend the model formulation to support this.  \n\n# 2.1. Speech Translation from ASR Phrase Lattices  \n\nWe assume we have an ASR system with target language acoustic models $P(A|t_{1}^{J})$ and a target language model. To describe how source language text might generate a a target language utterance $A$ , we define $\\bar{P}(A,t_{1}^{J},v_{1}^{\\bar{R}},x_{1}^{\\bar{K}},u_{1}^{K},s_{1}^{I})$ as  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{P(A|t_{1}^{J})\\,P(t_{1}^{J}|v_{1}^{R})\\,P(v_{1}^{R}|x_{1}^{K},u_{1}^{K})\\,P(x_{1}^{K}|u_{1}^{K})\\,P(u_{1}^{K}|s_{1}^{I})\\,P(s_{1}^{I})}}\\\\ &{\\zeta\\qquad\\qquad\\Omega\\qquad\\Phi\\qquad}&{{}R\\qquad\\qquad G}\\end{array}\n$$  \n\nwhere $\\mathcal{L}$ is an weighted acceptor containing the word sequences and acoustic scores from an lattice generated by the ASR system over the utterance $A$ . In translation from speech, the ideal translation $\\begin{array}{r}{\\widehat{s_{1}^{I}}=\\operatorname{argmax}_{s_{1}^{I}}P(A|s_{1}^{I})P(s_{1}^{I})}\\end{array}$ is approximated as  \n\n$$\n\\widehat{s_{1}^{I}}=\\operatorname*{argmax}_{s_{1}^{I}}\\{\\operatorname*{max}_{t_{1}^{J},v_{1}^{R},x_{1}^{K},u_{1}^{K},K}P(A,t_{1}^{J},v_{1}^{R},x_{1}^{K},u_{1}^{K},s_{1}^{I})\\}\\;.\n$$  \n\nAs an aside, this differs from translation of the ASR transcript, which would be $\\widehat{t_{1}^{J}}=\\mathrm{argmax}_{t_{1}^{J}}P(A,t_{1}^{J})$  \n\n$$\n\\widehat{s_{1}^{I}}=\\operatorname*{argmax}_{s_{1}^{I}}\\{\\operatorname*{max}_{v_{1}^{R},x_{1}^{K},u_{1}^{K},K}P(\\widehat{t_{1}^{J}},v_{1}^{R},x_{1}^{K},u_{1}^{K},s_{1}^{I})\\}\\;.\n$$  \n\nWe briefly digress to contrast our model to the previously mentioned lattice-based speech translation approaches [7, 8]. While different, those are based on joint generation and translation, i.e. a parameterized distribution $\\dot{P_{\\lambda}}(t_{1}^{J},s_{1}^{I}|A)$ describes the simultaneous generation of a source sentence and its translation. This differs from generative approaches, which rely on distinct parameterized distributions, e.g. $P_{\\lambda_{1}}(A|t_{1}^{J})P_{\\lambda_{2}}(t_{1}^{J}|\\dot{s}_{1}^{I})P_{\\lambda_{3}}(s_{1}^{I})$ . The two approaches have their advantages and disadvantages, but it is worth noting that they arise from fundamentally different formulations, and involve quite different estimation and decoding procedures.  \n\nResuming the discussion of implementing the speech translation process via WFSMs, we could replace the (unweighted) acceptor $T$ constructed for a single target sentence to be translated by the (weighted) acceptor for the word strings in the ASR lattice  \n\n$$\n\\boldsymbol{T}=G\\circ W\\circ R\\circ\\Phi\\circ\\Omega\\circ\\mathcal{L}\\;.\n$$  \n\nBut what is done follows the text translation approach: the Target Phrase Segmentation transducer is applied to the word lattice acceptor, as $\\Omega\\circ\\mathcal{L}$ , to generate a lattice of phrases, $Q$ . The translation lattice is then found as $T=G\\circ W\\circ R\\circ\\Phi\\circ Q$ , and the translation $\\widehat{s_{1}^{I}}$ is found as the minimum cost path through $\\tau$ .  \n\nThere are of course modeling and implementation issues that arise in translating ASR lattices relative to translating individual text strings. For example, it is considerably easier to enumerate all the phrases in a sentence than in a word lattice. This and other issues are non-trivial modeling problems, and our current approaches to them are discussed next. However, despite these modeling challenges, we emphasize that this approach to speech translation neatly avoids the difficult problem of developing statistical translation systems that can process ASR word lattices. That problem is replaced instead by a modeling problem, namely how to extract phrase sequences from word lattices.  \n\n# 2.2. Transforming ASR Word Lattices into Phrase Lattices  \n\nWe describe our initial approach to transforming ASR word lattices into phrase lattices suitable for translation by the TTM. Formally, we would like to extract the target phrase sequences under  \n\nthe posterior distribution provided by the ASR system:  \n\n$$\nQ=P(v_{1}^{R}|A)=\\frac{\\sum_{t_{1}^{J}}P(v_{1}^{R}|t_{1}^{J})\\,P(A|t_{1}^{J})\\,P(t_{1}^{J})}{P(A)}\n$$  \n\nbased on the acoustic scores $P(A|t_{1}^{J})$ and the target language model scores $P(t_{1}^{J})$ . The latter does not appear in the formulation of overall translation; however we include it simply because it improves translation performance, and note that proper inclusion of the target language model will require extension of the TTM itself.  \n\nIn addition to words, ASR lattices can contain silence markers, fillers, and sentence breaks. Since these do not occur within sentences in our bilingual text collections, it is difficult to extract phrases that cover them. We map these symbols to NULL. Consequently, some of the phrases extracted will span what the ASR system hypothesized as sentence breaks. This is less than ideal, and we note this as an opportunity to incorporate metadata extraction techniques to guide phrase extraction through improved detection of phrase and sentence boundaries [15].  \n\nAfter this initial processing, the list of all phrases is extracted from the word lattices, as the list of all phrases is extracted from the target sentence in text translation. To extract the phrases from the ASR word lattice, we use the GRM Library grmcount tool which counts subsequences in a WFSM [16].  \n\n# EECH TO TEXT TRANSLATION PERFORM  \n\nWe investigate the performance of our systems on the TC-STAR Chinese to English (C-E) Broadcast News translation task 1.  \n\n# 3.1. Mandarin Broadcast News Translation Task Description  \n\nThe speech-to-text translation corpus is based on six Mandarin news broadcasts which were manually segmented and transcribed into Chinese sentences for use as reference transcriptions for ASR system evaluation. Two English translations of each Chinese sentence transcription were commissioned for translation system scoring. Three documents form the Development Set, and the other three the Evaluation Set, as specified by the 2005 TC-STAR evaluation; they contain 525 and 494 sentences, respectively. The overall statistics are given in Table 1.  \n\n<html><body><table><thead><tr><td></td><td><b>C</b></td><td><b>E-1</b></td><td><b>E-2</b></td></tr></thead><tbody><tr><td>Dev</td><td>3,156 /12,648</td><td>3,232/12,865</td><td>3,107/12,177</td></tr><tr><td>Eval</td><td>2,993 /13,023</td><td>2,809 /13,199</td><td>2,771 / 13,101</td></tr></tbody></table></body></html>\n\nTable 1. Dev and Eval Set Vocabularies (types/tokens).  \n\nIn addition to the six audio documents, their Chinese text transcriptions, and their corresponding English translations, we also have Mandarin ASR lattices in HTK format [17]. These were generated by the LIMSI Mandarin Broadcast News System [18] incorporating cross-word triphone acoustic models and a 4-gram language model.  \n\nThe LIMSI Mandarin Broadcast News ASR system was applied to the complete audio document. The system was allowed generate its own acoustic segmentation independently of the manual acoustic segmentation performed during the initial transcription. Corresponding to this automatic segmentation, there are 231 ASR lattices for the Dev set and 181 ASR lattices for the Eval set.  \n\nSince the audio segmentation was performed automatically by the ASR system, the number of lattices is not the same as the number of manually segmented sentences in the reference transcriptions. Prior to scoring, the ASR hypotheses and the reference transcriptions are each concatenated in temporal order to form a single, long document. The ASR Character Error Rate (CER) over the Dev set was found to be $8.7\\%$ .  \n\n# 3.2. Mandarin Broadcast News Phrase-Based SMT System  \n\nTranslation experiments are based on the TTM phrase-based SMT system [9, 10], and the experiments reported here are performed on the basic system submitted by JHU/CU to the 2005 TC-STAR and NIST Chinese-English MT evaluations.  \n\nThe underlying training bitext consists of C-E parallel corpora provided by LDC (http://www.ldc.org), mainly consisting of FBIS, Xinhua, Hong Kong News, Sinorama news sources, the Chinese Treebank, and the Hong Kong Hansards and UN proceedings; the bitext contains 175M Chinese words and 200M English words. The Chinese text was word segmented using the LDC segmenter followed by rule-based number grouping. The English text was processed using a slightly modified version of the tokenizer distributed in the NIST MT-eval toolkit [19].  \n\nThe documents were aligned at the sentence and sub-sentence level [20] to produce 7M bilingual sentence or subsentence chunk pairs. The chunk-aligned bitext was then aligned at the word level under the Word-to-Phrase alignment model [14]. Phrase-pairs were extracted by commonly used heuristics [13]; phrase pairs were extracted only as needed to cover the Chinese phrases to be translated. This process was complicated by inconsistent Chinese tokenization and word segmentation schemes between the ASR system and the SMT bitext; this is discussed in the next section.  \n\nThe English language model training data consists of 380M words of text from the LDC English Gigaword (AFP and Xinhua), the English side of FBIS, and the online archives of People’s Daily. On the C-E task we estimated an interpolated 3-gram target LM with uniform weights over the three LM English sources. For LM training, the corpus was lower-cased and punctuation removed.  \n\nPrior to translation, the Chinese ASR lattices were converted into weighted finite state acceptors in AT&T FSM format [11, 12]; time information was removed, and the lattices were reduced in size by applying ϵ-removal, determinization, and minimization [11, 12]. Lattices were in joint-likelihood form with acoustic and language model scores were combined using a Word Insertion Penalty and a Grammar Scale Factor optimized for ASR Word Error Rate by LIMSI. The ASR word lattices are pruned as necessary, and after composition with the target phrase segmentation transducer, phrases are extracted up to 5 target word in length. Parameters were optimized over the dev set.  \n\nTranslation performance was measured under the BLEU metric [21] with respect to the two sets of English transcriptions. Casing was preserved in the reference translation, and the SMT output was re-cased using the SRILM disambig tool [22] with a modified Kneser-Ney 3-gram LM trained over the English LM text.  \n\nBaseline translation performance is reported by applying various translation system configurations to the Chinese reference transcriptions. In these baseline experiments, BLEU scores are reported at both the sentence level (sBLEU) and document level (dBLEU). However, since there is no easily found correspondence between the ASR acoustic segmentation and the manual segmentation from which the sentence level translations are derived, only dBLEU scores are reported for the speech translation experiments.  \n\n<html><body><table><thead><tr><td></td><td><b>Mandarin Source</b></td><td><b>DEV</b></td><td><b>EVAL</b></td></tr></thead><tbody><tr><td rowspan=\"3\">Monotone  Phrase</td><td>Ref. Transcription</td><td>12.8 / 16.1</td><td>14.1/18.8</td></tr><tr><td>ASR 1-Best</td><td>14.8</td><td>13.6</td></tr><tr><td>ASR lattice</td><td>15.0</td><td>13.8</td></tr><tr><td rowspan=\"3\">MJ-1 VT  Phrase Order</td><td>Ref. Transcription</td><td>12.9 / 16.1</td><td>14.1 /19.3</td></tr><tr><td>ASR 1-Best</td><td>15.0</td><td>13.8</td></tr><tr><td>ASR lattice</td><td>15.1</td><td>14.0</td></tr></tbody></table></body></html>\n\nTable 2. Mandarin Broadcast News Translation Performance.  \n\nTranslation performance is reported in Table 2 over the manual reference transcriptions (sBLEU/dBLEU scores are provided), the ASR 1-Best hypotheses, and the ASR lattices. Two configurations of the TTM are investigated. In the first, target phrases appear in monotone phrase order, i.e. Chinese phrases appear in English phrase order. In the second, MJ-1 VT phrase reordering allows target phrases swap places with their immediate neighbors as determined by reordering probabilities estimated over bitext (with a backoff swap probability of 0.02) [10]. Although improvements are not large, we find improvements in all scenarios by translating ASR lattices instead of ASR 1-Best hypothesis.  \n\nIn comparing the speech and text translation systems, we note that from the Dev Set reference transcriptions we extract 44,744 Chinese phrases; 11,617 of these appear in the bitext, accompanied by 59,589 English phrases (after pruning). By comparison, we extract 58,395 Chinese phrases from the ASR lattices - 1.3 times as many phrases as appear in the reference transcriptions. However, we are able to find only 12,983 of these Chinese phrases in the bitext, and these are accompanied by 60,574 English translations. In summary, we find that in translating the lattice we have increased the number of Chinese phrases and their English alternatives only slightly. There are two factors at work. The first is that our phrase extraction procedure was developed for phrases extracted from text; different modeling procedure will be needed to translate phrases hypothesized by ASR systems, which tend to be disfluent and are relatively unlikely to appear in training bitext text. The second, dominant, problem is the mismatch in tokenization and word segmentation between the ASR system and the Chinese side of the bitext. We anticipate performance improvements when we integrate ASR and SMT systems constructed with consistent text formatting.  \n\n# 4. CONCLUSION  \n\nWe have presented a modeling framework for statistical speechto-text translation as an extension of the phrase-based TTM text translation model. This formulation leads to a tight coupling of the ASR and SMT subsystems, both as statistical models and as implemented by the WFSM phrase-based translation system. We have identified and described weaknesses in this initial formulation and its implementation, and we intend to improve upon these in subsequent work. Mandarin-to-English Broadcast News translation experiments demonstrate that the approach is feasible, and we anticipate further improvements in translation performance by integrated development of the component ASR and SMT systems.", "appendix": ""}, {"title": "Investigating Self-Supervised Pre-Training for End-to-End Speech Translation", "authors": "Ha Nguyen, Fethi Bougares, N. Tomashenko, Yannick Estève, Laurent Besacier", "bibkey": "investigating_self_supervised_pre_training_for_end_to_end_speech_translation", "bibitem": "@article{NISPFEST,\n  url = {https://hal.archives-ouvertes.fr/hal-02962186/file/Paper_Template_for_INTERSPEECH_2019-3.pdf},\n  title = {Investigating Self-Supervised Pre-Training for End-to-End Speech Translation},\n  authors = {Ha Nguyen, Fethi Bougares, N. Tomashenko, Yannick Estève, Laurent Besacier},\n  bibkey = {NISPFEST},\n  journal = {Interspeech 2020}\n}", "url": "https://hal.archives-ouvertes.fr/hal-02962186/file/Paper_Template_for_INTERSPEECH_2019-3.pdf", "latex_url": null, "latex_path": null, "pdf_url": "https://hal.archives-ouvertes.fr/hal-02962186/file/Paper_Template_for_INTERSPEECH_2019-3.pdf", "pdf_path": "output/download_papers/Paper_Template_for_INTERSPEECH_2019-3/Paper_Template_for_INTERSPEECH_2019-3.pdf", "md_url": null, "latex_length": 0, "latex": null, "abstract": "# Investigating Self-supervised Pre-training for End-to-end Speech Translation  \n\nHa Nguyen, Fethi Bougares, Natalia Tomashenko, Yannick Estève, Laurent Besacier  \n\n# To cite this version:  \n\nHa Nguyen, Fethi Bougares, Natalia Tomashenko, Yannick Estève, Laurent Besacier. Investigating Self-supervised Pre-training for End-to-end Speech Translation. Interspeech 2020, Oct 2020, Shangai (Virtual Conf), China. ￿hal-02962186  \n\n# HAL Id: hal-02962186  \n\nhttps://hal.science/hal-02962186v1  \n\nSubmitted on 9 Oct 2020  \n\nHAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers.  \n\nL’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d’enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.  \n\n# Investigating Self-supervised Pre-training for End-to-end Speech Translation  \n\nHa Nguyen $^{1,2}$ , Fethi Bougares3, Natalia Tomashenko2, Yannick Est\\`eve2, Laurent Besacier1  \n\n1LIG - Universite´ Grenoble Alpes, France 2LIA - Avignon Universite´, France 3LIUM - Le Mans Universit´e, France  \n\nmanh-ha.nguyen@univ-grenoble-alpes.f  \n\n# Abstract  \n\n# 2. Related Works  \n\nSelf-supervised learning from raw speech has been proven beneficial to improve automatic speech recognition (ASR). We investigate here its impact on end-to-end automatic speech translation (AST) performance. We use a contrastive predictive coding (CPC) model pre-trained from unlabeled speech as a feature extractor for a downstream AST task. We show that selfsupervised pre-training is particularly efficient in low resource settings and that fine-tuning CPC models on the AST training data further improves performance. Even in higher resource settings, ensembling AST models trained with filter-bank and CPC", "abstract_length": 2049, "abstract_token": 500, "introduction": "Self-supervised learning using huge unlabeled data has been explored with very promising results for image processing [1] and natural language processing [2]. Recent works investigated selfsupervised representation learning from speech [3, 4, 5]. They were successful to improve performance on downstream tasks such as speech recognition. These recent works suggest that it is possible to reduce dependence on labeled data for building speech systems through acoustic representation learning. We investigate the possibility to leverage unlabeled speech for endto-end automatic speech translation (AST). We focus on scenarios where (a) recordings in source language are not transcribed1 (no ASR pre-training is possible), (b) only a small-medium amount of training data (speech aligned to translations) is available, (c) a larger amount of unlabeled speech can be used. This scenario is typical of situations when one builds a system that translates from speech in a language with poorly standardized orthography or even from an unwritten language. In summary, our contributions are: (1) we propose an indepth study on the impact of self-supervised pre-training for AST, (2) we show that fine-tuning pre-trained representations on the AST training data is beneficial and that self-supervised pre-training is particularly efficient in low resource settings, (3) even in high resource settings, ensembling models trained with filter-bank and self-supervised representations leads to near state-of-the-art models without using ASR pre-training, (4) we analyze the representations learnt and show that they allow to better discriminate phones, better align source and target sequences, and are more robust to speaker variability.", "introduction_length": 1724, "introduction_token": 327, "reference": "# 8. References  \n\n[1] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for contrastive learning of visual representations,” 2020.   \n[2] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training of deep bidirectional transformers for language understanding,” CoRR, vol. abs/1810.04805, 2018. [Online]. Available: http://arxiv.org/abs/1810.04805   \n[3] A. Baevski, M. Auli, and A. Mohamed, “Effectiveness of selfsupervised pre-training for speech recognition,” 2019.   \n[4] K. Kawakami, L. Wang, C. Dyer, P. Blunsom, and A. van den Oord, “Learning robust and multilingual speech representations,” 2020.   \n[5] Y.-A. Chung and J. Glass, “Generative pre-training for speech with autoregressive predictive coding,” 2019.   \n[6] Y. Chung, W. Hsu, H. Tang, and J. R. Glass, “An unsupervised autoregressive model for speech representation learning,” CoRR, vol. abs/1904.03240, 2019. [Online]. Available: http://arxiv.org/ abs/1904.03240   \n[7] Y.-A. Chung and J. Glass, “Improved speech representations with multi-target autoregressive predictive coding,” 2020.   \n[8] S. Schneider, A. Baevski, R. Collobert, and M. Auli, “wav2vec: Unsupervised Pre-Training for Speech Recognition,” in Proc. Interspeech 2019, 2019, pp. 3465–3469. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.2019-1873   \n[9] J. Kahn, M. Rivi\\`ere, W. Zheng, E. Kharitonov, Q. Xu, P.-E. Mazare´, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux, “Libri-light: A benchmark for asr with limited or no supervision,” 2019.   \n[10] M. Rivi\\`ere, A. Joulin, P.-E. Mazar´e, and E. Dupoux, “Unsupervised pretraining transfers well across languages,” 2020.   \n[11] M. Ravanelli, J. Zhong, S. Pascual, P. Swietojanski, J. Monteiro, J. Trmal, and Y. Bengio, “Multi-task self-supervised learning for robust speech recognition,” 2020.   \n[12] J. Engel, L. Hantrakul, C. Gu, and A. Roberts, “Ddsp: Differentiable digital signal processing,” 2020.   \n[13] S. Pascual, M. Ravanelli, J. Serr\\`a, A. Bonafonte, and Y. Bengio, “Learning problem-agnostic speech representations from multiple self-supervised tasks,” 2019.   \n[14] A. B´erard, O. Pietquin, C. Servan, and L. Besacier, “Listen and translate: A proof of concept for end-to-end speech-to-text translation,” in NIPS Workshop on End-to-end Learning for Speech and Audio Processing, 2016.   \n[15] R. J. Weiss, J. Chorowski, N. Jaitly, Y. Wu, and Z. Chen, “Sequence-to-sequence models can directly transcribe foreign speech,” in Proc. of INTERSPEECH, 2017.   \n[16] A. Be´rard, L. Besacier, A. C. Kocabiyikoglu, and O. Pietquin, “End-to-end automatic speech translation of audiobooks,” CoRR, vol. abs/1802.04200, 2018. [Online]. Available: http://arxiv.org/ abs/1802.04200   \n[17] S. Bansal, H. Kamper, K. Livescu, A. Lopez, and S. Goldwater, “Pre-training on high-resource speech recognition improves lowresource speech-to-text translation,” CoRR, vol. abs/1809.01431, 2018. [Online]. Available: http://arxiv.org/abs/1809.01431   \n[18] Y. Chung, W. Weng, S. Tong, and J. Glass, “Towards unsupervised speech-to-text translation,” CoRR, vol. abs/1811.01307, 2018. [Online]. Available: http://arxiv.org/abs/1811.01307   \n[19] Y. Jia, R. J. Weiss, F. Biadsy, W. Macherey, M. Johnson, Z. Chen, and Y. Wu, “Direct speech-to-speech translation with a sequence-to-sequence model,” CoRR, vol. abs/1904.06037, 2019. [Online]. Available: http://arxiv.org/abs/1904.06037   \n[20] M. A. Di Gangi, R. Cattoni, L. Bentivogli, M. Negri, and M. Turchi, “MuST-C: a Multilingual Speech Translation Corpus,” in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 2012–2017. [Online]. Available: https://www.aclweb.org/anthology/N19-1202   \n[21] Y. Jia, M. Johnson, W. Macherey, R. J. Weiss, Y. Cao, C. Chiu, N. Ari, S. Laurenzo, and Y. Wu, “Leveraging weakly supervised data to improve end-to-end speech-totext translation,” CoRR, vol. abs/1811.02050, 2018. [Online]. Available: http://arxiv.org/abs/1811.02050   \n[22] M. Sperber, G. Neubig, J. Niehues, and A. Waibel, “Attentionpassing models for robust and data-efficient end-to-end speech translation,” CoRR, vol. abs/1904.07209, 2019. [Online]. Available: http://arxiv.org/abs/1904.07209   \n[23] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “LibriSpeech: an ASR corpus based on public domain audio books,” in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 5206–5210.   \n[24] R. Sanabria, O. Caglayan, S. Palaskar, D. Elliott, L. Barrault, L. Specia, and F. Metze, “How2: a large-scale dataset for multimodal language understanding,” in ViGIL Workshop, NeurIPS, 2018.   \n[25] H. Nguyen, N. Tomashenko, M. Z. Boito, A. Caubriere, F. Bougares, M. Rouvier, L. Besacier, and Y. Esteve, “ON-TRAC consortium end-to-end speech translation systems for the IWSLT 2019 shared task,” in Proc. of IWSLT, 2019.   \n[26] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” in Proc. of ICLR, 2015.   \n[27] D. Bahdanau, K. Cho, and Y. Bengio, “Neural Machine Translation by Jointly Learning to Align and Translate,” in Proc. of ICLR, 2015.   \n[28] H. Inaguma, S. Kiyono, K. Duh, S. Karita, N. E. Y. Soplin, T. Hayashi, and S. Watanabe, “ESPnet-ST: All-in-one speech translation toolkit,” arXiv preprint arXiv:2004.10234, 2020.   \n[29] J. Niehues, R. Cattoni, S. St¨uker, M. Negri, M. Turchi, E. Salesky, R. Sanabria, L. Barrault, L. Specia, and M. Federico, “The iwslt 2019 evaluation campaign,” in Proceedings of the 16th International Workshop on Spoken Language Translation (IWSLT 2019), 2019.   \n[30] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett, and N. L. Dahlgren, “DARPA TIMIT acoustic phonetic continuous speech corpus cdrom,” 1993.   \n[31] N.-Q. Luong, L. Besacier, and B. Lecouteux, “Towards accurate predictors of word quality for machine translation: Lessons learned on french - english and english - spanish systems,” Data and Knowledge Engineering, 2015.   \n[32] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann et al., “The Kaldi speech recognition toolkit,” Tech. Rep., 2011.   \n[33] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, “X-vectors: Robust DNN embeddings for speaker recognition,” in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5329–5333.   \n[34] A. Nagrani, J. S. Chung, and A. Zisserman, “VoxCeleb: a largescale speaker identification dataset,” in Interspeech, 2017, pp. 2616–2620.   \n[35] N. Tomashenko, B. M. L. Srivastava, X. Wang, E. Vincent, A. Nautsch, J. Yamagishi, N. Evans, J. Patino, J.-F. Bonastre, P.- G. Noe´, and M. Todisco, “Introducing the VoicePrivacy initiative,” in Interspeech, 2020.", "reference_length": 7006, "reference_token": 2393, "txt_length": 24426, "txt_token": 7004, "txt": "# Investigating Self-supervised Pre-training for End-to-end Speech Translation  \n\nHa Nguyen, Fethi Bougares, Natalia Tomashenko, Yannick Estève, Laurent Besacier  \n\n# To cite this version:  \n\nHa Nguyen, Fethi Bougares, Natalia Tomashenko, Yannick Estève, Laurent Besacier. Investigating Self-supervised Pre-training for End-to-end Speech Translation. Interspeech 2020, Oct 2020, Shangai (Virtual Conf), China. ￿hal-02962186  \n\n# HAL Id: hal-02962186  \n\nhttps://hal.science/hal-02962186v1  \n\nSubmitted on 9 Oct 2020  \n\nHAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers.  \n\nL’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d’enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.  \n\n# Investigating Self-supervised Pre-training for End-to-end Speech Translation  \n\nHa Nguyen $^{1,2}$ , Fethi Bougares3, Natalia Tomashenko2, Yannick Est\\`eve2, Laurent Besacier1  \n\n1LIG - Universite´ Grenoble Alpes, France 2LIA - Avignon Universite´, France 3LIUM - Le Mans Universit´e, France  \n\nmanh-ha.nguyen@univ-grenoble-alpes.f  \n\n# Abstract  \n\n# 2. Related Works  \n\nSelf-supervised learning from raw speech has been proven beneficial to improve automatic speech recognition (ASR). We investigate here its impact on end-to-end automatic speech translation (AST) performance. We use a contrastive predictive coding (CPC) model pre-trained from unlabeled speech as a feature extractor for a downstream AST task. We show that selfsupervised pre-training is particularly efficient in low resource settings and that fine-tuning CPC models on the AST training data further improves performance. Even in higher resource settings, ensembling AST models trained with filter-bank and CPC representations leads to near state-of-the-art models without using any ASR pre-training. This might be particularly beneficial when one needs to develop a system that translates from speech in a language with poorly standardized orthography or even from speech in an unwritten language.  \n\nIndex Terms: self-supervised learning from speech, automatic speech translation, end-to-end models, low resource settings.  \n\n# 1. Introduction  \n\nSelf-supervised learning using huge unlabeled data has been explored with very promising results for image processing [1] and natural language processing [2]. Recent works investigated selfsupervised representation learning from speech [3, 4, 5]. They were successful to improve performance on downstream tasks such as speech recognition. These recent works suggest that it is possible to reduce dependence on labeled data for building speech systems through acoustic representation learning. We investigate the possibility to leverage unlabeled speech for endto-end automatic speech translation (AST). We focus on scenarios where (a) recordings in source language are not transcribed1 (no ASR pre-training is possible), (b) only a small-medium amount of training data (speech aligned to translations) is available, (c) a larger amount of unlabeled speech can be used. This scenario is typical of situations when one builds a system that translates from speech in a language with poorly standardized orthography or even from an unwritten language.  \n\nIn summary, our contributions are: (1) we propose an indepth study on the impact of self-supervised pre-training for AST, (2) we show that fine-tuning pre-trained representations on the AST training data is beneficial and that self-supervised pre-training is particularly efficient in low resource settings, (3) even in high resource settings, ensembling models trained with filter-bank and self-supervised representations leads to near state-of-the-art models without using ASR pre-training, (4) we analyze the representations learnt and show that they allow to better discriminate phones, better align source and target sequences, and are more robust to speaker variability.  \n\n# 2.1. Self-supervised learning from speech  \n\nSelf-supervised learning from speech consists in resolving pseudo-tasks not requiring human annotations as a pre-training to the real tasks to solve. These pseudo-tasks target predicting next samples or solving ordering problems. Autoregressive predictive coding (APC) [6, 7] considers the sequential structure of speech and predicts information about a future frame. An easier learning objective is introduced in Contrastive Predictive Coding (CPC) which consists in distinguishing a true future audio frame from negatives [3, 8, 9]. [5] shows that such representations are useful to improve several speech tasks while [4] extends those works by looking at the representations’ robustness to domain and language shifts. In the same vein, [10] compares self-supervised and supervised pre-training for ASR and shows that CPC pre-training extracts features that transfer well to other languages, being on par or even outperforming supervised pretraining. Another promising way is to use speech enhancement as a task for feature representation learning [11, 12]. Finally, several self-supervised tasks can be jointly tackled to discover better speech representations [13].  \n\n# 2.2. End-to-end Automatic Speech Translation  \n\nPrevious automatic speech-to-text translation (AST) systems operate in two steps: source language automatic speech recognition (ASR) and source-to-target text machine translation (MT). However, recent works have attempted to build end-toend AST without using source language transcription during learning or decoding [14, 15] or using it at training time only [16]. Recently several extensions of these pioneering works were introduced: low resource AST [17], unsupervised AST [18], end-to-end speech-to-speech translation (Translatotron) [19], multilingual AST [20]. Improvements of end-to-end AST were also proposed using weakly supervised data [21] or adding a second attention mechanism [22]. While supervised pretraining for AST was investigated (see for instance [16]), we are aware of a single research group [5, 7] that investigated selfsupervised pre-training for AST. However their experiments were done in a high resource setting and AST (for which only marginal gains were displayed) was solely investigated among other tasks, without an in-depth analysis of the representations learnt.  \n\n# 3. Self-supervised Pre-training from Speech  \n\n# 3.1. Contrastive predictive coding model  \n\nWe use the self-supervised pre-training model introduced in [8] $(w a v2\\nu e c)$ which is based on contrastive predictive coding. The model uses (1) an encoder network that converts the audio signal into a latent representation (from raw speech samples $x$ into a feature representation $z$ ), and (2) a context network that aggregates multiple time steps to build contextualized representations (from a sequence $z_{i-v},...,z_{i}$ into a context vector $c_{i}$ ).2The full model (encoder $^+$ context) is trained end-to-end to distinguish a sample $z_{i+k}$ that is $\\mathbf{k}$ steps in the future from negative samples $\\tilde{z}$ uniformly chosen from the same audio sequence. A contrastive loss is minimized for each step $k\\,=\\,1,...,K$ and the overall loss is summed over different step sizes (more details in [8]).  \n\nTable 1: Statistics of different How2 data partitions.   \n\n\n<html><body><table><thead><tr><td><b>Partition</b></td><td><b>#segments</b></td><td><b>#hours</b></td><td><b>#src words</b></td><td><b>#tgt words</b></td></tr></thead><tbody><tr><td>10%</td><td>17,751</td><td>28</td><td>313K</td><td>295K</td></tr><tr><td>20%</td><td>35,858</td><td>56</td><td>626K</td><td>591K</td></tr><tr><td>30%</td><td>53,698</td><td>84</td><td>887K</td><td>940K</td></tr><tr><td>60%</td><td>107,676</td><td>169</td><td>1778K</td><td>1883K</td></tr><tr><td>full</td><td>179,438</td><td>281</td><td>2963K</td><td>3139K</td></tr></tbody></table></body></html>  \n\n# 3.2. Pre-trained models for English  \n\nWe use an off-the-shelf model provided for English.3 It is trained on Librispeech corpus [23]. We also investigate if finetuning the model on our task specific data is beneficial. For this, we fine-tune wav2vec on the full speech corpora used for our AST experiments (see next section). It is important to note that no transcripts nor translations are needed for this step which requires only raw speech. After fine-tuning wav2vec, we input the representations produced by the context network $c_{i}$ to the AST encoder instead of filter-bank features (see Figure 1).  \n\n# 4. End-to-end Speech Translation Experiments  \n\n# 4.1. Experimental setup  \n\n# 4.1.1. Data  \n\nHow2 corpus [24] is used for our main experiments. This corpus contains about 297.6 hours of speech, which is transcribed and translated into 3.3 million of English words and 3.1 million of Portuguese words respectively.4 From this version of data, we first filter out too long sentences (sentences longer than 30 seconds or 400 characters). Then, in order to simulate lower resource scenarios, we randomly split the corpus into four subcorpora of roughly $10\\%$ , $20\\%$ , $30\\%$ , and $60\\%$ of the flitered full corpus. Our splits guarantee that smaller partitions are fully included in the bigger ones. The statistics of all the partitions and the filtered version of full corpora can be found in Table 1.  \n\n# 4.1.2. Speech features and data augmentation  \n\nAs shown in Figure 1, we extract either wav2vec features or filter-bank+pitch features (later denoted as fbanks) from speech input.5 Depending on the experiments, mean and variance normalization (MVN) is optionally applied to the generated features. For wav2vec feature extraction, we either use an offthe-shelf model trained on LibriSpeech [23] or a model finetuned on How2 training set. MVN parameters are estimated on the speech translation training set and then applied to all train/dev/test sets. Overall, we have 4 different self-supervised representations named wav2vec, wav2vec $^+$ norm, wav2vec $^+$ $F T$ (fined-tuned wav2vec) and wav $2\\nu e c+F T+n o r m$ . All those wav2vec features are of dimension 512. We compare the above representations to conventional filter-bank features. Similar to [25], we extract 80-dimensional Mel filter-bank features, concatenated with 3-dimensional pitch features from windows of $25m s$ , and a frame shift of 10ms. MVN is used in the same manner as for wav2vec features. This gives us 2 additional speech representations named fbanks and fbanks $^+$ norm respectively (their dimension is 83).6 Data augmentation through speed perturbation is also applied with factors of 0.9, 1.0, and 1.1 to the training data. We reuse the development set of our participation to the previous IWSLT2019 [25] (1, 984 sentences randomly excluded from the training set). How2 val set is used as our test data. As for target text processing, we normalize punctuation marks, and tokenize the text into character-level using Moses script.7  \n\n![](images/0af9298a5707cbd5c7517be8441bd5ead162fa01c523a262d4b21d9229ae68d1.jpg)  \nFigure 1: Architecture of the speech encoder: a stack of two VGG blocks followed by 5 BLSTM layers. We use as input (1) wav2vec features (that pass through an additional projection layer to reduce their dimension from 512 to 83), or (2) filterbank $^+.$ pitch features. The input features are optionally normalized (MVN).  \n\n# 4.2. Speech-to-text translation model  \n\n# 4.2.1. Architecture.  \n\nWe use an attention-based encoder-decoder architecture, whose encoder is illustrated in Figure 1. The encoder is a stack of two VGG-like [26] CNN blocks followed by five 1024-dimensional BLSTM layers. Each VGG block contains two 2D-convolution layers just before a 2D-maxpooling layer, which aims to reduce both time $(T)$ and frequency dimension $(D)$ of the input speech features by a factor of 2. These two VGG blocks transform input speech features’ shape from $(T\\times D)$ to $(T/4\\times D/4)$ . Bahdanau’s attention mechanism [27] is used in all our experiments. The decoder is a stack of two 1024-dimensional LSTM layers. This model performed well at the IWSLT2019 E2E AST track [25], thus it is completely reused for all the experiments with fbanks features presented throughout this paper. However $w a v2\\nu e c$ features have higher dimension (512) than fbanks (83). In order to compare both input representations with a similar parameter budget in the architecture (and also because training an architecture with input features of dimension 512 would be more computationally expensive), we add a projection block at the bottom of the encoder.8 This block (containing a linear layer followed by a ReLU) reduces the wav2vec’s feature size from 512 to 83 (see Figure 1).  \n\nTable 2: Detokenized case-sensitive BLEU scores measured on How2 val set of different models trained on different partitions of How2 corpus (EN-PT) with different speech features. FT means fine-tuned and norm stands for MVN normalization.   \n\n\n<html><body><table><thead><tr><td><b>No.</b></td><td><b>Feature</b></td><td><b>10% (28h)</b></td><td><b>20% (56h)</b></td><td><b>30% (84h)</b></td><td><b>60% (169h)</b></td><td><b>100% (281h)</b></td></tr></thead><tbody><tr><td>1</td><td>wav2vec</td><td>11.33</td><td>26.75</td><td>30.83</td><td>36.33</td><td>41.02</td></tr><tr><td>2</td><td>wav2vec + FT</td><td>12.52</td><td>27.30</td><td>32.11</td><td>37.78</td><td>42.32</td></tr><tr><td>3</td><td>wav2vec + norm</td><td>16.52</td><td>27.33</td><td>31.27</td><td>37.62</td><td>41.08</td></tr><tr><td>4</td><td>wav2vec + FT + norm</td><td>18.50</td><td>27.68</td><td>32.17</td><td>37.75</td><td>41.30</td></tr><tr><td>5</td><td>fbanks</td><td>1.03</td><td>18.61</td><td>27.32</td><td>37.23</td><td>41.63</td></tr><tr><td>6</td><td>fbanks + norm</td><td>2.11</td><td>24.58</td><td>30.21</td><td>37.56</td><td>42.51</td></tr><tr><td>7</td><td>Ensemble [5, 6]</td><td></td><td>25.28</td><td>31.90</td><td>40.39</td><td>44.35</td></tr><tr><td>8</td><td>Ensemble [4, 6]</td><td></td><td>29.87</td><td>34.67</td><td>41.22</td><td>45.02</td></tr><tr><td>9</td><td>Ensemble [1,2,3,4,5,6]</td><td></td><td>31.88</td><td>36.80</td><td>42.62</td><td>46.16</td></tr></tbody></table></body></html>  \n\n# 4.2.2. Hyperparameters’ details  \n\nModels are trained in maximum 20 epochs with early stopping after 3 epochs if the accuracy on the dev set does not improve. Adadelta is chosen as optimizer and dropout is set to 0.3 on the encoder side. We decode all our models with beam size of 10.  \n\n# 4.3. Experimental results on How2  \n\nOn each partition of How2 corpus, we train 6 models which take as input different speech representations presented in section 4.1.2, thus in total 30 models shown in Table 2. We evaluate on How2 val set, which contains 2, 022 segments (about 3.2 hours of speech), in the same conditions as our participation to IWSLT 2019 shared task. It is clear from the table that in low resource settings (28 and 56 hours), self-supervised representations $(w a\\nu2\\nu e c)$ significantly outperform fbanks. Figure 2a confirms this and shows that models trained with wav2vec representations converge better and faster. The impact of normalization and fine-tuning is also notable from both Table 2 and Figure 2a. In very low resource settings (like 28 hours), fine-tuning wav2vec can greatly help, and with normalization, the performance further improves. In higher resource settings (169 and 281 hours of translated speech), differences between wav2vec and fbanks fade away (and so does the impact of fine-tuning and normalization). However, our ensembling experiments of lines 7 and 8 on $100\\%$ of How2 show that it is beneficial to ensemble the best system $(f b a n k s{+}n o r m$ , line 6) with a system trained with wav2vec $(w a\\nu2\\nu e c{+}F T{+}n o r m$ , line 4) rather than a better model (fbanks, line 5) also based on filter-bank features, even though $w a\\nu2\\nu e c\\mathbf{+}F T\\mathbf{+}n o r m$ underperforms fbanks on this partition. Ensembling all our models (line 9) leads to $B L E U>30$ even in very low resource training conditions (56 hours). Finally, in order to compare ourselves with the stateof-the-art [28], we decode How2 dev5 (a.k.a How2 test), which consists of 2, 305 segments (about 3.7 hours of speech), using the ensemble of all our models trained on the full corpus (line 9). This gives us near state-of-the-art BLEU: we obtain 46.16 on How2 val and 47.17 on How2 dev5. This latter score on dev5 is to be compared with 48.04 reported with an ensemble model in [28] where ASR and MT pre-training were used, as well as data augmentation with SpecAugment.  \n\n![](images/68db28f02863d67052d452b8d1d52af8a5e2544bcb13af53f109e042f25c8e8a.jpg)  \nFigure 2: Learning curves (accuracy) of models trained on different partitions of How2.  \n\n# 4.4. Validation on two other language pairs  \n\nTo validate our results in low resource settings (56 and 84 hours), we train our models on two subsets of MuST-C [20] English-to-German and English-to-French training data (56 and 84 hours each, a training size similar to How2 $20\\%$ and $30\\%$ ). As illustrated by Table 3, MuST-C is more challenging than How2 (as confirmed by official IWSLT 2019 evaluation results [29]), but for both language pairs, wav2vec significantly outperform fbanks. This confirms that self-supervised pre-training is useful in low resource scenarios.  \n\n# 5. Analysis of Learnt Representations  \n\nThis section tries to answer the question why wav2vec representation performs better than filter-bank features . The following subsections present the experiments which show that wav2vec might be (1) better at discriminating phones, (2) better at aligning source and target sequences, and (3) more robust to speaker variability.  \n\n# 5.1. Better phone discrimination  \n\nWe first replicate an experiment from [8] for phoneme recognition on TIMIT [30]. Speech representations are extracted from train, dev and test split of TIMIT. A simple attentional encoderdecoder model is used: encoder with 4 BLSTM layers of hidden size 320, decoder with 1 LSTM layer and location-based attention [31]. The results of Table 4 confirm that wav2vec representations (normalized or not) are much better at recognizing phones than fbanks.  \n\nTable 3: AST BLEU on MuST-C for EN-DE and EN-FR. (a) MuST-C 56 hours.   \n\n\n<html><body><table><thead><tr><td><b>Lang</b></td><td><b>Features</b></td><td><b>tst-COMMON</b></td><td><b>tst-HE</b></td></tr></thead><tbody><tr><td rowspan=\"5\">EN-DE</td><td>wav2vec</td><td>7.56</td><td>7.21</td></tr><tr><td>wav2vec+norm</td><td>7.83</td><td>8.12</td></tr><tr><td>fbanks</td><td>1.50</td><td>1.09</td></tr><tr><td>fbanks+norm</td><td>4.89</td><td>4.87</td></tr><tr><td rowspan=\"5\">EN-FR</td><td>wav2vec</td><td>12.08</td><td>12.41</td></tr><tr><td>wav2vec+norm</td><td>12.58</td><td>12.58</td></tr><tr><td>fbanks</td><td>0.54</td><td>0.00</td></tr><tr><td>fbanks+norm</td><td>7.10</td><td>6.37</td></tr></tbody></table></body></html>  \n\n(b) MuST-C 84 hours.   \n\n\n<html><body><table><thead><tr><td><b>Lang</b></td><td><b>Features</b></td><td><b>tst-COMMON</b></td><td><b>tst-HE</b></td></tr></thead><tbody><tr><td rowspan=\"5\">EN-DE</td><td>wav2vec</td><td>10.57</td><td>10.43</td></tr><tr><td>wav2vec+norm</td><td>10.30</td><td>10.27</td></tr><tr><td>fbanks</td><td>0.74</td><td>0.66</td></tr><tr><td>fbanks+norm</td><td>7.68</td><td>7.84</td></tr><tr><td rowspan=\"5\">EN-FR</td><td>wav2vec</td><td>16.18</td><td>16.68</td></tr><tr><td>wav2vec+norm</td><td>16.84</td><td>16.37</td></tr><tr><td>fbanks</td><td>1.65</td><td>0.97</td></tr><tr><td>fbanks+norm</td><td>14.31</td><td>13.86</td></tr></tbody></table></body></html>  \n\nTable 4: Phone error rate $'P E R~\\%$ ) on TIMIT dev and test set.   \n\n\n<html><body><table><thead><tr><td><b>No.</b></td><td><b>Feature</b></td><td><b>TIMIT dev</b></td><td><b>TMIT test</b></td></tr></thead><tbody><tr><td>1</td><td>wav2vec</td><td>13.0</td><td>15.0</td></tr><tr><td>2</td><td>wav2vec + norm</td><td>13.9</td><td>15.8</td></tr><tr><td>3</td><td>fbanks</td><td>22.2</td><td>24.9</td></tr><tr><td>4</td><td>fbanks + norm</td><td>20.7</td><td>23.5</td></tr></tbody></table></body></html>  \n\n# 5.2. Better source-target alignments  \n\nWe evaluate the entropies of the soft alignments obtained with different speech representations in teacher forcing mode. Let $\\alpha_{t j}$ be the alignment score between target token $y_{t}$ and source speech frame $x_{j}$ , we evaluate the entropy of the probability distTrihbisu timoena $\\alpha_{t}$ ,e $\\begin{array}{r}{\\bar{H_{t}}\\;=\\;\\sum_{j=1}^{|x|}\\alpha_{t j}\\log\\alpha_{t j}}\\end{array}$ kfeonr s eavte trhye  tcarogreptu st olkeevenl. (How $10\\%$ ). A low entropy means the attention mechanism is confident in its source-target alignments (see example in Figure 3). Table 5 shows clearly that, in our low resource setting, wav2vec leads to better alignments (lower entropy) than fbanks. Fine-tuning and normalization of self-supervised representations also improve the soft alignments.  \n\nTable 5: Averaged entropies of soft-alignments on How2 dev and val set. AST models trained on $I O\\%$ partition of How2.   \n\n\n<html><body><table><thead><tr><td><b>No.</b></td><td><b>Feature</b></td><td><b>How2 dev</b></td><td><b>How2 val</b></td></tr></thead><tbody><tr><td>1</td><td>wav2vec</td><td>0.66</td><td>0.66</td></tr><tr><td>2</td><td>wav2vec + FT</td><td>0.65</td><td>0.65</td></tr><tr><td>3</td><td>wav2vec+norm</td><td>0.57</td><td>0.57</td></tr><tr><td>4</td><td>wav2vec + FT + norm</td><td>0.51</td><td>0.51</td></tr><tr><td>5</td><td>fbanks</td><td>0.89</td><td>0.90</td></tr><tr><td>6</td><td>fbanks + norm</td><td>0.93</td><td>0.93</td></tr></tbody></table></body></html>  \n\n![](images/addfaf86b0328d6eae6c34599ede915e52886b11d232e5a1cc7b4f549a04f045.jpg)  \nFigure 3: Soft alignments between source speech features and target text for sentence $^{\\epsilon}A$ outra pessoa perde.”  \n\n# 5.3. Better robustness to speaker variability  \n\nTable 6: Equal error rate $(E E R\\mathrm{~\\textperthousand~}$ ) on the VoxCeleb1 test and LibriSpeech test sets for female $(f)$ and male (m) speakers.  \n\n<html><body><table><thead><tr><td><b>No.</b></td><td><b>Feature</b></td><td><b>VoxCeleb</b></td><td><b>Libri (f)</b></td><td><b>Libri (m)</b></td></tr></thead><tbody><tr><td>1</td><td>wav2vec</td><td>22.75</td><td>11.22</td><td>2.23</td></tr><tr><td>2</td><td>wav2vec + norm</td><td>20.93</td><td>10.54</td><td>1.79</td></tr><tr><td>3</td><td>fbanks</td><td>15.78</td><td>5.47</td><td>0.89</td></tr><tr><td>4</td><td>fbanks + norm</td><td>16.25</td><td>3.47</td><td>0.67</td></tr></tbody></table></body></html>  \n\nTo investigate robustness to speaker variability, we trained several automatic speaker verification (ASV) systems using wav2vec or fbanks features. Models are trained on LibriSpeech train-clean-360 dataset [23] using Kaldi [32]. ASV systems are based on x-vectors and probabilistic linear discriminant analysis (PLDA) [33]. To extract $\\mathbf{X}$ -vectors, we used a time delay neural network (TDNN) model topology similar to the one described in [33]. Input features are fbanks or wav2vec (optionally normalized) while output corresponds to 921 speakers of the training corpus. ASV experiments are conducted on the VoxCeleb1 test [34] and LibriSpeech test-clean [23] sets.9 ASV results (equal error rate - EER) are presented in Table 6. We observe that in all experiments, models trained on wav2vec features provide significantly higher EER in comparison with fbanks. This confirms our hypothesis that wav2vec representations remove speaker information from speech signal.10  \n\n# 6. Conclusion  \n\nWe investigated the impact of self-supervised learning for endto-end AST. It was shown that representations based on contrastive predicting coding (CPC) improve results significantly compared to baseline filter-bank, in low-medium resource conditions $(t r a i n<100h)$ ). Our explanation is that self-supervised representations show better phone discrimination, source-target alignments and speaker robustness.  \n\n# 7. Acknowledgements  \n\nThis work was funded by the French Research Agency (ANR) through the ON-TRAC project under contract number ANR-18- CE23-0021.", "appendix": ""}, {"title": "Librispeech: An ASR corpus based on public domain audio books", "authors": null, "bibkey": "librispeech_an_asr_corpus_based_on_public_domain_audio_books", "bibitem": "@article{UnknownLAACBOPDAB,\n  url = {https://doi.org/10.1109/ICASSP.2015.7178964},\n  title = {Librispeech: An ASR corpus based on public domain audio books},\n  bibkey = {UnknownLAACBOPDAB}\n}", "url": "https://doi.org/10.1109/ICASSP.2015.7178964", "latex_url": null, "latex_path": null, "pdf_url": "https://sci.bban.top/pdf/10.1109/icassp.2015.7178964.pdf", "pdf_path": "output/download_papers/icassp.2015.7178964/icassp.2015.7178964.pdf", "md_url": null, "latex_length": 0, "latex": null, "abstract": "This paper introduces a new corpus of read English speech. suitable for training and evaluating speech recognition systems.  The Lib­ riSpeech corpus is derived from audiobooks that are part of the Lib­ riVox project, and contains 1000 hours of speech sampled at $16\\,\\mathrm{kHz}$ . We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built lan­ guage models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself.  We are also releasing Kaldi scripts that make it easy to build these systems. Index Terms- Speech Recognition, Corpus, LibriVox", "abstract_length": 712, "abstract_token": 153, "introduction": "the language models, which we make available with this corpus. Fi­ nally in Section 5 we present experimental results on models trained on this data set, using both the LibriSpeech dev and test sets and Wall Street Journal (WSJ) [5] test sets.", "introduction_length": 243, "introduction_token": 56, "reference": "# 7. REFERENCES  \n\nSimilarly LibriSpeech's language models are used with WSJ acoustic models to decode LibriSpeech's test sets. For these tests the results in Table 3 were obtained by rescoring with the full 4-gram language model from Section 4.  \n\nIn order to be able to rescore lattices using large language mod­ els in a memory efficient manner, we implemented a new rescoring tool, which is now part of the Kaldi toolkit. Table 4 shows the word  \n\n[I] K. Prahallad, Automatic building of synthetic voices from audio books, Ph.D. thesis, CMU, Pittsburgh, 2010.   \n[2] S. King and V. Karaiskos, \"The Blizzard Challenge 2012,\" in Proceedings Blizzard Workshop, 2012.   \n[3] \"Creative  Commons  Attribution  4.0  International  Public  License,\"  https:/ /creativecommons. org/ licenses/by/4. 0/, November 2013.   \n[4] D. Povey, A. Ghoshal, et aI., \"The Kaldi Speech Recognition Toolkit,\" in Proc. ASRU, 2011.  \n\n[23] X. Zhaug, J. Trmal, D. Povey, aud S. Khudaupur, \"Improving deep neural network acoustic models using generalized maxout networks,\"  in iEEE international Conference on Acoustics, Speech and Signal Processing, iCASSP 2014, Florence, italy, May 49,2014,2014, pp. 215-219.  \n\n[5] D. B. Paul aud J. M. Baker, \"The design for the Wall Street Journal-based CSR corpus,\" in Proceedings of the workshop on Speech and Natural Language. Association for Computational Linguistics, 1992, pp. 357-362.   \n[6] T J. Hazen, \"Automatic alignment aud error correction of hu­ mau generated trauscripts for long speech recordings,\"  in in Proc. interspeech, 2006.   \n[7] X. Anguera, J. Luque, aud C. Gracia, \"Audio-to-text align­ ment for speech recognition with very limited resources,\"  in interspeech, 2014.   \n[8] R. Sproat et aI., \"Normalization of non-staudard words,\" Com­ puter Speech & Language, vol. 15, no. 3, pp. 287-333, 2001.   \n[9] A. Stolcke,  \"SRILM - An Extensible Lauguage Modeling Toolkit,\" in iCSLP, 2002.   \n[10] I.H. Witten and TC. Bell, 'The zero-frequency problem: Es­ timating the probabilities of novel events in adaptive text com­ pression,\" IEEE Transactions on information Theory, vol. 37, no. 4,1991.   \n[11] M. Bisani and H. Ney, \"Joint-sequence models for grapheme­ to-phoneme conversion.,\" Speech Communication, vol. 50, no. 5, pp. 434--451, 2008.   \n[12] D. Povey aud D. Kauevsky aud B. Kingsbury aud B. Ramab­ hadrau aud G. Saon aud K. Visweswariah,  \"Boosted MMI for Feature and Model Space Discriminative Training,\"  in iCASSP, 2008.   \n[13] S. Davis aud P. Mermelstein, \"Comparison of parametric repre­ sentations for monosyllabic word recognition in continuously spoken sentences,\"  Acoustics, Speech and Signal Processing, iEEE Transactions on, vol. 28, no. 4, pp. 357-366, 1980.   \n[14] M.J.F. Gales,  \"Semi-tied covariauce matrices for hidden markov models,\" IEEE Transactions on Speech and Audio Pro­ cessing, vol. 7, pp. 272-281, 1999.   \n[15] T Smith and M. Waterman, \"Identification of common molec­ ular subsequences,\"  Journal of Molecular Biology, vol. 147, no. I, pp. 195-197, 1981.   \n[16] v.I. Levenshtein, \"Binary Codes Capable of Correcting Dele­ tions, Insertions aud Reversals,\" Soviet Physics Doklady, vol. 10, pp. 707, 1966.   \n[17] N. Braunschweiler, M. J. F. Gales, and S. Buchholz, \"Lightly supervised recognition for automatic alignment of large coher­ ent speech recordings.,\" in lNTERSPEECH. 2010, pp. 2222- 2225,ISCA.   \n[18] M. J. F. Gales and P. C. Woodland, \"Mean and Variance Adap­ tation Within the MLLR Framework,\" Computer Speech and Language, vol. 10, pp. 249-264, 1996.   \n[19] T Anastasakos, J. McDonough, R. Schwartz, and J. Makhoul, \"A Compact Model for Speaker-Adaptive Training,\" in iCSLP, 1996.   \n[20] S. Meignier and T Merlin, \"UUM SpkDiarization: an open source toolkit for diarization,\" in CMU SPUD Workshop, Dal­ las (Texas, USA), March 2010.   \n[21] R. Kneser aud H. Ney, \"Improved backing-off for m-gram lau­ guage modeling,\" in iCASSP, 1995, vol. 1, pp. 181-184.   \n[22] S. F. Chen and J. Goodman, \"An empirical study of smoothing techniques for lauguage modeling,\" in Proceedings of the 34th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1996, pp. 310-318.", "reference_length": 4202, "reference_token": 1310, "txt_length": 23308, "txt_token": 6074, "txt": "# ISPEECH: AN ASR CORPUS BASED ON PUBLIC DOMAIN AUDIO BOOK  \n\nVassil Panayotov, Guoguo Chen\\*, Daniel Povey\\*, Sanjeev Khudanpur\\*  \n\n\\*Center for Language and Speech Processing & Human Language Technology Center of Excellence The Johns Hopkins University, Baltimore, MD 21218, USA  \n\n# ABSTRACT  \n\nThis paper introduces a new corpus of read English speech. suitable for training and evaluating speech recognition systems.  The Lib­ riSpeech corpus is derived from audiobooks that are part of the Lib­ riVox project, and contains 1000 hours of speech sampled at $16\\,\\mathrm{kHz}$ . We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built lan­ guage models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself.  We are also releasing Kaldi scripts that make it easy to build these systems.  \n\nIndex Terms- Speech Recognition, Corpus, LibriVox  \n\n# 1. INTRODUCTION  \n\nthe language models, which we make available with this corpus. Fi­ nally in Section 5 we present experimental results on models trained on this data set, using both the LibriSpeech dev and test sets and Wall Street Journal (WSJ) [5] test sets.  \n\n# 2. AUDIO AUGNMENT  \n\nMost acoustic model training procedures expect that the training data come in the form of relatively short utterances, usually up to few tens of seconds in length, each with corresponding text. Therefore we need to align the audio recordings with the corresponding texts, and split them into short segments.  We also aim to exclude seg­ ments of audio that might not correspond exactly with the aligned text. Our procedure is similar to that described in [6], and consists of two stages. (Note: we have since become aware of a different, phone-based approach [7]).  \n\nThe rapid increase in the amount of multimedia content on the In­ ternet in recent years makes it feasible to automatically collect data for the purpose of training statistical models. This is particularly true when the source data is already organized into well curated, machine readable collections.  The LibriVox projece, a volunteer effort, is currently responsible for the creation of approximately 8000 public domain audio books, the majority of which are in English. Most of the recordings are based on texts from Project Gutenberg2, also in the public domain.  \n\nAlthough the use of audio books for building synthetic voices [1, 2] has previously been investigated, we are not aware of any freely available read speech corpus in English that is suitable for training and testing speech recognition systems, and which is as large scale as the one we present here. The volunteer-supported speech-gathering effort Voxforge , on which the acoustic models we used for align­ ment were trained, contains a certain amount of LibriVox audio, but the dataset is much smaller than the one we present here, with around 100 hours of English speech, and suffers from major gender and per­ speaker duration imbalances.  \n\nThis paper presents the LibriSpeech corpus, which is a read speech data set based on LibriVox's audio books.  The corpus is  freely  available4  under the very  permissive CC BY 4.0 li­ cense [3] and there are example scripts in the open source Kaldi ASR toolkit [4] that demonstrate how high quality acoustic models can be trained on this data.  \n\nSection 2 presents the long audio alignment procedure that we used in the creation of this corpus. Section 3 describes the structure of the corpus. In Section 4 we describe the process we used to build  \n\n# 2.1. Text preprocessing, lexicon and LM creation  \n\nEach book's text is normalized by converting it into upper-case, re­ moving the punctuation, and expanding common abbreviations and non-standard words [8]. Then the SRILM toolkit [9] is used to train a Witten-Bell [10] smoothed bigram language model on the text of that book. We base our lexicon on CMUdict, from which we remove the numeric stress markers; the pronunciations for out-of-vocabulary (OOV) words are generated with the Sequitur G2P toolkit [11]. In order to avoid possible problems with recognizing excessively long audio recordings, the audio chapters are split into segments of up to 30 minutes in length.  The audio is then recognized using the gmm-decode-faster decoder from the Kaldi toolkit, trained on the VoxForge dataset.  For this first decoding pass we use a triphone model discriminatively trained with Boosted MMI [12], based on MFCC [13] features processed with frame-splicing over 7 frames, followed by LDA, followed by a global semi-tied covariance (STC) transform [14].  \n\n# 2.2. First alignment stage  \n\nWe use the Smith-Waterman alignment algorithm [15] to find the best single region of al ignment between the recognized audio and the chapter text. This is like doing Levenshtein alignment [16], except we do not require it to consume the whole reference or hypothesis from the beginning to end, and it also has tunable rather than fixed weights for the different kinds of errors. From this we take the largest single region of similarity (which in most cases would be the entire chapter) and discard the rest, if any. Within that region of similarity, we mark a transcript word as being part of an \"island of confidence\" if it is part of an exact match with the reference whose length is 12 phones or more. We now split the audio into shorter segments, of 35 seconds or less, using a dynamic programming algorithm. We  \n\n![](images/a7045346c162c6a4e2590af9d650985c83193a4aac0de62b51f45f137cf1db14.jpg)  \nFig. 1. Example grammar (G) acceptor for the second stage of the alignment algorithm   \nFig. 2. Examples of typical text-audio discrepancies detected in second stage decoding. a) Chapter 1 of \"North anger Abbey \" by J. Austen, read by Kara Shallenberg; b) Chapter 23 of \"Frankenstein \" by M. Shelley, read by Hugh McGuire; c) Chapter 15 of \"Frankenstein \" by M. Shelley, read by Gord Mackenzie  \n\nReference: A family of ten children will be always called a fine family . . .   \na) Hypothesis: _sil A FAMILY OF TEN CHILDREN WILL #del ALWAYS #insOOl6 _b jy #insOOl7 CALLED A FINE FAMILY _sil Explanation: Transposition of \"be \" and \"always\". Reference: . . .  upon her arm and ... 1 rushed towards her . . .   \nb) Hypothesis: _sil UPON HER ARM #ins0020 _s #ins0021 _sil AND ... I RUSHED _sil #ins0054 -'ly _r -'lh _sh _t #ins0055 TOWARDS HER Explanation: Reader pronounces \"arms \" instead of \"arm\" and repeats \"I rushed \". Reference: Morning dmvned before 1 arrived at the village ofChamounix . . .   \nc) Hypothesis: _sil MORNING DAWNED BEFORE I ARRIVED AT THE VILLAGE OF -sil #insOOl8 _sh -'lh Jl1 -ow _n jy #ins0021 Explanation: G2P error- the auto-generated dictionary entry is \"CHAMOUNIX CH AE M UW N IH K S\", which is wrong.  \n\nonly allow it to split on silence intervals whose length is at least 0.5 second and which are inside an island of confidence.  This allows us to generate, with reasonable confidence, a candidate text for each split piece of audio.  \n\n# 2.3. Second alignment stage  \n\nThe goal of the second stage is to filter out segments where the can­ didate text obtained by the first stage has a high likelihood of be­ ing inaccurate. Possible sources of text-audio mismatch include in­ accuracies in Project Gutenberg texts, reader-introduced insertions, deletions, substitutions and transpositions, and involuntary disfluen­ cies [1,17]. Other significant sources of mismatch that we noticed are inaccurate text normalization and grapheme-to-phoneme errors in the automatically generated pronunciations.  \n\nIn this second stage of alignment, we use a custom-generated decoding graph for each segment. The decoding graph, diagrammed in Figure I, is formed from a combination of the linear sequence of words in the transcript with a generic phone-level bigram lan­ guage model. Our aim is to use the phone-level bigram to allow ar­ bitrary insertions between words in the transcript, or replacement of words in the transcript; we will reject any utterance whose decoding shows any deviation from the transcript. We also experimented with a single-phone filler model to model errors, but found the phone­ level bigram was more effective at finding segments with inaccurate transcripts.  \n\nThe most obvious way to generate the decoding graph would be to include multiple copies of the phone-level bigram graph, but this would lead to very large decoding graphs.  Instead we use a single copy of the bigram part of the decoding graph (Figure I), but we modify the decoder so that after entering the bigram part of the model from word-position $_x$ in the transcript, we may only return at position $_x$ (corresponding to an insertion between words) or position $_{x+1}$ (corresponding to a substitution or deletion of a word). This is like a pushdown transducer that can only store one item in the pushdown store.  \n\nIn this second  decoding  stage, we  use  a speaker-adapted model [IS, 19] with fMLLR transforms estimated at the speaker level, based on the transcript generated by the first decoding pass.  \n\nIn most cases this algorithm succeeds in detecting text-audio mismatches, especially for native speakers. There are also instances of false rejections. A common problem is, for example, the \"assimi­ lation \" of a short, 1-2 phone word into a neighboring silence period, which leads to an erroneous detection of deletion from the audio. However, since the original amount of audio in the audiobooks is so large, we can afford to lose a certain percentage of it. Figure 2 shows examples of the kinds of errors that we typically find by applying this method.  \n\nThe whole alignment process took approximately 65 hours on two Amazon EC2 cc2.Sxlarge instances, to produce an initial set of aligned audio of size approximately 1200 hours.  \n\n# 2.4. Data segmentation  \n\nThe second stage of alignment, which we described above, gives us a subset of the audio segments of length up to 35 seconds, that have a good likelihood of having accurate transcripts. Next we break these long segments up into smaller segments. We used two different methods for this. For training data, our rule was to split on any si­ lence interval longer than 0.3 seconds. For test data, we only allowed splits if those intervals coincided with a sentence break in the refer­ ence text. The idea was that data split at sentence breaks is likely to be easier to recognize from a language modeling point of view.  \n\n# 3. DATA SELECTION AND CORPUS STRUCTURE  \n\n# 3.1. Data selection  \n\nTo select the audio recordings for inclusion into the corpus we use LibriVox's $\\mathrm{API}^{5}$ to collect information about the readers the audio book projects in which they participated, and the chapte:s of books that they read. The URLs for audio files and reference texts were obtained by matching the information from LibriVox's API with the metadata records from the Internet Archive6 and Project Gutenberg's RDF/XML files7• For a small fraction of audio books no exact match for the title was found in Project Gutenberg, so to improve coverage we allowed a fuzzy matching of titles.  \n\nIn order to guarantee that there was no speaker overlap between the training, development and test sets, we wanted to ensure that each recording is unambiguously attributable to a single speaker. To that end we exclude such LibriVox genres as, for example, \"Dra­ matic Reading \", which include predominantly multi-reader audio chapters. As an extra precaution, in the final post-processing step of the alignment processing the recordings are processed with the LIUM speaker diarization toolkit [20] to automatically detect multi­ speaker chapters. A custom GUI application was written, that makes use of the text-audio alignment information and the speaker diariza­ tion information, to allow for quick inspection and filtering out of the remaining multi-speaker recordings. This application also made it possible to quickly produce gender information for the speakers and to discard a small number of recordings that had excessive au­ dio quality problems.  \n\nWe ensured a gender balance at the speaker level and in terms of the amount of data available for each gender.  \n\n# 3.2. Corpus partitions  \n\nThe size of the corpus makes it impractical, or at least inconvenient for some users, to distribute it as a single large archive.  Thus the training portion of the corpus is split into three subsets, with approx­ imate size 100, 360 and 500 hours respectively. A simple automatic procedure was used to select the audio in the first two sets to be, on average, of higher recording quality and with accents closer to US English. An acoustic model was trained on WSJ's si-84 data subset and was used to recognize the audio in the corpus, using a bigram LM estimated on the text of the respective books. We computed the Word Error Rate (WER) of this automatic transcript relative to our reference transcripts obtained from the book texts.  \n\nThe speakers in the corpus were ranked according to the WER of the WSJ model's transcripts, and were divided roughly in the middle, with the lower-WER speakers designated as \"clean \" and the higher­ WER speakers designated as \"other\".  From the \"clean \" pool, 20 male and 20 female speakers were drawn at random and assigned to a development set. The same was repeated to form a test set. For each dev or test set speaker, approximately eight minutes of speech are used, for total of approximately 5 hours and 20 minutes each. Note that, as mentioned in Section 2.4, we use a different segmentation procedure for development and test data, than for training data.  \n\nThe rest of the audio in the \"clean \" pool was randomly split into two training sets with approximate size 100 and 360 hours respec­ tively. For each speaker in these training sets the amount of speech was limited to 25 minutes, in order to avoid major imbalances in per-speaker audio duration.  \n\n<html><body><table><thead><tr><td><b>subset</b></td><td><b>hours</b></td><td><b>per-spk minutes</b></td><td><b>female spkrs</b></td><td><b>male spkrs</b></td><td><b>total spkrs</b></td></tr></thead><tbody><tr><td>dev-clean</td><td>5.4</td><td>8</td><td>20</td><td>20</td><td>40</td></tr><tr><td>test-clean</td><td>5.4</td><td>8</td><td>20</td><td>20</td><td>40</td></tr><tr><td>dev-other</td><td>5.3</td><td>10</td><td>16</td><td>17</td><td>33</td></tr><tr><td>test-other</td><td>5.1</td><td>10</td><td>17</td><td>16</td><td>33</td></tr><tr><td>train-clean-100</td><td>100.6</td><td>25</td><td>125</td><td>126</td><td>251</td></tr><tr><td>train-clean-360</td><td>363.6</td><td>25</td><td>439</td><td>482</td><td>921</td></tr><tr><td>train-other-500</td><td>496.7</td><td>30</td><td>564</td><td>602</td><td>1166</td></tr></tbody></table></body></html>  \n\nThe \"other \" pool was similarly split into test and development sets, and a single training set of approximately 500 hours. For this pool, however we did not choose the development and test sets at random; instead we deliberately chose more challenging data. The WER we computed using the WSJ models was used to rank the speakers in order of increasing difficulty, and the speakers for the test and development set were randomly chosen from the third quar­ tile of this sorted list. Table 1 provides a summary of all subsets in the corpus.  \n\n# 4. LANGUAGE MODELS  \n\nTo make it easy to reproduce the results we report here, we have re­ leased language model training data and pre-built language models online8, along with the text data that we used to build the language models. The language model training material is carefully selected to avoid any overlap with the texts which appear in the test and de­ velopment sets.  \n\nThe source material for these language models is Project Guten­ berg books. All books, in their entirety, on which the test and de­ velopment sets are based were filtered out, as well as any book whose title has cosine similarity, over letter 3-grams, greater tha 0.7 with any of the titles of these books.  After filtering on titles, the text of approximately 22 000 candidate books was downloaded from Project Gutenberg. An inverted index of all 5-grams, with stop words deleted, is built for the books in the test and development sets.  All candidate books are then checked against this index and each book for which more than one percent of the 5-grams which appear in it, appear in any of the books in the test and development sets, is removed from the candidate set. The method is effective for finding shared text such as, for example, popular fairy tales which are present in more than one fairy tales collection, long citations of poems in other works, and so on. We used other heuristics to also filter out texts such as numeric tables, sequences from the Human Genome Project, and other types of documents that were deemed inappropriate for language model training.  \n\nAfter the above steps, approximately 14 500 public domain books, containing around 803 million tokens in total and 900 000 unique words, remained.  \n\nTo select a lexicon, the words in the corpus were ranked by fre­ quency, and the 200 000 most frequent words were selected. Around one third of these words are present in the eMU pronunciation dic­ tionary, accounting for around $97.5\\%$ of all tokens in the evaluation sets; we generated pronunciations for the remaining words using the Sequitur G2P toolkit [II]. Modified Kneser-Ney smoothed 3- and 4-grams [21, 22] are trained. The perplexity for the 3-gram model is 170, and the out of vocabulary token rate is approximately $0.4\\%$ on average. For the 4-gram language model the perplexity is around 150.  \n\n# 5. EXPERIMENTS  \n\nIn this section we present decoding results using models trained using various amounts of LibriSpeech data, and on WSJ data, on both LibriSpeech and WSJ test sets. The recordings available from LibriVox are not completely ideal for training acoustic models for other domains, because the audio is MP3-compressed and because the site's guidelines for upload recommend noise rem oval 9 and vol­ ume normalizationlO  These practices are not consistently enforced, however, so there is a significant fraction of noisy and non-processed audio available, combined with audio that has been subjected to au­ tomatic noise removal.  \n\nTable 3.  WERs on LibriSpeech's test sets; all results are obtained by rescoring with a 4-gram language model.   \n\n\n<html><body><table><thead><tr><td colspan=\"2\"><b>Acoustic model</b></td><td><b>dev- clean</b></td><td><b>test- clean</b></td><td><b>dev- other</b></td><td><b>test- other</b></td></tr></thead><tbody><tr><td rowspan=\"7\">LS</td><td>SAT 100h</td><td>8.19</td><td>9.32</td><td>29.31</td><td>31.52</td></tr><tr><td>SAT 460h</td><td>7.26</td><td>8.34</td><td>26.27</td><td>28.11</td></tr><tr><td>SAT 960h</td><td>7.08</td><td>8.04</td><td>21.14</td><td>22.65</td></tr><tr><td>DNN 100h</td><td>5.93</td><td>6.59</td><td>20.42</td><td>22.52</td></tr><tr><td>DNN 460h</td><td>5.27</td><td>5.78</td><td>17.67</td><td>19.12</td></tr><tr><td>DNN 960h</td><td>4.90</td><td>5.51</td><td>12.98</td><td>13.97</td></tr><tr><td rowspan=\"2\">WSJ</td><td>SAT si-284</td><td>10.87</td><td>12.44</td><td>39.44</td><td>41.26</td></tr><tr><td>DNN si-284</td><td>7.80</td><td>8.49</td><td>27.39</td><td>30.01</td></tr></tbody></table></body></html>  \n\nIn order to assess the performance of the acoustic models on non-compressed audio we use the Wall Street Journal read speech corpus [5], as a baseline. We employ language models, trained on the text material the WSJ corpus provides, in conjunction with acoustic models trained on the LibriSpeech data to decode WSJ's test sets, and compare the results with those for state-of-the-art models trained on WSJ's own si-284 set (which contains 82 hours of speech data). The WSJ results we present in Table 2 are for the \"open-vocabulary \" (60K) test condition, using not the standard 60K word dictionary supplied with WSJ but an extended version that we built to cover more of the words that appear in the WSJ language models. For the language model we used a pruned version of the standard trigram language model that is distributed with the WSJ corpus. The acoustic models, referred to as SAT in the tables, are speaker-adapted GMM models [18, 19], and those referred to as DNN, are based on deep neural networks with p-norm non-linearities [23], trained and tested on top of fMLLR features. The models estimated on LibriSpeech's training data are named after the amount of audio they were built on. The models marked with $46\\theta h$ are trained on the union of the \"train-clean- $100^{\\circ}$ and \"train-clean- $360^{\\circ}$ subsets, and those marked with $960h$ are trained on all of LibriSpeech's training sets.  \n\nTable 2.  WERs on WSJ's test sets under the \"open vocabulary \" (60K) test condition   \n\n\n<html><body><table><thead><tr><td colspan=\"2\"><b>Acoustic model</b></td><td><b>eval'92</b></td><td><b>dev'93</b></td><td><b>eval'93</b></td></tr></thead><tbody><tr><td rowspan=\"7\">LS</td><td>SAT 100h</td><td>5.72</td><td>10.10</td><td>9.14</td></tr><tr><td>SAT 460h</td><td>5.49</td><td>8.96</td><td>7.69</td></tr><tr><td>SAT 960h</td><td>5.33</td><td>8.87</td><td>8.32</td></tr><tr><td>DNN 100h</td><td>4.08</td><td>7.31</td><td>6.73</td></tr><tr><td>DNN 460h</td><td>3.90</td><td>6.75</td><td>5.95</td></tr><tr><td>DNN 960h</td><td>3.63</td><td>6.52</td><td>5.66</td></tr><tr><td rowspan=\"2\">WSJ</td><td>SAT si-284</td><td>6.26</td><td>9.39</td><td>9.19</td></tr><tr><td>DNN si-284</td><td>3.92</td><td>6.97</td><td>5.74</td></tr></tbody></table></body></html>  \n\nerror rates for language models of different size. The first pass de­ coding is performed using the 3-gram model pruned with threshold $3\\,\\times\\,\\bar{10}^{-7}$ using SRILM's pruning method; the other numbers are obtained through lattice rescoring.  \n\nTable 4. LM rescoring results for the 960 hour DNN model   \n\n\n<html><body><table><thead><tr><td><b>Language model</b></td><td><b>dev- clean</b></td><td><b>test- clean</b></td><td><b>dev- other</b></td><td><b>test- other</b></td></tr></thead><tbody><tr><td>3-gram prn. thresh. 3e-7</td><td>7.54</td><td>8.02</td><td>18.51</td><td>19.41</td></tr><tr><td>3-gram prn. thresh. 1e-7</td><td>6.57</td><td>7.21</td><td>16.72</td><td>17.66</td></tr><tr><td>3-gram full</td><td>5.14</td><td>5.74</td><td>13.89</td><td>14.77</td></tr><tr><td>4-gram full</td><td>4.90</td><td>5.51</td><td>12.98</td><td>13.97</td></tr></tbody></table></body></html>  \n\n# 6. CONCLUSIONS  \n\nWe have automatically aligned and segmented English read speech from audiobooks with the corresponding book text, and filtered out segments with noisy transcripts, in order to produce a corpus of En­ glish read speech suitable for training speech recognition systems. We have demonstrated that models trained with our corpus do better on the standard Wall Street Journal (WSJ) test sets than models built on WSJ itself - the larger size of our corpus (1000 hours, versus the 82 hours of WSJ's si-284 data) outweighs the audio mismatch. We are releasing this corpus onlinell and have introduced scripts into the Kaldi speech recognition toolkit so that others can easily repli­ cate these results.", "appendix": ""}, {"title": "Speech Translation and the End-to-End Promise: Taking Stock of Where We Are", "authors": "Matthias Sperber, Matthias Paulik", "bibkey": "speech_translation_and_the_end_to_end_promise_taking_stock_of_where_we_are", "bibitem": "@article{sperber2020speech,\n  url = {http://arxiv.org/abs/2004.06358v1},\n  title = {Speech Translation and the End-to-End Promise: Taking Stock of Where We Are},\n  authors = {Matthias Sperber, Matthias Paulik},\n  abstract = {Over its three decade history, speech translation has experienced several shifts in its primary research themes; moving from loosely coupled cascades of speech recognition and machine translation, to exploring questions of tight coupling, and finally to end-to-end models that have recently attracted much attention. This paper provides a brief survey of these developments, along with a discussion of the main challenges of traditional approaches which stem from committing to intermediate representations from the speech recognizer, and from training cascaded models separately towards different objectives.   Recent end-to-end modeling techniques promise a principled way of overcoming these issues by allowing joint training of all model components and removing the need for explicit intermediate representations. However, a closer look reveals that many end-to-end models fall short of solving these issues, due to compromises made to address data scarcity. This paper provides a unifying categorization and nomenclature that covers both traditional and recent approaches and that may help researchers by highlighting both trade-offs and open research questions.},\n  arxiv_id = {2004.06358v1},\n  subject = {cs.CL},\n  submission_date = {2020-04-14T08:43:51Z}\n}", "url": "http://arxiv.org/abs/2004.06358v1", "latex_url": "http://arxiv.org/src/2004.06358v1", "latex_path": "output/download_papers/2004.06358v1/2004.06358v1", "pdf_url": "http://arxiv.org/pdf/2004.06358v1", "pdf_path": "output/download_papers/2004.06358v1/2004.06358v1.pdf", "md_url": null, "latex_length": 0, "latex": "", "abstract": "Over its three decade history, speech translation has experienced several shifts in its primary research themes; moving from loosely coupled cascades of speech recognition and machine translation, to exploring questions of tight coupling, and finally to end-to-end models that have recently attracted much attention. This paper provides a brief survey of these developments, along with a discussion of the main challenges of traditional approaches which stem from committing to intermediate representations from the speech recognizer, and from training cascaded models separately towards different objectives.   Recent end-to-end modeling techniques promise a principled way of overcoming these issues by allowing joint training of all model components and removing the need for explicit intermediate representations. However, a closer look reveals that many end-to-end models fall short of solving these issues, due to compromises made to address data scarcity. This paper provides a unifying categorization and nomenclature that covers both traditional and recent approaches and that may help researchers by highlighting both trade-offs and open research questions.", "abstract_length": 1167, "abstract_token": 191, "introduction": "Speech translation (ST), the task of translating acoustic speech signals into text in a foreign language, is a complex and multi-faceted task that builds upon work in automatic speech recognition (ASR) and machine translation (MT). ST applications are diverse and include travel assistants (Takezawa et al., 1998), simultaneous lecture translation (Fu¨gen, 2008), movie dubbing/subtitling (Saboo and Baumann, 2019; Matusov et al., 2019), language documentation and crisis response (Bansal et al., 2017), and developmental efforts (Black et al., 2002). Until recently, the only feasible approach has been the cascaded approach that applies an ASR to the speech inputs, and then passes the results on to an MT system. Progress in ST has come from two fronts: general improvements in ASR and MT models, and moving from the loosely-coupled cascade in its most basic form toward a tighter coupling. However, despite considerable efforts toward tight coupling, a large share of the progress has arguably been owed simply to general ASR and MT improvements.1 Recently, new modeling techniques and in particular end-to-end trainable encoder-decoder models have fueled hope for addressing challenges of ST in a more principled manner. Despite these hopes, the empirical evidence indicates that the success of such efforts has so far been mixed (Weiss et al., 2017; Niehues et al., 2019). In this paper, we will attempt to uncover potential reasons for this. We start by surveying models proposed throughout the three-decade history of ST. By contrasting the extreme points of loosely coupled cascades vs. purely end-to-end trained direct models, we identify foundational challenges: erroneous early decisions, mismatch between spokenstyle ASR outputs and written-style MT inputs, and loss of speech information (e.g. prosody) on the one hand, and data scarcity on the other hand. We then show that to improve data efficiency, most endto-end models employ techniques that re-introduce issues generally attributed to cascaded ST. Furthermore, this paper proposes a categorization of ST research into well-defined terms for the particular challenges, requirements, and techniques that are being addressed or used. This multidimensional categorization suggests a modeling space with many intermediate points, rather than a dichotomy of cascaded vs. end-to-end models, and reveals a number of trade-offs between different modeling choices. This implies that additional work to more explicitly analyze the interactions between these trade-offs, along with further model explorations, can help to determine more favorable points in the modeling space, and ultimately the most favorable model for a specific ST application.", "introduction_length": 2706, "introduction_token": 546, "reference": "# References  \n\nP. D. Aguero, Jordi Adell, and Antonio Bonafonte. 2006. Prosody Generation for Speech-to-Speech Translation. In International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Toulouse, France.  \n\nAntonios Anastasopoulos and David Chiang. 2018. Tied Multitask Learning for Neural Speech Translation. In North American Chapter of the Association for Computational Linguistics (NAACL), New Orleans, USA.  \n\nGopala Krishna Anumanchipalli, Luis C. Oliveira, and Alan W. Black. 2012. Intent transfer in speech-tospeech machine translation. In Workshop on Spoken  \n\nLanguage Technology (SLT), pages 153–158, Miami, USA.  \n\nNecip Fazil Ayan, Arindam Mandal, Michael Frandsen, Jing Zheng, Peter Blasco, Andreas Kathol, Frederic Bechet, Benoit Favre, Alex Marin, Tom Kwiatkowski, Mari Ostendorf, Luke Zettlemoyer, Philipp Salletmayr, Julia Hirschberg, and Svetlana Stoyanchev. 2013. ’Can you give me another word for hyperbaric?’: Improving speech translation using targeted clarification questions. In International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8391–8395, Vancouver, Canada.  \n\nParnia Bahar, Albert Zeyer, Ralf Schlu¨ter, and Hermann Ney. 2019. On Using SpecAugment for Endto-End Speech Translation. In International Workshop on Spoken Language Translation (IWSLT), Hongkong.  \n\nSrinivas Bangalore and Giuseppe Riccardi. 2001. A Finite-State Approach to Machine Translation. In North American Chapter of the Association for Computational Linguistics (NAACL), Pittsburgh, USA.  \n\nSameer Bansal, Herman Kamper, Karen Livescu, Adam Lopez, and Sharon Goldwater. 2018. LowResource Speech-to-Text Translation. In Annual Conference of the International Speech Communication Association (InterSpeech), Hyderabad, India.  \n\nSameer Bansal, Herman Kamper, Karen Livescu, Adam Lopez, and Sharon Goldwater. 2019. Pretraining on high-resource speech recognition improves low-resource speech-to-text translation. In North American Chapter of the Association for Computational Linguistics (NAACL), Minneapolis, USA.  \n\nSameer Bansal, Herman Kamper, Adam Lopez, and Sharon Goldwater. 2017. Towards speech-to-text translation without speech recognition. In European Chapter of the Association for Computational Linguistics (EACL).  \n\nDaniel Beck, Trevor Cohn, and Gholamreza Haffari. 2019. Neural Speech Translation using Lattice Transformations and Graph Networks. In Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13), pages 26–31, Hongkong.  \n\nBenjamin Beilharz, Xin Sun, Sariya Karimova, and Stefan Riezler. 2020. LibriVoxDeEn: A Corpus for German-to-English Speech Translation and Speech Recognition. In Language Resources and Evaluation (LREC), Marseille, France.  \n\nYoshua Bengio, Nicholas Le´onard, and Aaron Courville. 2013. Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation. arXiv:1308.3432.  \n\nAlexandre Be´rard, Laurent Besacier, Ali Can Kocabiyikoglu, and Olivier Pietquin. 2018. End-toEnd Automatic Speech Translation of Audiobooks.  \n\nIn International Conference on Acoustics, Speech and Signal Processing (ICASSP), Calgary, Canada.  \n\nAlexandre Berard, Olivier Pietquin, Christophe Servan, and Laurent Besacier. 2016. Listen and Translate: A Proof of Concept for End-to-End Speechto-Text Translation. In NIPS Workshop on Endto-end Learning for Speech and Audio Processing, Barcelona, Spain.  \n\nNicola Bertoldi and Marcello Federico. 2005. A New Decoder for Spoken Language Translation Based on Confusion Networks. In Automatic Speech Recognition & Understanding (ASRU), pages 86–91, San Juan, Puerto Rico.  \n\nAlan W. Black, Ralf D. Brown, Robert Frederking, Kevin Lenzo, John Moody, Alexander Rudnicky, Rita Singh, and Eric Steinbrecher. 2002. Rapid Development of Speech-to-Speech Translation Systems. In International Conference on Spoken Language Processing (ICSLP), Denver, USA.  \n\nMarcely Zanon Boito, William N. Havard, Mahault Garnerin, E´ric Le Ferrand, and Laurent Besacier. 2020. MaSS: A Large and Clean Multilingual Corpus of Sentence-aligned Spoken Utterances Extracted from the Bible. In Language Resources and Evaluation (LREC), Marseille, France.  \n\nFrancisco Casacuberta, Hermann Ney, Franz Josef Och, Enrique Vidal, J. M. Vilar, S. Barrachina, I. Garc´ıaVarea, D. Llorens, C. Mart´ınez, S. Molau, F. Nevado, M. Pastor, D. Pico´, A. Sanchis, and C. Tillmann. 2004. Some approaches to statistical and finite-state speech-to-speech translation. Computer Speech and Language, 18(1):25–47.  \n\nQiao Cheng, Meiyuan Fang, Yaqian Han, Jin Huang, and Yitao Duan. 2019. Breaking the Data Barrier: Towards Robust Speech Translation via Adversarial Stability Training. In International Workshop on Spoken Language Translation (IWSLT), Hongkong.  \n\nYong Cheng, Zhaopeng Tu, Fandong Meng, Junjie Zhai, and Yang Liu. 2018. Towards Robust Neural Machine Translation. In Association for Computational Linguistic (ACL), Melbourne, Australia.  \n\nEunah Cho, Christian Fu¨gen, Teresa Hermann, Kevin Kilgour, Mohammed Mediani, Christian Mohr, Jan Niehues, Kay Rottmann, Christian Saam, Sebastian Stu¨ker, and Alex Waibel. 2013. A real-world system for simultaneous translation of German lectures. In Annual Conference of the International Speech Communication Association (InterSpeech), pages 3473– 3477, Lyon, France.  \n\nHoang Cuong and Khalil Sima’an. 2018. A survey of domain adaptation for statistical machine translation. In Conference on Computational Linguistics (COLING), Santa Fe, USA.  \n\nMattia Antonino Di Gangi, Viet-Nhat Nguyen, Matteo Negri, and Marco Turchi. 2020. Instance-Based  \n\nModel Adaptation For Direct Speech Translation. In International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain.  \n\nPaul R Dixon, Andrew Finch, H Chiori, and H Kashioka. 2011. Investigation on the Effects of ASR Tuning on Speech Translation Performance. In International Workshop on Spoken Language Translation (IWSLT), pages 167–174, San Francisco, USA.  \n\nQuoc Truong Do, Tomoki Toda, Graham Neubig, Sakriani Sakti, and Satoshi Nakamura. 2017. Preserving Word-level Emphasis in Speech-to-speech Translation. IEEE Transactions on Audio, Speech and Language Processing, 25(3):544–556.  \n\nLong Duong, Antonios Anastasopoulos, David Chiang, Steven Bird, and Trevor Cohn. 2016. An Attentional Model for Speech Translation Without Transcription. In North American Chapter of the Association for Computational Linguistics (NAACL), pages 949–959, San Diego, USA.  \n\nMarcello Federico, Robert Enyedi, and Roberto Barrachicote. 2020. From Speech-to-Speech Translation to Automatic Dubbing. arXiv:2001.06785v3.  \n\nErin Fitzgerald, Keith Hall, and Frederick Jelinek. 2009. Reconstructing false start errors in spontaneous speech text. In European Chapter of the Association for Computational Linguistics (EACL), pages 255–263, Athens, Greece.  \n\nChristian Fu¨gen. 2008. A System for Simultaneous Translation of Lectures and Speeches. Ph.D. thesis, University of Karlsruhe.  \n\nAntonino Mattia di Gangi, Roldano Cattoni, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2019a. MuST-C : a Multilingual Speech Translation Corpus. In North American Chapter of the Association for Computational Linguistics (NAACL), Minneapolis, USA.  \n\nMattia A. di Gangi, Matteo Negri, Roldano Cattoni, Roberto Dessi, and Marco Turchi. 2019b. Enhancing Transformer for End-to-end Speech-to-Text Translation. In Machine Translation Summit XVII Volume 1: Research Track, pages 21–31, Dublin, Ireland.  \n\nMattia Antonino di Gangi, Matteo Negri, and Marco Turchi. 2019c. One-to-Many Multilingual Endto-End Speech Translation. In Automatic Speech Recognition & Understanding (ASRU), December.  \n\nPierre Godard, Gilles Adda, Martine Adda-Decker, J. Benjumea, L. Besacier, J. Cooper-Leavitt, G-N. Kouarata, L. Lamel, H. Maynard, M. Mueller, A. Rialland, S. Stueker, F. Yvon, and M. Zanon-Boito. 2018. A Very Low Resource Language Speech Corpus for Computational Language Documentation Experiments. In Language Resources and Evaluation (LREC), Miyazaki, Japan.  \n\nHe He, Jordan Boyd-Graber, and Hal. 2016. Interpretese vs. Translationese : The Uniqueness of Human Strategies in Simultaneous Interpretation. In North American Chapter of the Association for Computational Linguistics (NAACL), pages 971–976, San Diego, USA.  \n\nXiaodong He, Li Deng, and Alex Acero. 2011. Why word error rate is not a good metric for speech recognizer training for the speech translation task? In International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5632–5635, Prague, Czech Republic.  \n\nRoger Hsiao, Ashish Venugopal, Thilo Ko¨hler, Ting Zhang, Paisarn Charoenpornsawat, Andreas Zollmann, Stephan Vogel, Alan W. Black, Tanja Schultz, and Alex Waibel. 2006. Optimizing components for handheld two-way speech translation for an EnglishIraqi Arabic system. In Annual Conference of the International Speech Communication Association (InterSpeech), pages 765–768, Pittsburgh, USA.  \n\nHirofumi Inaguma, Kevin Duh, Tatsuya Kawahara, and Shinji Watanabe. 2019. Multilingual End-to-End Speech Translation. In Automatic Speech Recognition & Understanding (ASRU), Singapore.  \n\nSathish Indurthi, Houjeung Han, Nikhil Kumar Lakumarapu, Beomseok Lee, Insoo Chung, Sangha Kim, and Chanwoo Kim. 2020. Data Efficient Direct Speech-to-Text Translation with Modality Agnostic Meta-Learning. In Conference on Artificial Intelligence (AAAI), New York City, USA.  \n\nJavier Iranzo-Sa´nchez, Joan Albert Silvestre-Cerda\\`, Javier Jorge, Nahuel Rosello´, Adria\\` Gime´nez, Albert Sanchis, Jorge Civera, and Alfons Juan. 2020. Europarl-ST: A Multilingual Corpus For Speech Translation Of Parliamentary Debates. In International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain.  \n\nYe Jia, Melvin Johnson, Wolfgang Macherey, Ron J Weiss, Yuan Cao, Chung-cheng Chiu Naveen, Ari Stella, and Laurenzo Yonghui. 2019a. Leveraging Weakly Supervised Data to Improve End-to-End Speech-to-Text Translation. In International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, United Kingdom.  \n\nYe Jia, Ron J. Weiss, Fadi Biadsy, Wolfgang Macherey, Melvin Johnson, Zhifeng Chen, and Yonghui Wu. 2019b. Direct speech-to-speech translation with a sequence-to-sequence model. In Annual Conference of the International Speech Communication Association (InterSpeech), Graz, Austria.  \n\nJie Jiang, Zeeshan Ahmed, Julie Carson-Berndsen, Peter Cahill, and Andy Way. 2011. Phonetic Representation-Based Speech Translation. In Machine Translation Summit (MTS), pages 81–88, Xiamen, China.  \n\nDenis Jouvet. 2019. Speech Processing and Prosody. In International Conference of Text, Speech and Dialogue (TSD), Ljubljana, Slovenia.  \n\nTakatomo Kano, Sakriani Sakti, and Satoshi Nakamura. 2017. Structured-based Curriculum Learning for End-to-end English-Japanese Speech Translation. In Annual Conference of the International Speech Communication Association (InterSpeech), pages 2630–2634.  \n\nTakatomo Kano, Shinnosuke Takamichi, Sakriani Sakti, Graham Neubig, Tomoki Toda, and Satoshi Nakamura. 2018. End to end Model for CrossLingual Transformation of Paralinguistic Information. Machine Translation, 32(4):353–368.  \n\nAli Can Kocabiyikoglu, Laurent Besacier, and Olivier Kraif. 2018. Augmenting Librispeech with French Translations: A Multimodal Corpus for Direct Speech Translation Evaluation. In Language Resources and Evaluation (LREC), Miyazaki, Japan.  \n\nWouter M. Kouw and Marco Loog. 2018. An introduction to domain adaptation and transfer learning. Technical report, Delft University of Technology, Delft, Netherlands.  \n\nAlon Lavie, Donna Gates, Marsal Gavalda, Laura Mayfield, Alex Waibel, and Lori Levin. 1996. Multilingual Translation of Spontaneously Spoken Language in a Limited Domain. In International Conference on Computational Linguistics, pages 252–255, Copenhagen, Denmark.  \n\nRobin J Lickley. 1994. Detecting disfluency in spontaneous speech. Ph.D. thesis, University of Edinburgh.  \n\nFu-hua Liu, Liang Gu, Yuqing Gao, and Michael Picheny. 2003. Use of Statistical N-Gram Models in Natural Language Generation for Machine Translation. In International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 636– 639, Hongkong.  \n\nYuchen Liu, Hao Xiong, Zhongjun He, Jiajun Zhang, Hua Wu, Haifeng Wang, and Chengqing Zong. 2019. End-to-End Speech Translation with Knowledge Distillation. In Annual Conference of the International Speech Communication Association (InterSpeech), Graz, Austria.  \n\nEvgeny Matusov, Bjo¨rn Hoffmeister, and Hermann Ney. 2008. ASR word lattice translation with exhaustive reordering is possible. In Annual Conference of the International Speech Communication Association (InterSpeech), pages 2342–2345, Brisbane, Australia.  \n\nEvgeny Matusov, Arne Mauser, and Hermann Ney. 2006. Automatic Sentence Segmentation and Punctuation Prediction for Spoken Language Translation. In International Workshop on Spoken Language Translation (IWSLT), pages 158–165, Kyoto, Japan.  \n\nEvgeny Matusov, Hermann Ney, and Ralph Schluter. 2005. Phrase-based translation of speech recognizer word lattices using loglinear model combination. In Automatic Speech Recognition and Understanding (ASRU), pages 110–115, San Juan, Puerto Rico.  \n\nEvgeny Matusov, Patrick Wilken, and Yota Georgakopoulou. 2019. Customizing Neural Machine Translation for Subtitling. In Conference on Machine Translation (WMT), pages 82–93, Florence, Italy.  \n\nPeter Newmark. 1988. Approaches to Translation. Prentice Hall, Hertfordshire.  \n\nHermann Ney. 1999. Speech Translation: Coupling of Recognition and Translation. In International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 517–520, Phoenix, USA.  \n\nJan Niehues, Roldano Cattoni, Sebastian Stu¨ker, Matteo Negri, Marco Turchi, Thanh-Le Ha, Elizabeth Salesky, Ramon Sanabria, Loic Barrault, Lucia Specia, and Marcello Federico. 2019. The IWSLT 2019 Evaluation Campaign. In International Workshop on Spoken Language Translation (IWSLT), Hongkong.  \n\nJan Niehues, Ngoc-Quan Pham, Thanh-Le Ha, Matthias Sperber, and Alex Waibel. 2018. LowLatency Neural Speech Translation. In Annual Conference of the International Speech Communication Association (InterSpeech), Hyderabad, India.  \n\nMatthias Paulik. 2010. Learning Speech Translation from Interpretation. Ph.D. thesis, University of Karlsruhe.  \n\nStephan Peitz, Simon Wiesler, Markus NussbaumThom, and Hermann Ney. 2012. Spoken Language Translation Using Automatically Transcribed Text in Training. In International Workshop on Spoken Language Translation (IWSLT) 2011, pages 276– 283, Hongkong.  \n\nAlicia Pe´rez, Victor Guijarrubia, Raquel Justo, M. Ine´s Torres, and Francisco Casacuberta. 2007. A comparison of linguistically and statistically enhanced models for speech-to-speech machine translation. In International Workshop on Spoken Language Translation (IWSLT), Trento, Italy.  \n\nNgoc-quan Pham, Thai-son Nguyen, Thanh-Le Ha, Juan Hussain, Felix Schneider, Jan Niehues, Sebastian Stu¨ker, and Alexander Waibel. 2019. The IWSLT 2019 KIT Speech Translation System. In International Workshop on Spoken Language Translation (IWSLT), Hongkong.  \n\nJuan Pino, Liezl Puzon, Jiatao Gu, Xutai Ma, Arya D. McCarthy, and Deepak Gopinath. 2019. Harnessing Indirect Training Data for End-to-End Automatic Speech Translation: Tricks of the Trade. In International Workshop on Spoken Language Translation (IWSLT), Hongkong.  \n\nMatt Post, Gaurav Kumar, Adam Lopez, Damianos Karakos, Chris Callison-Burch, and Sanjeev Khudanpur. 2013. Improved Speech-to-Text Translation with the Fisher and Callhome Spanish–English Speech Translation Corpus. In International Workshop on Spoken Language Translation (IWSLT), Heidelberg, Germany.  \n\nPablo Romero-Fresco. 2009. More haste less speed: Edited versus verbatim respoken subtitles. Vigo International Journal of Applied Linguistics, 6:109– 134.  \n\nNicholas Ruiz and Marcello Federico. 2014. Assessing the impact of speech recognition errors on machine translation quality. Conference of the Association for Machine Translation in the Americas (AMTA), 1(November):261–274.  \n\nNicholas Ruiz, Qin Gao, William Lewis, and Marcello Federico. 2015. Adapting Machine Translation Models toward Misrecognized Speech with Text-to-Speech Pronunciation Rules and Acoustic Confusability. In Annual Conference of the International Speech Communication Association (InterSpeech), pages 2247–2251, Dresden, Germany.  \n\nAshutosh Saboo and Timo Baumann. 2019. Integration of Dubbing Constraints into Machine Translation. In Conference on Machine Translation (WMT), pages 94–101, Florence, Italy.  \n\nShirin Saleem, Szu-Chen Jou, Stephan Vogel, and Tanja Schultz. 2004. Using Word Lattice Information for a Tighter Coupling in Speech Translation Systems. In International Conference on Spoken Language Processing (ICSLP), pages 41–44, Jeju Island, Korea.  \n\nElizabeth Salesky, Matthias Sperber, and Alan W Black. 2019a. Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation. In Association for Computational Linguistics (ACL), pages 1835–1841, Florence, Italy.  \n\nElizabeth Salesky, Matthias Sperber, and Alex Waibel. 2019b. Fluent Translations from Disfluent Speech in End-to-End Speech Translation. In North American Chapter of the Association for Computational Linguistics (NAACL), Minneapolis, USA.  \n\nMatthias Sperber, Graham Neubig, Jan Niehues, and Alex Waibel. 2017a. Neural Lattice-to-Sequence Models for Uncertain Inputs. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1380–1389, Copenhagen, Denmark.  \n\nMatthias Sperber, Graham Neubig, Jan Niehues, and Alex Waibel. 2019a. Attention-Passing Models for Robust and Data-Efficient End-to-End Speech Translation. Transactions of the Association for Computational Linguistics (TACL).  \n\nMatthias Sperber, Graham Neubig, Ngoc-Quan Pham, and Alex Waibel. 2019b. Self-Attentional Models for Lattice Inputs. In Association for Computational Linguistic (ACL), pages 1185–1197, Florence, Italy.  \n\nMatthias Sperber, Jan Niehues, and Alex Waibel. 2017b. Toward Robust Neural Machine Translation for Noisy Input Sequences. In International Workshop on Spoken Language Translation (IWSLT), Tokyo, Japan.  \n\nF. W. M. Stentiford and M. G. Steer. 1988. Machine Translation of Speech. British Telecom technology journal, 6(2):116–123.  \n\nMihaela C. Stoian, Sameer Bansal, and Sharon Goldwater. 2020. Analyzing ASR pretraining for lowresource speech-to-text translation. In International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain.  \n\nUmut Sulubacak, Ozan Caglayan, Stig-Arne Gro¨nroos, Aku Rouhe, Desmond Elliott, Lucia Specia, and Jo¨rg Tiedemann. 2019. Multimodal Machine Translation through Visuals and Speech. arXiv:1911.12798.  \n\nEiichiro Sumita, Tohru Shimizu, and Satoshi Nakamura. 2007. NICT-ATR speech-to-speech translation system. In Association for Computational Linguistic (ACL), pages 25–28, Prague, Czech Republic. Association for Computational Linguistics.  \n\nTzu-Wei Sung, Jun-You Liu, Hung-yi Lee, and Linshan Lee. 2019. Towards End-to-End Speechto-Text Translation with Two-Pass Decoding. In ICASSP, pages 7175–7179, Brighton, United Kingdom.  \n\nToshiyuki Takezawa, Tsuyoshi Morimoto, Yoshinori Sagisaka, Nick Campbell, Hitoshi Iida, Fumiaki Sugaya, Akio Yokoo, and Seiichi Yamamoto. 1998. A Japanese-to-English Speech Translation System: ATR-MATRIX. In International Conference on Spoken Language (ICSLP), Sydney, Australia.  \n\nAndros Tjandra, Sakriani Sakti, and Satoshi Nakamura. 2019. Speech-to-speech Translation between Untranscribed Unknown Languages. In Automatic Speech Recognition & Understanding (ASRU), Singapore.  \n\nYulia Tsvetkov, Florian Metze, and Chris Dyer. 2014. Augmenting translation models with simulated acoustic confusions for improved spoken language translation. In Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 616–625, Gothenburg, Sweden.  \n\nEnrique Vidal. 1997. Finite-State Speech-to-Speech Translation. In International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 111–114, Munich, Germany.  \n\nAlex Waibel, Ajay N. Jain, Arthur E. McNair, Hiroaki Saito, Alexander G. Hauptmann, and Joe Tebelskis. 1991. JANUS: a speech-to-speech translation system using connectionist and symbolic processing strategies. In International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 793–796, Toronto, Canada.  \n\nChengyi Wang, Yu Wu, Shujie Liu, Zhenglu Yang, and Ming Zhou. 2020. Bridging the Gap between PreTraining and Fine-Tuning for End-to-End Speech Translation. In Conference on Artificial Intelligence (AAAI), New York City, USA.  \n\nYe-Yi Wang and Alex Waibel. 1998. Modeling with structures in statistical machine translation. In Association for Computational Linguistics and International Conference on Computational Linguistics (COLING-ACL), pages 1357–1363, Montre´al, Canada.  \n\nRon J. Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, and Zhifeng Chen. 2017. Sequence-toSequence Models Can Directly Transcribe Foreign Speech. In Annual Conference of the International Speech Communication Association (InterSpeech), Stockholm, Sweden.  \n\nM. Woszczyna, N. Coccaro, A. Eisele, A. Lavie, A. McNair, T. Polzin, I. Rogina, C. P. Rose, T. Sloboda, M. Tomita, J. Tsutsumi, N. Aoki-Waibel, A. Waibel, and W. Ward. 1993. Recent advances in JANUS: A Speech Translation System. In Workshop on Human Language Technology (HLT), pages 211–216, Plainsboro, USA.  \n\nPei Zhang, Boxing Chen, Niyu Ge, and Kai Fan. 2019. Lattice Transformer for Speech Translation. In Association for Computational Linguistic (ACL), pages 6475–6484, Florence, Italy.  \n\nRuiqiang Zhang, Genichiro Kikui, Hirofumi Yamamoto, and Wai-Kit Lo. 2005. A Decoding Algorithm for Word Lattice Translation in Speech Translation. In International Workshop on Spoken Language Translation (IWSLT), pages 23–29, Pittsburgh, USA.", "reference_length": 22081, "reference_token": 5777, "txt_length": 36956, "txt_token": 8203, "txt": "# Speech Translation and the End-to-End Promise: Taking Stock of Where We Are  \n\nMatthias Sperber Apple sperber@apple.com  \n\nMatthias Paulik Apple mpaulik@apple.com  \n\n# Abstract  \n\nOver its three decade history, speech translation has experienced several shifts in its primary research themes; moving from loosely coupled cascades of speech recognition and machine translation, to exploring questions of tight coupling, and finally to end-to-end models that have recently attracted much attention. This paper provides a brief survey of these developments, along with a discussion of the main challenges of traditional approaches which stem from committing to intermediate representations from the speech recognizer, and from training cascaded models separately towards different objectives.  \n\nRecent end-to-end modeling techniques promise a principled way of overcoming these issues by allowing joint training of all model components and removing the need for explicit intermediate representations. However, a closer look reveals that many end-to-end models fall short of solving these issues, due to compromises made to address data scarcity. This paper provides a unifying categorization and nomenclature that covers both traditional and recent approaches and that may help researchers by highlighting both trade-offs and open research questions.  \n\n# 1 Introduction  \n\nSpeech translation (ST), the task of translating acoustic speech signals into text in a foreign language, is a complex and multi-faceted task that builds upon work in automatic speech recognition (ASR) and machine translation (MT). ST applications are diverse and include travel assistants (Takezawa et al., 1998), simultaneous lecture translation (Fu¨gen, 2008), movie dubbing/subtitling (Saboo and Baumann, 2019; Matusov et al., 2019), language documentation and crisis response (Bansal et al., 2017), and developmental efforts (Black et al., 2002).  \n\nUntil recently, the only feasible approach has been the cascaded approach that applies an ASR to the speech inputs, and then passes the results on to an MT system. Progress in ST has come from two fronts: general improvements in ASR and MT models, and moving from the loosely-coupled cascade in its most basic form toward a tighter coupling. However, despite considerable efforts toward tight coupling, a large share of the progress has arguably been owed simply to general ASR and MT improvements.1  \n\nRecently, new modeling techniques and in particular end-to-end trainable encoder-decoder models have fueled hope for addressing challenges of ST in a more principled manner. Despite these hopes, the empirical evidence indicates that the success of such efforts has so far been mixed (Weiss et al., 2017; Niehues et al., 2019).  \n\nIn this paper, we will attempt to uncover potential reasons for this. We start by surveying models proposed throughout the three-decade history of ST. By contrasting the extreme points of loosely coupled cascades vs. purely end-to-end trained direct models, we identify foundational challenges: erroneous early decisions, mismatch between spokenstyle ASR outputs and written-style MT inputs, and loss of speech information (e.g. prosody) on the one hand, and data scarcity on the other hand. We then show that to improve data efficiency, most endto-end models employ techniques that re-introduce issues generally attributed to cascaded ST.  \n\nFurthermore, this paper proposes a categorization of ST research into well-defined terms for the particular challenges, requirements, and techniques that are being addressed or used. This multidimensional categorization suggests a modeling space with many intermediate points, rather than a dichotomy of cascaded vs. end-to-end models, and reveals a number of trade-offs between different modeling choices. This implies that additional work to more explicitly analyze the interactions between these trade-offs, along with further model explorations, can help to determine more favorable points in the modeling space, and ultimately the most favorable model for a specific ST application.  \n\n# 2 Chronological Survey  \n\nThis chapter surveys the historical development of ST and introduces key concepts that will be expanded upon later.2  \n\n# 2.1 Loosely Coupled Cascades  \n\nEarly efforts to realize ST (Stentiford and Steer, 1988; Waibel et al., 1991) introduced what we will refer to as the loosely coupled cascade in which separately built ASR and MT systems are employed and the best hypothesis of the former is used as input to the latter. The possibility of speech-to-speech translation, which extends the cascade by appending a text-to-speech component, was also considered early on (Waibel et al., 1991).  \n\nThese early systems were especially susceptible to errors propagated from the ASR, given the widespread use of interlingua-based MT which relied on parsers unable to handle mal-formed inputs (Woszczyna et al., 1993; Lavie et al., 1996; Liu et al., 2003). Subsequent systems Wang and Waibel (1998); Takezawa et al. (1998); Black et al. (2002); Sumita et al. (2007), relying on data driven, statistical MT, somewhat alleviated the issue, and also in part opened the path towards tighter integration.  \n\n# 2.2 Toward Tight Integration  \n\nResearchers soon turned to the question of how to avoid early decisions and the problem of error propagation. While the desirable solution of full integration over transcripts is intractable (Ney, 1999), approximations are possible. Vidal (1997); Bangalore and Riccardi (2001); Casacuberta et al. (2004); Pe´rez et al. (2007) compute a composition of FSTbased ASR and MT models, which approximates the full integration up to search heuristics, but suffers from limited reordering capabilities. A much simpler, though computationally expensive, solution is the $n$ -best translation approach which replaces the sum over all possible transcripts by a sum over only the $n$ -best ASR outputs (Woszczyna et al., 1993; Lavie et al., 1996). Follow-up work suggested lattices and confusion nets (Saleem et al., 2004; Zhang et al., 2005; Bertoldi and Federico, 2005) as more effective and efficient alternatives to $n$ -best lists. Lattices proved flexible enough for integration into various translation models, from word-based translation models to phrasebased ST Matusov et al. (2005, 2008) to neural lattice-to-sequence models (Sperber et al., 2017a, 2019b; Zhang et al., 2019; Beck et al., 2019).  \n\nAnother promising idea was to limit the detrimental effects of early decisions, rather than attempting to avoid early decisions. One way of achieving this is to train robust translation models by introducing synthetic ASR errors into the source side of MT corpora (Peitz et al., 2012; Tsvetkov et al., 2014; Ruiz et al., 2015; Sperber et al., 2017b; Cheng et al., 2018, 2019). A different route is taken by Dixon et al. (2011); He et al. (2011) who directly optimize ASR outputs towards translation quality.  \n\nBeyond early decisions, research moved towards tighter coupling by addressing issues arising from ASR and MT models being trained separately and on different types of corpora. Domain adaptation techniques were used by Liu et al. (2003); F¨ugen (2008) to adapt models to the spoken language domain. Matusov et al. (2006); Fu¨gen (2008) propose re-segmenting the ASR output and inserting punctuation, so as to provide the translation model with well-formed text inputs. In addition, disfluency removal (Fitzgerald et al., 2009) was proposed to avoid translation errors caused by disfluencies that are often found in spoken language.  \n\nAguero et al. (2006); Anumanchipalli et al. (2012); Do et al. (2017); Kano et al. (2018) propose prosody transfer for speech-to-speech translation by determining source-side prosody and applying transformed prosody characteristics to the aligned target words.  \n\n# 2.3 Speech Translation Corpora  \n\nIt is important to realize that all efforts to this point had used separate ASR and MT corpora for training. This often led to a mismatch between ASR trained on data from the spoken domain, and MT trained on data from the written domain. End-toend ST data (translated speech utterances) was only available in small quantities for test purposes.  \n\nPaulik (2010) proposes the use of audio recordings of interpreter-mediated communication scenarios, which is not only potentially easier to obtain, but also does not exhibit such domain mismatches. Post et al. (2013) manually translate an ASR corpus to obtain an end-to-end ST corpus, and show that training both ASR and MT on the same corpus considerably improves results compared to using out-of-domain MT data. Unfortunately, high annotation costs prevent scaling of the latter approach, so follow-up work concentrates on compiling ST corpora from available web sources (Godard et al., 2018; Kocabiyikoglu et al., 2018; di Gangi et al., 2019a; Boito et al., 2020; Beilharz et al., 2020; Iranzo-S´anchez et al., 2020). Note that despite these efforts, publicly available ST corpora are currently strongly limited in terms of both size and language coverage. For practical purposes, the use of separate ASR and MT corpora is therefore currently unavoidable.  \n\n# 2.4 End-to-End Models  \n\nThe availability of end-to-end ST corpora, along with the success of end-to-end models for MT and ASR, led researchers to explore ST models trained in an end-to-end fashion. This was fueled by a hope to solve the issues addressed by prior research in a principled and more effective way. Duong et al. (2016); Berard et al. (2016); Bansal et al. (2018) explore direct ST models that translate speech without using explicitly generated intermediate ASR output. In contrast, Kano et al. (2017); Anastasopoulos and Chiang (2018); Wang et al. (2020) explore end-to-end trainable cascades and triangle models, i.e. models that do rely on transcripts, but are optimized in part through end-to-end training. Multi-task training and pre-training were proposed as a way to incorporate additional ASR and MT data and reduce dependency on scarce end-to-end data (Weiss et al., 2017; B´erard et al., 2018; Bansal et al., 2019; Stoian et al., 2020; Wang et al., 2020). As these techniques were not able to exploit ASR and MT data as effectively as the loosely coupled cascade, other approaches like subtask training for end-to-end-trainable cascades (Sperber et al., 2019a), data augmentation (Jia et al., 2019a; Pino et al., 2019), knowledge distillation (Liu et al., 2019), and meta-learning (Indurthi et al., 2020) were proposed. Salesky et al.  \n\n![](images/cfb3bda8dd4f2684b25bde20cba4f99c426bcd331d9ec7d866f8ace4056244ca.jpg)  \nFigure 1: Illustration of inference strategies (§4.2): Committed/marginalizing cascade (CC/MC), direct (Di), committed/marginalizing triangle (CT/MT), joint $(\\upsigma+)$ . Double lines differentiate the observed variable (speech input $X$ ) from random variables (intermediate representations $I R$ and translations $T$ ). Shaded circles marginalize over random variables.  \n\n(2019a) propose pre-segmenting speech frames, (Jia et al., 2019b; Tjandra et al., 2019) explore speech-to-speech translation. Sung et al. (2019); di Gangi et al. (2019b); Di Gangi et al. (2020); Bahar et al. (2019); Inaguma et al. (2019); di Gangi et al. (2019c) transfer ideas from MT and ASR fields to ST.  \n\n# 3 Central Challenges  \n\nGiven the abundance of prior work, a clear picture on where we currently stand is needed. For purposes of identifying the key challenges in ST research, this section will contrast the extreme cases of the loosely coupled cascade (CC in Fig. $1)^{3}$ against the vanilla direct model (Di in Fig. 1).4 We emphasize that these models are only extreme points in a modeling space with many intermediate points, as we will see in $\\S4$ . We assume appropriate speech features $X$ as inputs. $T,{\\hat{T}}\\in{\\bar{T}}$ denote candidate/best translations, respectively, from the MT hypothesis space. $S{\\in}{\\mathcal{H}}$ denotes a graphemic transcript from the ASR hypothesis space.  \n\n# 3.1 Challenges of Loosely Coupled Cascades  \n\nThe loosely coupled cascade justifies its decomposition into MT model $P_{\\mathrm{MT}}\\left(T|S\\right)$ and ASR model $P_{\\mathrm{ASR}}\\left(S|X\\right)$ as follows:  \n\n3ASR and MT models trained separately on different corpora; intermediate representation is ASR 1-best output.  \n\n4Encoder-decoder model trained on speech utterances paired with translations; no intermediate representations used.  \n\n$$\n\\begin{array}{r l}&{\\hat{T}=\\underset{T\\in\\mathcal{T}}{\\mathrm{argmax}}\\,P\\left(T\\,|\\,X\\right)}\\\\ &{\\quad=\\underset{T\\in\\mathcal{T}}{\\mathrm{argmax}}\\,\\underset{S\\in\\mathcal{H}}{\\sum}\\,P\\left(T|S,X\\right)P\\left(S|X\\right)}\\\\ &{\\quad\\quad\\overset{T\\in\\mathcal{T}}{\\sim}\\underset{T\\in\\mathcal{T}}{\\mathrm{argmax}}\\,\\underset{S\\in\\mathcal{H}}{\\sum}P_{\\mathrm{MT}}\\left(T|S\\right)P_{\\mathrm{ASR}}\\left(S|X\\right)}\\\\ &{\\quad\\approx\\underset{T\\in\\mathcal{T}}{\\mathrm{argmax}}\\,\\underset{S\\in\\mathcal{H}^{\\prime}}{\\sum}\\,P_{\\mathrm{MT}}\\left(T|S\\right)P_{\\mathrm{ASR}}\\left(S|X\\right)}\\end{array}\n$$  \n\nNote that here the set $\\mathcal{H^{\\prime}}$ contains only a single entry, the 1-best ASR output. The approximations in these derivations directly result in the following three foundational challenges:  \n\nErroneous early decisions: Committing to a potentially erroneous $S$ during inference. This leads to the well-known problem of error propagation (Ruiz and Federico, 2014) and is caused by avoiding the intractable full integration over transcripts (Eq. 3) and using only the 1-best ASR output instead (Eq. 4). Typical countermeasures include increasing $\\mathcal{H^{\\prime}}$ to cover a larger space using lattices or confusion nets, or improving the robustness of MT models.  \n\nMismatched source-language: ASR and MT components model the source-language (transcript) priors $P_{\\mathrm{MT}}(S)$ and $P_{\\mathrm{ASR}}(S)$ differently.5 Causes include both modeling assumptions, e.g. ASR modeling only unpunctuated transcripts; and mismatched training data, leading to stylistic and topical divergence. Typical countermeasures are domain adaptation techniques, disfluency removal, text normalization, and segmentation/punctuation insertion.  \n\nInformation loss: Assumed conditional independence between inputs and outputs, given the transcript: $\\left(T\\perp X\\right)|\\,S$ . This can be seen in Eq. 3 and results in any information not represented in $S$ to be lost for the translation step. In particular, the MT model is unaware of prosody which structures and disambiguates the utterances, thus playing a role similar to punctuation in written texts; and provides ways to emphasize words or parts of the messages that the speaker think are important. Prosody also conveys information on the speaker’s attitude and emotional state (Jouvet, 2019).  \n\n# 3.2 Challenges of the Vanilla Direct Model  \n\nConsider instead the other extreme case: an encoder-decoder model trained to directly produce translations from speech (Eq. 1). Because this model avoids the decomposition in Eq. 2-4, it is not subject to the three issues outlined in $\\S3.1$ . Unfortunately, this second extreme case is often impractical due to its dependency on scarce end-to-end ST training corpora $(\\S2.3)$ , rendering this model unable to compete with cascaded models that are trained on abundant ASR and MT training data.  \n\nMost recent works therefore depart from this purely end-to-end trained direct model, and incorporate ASR and MT back into training, e.g. through weakly supervised training, or by exploring end-toend trainable cascades or triangle models (CT/MT in Fig. 1). This departure raises two questions: (1) To what extent does the re-introduction of ASR and MT data cause challenges similar to those found in loosely coupled cascades? (2) Are techniques such as weakly supervised training effective enough to allow competing with the loosely coupled cascade? To address the second question, we propose the notion of data efficiency as a fourth key challenge.  \n\nData efficiency: The increase in accuracy achievable through the addition of a certain amount of training data. To assess data efficiency, data ablations that contrast models over at least two data conditions are required. We argue that empirical evidence along these lines will help considerably in making generalizable claims about the relative performance between two ST models. Generalizable findings across data conditions are critical given that ST models are trained on at least three types of corpora (ASR, MT, and end-to-end corpora), whose availability vastly differs across languages.  \n\n# 3.3 Data Efficiency vs. Modeling Power – A Trade-Off?  \n\nConsider how the incorporation of MT and ASR data into ST models of any kind may inherently cause the problems as outlined in $\\S3.1$ : Training on MT data may weaken the model’s sensitivity to prosody; the effectiveness of training on $\\mathrm{ASR+MT}$ data may be impacted by mismatched source-language issues; even some types of endto-end-trainable models make (non-discrete) early decisions that are potentially erroneous.  \n\nThis suggests a potential trade-off between data efficiency and modeling power. In order to find models that trade off advantages and disadvantages in the most favorable way, it is therefore necessary to thoroughly analyze models across the dimensions of early decisions, mismatched sourcelanguage, information loss, and data efficiency.  \n\nTable 1: Motivating examples for prosody-aware translation from English to Japanese. In the first example, prosody disambiguates whether the speaker is talking about Lucy as a third person or directly addressing Lucy. In the second example, prosody disambiguates whether cheese or jam is an open set or a closed set. In both cases, the surface form of the Japanese translation requires considerable changes depending on the prosody.   \n\n\n<html><body><table><thead><tr><td><b>English</b></td><td><b>Japanese</b></td></tr></thead><tbody><tr><td>this is my niece , lucy</td><td>の ）シ lucy,kono ko ga watashi no suekko desu kochirawa suekkono lucydesu 子</td></tr><tr><td>this is my niece , lucy</td><td>の子私の子</td></tr><tr><td>will you have Acheese or Ajam 千一</td><td> chizutoka jamu toka,doreni shimasuka</td></tr></tbody></table></body></html>  \n\nAnalyzing early decisions: Problems due to erroneous early decisions are inference-time phenomena in which upstream ASR errors are responsible for errors in the final translation outputs. It follows that the problem disappears for hypothetical utterances for which the ASR can generate error-free intermediate representations. Thus, models that do not suffer from erroneous early decisions will expectedly exhibit an advantage over other models especially for acoustically challenging inputs, and less so for inputs with clean acoustics. This angle can provide us with strategies for isolating errors related to this particular phenomenon. Prior work in this spirit has demonstrated that lattice-tosequence translation is in fact beneficial especially for acoustically challenging inputs (Sperber et al., 2017a), and that cascaded models with non-discrete intermediate representations are less sensitive to artificially perturbed intermediate representations than if using discrete transcripts as an intermediate representation (Sperber et al., 2019a).  \n\nAnalyzing mismatched source-language: Endto-end ST corpora allow for controlled experiments in which one can switch between matched vs. mismatched (out-of-domain) MT corpora. Post et al. (2013) demonstrated that using a matched corpus can strongly improve translation quality for loosely coupled cascades. We are not aware of such analyses in more recent work.  \n\nAnalyzing information loss: Prior work (Aguero et al., 2006; Anumanchipalli et al., 2012; Do et al., 2017; Kano et al., 2018) has addressed prosody transfer in speech-to-speech translation, but to our knowledge the question of how such information should inform textual translation decisions is still unexplored. Table 1 shows examples that may motivate future work in this direction.  \n\nAnalyzing data efficiency: While several prior works aim at addressing this problem, often only a single data condition is tested, limiting the generalizability of findings. We are aware of three recent works that do analyze data efficiency across several data conditions (Jia et al., 2019a; Sperber et al., 2019a; Wang et al., 2020). Findings indicate that both pretraining and data synthesizing outperform multi-task training in terms of data efficiency, and that end-to-end trainable cascades are on par with loosely coupled cascades, while strongly outperforming multi-task training.  \n\n# 4 Modeling Techniques  \n\nLet us now break apart modeling techniques from prior literature into four overarching categories, with the aim of exposing the ST modeling space between the extreme points of vanilla direct models and loosely coupled cascades.  \n\n# 4.1 Intermediate Representations  \n\nAlmost all models use intermediate representations (IRs) in some form: non-direct models to support both training and inference, and direct models to overcome data limitations. IRs are often speech transcripts, but not necessarily so. A number of factors must be considered for choosing an appropriate IR, such as availability of supervised data, inference accuracy, expected impact of erroneous early decisions, and the feasibility of backpropagation through the IR for end-to-end training. We list several possibilities below:  \n\nTranscripts: Generally used in the loosely coupled cascade. Being a discrete representation, this option prevents end-to-end training via backpropagation, although future work may experiment with work-arounds such as the straight-through gradient estimator (Bengio et al., 2013). Besides graphemic transcripts, phonetic transcripts are another option (Jiang et al., 2011).  \n\nHidden representations: Kano et al. (2017); Anastasopoulos and Chiang (2018); Sperber et al. (2019a) propose the use of hidden representations that are the by-product of a neural decoder generating an auxiliary IR such as a transcript. Advantages of this representation are differentiability, prevention of information loss, and weakened impact of erroneous early decisions. A downside is that endto-end ST data is required for training.  \n\nLattices: Lattices compactly represent the space over multiple sequences, and therefore weaken the impact of erroneous early decisions. Future work may explore lattices over continuous, hidden representations, and end-to-end training for ST models with lattices as intermediate representation.  \n\nOther: Prior work further suggests presegmented speech frames (Salesky et al., 2019a) or unsupervised speech-unit clusters (Tjandra et al., 2019) as intermediate representation. Further possibilities may be explored in future work.  \n\n# 4.2 Inference Strategies  \n\nThe conditioning graph (Fig. 1) reveals independence assumptions and use of IRs at inference time. Some strategies avoid the problem of early decisions (MC, Di, MT, Jt), while others remove the conditional independence assumption between inputs and outputs (Di, CT, MT, Jt).  \n\nCommitted cascade (CC): Compute one IR, rely on it to generate outputs (Eq. 4). Includes both the loosely coupled cascade, and recent end-to-end trainable cascaded models such as by Kano et al. (2017); Sperber et al. (2019a).  \n\nMarginalizing cascade (MC): Compute outputs by relying on IRs, but marginalize over them instead of committing to one (Eq. 3). As marginalization is intractable, approximations such as $n$ -best translation or lattice translation are generally used.  \n\nDirect (Di): Compute outputs without relying on IRs (Eq. 1). To address data limitations, techniques such as multi-task training or data augmentation can be used, but may reintroduce certain biases.  \n\nCommitted triangle $\\bf(C I r)$ : Commit to an IR, then produce outputs by conditioning on both inputs and intermediate representation. Anastasopoulos and Chiang (2018), who introduce the triangle model, use it in its marginalizing form (see below). Unexplored variations include the use of discrete transcripts as IR, which interestingly could be seen as a strict generalization of the loosely coupled cascade and should therefore never perform worse than it if trained properly.  \n\nMarginalizing triangle $(\\bf{M T}\\pmb{\\Sigma})$ : Produce output by conditioning on both input and IR, while marginalizing over the latter (Eq. 2). Anastasopoulos and Chiang (2018) marginalize by taking an $n$ -best list, with $n$ set to only 4 for computational reasons. This raises the question of whether the more computationally efficient lattices could be employed instead. Similar considerations apply to the end-to-end trainable marginalizing cascade.  \n\nJoint (Jt): Changes the problem formulation to $\\hat{S},\\hat{T}\\,=\\,\\operatorname{argmax}_{S\\in\\mathcal{H},T\\in\\mathcal{T}}P r\\left(S,T\\mid X\\right)$ . This is a useful optimization for many applications which display both transcripts and translations to the user, yet to our knowledge has never been explicitly addressed by researchers.  \n\n# 4.3 Training Strategies  \n\nThis group of techniques describes the types of supervision signals applied during training.  \n\nSubtask training: Training of sub-components by pairing IRs with either the speech inputs or the output translations. Loosely coupled cascades rely on this training technique while recently proposed cascaded and triangle models often combine subtask training and end-to-end training.  \n\nAuxiliary task training: Training by pairing either model inputs or outputs with data from an arbitrary auxiliary task through multi-task training.6 This technique has been used in two ways in literature: (1) To incorporate ASR and MT data into direct models by using auxiliary models that share parts of the parameters with the main model (Weiss et al., 2017). Auxiliary models are introduced for training purposes only, and discarded during inference. This approach has been found inferior at exploiting ASR and MT data when compared to subtask training (Sperber et al., 2019a). (2) To incorporate various types of less closely related training data, such as the use of multitask training to exploit ASR data from an unrelated third language (Bansal et al., 2019; Stoian et al., 2020).  \n\nEnd-to-end: Supervision signal that directly pairs speech inputs and output translations. This technique is appealing because it jointly optimizes all involved parameters and may lead to better optima. The main limitation is lack of appropriate data, which can be addressed by combined training with one of the alternative supervision types, or by training on augmented data, as discussed next.  \n\n# 4.4 End-to-End Training Data  \n\nManual: Speech utterances for training are translated (and possibly transcribed) by humans. This is the most desirable case, but such data is currently scarce. While we have seen growth in data sources in the past two years (§2.3), collecting more data is an extremely important direction for future work.  \n\nAugmented: Data obtained by either augmenting an ASR corpus with automatic translations, or augmenting an MT corpus with synthesized speech. This has been shown more data efficient than multitask training in the context of adding large MT and ASR corpora (Jia et al., 2019a). Pino et al. (2019) find that augmented ASR corpora are more effective than augmented MT corpora. This approach allows training direct models and end-to-end models even when no end-to-end data is available. Knowledge distillation can be seen as an extension (Liu et al., 2019). An important problem that needs analysis is to what extent mismatched source-language and information loss degrade the augmented data.  \n\nZero-Shot: Using no end-to-end data during training. While augmented data can be used in most situations in which no manual data is available, it suffers from certain biases that may harm the ST model. Similarly to how zero-shot translation enables translating between unseen combinations of source and target languages, it may be worth exploring whether some recent models, such as direct models or cascades with non-discrete IRs, can be trained without resorting to any end-to-end data for the particular language pair of interest.  \n\n# 5 Applications and Requirements  \n\nWhile we previously described the task of ST simply as the task of generating accurate text translations from speech inputs, the reality is in fact much more complicated. Future work may exploit new modeling techniques to explicitly address the aspects drawn out below.  \n\n# 5.1 Mode of Delivery  \n\nBatch mode: A (potentially large) piece of recorded speech is translated as a whole. Segmentation into utterances may or may not be given. This mode allows access to future context, and imposes no strict computational restrictions. Typical applications include movie subtitling (Matusov et al., 2019) and dubbing (Saboo and Baumann, 2019; Federico et al., 2020).  \n\nConsecutive: Real-time situation where inputs are provided as complete utterances or other translatable units, and outputs must be produced with low latency. A typical example is a two-way translation system on a mobile device (Hsiao et al., 2006). This is the only mode of delivery that allows interaction between speaker and translator (Ayan et al., 2013).  \n\nSimultaneous: Real-time situation where latency is crucial and outputs are produced incrementally based on incoming audio stream. Simultaneous translation is faced with an inherent delay vs. accuracy trade-off, such as in a typical lecture translation application (F¨ugen, 2008). In addition to computational latency, which is relevant also with consecutive translation, simultaneous translation suffers from inherent modeling latency caused by factors including reordering.  \n\n# 5.2 Output Medium  \n\nText: This is a standard setting, but is nevertheless worth discussing in more detail for at least two reasons: (1) as is well-known in the subtitling industry, reading speeds can be slower than speaking and listening speeds (Romero-Fresco, 2009), implying that a recipient may not be able to follow verbatim text translations in case of fast speakers, and that summarization may be warranted. (2) Text display makes repair strategies possible that are quite distinct from spoken outputs: One can alter, highlight, or remove past outputs. One possible way of exploiting this is Niehues et al. (2018)’s strategy of simultaneous translation through re-translation.  \n\nTable 2: Examples for faithful Spanish to English translations, taken from (Salesky et al., 2019b).   \n\n\n<html><body><table><thead><tr><td><b>ES EN</b></td><td><b>también tengo um eh estoy tomando una clase .. i also have um eh i'm taking a marketing class ..</b></td></tr></thead><tbody><tr><td>ES</td><td>porque que va, mja ya te acuerda que ..</td></tr><tr><td>EN</td><td>because what is, mhm do you recall now that ..</td></tr></tbody></table></body></html>  \n\nSpeech: Speech outputs have been used since the early days (Lavie et al., 1996), but whether to apply text-to-speech on top of translated text has often been seen as a question to leave to user interface designers. Here, we argue that ST researchers should examine in what ways speech outputs should differ from text outputs. For example, is disfluency removal (Fitzgerald et al., 2009) beneficial for speech outputs, given that human listeners are naturally able to repair disfluencies (Lickley, 1994)? Further examples that need more exploration are prosody transfer (Aguero et al., 2006) and models that directly translate speech-to-speech (Jia et al., 2019b).  \n\n# 5.3 The Role of Transcripts  \n\nMandatory transcripts: User interface displays both transcripts and translations to the user. This scenario has been implemented in many applications (Hsiao et al., 2006; Cho et al., 2013), but has received little attention in the context of end-to-end ST research. It ties together with the joint inference model (§4.3). Note that with loosely coupled cascades, there is little need to consider this scenario explicitly because the application can simply display the by-product transcripts to the user. But this is not easily possible with direct models or with models using IRs other than transcripts.  \n\nAuxiliary transcripts: Transcriptions are not needed as user-facing model outputs, but may be exploited as IRs during training and possibly inference. This is the most typical formal framing of the ST task, assuming that transcribed training data is useful mainly for purposes of improving the final translation.  \n\nTranscript-free: No transcribed training data exists, so the model cannot rely on supervised transcripts as IR. The main scenario is endangered language preservation for languages without written script, where it is often easier to collect translated speech than transcribed speech (Duong et al., 2016).  \n\n# 5.4 Translation Method  \n\nThe method of translation is an especially relevant factor in ST, which commonly includes a transfer from the spoken into the written domain. Here, we provide two reference points for the method of translation, while referring to Newmark (1988) for a more nuanced categorization.  \n\nFaithful: Keeps the contextual meaning of the original as precisely as possible within the grammatical constraints of the target language. With text as output medium, faithful translation may result in poor readability, e.g. due to the translation of disfluencies (Table 2). Arguably the most appropriate output medium for faithful ST would be speech, although user studies are needed to confirm this. Another application are high-stake political meetings in which translations must stay as close to the original sentence as possible. As we move toward more distant language pairs, the practicability of faithful translation of spoken language with disfluencies becomes increasingly questionable.  \n\nCommunicative: Renders the contextual meaning of the original such that both content and style are acceptable and comprehensible by the target audience. An important example for improving communicativeness is disfluency removal (Fitzgerald et al., 2009). Given that human translators and interpreters adapt their translation method depending on factors that include input and output medium (He et al., 2016), more research is needed beyond disfluency removal. Communicative translations are especially relevant in casual contexts where convenience and low cognitive effort are mandative. Arguably the closest neighbor of spoken language style in the text realm is social media, it would be interesting to attempt speech-to-text translation with social-media style outputs.  \n\n# 6 Discussion  \n\nRecent works on end-to-end modeling techniques are motivated by the prospect of overcoming the loosely coupled cascade’s inherent issues, yet of the issues outlined in $\\S2.1$ , often only the goal of avoiding early decisions is mentioned motivationally. While early decisions and data efficiency have been recognized as central issues, empirical insights are still limited and further analysis is needed. Mismatched source-language and information loss are often not explicitly analyzed.  \n\nWe conjecture that the apparent trade-off between data efficiency and modeling power may explain the mixed success in outperforming the loosely coupled cascade. In order to make progress in this regard, the involved issues (early decisions, mismatched source-language, information loss, data efficiency) need to be precisely analyzed (§3), and more model variants (§4) should be explored. As a possible starting point one may aim to extend, rather than alter, traditional models, e.g. applying end-to-end training as a fine-tuning step, employing a direct model for rescoring, or adding a triangle connection to a loosely coupled cascade. We further suggest that more principled solutions to the different application-specific requirements (§5) should be attempted. Perhaps it is possible to get rid of segmentation as a separate step in batch delivery mode, or perhaps text as output medium can be used to visualize repairs more effectively. Several of the application-specific requirements demand user studies and will not be sufficiently solved by relying on automatic metrics only.  \n\n# 7 Conclusion  \n\nWe started this paper with a chronological survey of three decades of ST research, focusing on carving out the key concepts. We then provided definitions of the central challenges, techniques, and requirements, motivated by the observation that recent work does not sufficiently analyze these challenges. We exposed a significant space of both modeling ideas and application-specific requirements left to be addressed in future research.  \n\nOur hope is to encourage meaningful and generalizable comparisons on our quest toward overcoming the long-standing issues found in ST models.", "appendix": ""}, {"title": "wav2vec: Unsupervised Pre-Training for Speech Recognition", "authors": "Steffen Schneider, Alexei Baevski, Ronan Collobert, Michael Auli", "bibkey": "wav2vec_unsupervised_pre_training_for_speech_recognition", "bibitem": "@article{AuliWUPFSR,\n  url = {https://arxiv.org/pdf/1904.05862},\n  title = {wav2vec: Unsupervised Pre-Training for Speech Recognition},\n  authors = {Steffen Schneider, Alexei Baevski, Ronan Collobert, Michael Auli},\n  bibkey = {AuliWUPFSR},\n  journal = {Interspeech 2019}\n}", "url": "https://arxiv.org/pdf/1904.05862", "latex_url": null, "latex_path": null, "pdf_url": "https://arxiv.org/pdf/1904.05862", "pdf_path": "output/download_papers/1904.05862v4/1904.05862v4.pdf", "md_url": null, "latex_length": 0, "latex": null, "abstract": "We explore unsupervised pre-training for speech recognition by learning representations of raw audio. wav2vec is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training. We pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task. Our experiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline by up to $36\\,\\%$ when only a few hours of transcribed data is available. Our approach achieves $2.43\\,\\%$ WER on the nov92 test set. This outperforms Deep Speech 2, the best reported character-based system in the literature while using two orders of magnitude less labeled training data.1", "abstract_length": 750, "abstract_token": 155, "introduction": "Current state of the art models for speech recognition require large amounts of transcribed audio data to attain good performance (Amodei et al., 2016). Recently, pre-training of neural networks has emerged as an effective technique for settings where labeled data is scarce. The key idea is to learn general representations in a setup where substantial amounts of labeled or unlabeled data is available and to leverage the learned representations to improve performance on a downstream task for which the amount of data is limited. This is particularly interesting for tasks where substantial effort is required to obtain labeled data, such as speech recognition. In computer vision, representations for ImageNet (Deng et al., 2009) and COCO (Lin et al., 2014) have proven to be useful to initialize models for tasks such as image captioning (Vinyals et al., 2016) or pose estimation (Pavllo et al., 2019). Unsupervised pre-training for computer vision has also shown promise (Doersch et al., 2015; He´naff et al., 2019). In natural language processing (NLP), unsupervised pre-training of language models (Devlin et al., 2018; Radford et al., 2018; Baevski et al., 2019) improved many tasks such as text classification, phrase structure parsing and machine translation (Edunov et al., 2019; Lample & Conneau, 2019). In speech processing, pre-training has focused on emotion recogniton (Lian et al., 2018), speaker identification (Ravanelli & Bengio, 2018), phoneme discrimination (Synnaeve & Dupoux, 2016a; van den Oord et al., 2018) as well as transferring ASR representations from one language to another (Kunze et al., 2017). There has been work on unsupervised learning for speech but the resulting representations have not been applied to improve supervised speech recognition (Synnaeve & Dupoux, 2016b; Kamper et al., 2017; Chung et al., 2018; Chen et al., 2018; Chorowski et al., 2019). In this paper, we apply unsupervised pre-training to improve supervised speech recognition. This enables exploiting unlabeled audio data which is much easier to collect than labeled data. Our model, wav2vec, is a convolutional neural network that takes raw audio as input and computes a general representation that can be input to a speech recognition system. The objective is a contrastive loss that requires distinguishing a true future audio sample from negatives (Collobert et al., 2011; Mikolov et al., 2013; van den Oord et al., 2018). Different to previous work (van den Oord et al., 2018), we move beyond frame-wise phoneme classification and apply the learned representations to improve strong supervised ASR systems. wav2vec relies on a fully convolutional architecture which can be easily parallelized over time on modern hardware compared to recurrent models used in previous work (§2). ![](images/526c534d2238c72f5d7db6a1f45105c1c6bffb22fe5975ae7e8dc890ad6df46c.jpg) Figure 1: Illustration of pre-training from audio data $\\mathcal{X}$ which is encoded with two convolutional neural networks that are stacked on top of each other. The model is optimized to solve a next time step prediction task. Experimental results on the WSJ benchmark demonstrate that pre-trained representations estimated on about 1,000 hours of unlabeled speech can substantially improve a character-based ASR system and outperform the best character-based result in the literature, Deep Speech 2, improving WER from $3.1\\,\\%$ to $2.43\\,\\%$ . On TIMIT, pre-training enables us to match the best reported result in the literature. In a simulated low-resource setup with only eight hours of transcribed audio data, wav2vec reduces WER by up to $36\\,\\%$ compared to a baseline model that relies on labeled data only $(\\S3,\\S4)$ .", "introduction_length": 3707, "introduction_token": 883, "reference": "# REFERENCES  \n\nDario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-to-end speech recognition in english and mandarin. In Proc. of ICML, 2016.   \nAlexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. Cloze-driven pretraining of self-attention networks. arXiv, abs/1903.07785, 2019.   \nYi-Chen Chen, Chia-Hao Shen, Sung-Feng Huang, Hung-yi Lee, and Lin-Shan Lee. Almost-unsupervised speech recognition with close-to-zero resource based on phonetic structures learned from very small unpaired speech and text data. arXiv, abs/1810.12566, 2018.   \nJan Chorowski, Ron J. Weiss, Samy Bengio, and Aa¨ron van den Oord. Unsupervised speech representation learning using wavenet autoencoders. arXiv, abs/1901.08810, 2019.   \nYu-An Chung, Wei-Hung Weng, Schrasing Tong, and James R. Glass. Unsupervised cross-modal alignment of speech and text embedding spaces. arXiv, abs/1805.07467, 2018.   \nRonan Collobert, Jason Weston, Le´on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. JMLR, 2011.   \nRonan Collobert, Christian Puhrsch, and Gabriel Synnaeve. Wav2letter: an end-to-end convnet-based speech recognition system. arXiv, abs/1609.03193, 2016.   \nRonan Collobert, Awni Hannun, and Gabriel Synnaeve. A fully differentiable beam search decoder. arXiv, abs/1902.06022, 2019.   \nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In Proc. of ICML, 2017.   \nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proc. of CVPR, 2009.   \nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv, abs/1810.04805, 2018.  \n\n# 5 CONCLUSIONS  \n\nCarl Doersch, Abhinav Gupta, and Alexei A. Efros. Unsupervised visual representation learning by context prediction. In Proc. of ICCV, 2015.   \nSergey Edunov, Alexei Baevski, and Michael Auli. Pre-trained language model representations for language generation. In Proc. of NAACL, 2019.   \nJohn S. Garofolo, David Graff, Doug Paul, and David S. Pallett. CSR-I (WSJ0) Complete LDC93S6A. Web Download. Linguistic Data Consortium, 1993a.   \nJohn S. Garofolo, Lori F. Lamel, William M. Fisher, Jonathon G. Fiscus, David S. Pallett, and Nancy L. Dahlgren. The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus CDROM. Linguistic Data Consortium, 1993b.   \nPegah Ghahremani, Vimal Manohar, Hossein Hadian, Daniel Povey, and Sanjeev Khudanpur. Investigation of transfer learning for asr using lf-mmi trained neural networks. In Proc. of ASRU, 2017.   \nHossein Hadian, Hossein Sameti1, Daniel Povey, and Sanjeev Khudanpur. End-to-end speech recognition using lattice-free mmi. In Proc. of Interspeech, 2018.   \nKenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. Scalable modified Kneser-Ney language model estimation. In Proc. of ACL, 2013.   \nOlivier J. He´naff, Ali Razavi, Carl Doersch, S. M. Ali Eslami, and A¨aron van den Oord. Data-efficient image recognition with contrastive predictive coding. arXiv, abs/1905.09272, 2019.   \nHerman Kamper, Aren Jansen, and Sharon Goldwater. A segmental framework for fully-unsupervised largevocabulary speech recognition. Comput. Speech Lang., 46(C), November 2017.   \nDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proc. of ICLR, 2015.   \nJulius Kunze, Lous Kirsch, Ilia Kurenkov, Andreas Krug, Jens Johannsmeier, and Sebastian Stober. Transfer learning for speech recognition on a budget. In Proc. of Workshop on Representation Learning for NLP, 2017.   \nGuillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv, abs/1901.07291, 2019.   \nZheng Lian, Ya Li, Jianhua Tao, and Jian Huang. Improving speech emotion recognition via transformer-based predictive coding through transfer learning. arXiv, abs/1811.07691, 2018.   \nTatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert. Who needs words? lexicon-free speech recognition. In Proc. of Interspeech, 2019.   \nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Proc. of ECCV, 2014.   \nIlya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with restarts. arXiv, abs/1608.03983, 2016.   \nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Proc. of NIPS, 2013.   \nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proc. of NAACL System Demonstrations, 2019.   \nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In Proc. of ICASSP, pp. 5206–5210. IEEE, 2015.   \nDario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli. 3d human pose estimation in video with temporal convolutions and semi-supervised training. In Proc. of CVPR, 2019.   \nVineel Pratap, Awni Hannun, Qiantong Xu, Jeff Cai, Jacob Kahn, Gabriel Synnaeve, Vitaliy Liptchinsky, and Ronan Collobert. wav2letter++: The fastest open-source speech recognition system. arXiv, abs/1812.07625, 2018.   \nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. https://s3-us-west-2.amazonaws.com/openai-assets/ research-covers/language-unsupervised/language_understanding_paper.pdf, 2018.   \nMirco Ravanelli and Yoshua Bengio. Learning speaker representations with mutual information. arXiv, abs/1812.00271, 2018.   \nMirco Ravanelli, Philemon Brakel, Maurizio Omologo, and Yoshua Bengio. Light gated recurrent units for speech recognition. IEEE Transactions on Emerging Topics in Computational Intelligence, 2(2):92–102, 2018.   \nGabriel Synnaeve and Emmanuel Dupoux. A temporal coherence loss function for learning unsupervised acoustic embeddings. In Proc. of SLTU, 2016a.   \nGabriel Synnaeve and Emmanuel Dupoux. A temporal coherence loss function for learning unsupervised acoustic embeddings. Procedia Computer Science, 81:95–100, 2016b.   \nA¨aron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv, abs/1807.03748, 2018.   \nOriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: Lessons learned from the 2015 MS COCO image captioning challenge. arXiv, abs/1609.06647, 2016. p C Woodland, Julian J Odell, Valtcho Valtchev, and Steve J Young. Large vocabulary continuous speec cognition using htk. In Proc. of ICASSP, 1994.   \nYuxin Wu and Kaiming He. Group normalization. arXiv, abs/1803.08494, 2018.   \nNeil Zeghidour, Nicolas Usunier, Iasonas Kokkinos, Thomas Schaiz, Gabriel Synnaeve, and Emmanuel Dupoux. Learning filterbanks from raw speech for phone recognition. In Proc. of (ICASSP), 2018a.   \nNeil Zeghidour, Qiantong Xu, Vitaliy Liptchinsky, Nicolas Usunier, Gabriel Synnaeve, and Ronan Collobert. Fully convolutional speech recognition. arXiv, abs/1812.06864, 2018b.", "reference_length": 7402, "reference_token": 2225, "txt_length": 26210, "txt_token": 7720, "txt": "# WAV2VEC: UNSUPERVISED PRE-TRAINING FOR SPEECH RECOGNITION  \n\nSteffen Schneider, Alexei Baevski, Ronan Collobert, Michael Auli Facebook AI Research  \n\n# ABSTRACT  \n\nWe explore unsupervised pre-training for speech recognition by learning representations of raw audio. wav2vec is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training. We pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task. Our experiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline by up to $36\\,\\%$ when only a few hours of transcribed data is available. Our approach achieves $2.43\\,\\%$ WER on the nov92 test set. This outperforms Deep Speech 2, the best reported character-based system in the literature while using two orders of magnitude less labeled training data.1  \n\n# 1 INTRODUCTION  \n\nCurrent state of the art models for speech recognition require large amounts of transcribed audio data to attain good performance (Amodei et al., 2016). Recently, pre-training of neural networks has emerged as an effective technique for settings where labeled data is scarce. The key idea is to learn general representations in a setup where substantial amounts of labeled or unlabeled data is available and to leverage the learned representations to improve performance on a downstream task for which the amount of data is limited. This is particularly interesting for tasks where substantial effort is required to obtain labeled data, such as speech recognition.  \n\nIn computer vision, representations for ImageNet (Deng et al., 2009) and COCO (Lin et al., 2014) have proven to be useful to initialize models for tasks such as image captioning (Vinyals et al., 2016) or pose estimation (Pavllo et al., 2019). Unsupervised pre-training for computer vision has also shown promise (Doersch et al., 2015; He´naff et al., 2019). In natural language processing (NLP), unsupervised pre-training of language models (Devlin et al., 2018; Radford et al., 2018; Baevski et al., 2019) improved many tasks such as text classification, phrase structure parsing and machine translation (Edunov et al., 2019; Lample & Conneau, 2019). In speech processing, pre-training has focused on emotion recogniton (Lian et al., 2018), speaker identification (Ravanelli & Bengio, 2018), phoneme discrimination (Synnaeve & Dupoux, 2016a; van den Oord et al., 2018) as well as transferring ASR representations from one language to another (Kunze et al., 2017). There has been work on unsupervised learning for speech but the resulting representations have not been applied to improve supervised speech recognition (Synnaeve & Dupoux, 2016b; Kamper et al., 2017; Chung et al., 2018; Chen et al., 2018; Chorowski et al., 2019).  \n\nIn this paper, we apply unsupervised pre-training to improve supervised speech recognition. This enables exploiting unlabeled audio data which is much easier to collect than labeled data. Our model, wav2vec, is a convolutional neural network that takes raw audio as input and computes a general representation that can be input to a speech recognition system. The objective is a contrastive loss that requires distinguishing a true future audio sample from negatives (Collobert et al., 2011; Mikolov et al., 2013; van den Oord et al., 2018). Different to previous work (van den Oord et al., 2018), we move beyond frame-wise phoneme classification and apply the learned representations to improve strong supervised ASR systems. wav2vec relies on a fully convolutional architecture which can be easily parallelized over time on modern hardware compared to recurrent models used in previous work (§2).  \n\n![](images/526c534d2238c72f5d7db6a1f45105c1c6bffb22fe5975ae7e8dc890ad6df46c.jpg)  \nFigure 1: Illustration of pre-training from audio data $\\mathcal{X}$ which is encoded with two convolutional neural networks that are stacked on top of each other. The model is optimized to solve a next time step prediction task.  \n\nExperimental results on the WSJ benchmark demonstrate that pre-trained representations estimated on about 1,000 hours of unlabeled speech can substantially improve a character-based ASR system and outperform the best character-based result in the literature, Deep Speech 2, improving WER from $3.1\\,\\%$ to $2.43\\,\\%$ . On TIMIT, pre-training enables us to match the best reported result in the literature. In a simulated low-resource setup with only eight hours of transcribed audio data, wav2vec reduces WER by up to $36\\,\\%$ compared to a baseline model that relies on labeled data only $(\\S3,\\S4)$ .  \n\n# 2 PRE-TRAINING APPROACH  \n\nGiven an audio signal as input, we optimize our model (§2.1) to predict future samples from a given signal context. A common problem with these approaches is the requirement to accurately model the data distribution $p(\\mathbf x)$ , which is challenging. We avoid this problem by first encoding raw speech samples $\\mathbf{x}$ into a feature representation $\\mathbf{z}$ at a lower temporal frequency and then implicitly model a density ratio $p(\\mathbf{z}_{i+k}|\\mathbf{z}_{i}\\ldots\\mathbf{\\bar{z}}_{i-r})/p(\\mathbf{z}_{i+k})$ similar to van den Oord et al. (2018).  \n\n# 2.1 MODEL  \n\nOur model takes raw audio signal as input and then applies two networks. The encoder network embeds the audio signal in a latent space and the context network combines multiple time-steps of the encoder to obtain contextualized representations (Figure 1). Both networks are then used to compute the objective function $(\\S2.2)$ .  \n\nGiven raw audio samples $\\mathbf{x}_{i}\\,\\in\\,{\\boldsymbol{X}}$ , we apply the encoder network $f:\\mathcal{X}\\mapsto\\mathcal{Z}$ parameterized as a five-layer convolutional network (van den Oord et al., 2018). Alternatively, one could use other architectures such as the trainable frontend of Zeghidour et al. (2018a). The encoder layers have kernel sizes $(10,8,4,4,4)$ and strides $(5,4,2,2,2)$ . The output of the encoder is a low frequency feature representation $\\mathbf{z}_{i}\\in\\mathcal{Z}$ which encodes about $30\\,\\mathrm{ms}$ of $16\\,\\mathrm{kHz}$ of audio and the striding results in representations $\\mathbf{z}_{i}$ every $10\\mathrm{ms}$ .  \n\nNext, we apply the context network $g\\ :\\ {\\mathcal{Z}}\\ \\mapsto\\ C$ to the output of the encoder network to mix multiple latent representations $\\mathbf{z}_{i}\\ldots\\mathbf{z}_{i-v}$ into a single contextualized tensor $\\mathbf{c}_{i}=g(\\mathbf{z}_{i}\\ldots\\mathbf{z}_{i-v})$ for a receptive field size $v$ . The context network has nine layers with kernel size three and stride one. The total receptive field of the context network is about $210\\,\\mathrm{ms}$ .  \n\nThe layers in both the encoder and context networks consist of a causal convolution with 512 channels, a group normalization layer and a ReLU nonlinearity. We normalize both across the feature and temporal dimension for each sample which is equivalent to group normalization with a single normalization group (Wu & He, 2018). We found it important to choose a normalization scheme that is invariant to the scaling and the offset of the input. This choice resulted in representations that generalize well across datasets.  \n\nFor training on larger datasets, we also consider a model variant (“wav2vec large”) with increased capacity, using two additional linear transformations in the encoder and a considerably larger context network comprised of twelve layers with increasing kernel sizes $(2,3,\\ldots,13)$ . We found it important to introduce skip connections in the aggregator to help convergence in this case. The total receptive field in the last context network layer is hereby increased to about $810\\,\\mathrm{ms}$ .  \n\n# 2.2 OBJECTIVE  \n\nWe train the model to distinguish a sample $\\mathbf{z}_{i+k}$ that is $\\boldsymbol{\\mathrm{k}}$ steps in the future from distractor samples $\\widetilde{\\mathbf z}$ drawn from a proposal distribution $p_{n}$ , by minimizing the contrastive loss for each step $k\\,=$ $1,\\dots,K$ :  \n\n$$\n\\mathcal{L}_{k}=-\\sum_{i=1}^{T-k}\\Big(\\log\\sigma(\\mathbf{z}_{i+k}^{\\top}h_{k}(\\mathbf{c}_{i}))+\\lambda\\mathbb{E}\\left[\\log\\sigma(-\\tilde{\\mathbf{z}}^{\\top}h_{k}(\\mathbf{c}_{i}))\\right]\\Big)\n$$  \n\nwhere we denote the sigmoid $\\sigma(x)=1/(1+\\exp(-x))$ , and where $\\sigma(\\mathbf{z}_{i+k}^{\\top}h_{k}(\\mathbf{c}_{i}))$ is the probability of $\\mathbf{z}_{i+k}$ being the true sample. We consider a step-specific affine transformation $h_{k}(\\mathbf{c}_{i})=W_{k}\\mathbf{c}_{i}{+}\\mathbf{b}_{k}$ for each step $k$ , that is applied to $\\mathbf{c}_{i}$ (van den Oord et al., 2018). We optimize the loss $\\begin{array}{r}{\\mathcal{L}=\\sum_{k=1}^{K}\\mathcal{L}_{k}}\\end{array}$ , summing (1) over different step sizes. In practice, we approximate the expectation by sa mpling ten negatives examples by uniformly choosing distractors from each audio sequence, i.e., $\\begin{array}{r}{p_{n}(\\mathbf{\\bar{z}})\\bar{=}\\;\\frac{1}{T}}\\end{array}$ , where $T$ is the sequence length and we set $\\lambda$ to the number of negatives.2  \n\nAfter training, we input the representations $\\mathbf{c}_{i}$ produced by the context network to the acoustic model instead of log-mel filterbank features.  \n\n# 3 EXPERIMENTAL SETUP  \n\n# 3.1 DATA  \n\nWe consider the following corpora: For phoneme recognition on TIMIT (Garofolo et al., 1993b) we use the standard train, dev and test split where the training data contains just over three hours of audio data. Wall Street Journal (WSJ; Garofolo et al. (1993a); Woodland et al. (1994)) comprises about 81 hours of transcribed audio data. We train on si284, validate on nov93dev and test on nov92. Librispeech (Panayotov et al., 2015) contains a total of 960 hours of clean and noisy speech for training. For pre-training, we use either the full 81 hours of the WSJ corpus, an 80 hour subset of clean Librispeech, the full 960 hour Librispeech training set or a combination of all of them.  \n\nTo train the baseline acoustic model we compute 80 log-mel filterbank coefficients for a $25\\,\\mathrm{ms}$ sliding window with stride $10\\,\\mathrm{ms}$ . Final models are evaluated in terms of both word error rate (WER) and letter error rate (LER).  \n\n# 3.2 ACOUSTIC MODELS  \n\nWe use the wav2letter $^{++}$ toolkit for training and evaluation of acoustic models (Pratap et al., 2018). For the TIMIT task, we follow the character-based wav2letter $^{++}$ setup of Zeghidour et al. (2018a) which uses seven consecutive blocks of convolutions (kernel size 5 with 1,000 channels), followed by a PReLU nonlinearity and a dropout rate of 0.7. The final representation is projected to a 39- dimensional phoneme probability. The model is trained using the Auto Segmentation Criterion (ASG; Collobert et al., 2016)) using SGD with momentum.  \n\nOur baseline for the WSJ benchmark is the wav2letter $^{++}$ setup described by Collobert et al. (2019) which is a 17 layer model with gated convolutions (Dauphin et al., 2017). The model predicts probabilities for 31 graphemes, including the standard English alphabet, the apostrophe and period, two repetition characters (e.g. the word ann is transcribed as an1), and a silence token (|) used as word boundary.  \n\ncoustic models are trained on 8 NVIDIA V100 GPUs using the distributed training implemen ns of fairseq and wav2letter $^{++}$ . When training acoustic models on WSJ, we use plain SGD wit learning rate 5.6 as well as gradient clipping (Collobert et al., 2019) and train for 1,000 epochs with a total batch size of 64 audio sequences. We use early stopping and choose models based on validation WER after evaluating checkpoints with a 4-gram language model. For TIMIT we use learning rate 0.12, momentum 0.9 and we train for 1,000 epochs on 8 GPUs with a batch size of 16 audio sequences.  \n\nTable 1: Replacing log-mel filterbanks (Baseline) by pre-trained embeddings improves WSJ performance on test (nov92) and validation (nov93dev) in terms of both LER and WER. We evaluate pre-training on the acoustic data of part of clean and full Librispeech as well as the combination of all of them. † indicates results with phoneme-based models.   \n\n\n<html><body><table><thead><tr><td></td><td></td><td colspan=\"2\"><b>nov93dev</b></td><td colspan=\"2\"><b>nov92</b></td></tr><tr><td></td><td></td><td><b>LER</b></td><td><b>WER</b></td><td><b>LER</b></td><td><b>WER</b></td></tr></thead><tbody><tr><td>Deep Speech 2 (12K h labeled speech; Amodei et al., 2016)</td><td></td><td>-</td><td>4.42</td><td>-</td><td>3.1</td></tr><tr><td>Trainable frontend (Zeghidour et al., 2018a)</td><td></td><td>-</td><td>6.8</td><td>-</td><td>3.5</td></tr><tr><td>Lattice-free MMI (Hadian et al., 2018)</td><td></td><td>-</td><td>5.66t</td><td>-</td><td>2.8t </td></tr><tr><td>Supervised transfer-learning (Ghahremani et al., 2017)</td><td></td><td>-</td><td>4.99t</td><td>-</td><td>2.53t</td></tr><tr><td colspan=\"6\">4-GRAM LM (Heafield et al., 2013)</td></tr><tr><td>Baseline</td><td>960h</td><td>3.32</td><td>8.57</td><td>2.19</td><td>5.64</td></tr><tr><td>wav2vec</td><td>Librispeech</td><td>3.71</td><td>9.11</td><td>2.17</td><td>5.55</td></tr><tr><td>wav2vec</td><td>Librispeech</td><td>2.85</td><td>7.40</td><td>1.76</td><td>4.57</td></tr><tr><td>wav2vec</td><td>Libri + WSJ</td><td>2.91 1,041 h 960h</td><td>7.59</td><td>1.67</td><td>4.61</td></tr><tr><td>wav2vec large</td><td> Librispeech</td><td>2.73</td><td>6.96</td><td>1.57</td><td>4.32</td></tr><tr><td colspan=\"6\">WoRD CoNvLM (Zeghidour et al., 2018b)</td></tr><tr><td>Baseline</td><td></td><td>2.57</td><td>6.27</td><td>1.51</td><td>3.60</td></tr><tr><td>wav2vec</td><td>Librispeech</td><td>2.22</td><td>5.39</td><td>1.25</td><td>2.87</td></tr><tr><td>wav2vec large</td><td>Librispeech</td><td>2.13</td><td>5.16</td><td>1.02</td><td>2.53</td></tr><tr><td colspan=\"6\">CHAR CoNvLM (Likhomanenko et al., 2019)</td></tr><tr><td>Baseline</td><td>960h</td><td>2.77</td><td>6.67</td><td>1.53</td><td>3.46</td></tr><tr><td>wav2vec</td><td>Librispeech</td><td>2.14 960h</td><td>5.31</td><td>1.15</td><td>2.78</td></tr><tr><td>wav2vec large</td><td>Librispeech</td><td>960h 2.11</td><td>5.10</td><td>0.99</td><td>2.43</td></tr></tbody></table></body></html>  \n\n# 3.3 DECODING  \n\nFor decoding the emissions from the acoustic model we use a lexicon as well as a separate language model trained on the WSJ language modeling data only. We consider a 4-gram KenLM language model (Heafield et al., 2013), a word-based convolutional language model (Collobert et al., 2019), and a character based convolutional language model (Likhomanenko et al., 2019). We decode the word sequence $\\mathbf{y}$ from the output of the context network c or log-mel filterbanks using the beam search decoder of Collobert et al. (2019) by maximizing  \n\n$$\n\\operatorname*{max}_{\\mathbf{y}}f_{\\mathrm{AM}}(\\mathbf{y}|\\mathbf{c})+\\alpha\\log p_{\\mathrm{LM}}(\\mathbf{y})+\\beta|\\mathbf{y}|-\\gamma\\sum_{i=1}^{T}[\\pi_{i}=\\mathbf{\\epsilon}^{\\ast}|\\mathbf{\\epsilon}]\n$$  \n\nwhere $f_{\\mathrm{AM}}$ is the acoustic model, $p_{\\mathrm{{LM}}}$ is the language model, $\\pi=\\pi_{1},...,\\pi_{L}$ are the characters of $\\mathbf{y}$ . Hyper-parameters $\\alpha$ , $\\beta$ and $\\gamma$ are weights for the language model, the word penalty, and the silence penalty.  \n\nFor decoding WSJ, we tune the hyperparameters $\\alpha,\\,\\beta$ and $\\gamma$ using a random search. Finally, we decode the emissions from the acoustic model with the best parameter setting for $\\alpha$ , $\\beta$ and $\\gamma$ . We use a beam size of 4,000 and beam score threshold of 250 for the word based language models, and a beam size of 1,500 with beam score threshold 40 for the character based language model.  \n\n# 3.4 PRE-TRAINING MODELS  \n\nThe pre-training models are implemented in PyTorch in the fairseq toolkit (Ott et al., 2019). We optimize them with Adam (Kingma & Ba, 2015) and a cosine learning rate schedule (Loshchilov & Hutter, 2016) annealed over $40\\mathrm{k}$ update steps for both WSJ and the clean Librispeech training datasets or over $400\\mathrm{k}$ steps for full Librispeech. We start with a learning rate of $1\\times\\dot{1}0^{-7}$ , and then gradually warm it up for 500 updates up to $5\\times10^{-3}$ and then decay it following the cosine curve up to $1\\times10^{-6}$ . To compute the objective, we sample ten negatives for each of the $K=12$ tasks.  \n\n![](images/604f513d34d61389164793dbb4a8dd0caf6f8107f1bae70c45edffb87d303cfe.jpg)  \nFigure 2: Pre-training substanstially improves WER in simulated low-resource setups on the audio data of WSJ compared to wav2letter $^{++}$ with log-mel filterbanks features (Baseline). Pre-training on the audio data of the full $960\\,\\mathrm{h}$ Librispeech dataset (wav2vec Libri) performs better than pre-training on the $81\\,\\mathrm{h}$ WSJ dataset (wav2vec WSJ).  \n\nWe train the first wav2vec variant on 8 GPUs and put audio sequences amounting up to $1.5\\mathrm{M}$ frames on each GPU. Sequences are grouped by length and we crop each to a maximum size of $150\\mathbf{k}$ frames, or the length of the shortest sequence in the batch, whichever is smaller. Cropping removes speech signal from either the beginning or the end of the sequence and we randomly decide the cropping offsets for each sample; we re-sample every epoch. This is a form of data augmentation but also ensures equal length of all sequences on a GPU and removes on average $25\\,\\%$ of the training data. After cropping, the total effective batch size across GPUs is about 556 seconds of speech. For the large model variant, we train on 16 GPUs, doubling the effective batch size.  \n\n# 4 RESULTS  \n\nDifferent to van den Oord et al. (2018), we evaluate the pre-trained representations directly on downstream speech recognition tasks. We measure speech recognition performance on the WSJ benchmark and simulate various low resource setups (§4.1). We also evaluate on the TIMIT phoneme recognition task (§4.2) and ablate various modeling choices (§4.3).  \n\n# 4.1 PRE-TRAINING FOR THE WSJ BENCHMARK  \n\nWe consider pre-training on the audio data (without labels) of WSJ, part of clean Librispeech (about $80\\,\\mathrm{h})$ ) and full Librispeech as well as a combination of all datasets (§3.1). For the pre-training experiments we feed the output of the context network to the acoustic model, instead of log-mel filterbank features.  \n\nTable 1 shows that pre-training on more data leads to better accuracy on the WSJ benchmark. Pretrained representations can substantially improve performance over our character-based baseline which is trained on log-mel filterbank features. This shows that pre-training on unlabeled audio data can improve over the best character-based approach, Deep Speech 2 (Amodei et al., 2016), by 0.67 WER on nov92. In comparison to Hadian et al. (2018), wav2vec performs as well as their phoneme-based model and wav2vec large outperforms it by 0.37 WER. The phoneme-based approach of Ghahremani et al. (2017) pre-trains on the labeled version of Librispeech and then finetunes on WSJ. wav2vec large still outperforms Ghahremani et al. (2017) despite a weaker baseline model and not using Librispeech transcriptions.  \n\nWhat is the impact of pre-trained representations with less transcribed data? In order to get a better understanding of this, we train acoustic models with different amounts of labeled training data and measure accuracy with and without pre-trained representations (log-mel filterbanks). The pretrained representations are trained on the full Librispeech corpus and we measure accuracy in terms of WER when decoding with a 4-gram language model. Figure 2 shows that pre-training reduces WER by $36\\,\\%$ on nov92 when only about eight hours of transcribed data is available. Pre-training only on the audio data of WSJ (wav2vec WSJ) performs worse compared to the much larger Librispeech (wav2vec Libri). This further confirms that pre-training on more data is important to good performance. Similar to He´naff et al. (2019), we noticed that fine-tuning the embedding network does not meaningfully improve performance while substantially increasing the acoustic model training time.  \n\n<html><body><table><thead><tr><td></td><td><b>dev</b></td><td><b>test</b></td></tr></thead><tbody><tr><td>CNN + TD-filterbanks (Zeghidour et al., 2018a)</td><td>15.6</td><td>18.0</td></tr><tr><td>Li-GRU + MFCC (Ravanelli et al., 2018)</td><td>一</td><td>16.7 ± 0.26</td></tr><tr><td>Li-GRU + FBANK (Ravanelli et al., 2018)</td><td>15.8 ± 0.10</td></tr><tr><td>Li-GRU + fMLLR (Ravanelli et al., 2018)</td><td>一</td><td>14.9 ± 0.27</td></tr><tr><td>Baseline</td><td>16.9 ± 0.15</td><td>17.6 ± 0.11</td></tr><tr><td>wav2vec (Librispeech 80h)</td><td>15.5 ± 0.03</td><td>17.6 ± 0.12</td></tr><tr><td>wav2vec (Librispeech 960h)</td><td>13.6 ± 0.20</td><td>15.6 ± 0.23</td></tr><tr><td>wav2vec (Librispeech + WSJ)</td><td>12.9 ± 0.18</td></tr></tbody></table></body></html>  \n\nTable 2: Results for phoneme recognition on TIMIT in terms of PER. All our models use the CNN8L-PReLU-do0.7 architecture (Zeghidour et al., 2018a).   \nTable 3: Effect of different number of negative samples during pre-training for TIMIT on the development set.   \n\n\n<html><body><table><thead><tr><td><b>negatives</b></td><td><b>dev PER</b></td><td><b>train time (h)</b></td></tr></thead><tbody><tr><td>1</td><td>16.3</td><td>6.1</td></tr><tr><td>2</td><td>15.8</td><td>6.3</td></tr><tr><td>5</td><td>15.9</td><td>8.2</td></tr><tr><td>10</td><td>15.5</td><td>10.5</td></tr><tr><td>20</td><td>15.7</td><td>15.3</td></tr></tbody></table></body></html>  \n\n# 4.2 PRE-TRAINING FOR TIMIT  \n\nOn the TIMIT task we use a 7-layer wav2letter $^{++}$ model with high dropout (§3; Synnaeve & Dupoux (2016b)). Table 2 shows that wav2vec pre-training on Librispeech and WSJ audio data can lead to results matching the state of the art. Accuracy steadily increases with more data for pre-training and the best accuracy is achieved with the largest amount of data for pre-training.  \n\n# 4.3 ABLATIONS  \n\nIn this section we analyze some of the design choices we made for wav2vec. We pre-train on the 80 hour subset of clean Librispeech and evaluate on TIMIT. Table 3 shows that increasing the number of negative samples only helps up to ten samples. Thereafter, performance plateaus while training time increases. We suspect that this is because the training signal from the positive samples decreases as the number of negative samples increases. In this experiment, everything is kept equal except for the number of negative samples.  \n\nNext, we analyze the effect of data augmentation through cropping audio sequences (§3.4). When creating batches we crop sequences to a pre-defined maximum length. Table 4 shows that a crop size of 150k frames results in the best performance. Not restricting the maximum length (None) gives an average sequence length of about $207\\mathbf{k}$ frames and results in the worst accuracy. This is most likely because the setting provides the least amount of data augmentation.  \n\nTable 5 also shows that predicting more than 12 steps ahead in the future does not result in better performance and increasing the number of steps increases training time.  \n\nTable 4: Effect of different crop sizes (cf. Table 3).   \n\n\n<html><body><table><thead><tr><td><b>Crop size</b></td><td><b>dev PER</b></td></tr></thead><tbody><tr><td>None (Avg. 207k)</td><td>16.3</td></tr><tr><td>100k</td><td>16.1</td></tr><tr><td>15.5</td></tr><tr><td>150k</td></tr><tr></tr><tr><td>200k</td></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></body></html>  \n\nTable 5: Effect of different number of tasks $K$ (cf. Table 3).   \n\n\n<html><body><table><thead><tr><td><b># Tasks</b></td><td><b>dev PER</b></td><td><b>dev PER</b></td></tr></thead><tbody><tr><td>8</td><td>15.9</td></tr><tr><td>12</td></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></body></html>  \n\nWe introduce wav2vec, the first application of unsupervised pre-training to speech recognition with a fully convolutional model. Our approach achieves $2.43\\,\\%$ WER on the test set of WSJ, a result that outperforms the next best known character-based speech recognition model in the literature (Amodei et al., 2016) while using two orders of magnitude less transcribed training data. We show that more data for pre-training improves performance and that this approach not only improves resource-poor setups, but also settings where all WSJ training data is used. In future work, we will investigate different architectures which is likely to further improve performance.", "appendix": "# ACKNOWLEDGEMENTS  \n\nWe thank the Speech team at FAIR, especially Jacob Kahn, Vineel Pratap and Qiantong Xu for help with wav2letter $^{++}$ experiments, and Tatiana Likhomanenko for providing convolutional language models for our experiments."}, {"title": "Analyzing ASR pretraining for low-resource speech-to-text translation", "authors": "Mihaela C. Stoian, Sameer Bansal, Sharon Goldwater", "bibkey": "analyzing_asr_pretraining_for_low_resource_speech_to_text_translation", "bibitem": "@article{Stoian_ICASSPP2020,\n  url = {http://arxiv.org/abs/1910.10762v2},\n  title = {Analyzing ASR pretraining for low-resource speech-to-text translation},\n  authors = {Mihaela C. Stoian, Sameer Bansal, Sharon Goldwater},\n  abstract = {Previous work has shown that for low-resource source languages, automatic speech-to-text translation (AST) can be improved by pretraining an end-to-end model on automatic speech recognition (ASR) data from a high-resource language. However, it is not clear what factors --e.g., language relatedness or size of the pretraining data-- yield the biggest improvements, or whether pretraining can be effectively combined with other methods such as data augmentation. Here, we experiment with pretraining on datasets of varying sizes, including languages related and unrelated to the AST source language. We find that the best predictor of final AST performance is the word error rate of the pretrained ASR model, and that differences in ASR/AST performance correlate with how phonetic information is encoded in the later RNN layers of our model. We also show that pretraining and data augmentation yield complementary benefits for AST.},\n  arxiv_id = {1910.10762v2},\n  subject = {cs.CL},\n  submission_date = {2019-10-23T18:37:56Z}\n}", "url": "http://arxiv.org/abs/1910.10762v2", "latex_url": "http://arxiv.org/src/1910.10762v2", "latex_path": "output/download_papers/1910.10762v2/1910.10762v2", "pdf_url": "http://arxiv.org/pdf/1910.10762v2", "pdf_path": "output/download_papers/1910.10762v2/1910.10762v2.pdf", "md_url": null, "latex_length": 0, "latex": "", "abstract": "Previous work has shown that for low-resource source languages, automatic speech-to-text translation (AST) can be improved by pretraining an end-to-end model on automatic speech recognition (ASR) data from a high-resource language. However, it is not clear what factors --e.g., language relatedness or size of the pretraining data-- yield the biggest improvements, or whether pretraining can be effectively combined with other methods such as data augmentation. Here, we experiment with pretraining on datasets of varying sizes, including languages related and unrelated to the AST source language. We find that the best predictor of final AST performance is the word error rate of the pretrained ASR model, and that differences in ASR/AST performance correlate with how phonetic information is encoded in the later RNN layers of our model. We also show that pretraining and data augmentation yield complementary benefits for AST.", "abstract_length": 930, "abstract_token": 177, "introduction": "Low-resource automatic speech-to-text translation (AST) has recently gained traction as a way to bring NLP tools to under-represented languages. An end-to-end approach [1–7] is particularly appealing for source languages with no written form, or for endangered languages where translations into a high-resource language may be easier to collect than transcriptions [8]. However, building high-quality endto-end AST with little parallel data is challenging, and has led researchers to explore how other sources of data could be used to help. A number of methods have been investigated. Several of these use transcribed source language audio and/or translated source language text in a multitask learning scenario [4, 6, 9] or to pre-train parts of the model before fine-tuning on the end-to-end AST task [4]. Others assume, as we do here, that no additional source language resources are available, in which case transfer learning using data from language(s) other than the source language is a good option. In particular, several researchers have shown that low-resource AST can be improved by pretraining on an ASR task in some other language, then transferring the encoder parameters to initialize the AST model. For example, Bansal et al. [5] showed that pre-training on either English or French ASR improved their Spanish-English AST system (trained on 20 hours of parallel data) and Tian [10] got improvements on an 8-hour Swahili-English AST dataset using English ASR pretraining. Overall these results show that pretraining helps, but leave open the question of what factors affect the degree of improvement. For example, does language relatedness play a role, or simply the amount of pretraining data? Bansal et al. showed bigger AST gains as the amount of English pretraining data increased from 20 to 300 hours, and also found a slightly larger improvement when pretraining on 20 hours of English versus 20 hours of French, but they pointed out that the Spanish data contains many English code-switched words, which could explain the latter result. In related work on multilingual pretraining for low-resource ASR, Adams et al. [11] showed that pre-training on more languages helps, but it is not clear whether the improvement is due to including more languages, or just more data. To begin to tease apart these issues, we focus here on monolingual pretraining for low-resource AST, and investigate two questions. First, can we predict what sort of pretraining data is best for a particular AST task? Does it matter if the pretraining language is related to the AST source language (defined here as part of the same language family, since phonetic similarity is difficult to measure), or is the amount of pretraining data (or some other factor) more important? Second, can pretraining be effectively combined with other methods, such as data augmentation, in order to further improve AST results? To answer these questions, we use the same AST architecture and Spanish-English parallel data as Bansal et al. [5], but pretrain the encoder using a number of different ASR datasets: the 150- hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data. We find that pretraining on a larger amount of data from an unrelated language is much better than pretraining on a smaller amount of data from a related language. Moreover, even when controlling for the amount of data, the WER of the ASR model from pretraining seems to be a better predictor of final AST performance than does language relatedness. Indeed, we show that there is a very strong correlation between the WER of the pretraining model and BLEU score of the final AST model—i.e., the best pretraining strategy may simply be to use datasets and methods that will yield the lowest ASR WER during pretraining. However, we also found that AST results can be improved further by augmenting the AST data using standard speed perturbation techniques [12]. Our best results using non-English pretraining data improve the test set BLEU scores of an AST system trained on 20 hours of parallel data from 10.2 to 14.3, increasing to 15.8 with data augmentation. Finally, we analyze the representations learned by the models and show that better performance seems to correlate with the extent to which phonetic information is encoded in a linearly separable way in the later RNN layers.", "introduction_length": 4388, "introduction_token": 910, "reference": "# 8. REFERENCES  \n\n[1] A. B´erard, O. Pietquin, C. Servan, and L. Besacier, “Listen and translate: A proof of concept for end-to-end speech-totext translation,” in NIPS Workshop on end-to-end learning for speech and audio processing, 2016.   \n[2] R. J. Weiss, J. Chorowski, N. Jaitly, Y. Wu, and Z. Chen, “Sequence-to-sequence models can directly transcribe foreign speech,” in Proc. Interspeech, 2017.   \n[3] S. Bansal, H. Kamper, K. Livescu, A. Lopez, and S. Goldwater, “Low-resource speech-to-text translation,” in Proc. Interspeech, 2018.   \n[4] A. B´erard, L. Besacier, A. C. Kocabiyikoglu, and O. Pietquin, “End-to-end automatic speech translation of audiobooks,” in Proc. ICASSP, 2018.   \n[5] S. Bansal, H. Kamper, K. Livescu, A. Lopez, and S. Goldwater, “Pre-training on high-resource speech recognition improves lowresource speech-to-text translation,” in Proc. NAACL, 2019.   \n[6] M. Sperber, G. Neubig, J. Niehues, and A. Waibel, “Attentionpassing models for robust and data-efficient end-to-end speech translation,” in Trans. ACL, 2019.   \n[7] E. Salesky, M. Sperber, and A. Waibel, “Fluent translations from disfluent speech in end-to-end speech translation,” in Proc. NAACL, 2019.   \n[8] P. Godard, G. Adda, M. Adda-Decker et al., “A very low resource language speech corpus for computational language documentation experiments,” in Proc. LREC, 2018.   \n[9] A. Anastasopoulos and D. Chiang, “Tied multitask learning for neural speech translation,” in Proc. NAACL HLT, 2018.   \n[10] Y. Tian, “How does pre-training improve low-resource speechto-text translation? — a case study on a Swahili-English dataset,” Master’s thesis, University of Edinburgh, 2019.   \n[11] O. Adams, M. Wiesner, S. Watanabe, and D. Yarowsky, “Massively multilingual adversarial speech recognition,” in Proc. NAACL, 2019.   \n[12] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, “Audio augmentation for speech recognition,” in Proc. Interspeech, 2015.   \n[13] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely, “The Kaldi Speech Recognition Toolkit,” in Proc. ASRU, 2011.   \n[14] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a method for automatic evaluation of machine translation,” in Proc. ACL, 2002.   \n[15] M. Post, G. Kumar, A. Lopez, D. Karakos, C. CallisonBurch, and S. Khudanpur, “Fisher and CALLHOME SpanishEnglish Speech Translation,” 2014, https://catalog.ldc.upenn. edu/LDC2014T23.   \n[16] T. Schultz, “Globalphone: a multilingual speech and text database developed at Karlsruhe University,” in ICSLP, 2002.   \n[17] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, “Aishell-1: An open-source Mandarin speech corpus and a speech recognition baseline,” in O-COCOSDA, 2017.   \n[18] J. Godfrey and E. Holliman, “Switchboard-1 Release 2 (LDC97S62),” 1993, https://catalog.ldc.upenn.edu/LDC97S62.   \n[19] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of rare words with subword units,” in Proc. ACL, 2016.   \n[20] V. Nair and G. E. Hinton, “Rectified linear units improve restricted Boltzmann machines,” in Proc. ICML, 2010.   \n[21] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” in Proc. ICML, 2015.   \n[22] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Computation, 1997.   \n[23] R. J. Williams and D. Zipser, “A learning algorithm for continually running fully recurrent neural networks,” Neural Computation, 1989.   \n[24] M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches to attention-based neural machine translation,” in Proc. EMNLP, 2015.   \n[25] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in Proc. ICLR, 2015.   \n[26] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: A simple way to prevent neural networks from overfitting,” J. Mach. Learn. Res., 2014.   \n[27] Y. Gal, “A theoretically grounded application of dropout in recurrent neural networks,” in Proc. NIPS, 2016.   \n[28] Y. Wu, M. Schuster, Z. Chen et al., “Google’s neural machine translation system: Bridging the gap between human and machine translation,” CoRR, vol. abs/1609.08144, 2016.   \n[29] S. Toshniwal, T. N. Sainath, R. J. Weiss, B. Li, P. Moreno, E. Weinstein, and K. Rao, “Multilingual Speech Recognition with A Single End-To-End Model,” in Proc. ICASSP, 2018.   \n[30] S. Zhou, S. Xu, and B. Xu, “Multilingual end-to-end speech recognition with a single transformer on low-resource languages,” in arXiv, 2018, preprint arXiv:1806.05059.   \n[31] J. Cho, M. K. Baskar, R. Li, M. Wiesner, S. Mallidi, N. Yalta, M. Karafia´t, S. Watanabe, and T. Hori, “Multilingual sequenceto-sequence speech recognition: Architecture, transfer learning, and language modeling,” in Proc. SLT, 2018.   \n[32] S. Dalmia, R. Sanabria, F. Metze, and A. W. Black, “Sequencebased multi-lingual low resource speech recognition,” in ICASSP, 2018.   \n[33] E. Hermann, H. Kamper, and S. Goldwater, “Multilingual and unsupervised subword modeling for zero-resource languages,” 2018, arXiv preprint arXiv:1811.04791.   \n[34] E. Hermann and S. Goldwater, “Multilingual bottleneck features for subword modeling in zero-resource languages,” in Proc. Interspeech, 2018.   \n[35] Y. Belinkov and J. R. Glass, “Analyzing hidden representations in end-to-end automatic speech recognition systems,” in NIPS, 2017.   \n[36] D. Hupkes, S. Veldhoen, and W. H. Zuidema, “Visualisation and ‘diagnostic classifiers’ reveal how recurrent and recursive neural networks process hierarchical structure,” CoRR, vol. abs/1711.10203, 2017.", "reference_length": 5654, "reference_token": 1821, "txt_length": 25520, "txt_token": 6275, "txt": "# ANALYZING ASR PRETRAINING FOR LOW-RESOURCE SPEECH-TO-TEXT TRANSLATION  \n\nMihaela C. Stoian, Sameer Bansal, Sharon Goldwater  \n\nSchool of Informatics, University of Edinburgh, UK  \n\n{c.mihaela.stoian, sameer.bansal}@ed.ac.uk, sgwater@inf.ed.ac.uk  \n\n# ABSTRACT  \n\nPrevious work has shown that for low-resource source languages, automatic speech-to-text translation (AST) can be improved by pretraining an end-to-end model on automatic speech recognition (ASR) data from a high-resource language. However, it is not clear what factors—e.g., language relatedness or size of the pretraining data— yield the biggest improvements, or whether pretraining can be effectively combined with other methods such as data augmentation. Here, we experiment with pretraining on datasets of varying sizes, including languages related and unrelated to the AST source language. We find that the best predictor of final AST performance is the word error rate of the pretrained ASR model, and that differences in ASR/AST performance correlate with how phonetic information is encoded in the later RNN layers of our model. We also show that pretraining and data augmentation yield complementary benefits for AST.  \n\nIndex Terms— speech-to-text translation, transfer learning, pretraining, speech recognition, data augmentation.  \n\n# 1. INTRODUCTION  \n\nLow-resource automatic speech-to-text translation (AST) has recently gained traction as a way to bring NLP tools to under-represented languages. An end-to-end approach [1–7] is particularly appealing for source languages with no written form, or for endangered languages where translations into a high-resource language may be easier to collect than transcriptions [8]. However, building high-quality endto-end AST with little parallel data is challenging, and has led researchers to explore how other sources of data could be used to help.  \n\nA number of methods have been investigated. Several of these use transcribed source language audio and/or translated source language text in a multitask learning scenario [4, 6, 9] or to pre-train parts of the model before fine-tuning on the end-to-end AST task [4]. Others assume, as we do here, that no additional source language resources are available, in which case transfer learning using data from language(s) other than the source language is a good option. In particular, several researchers have shown that low-resource AST can be improved by pretraining on an ASR task in some other language, then transferring the encoder parameters to initialize the AST model. For example, Bansal et al. [5] showed that pre-training on either English or French ASR improved their Spanish-English AST system (trained on 20 hours of parallel data) and Tian [10] got improvements on an 8-hour Swahili-English AST dataset using English ASR pretraining.  \n\nOverall these results show that pretraining helps, but leave open the question of what factors affect the degree of improvement. For example, does language relatedness play a role, or simply the amount of pretraining data? Bansal et al. showed bigger AST gains as the amount of English pretraining data increased from 20 to 300 hours, and also found a slightly larger improvement when pretraining on 20 hours of English versus 20 hours of French, but they pointed out that the Spanish data contains many English code-switched words, which could explain the latter result. In related work on multilingual pretraining for low-resource ASR, Adams et al. [11] showed that pre-training on more languages helps, but it is not clear whether the improvement is due to including more languages, or just more data.  \n\nTo begin to tease apart these issues, we focus here on monolingual pretraining for low-resource AST, and investigate two questions. First, can we predict what sort of pretraining data is best for a particular AST task? Does it matter if the pretraining language is related to the AST source language (defined here as part of the same language family, since phonetic similarity is difficult to measure), or is the amount of pretraining data (or some other factor) more important? Second, can pretraining be effectively combined with other methods, such as data augmentation, in order to further improve AST results?  \n\nTo answer these questions, we use the same AST architecture and Spanish-English parallel data as Bansal et al. [5], but pretrain the encoder using a number of different ASR datasets: the 150- hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data. We find that pretraining on a larger amount of data from an unrelated language is much better than pretraining on a smaller amount of data from a related language. Moreover, even when controlling for the amount of data, the WER of the ASR model from pretraining seems to be a better predictor of final AST performance than does language relatedness. Indeed, we show that there is a very strong correlation between the WER of the pretraining model and BLEU score of the final AST model—i.e., the best pretraining strategy may simply be to use datasets and methods that will yield the lowest ASR WER during pretraining. However, we also found that AST results can be improved further by augmenting the AST data using standard speed perturbation techniques [12]. Our best results using non-English pretraining data improve the test set BLEU scores of an AST system trained on 20 hours of parallel data from 10.2 to 14.3, increasing to 15.8 with data augmentation.  \n\nFinally, we analyze the representations learned by the models and show that better performance seems to correlate with the extent to which phonetic information is encoded in a linearly separable way in the later RNN layers.  \n\n# 2. METHODOLOGY  \n\nFor both ASR and AST tasks we use the same end-to-end system architecture shown in Figure 1: the encoder-decoder model from [5], which itself is adapted from [2], [4] and [3]. Details of the architecture and training parameters are described in Section 3.4.  \n\n![](images/d515d367c759243123d13a791c6eaf3d33cb3a6c2838b9b8ce40dad2a90ef03c.jpg)  \nFig. 1: Encoder-decoder architecture used for both ASR and AST.  \n\nAfter pretraining an ASR model, we transfer only its encoder parameters to the AST task. Previous experiments [5] showed that the encoder accounts for most of the benefits of transferring the parameters. Transferring also the decoder and attention mechanism does bring some improvements, but is only feasible when the ASR pretraining language is the same as the AST target language, which is not true in most of our experiments.  \n\nIn addition to pretraining, we experimented with data augmentation. Specifically, we augmented the AST data using Kaldi’s [13] 3-way speed perturbation, adding versions of the AST data where the audio is sped down and up by a factor of 0.9 and 1.1, respectively.1  \n\nTo evaluate ASR performance we compute the word error rate (WER).2To evaluate AST performance we calculate the 4-gram BLEU score [14] on four reference translations.3  \n\n# 3. EXPERIMENTAL SETUP  \n\n# 3.1. Parallel data  \n\nFor the AST models, we use Spanish-English parallel data from Fisher corpus [15], containing 160 hours of Spanish telephone speech translated into English text. To simulate low-resource settings, we randomly downsample the original corpus to 20 hours of training data. Each of the dev and test sets comprise 4.5 hours of speech.  \n\n# 3.2. Pretraining data  \n\nSince we focus on investigating factors that might affect the AST improvements over the baseline when pretraining, we have chosen ASR datasets for pretraining that contrast in the number of hours and/or in the language similarity with Spanish. Statistics for each dataset are in the left half of Table 1, with further details below.  \n\nTo look at a range of languages with similar amounts of data, we used GlobalPhone corpora from seven languages [16], each with around 20 hours of speech: Mandarin Chinese (zh), Croatian (hr), Czech (cs), French (fr), Polish (pl), Portuguese (pt), and Swedish (sv). French and Portuguese, like the source language (Spanish), belong to the Romance family of languages, while the other languages are less related—especially Chinese, which is not an Indo-European language. GlobalPhone consists of read speech recorded using similar conditions across languages, and the transcriptions for Chinese are Romanized, with annotated word boundaries.  \n\nTable 1: Dataset statistics (left); dev set results from ASR pretraining and from the final AST system (right). AST results in all rows except the first are from pretraining using the dataset listed in that row, followed by fine-tuning using ast-20h. Numbers in brackets are the improvement over the baseline.   \n\n\n<html><body><table><thead><tr><td></td><td></td><td><b>DATA</b></td><td><b>RESULTS</b></td><td></td></tr><tr><td><b>Dataset</b></td><td><b>Hrs.</b></td><td><b>Spks.</b></td><td><b>ASR (WER)</b></td><td><b>AST (BLEU)</b></td></tr></thead><tbody><tr><td>ast-20h</td><td>20</td><td></td><td>一</td><td>10.3</td></tr><tr><td>zh-ai-small</td><td>20</td><td>81</td><td>38.7</td><td>12.4 (+2.1)</td></tr><tr><td>zh-ai-large</td><td>150</td><td>340</td><td>22.5</td><td>14.6 (+4.3)</td></tr><tr><td>zh-ai-hanzi</td><td>150</td><td>340</td><td>25.3</td><td>13.2 (+2.9)</td></tr><tr><td>hr-gp</td><td>12</td><td>72</td><td>71.5</td><td>10.7 (+0.4)</td></tr><tr><td>SV-gp</td><td>18</td><td>79</td><td>59.4</td><td>12.3 (+2.0)</td></tr><tr><td>pl-gp</td><td>19</td><td>79</td><td>59.6</td><td>10.8 (+0.5)</td></tr><tr><td>pt-gp</td><td>23</td><td>86</td><td>80.5</td><td>10.5 (+0.2)</td></tr><tr><td>fr-gp</td><td>25</td><td>84</td><td>31.1</td><td>12.5 (+2.2)</td></tr><tr><td>zh-gp</td><td>26</td><td>111</td><td>51.5</td><td>12.0 (+1.7)</td></tr><tr><td> cs-gP</td><td>27</td><td>82</td><td>53.7</td><td>11.1 (+0.8)</td></tr><tr><td>multilin6</td><td>124</td><td>482</td><td>44.2</td><td>13.3 (+3.0)</td></tr></tbody></table></body></html>  \n\nTo explore the effects of using a large amount of pretraining data from an unrelated language, we used the AISHELL-1 corpus of Mandarin Chinese [17], which contains 150 hours of read speech. Transcriptions with annotated word boundaries are available in both Hanzi (Chinese characters) and Romanized versions, and we built models with each. To compare to the GlobalPhone data, we also created a 20-hour subset of the Romanized AISHELL $z h$ -ai-small) by randomly selecting utterances from a subset of the speakers (81, roughly the number present in most of the GlobalPhone datasets).  \n\nFinally, to reproduce one of the experiments from [5], we pretrained one model using 300 hours of Switchboard English [18]. This data is the most similar to the AST speech data in terms of style and channel (both are conversational telephone speech). However, as noted by [5], the Fisher Spanish speech contains many words that are actually in English (code-switching), so pretraining on English may provide an unfair advantage relative to other languages.  \n\n# 3.3. Preprocessing  \n\nWe compute 13-dim MFCCs and cepstral mean and variance normalization along speakers using Kaldi [13] on our ASR and AST audio. To shorten the training time, we trimmed utterances from the AST data to 16 seconds (or 12 seconds for the 160h augmented dataset).  \n\nTo account for unseen words in the test data, we model the ASR and AST text outputs via sub-word units using byte-pair encoding (BPE) [19]. We do this separately for each dataset as BPE works best as a language-specific tool (i.e. it depends on the frequency of different subword units, which varies with the language). We use 1k merge operations in all cases except Hanzi, where there are around 3000 symbols initially (vs around 60 in the other datasets). For Hanzi we ran experiments with both 1k and $15\\mathbf{k}$ merge operations. For Chinese Romanized transcriptions we removed tone diacritics.  \n\n# 3.4. Model architecture and training  \n\nFollowing the architecture and training procedure described in [5], input speech features are fed into a stack of two CNN layers. In each CNN layer we stride the input with a factor of 2 along time, apply  \n\n![](images/f45aab23d8ebc8c48fcf3ab4a13a5fa04c1ca40d45de497109786e041d5ff86a.jpg)  \nFig. 2: WER of each ASR model vs BLEU score of the corresponding pre-trained AST model, computed in both cases on dev sets. Diamond markers are AISHELL data sets; circles are from GlobalPhone. The points in the circled group come from different runs on the same dataset but with different BPE or learning rate schedules. The Spearman rank correlation of these points is -0.97; the correlation is -0.92 when using test sets to compute both ASR and BLEU.  \n\nReLU activation [20] followed by batch normalization [21]. The CNN output is fed into a three-layer bi-directional long short-term memory network (LSTM) [22], with 512 hidden layer dimensions. For decoding, we use the predicted token $20\\%$ of the time and the training token $80\\%$ of the time [23] as input to a 128-dimensional embedding layer followed by a three-layer LSTM, with 256 hidden layer dimensions, and combine this with the output from the attention mechanism [24] to predict the word at the current time step.  \n\nWe use code and hyperparameter settings from $[5]^{4}$ : the Adam optimizer [25] with an initial learning rate of 0.001 and decay it by a factor of 0.5 based on the dev set BLEU score. When training AST models, we regularize using dropout [26] with a ratio of 0.3 over the embedding and LSTM layers [27]; weight decay with a rate of 0.0001; and, after the first 20 epochs, $30\\%$ of the time we replace the predicted output word by a random word from the target vocabulary. At test time we use beam decoding with a beam size of 5 and length normalization [28] with a weight of 0.6.  \n\n# 4. RESULTS AND DISCUSSION  \n\n# 4.1. Baseline and ASR results  \n\nOur baseline 20-hour AST system obtains a BLEU score of 10.3 (Table 1, first row), 0.5 BLEU point lower than that reported by [5]. This discrepancy might be due to differences in subsampling from the 160-hour AST dataset to create the 20-hour subset, or from Kaldi parameters when computing the MFCCs.  \n\nWERs for our pre-trained models (Table 1) vary from 22.5 for the large AISHELL dataset with Romanized transcript to 80.5 for Portuguese GlobalPhone. These are considerably worse than stateof-the-art ASR systems (e.g., Kaldi recipes can achieve WER of 7.5 on AISHELL and 26.5 on Portuguese GlobalPhone), but we did not optimize our architecture or hyperparameters for the ASR task since our main goal is to analyze the relationship between pretraining and AST performance (and in order to use pretraining, we must use a seq2seq model with the architecture as for AST).  \n\n# 4.2. Pretraining the AST task on ASR models  \n\nAST results for our pre-trained models are given in Table 1. Pretraining improves AST performance in every case, with improvements ranging from 0.2 (pt-gp) to 4.3 (zh-ai-large). These results make it clear that language relatedness does not play a strong role in predicting AST improvements, since on the similar-sized GlobalPhone datasets, the two languages most related to Spanish (French and Portuguese) yield the highest and lowest improvements, respectively. Moreover, pretraining on the large Chinese dataset yields a bigger improvement than either of these—4.3 BLEU points. This is nearly as much as the 6 point improvement reported by [5] when pretraining on 100 hours of English data, which is especially surprising given not only that Chinese is very different from Spanish, but also that the Spanish data contains some English words.  \n\nThis finding seems to suggest that data size is more important than language relatedness for predicting the effects of pretraining. However, there are big differences even amongst the languages with similar amounts of pretraining data. Analyzing our results further, we found a striking correlation between the WER of the initial ASR model and the BLEU score of the AST system pretrained using that model, as shown in Figure 2. Therefore, although pretraining data size clearly influences AST performance, this appears to be mainly due to its effect on WER of the ASR model. We therefore hypothesize that WER is a better direct predictor of AST performance than either data size or language relatedness.  \n\n# 4.3. Multilingual pretraining  \n\nAlthough our main focus is monolingual pretraining, we also looked briefly at multilingual pretraining, inspired by recent work on multilingual ASR [29, 30] and evidence that multilingual pretraining followed by fine-tuning on a distinct target language can improve ASR on the target language [11,31,32]. These experiments did not directly compare pretraining using a similar amount of monolingual data, but such a comparison was done by [33,34] in their work on learning feature representations for a target language with no transcribed data. They found a benefit for multilingual vs monolingual pretraining given the same amount of data.  \n\nFollowing up on this work, we tried pretraining using 124 hours of multilingual data (all GlobalPhone languages except Chinese), roughly the amount of data in our large Chinese models. We combined all the data together and trained an ASR model using a common target BPE with 6k merge operations, then transferred only the encoder to the AST model. However, we did not see a benefit to the multilingual training (Table 1, final row); in fact the resulting AST model was slightly worse than the $z h$ -ai-large model (BLEU of 13.3 vs 14.6). Other configurations of multilingual training might still outperform their monolingual counterparts, but we leave this investigation as future work.  \n\n# 4.4. Augmenting the parallel data  \n\nTable 2 (top) shows how data augmentation affects the results of the baseline 20h AST system, as well as three of the best-performing pretrained models from Table 1. For these experiments only, we changed the learning rates of the augmented-data systems so that all models took about the same amount of time to train (see Figure 3). Despite a more aggressive learning schedule, the performance of the augmented-data systems surpasses that of the baseline and pretrained models, even those trained on the largest ASR sets (150-hr Chinese and ${300}{-}\\mathrm{hr}$ English).  \n\nFor comparison to other work, Table 2 (bottom) gives results for AST models trained on the full 160 hours of parallel data, including models with both pretraining and data augmentation. For the latter, we used the original learning schedule, but had to stop training early due to time constraints (after 15 days, compared to 8 days for complete training of the non-augmented 160h models). We find that both pretraining and augmentation still help, providing a combined gain of 3.8 (3.2) BLEU points over the baseline on the dev (test) set.  \n\n<html><body><table><thead><tr><td></td><td colspan=\"2\"><b>dev set</b></td><td colspan=\"2\"><b>test set</b></td></tr><tr><td><b>Pretrain</b></td><td><b> No aug.</b></td><td><b>With aug.</b></td><td><b> No aug.</b></td><td><b>With aug.</b></td></tr></thead><tbody><tr><td>一</td><td>10.3</td><td>13.0 (+2.7)</td><td>10.2</td><td>13.3 (+3.1)</td></tr><tr><td>fr-gp</td><td>12.5</td><td>13.7 (+1.2)</td><td>12.6</td><td>14.3 (+1.7)</td></tr><tr><td>zh-ai-lrg</td><td>14.6</td><td>15.5 (+0.9)</td><td>14.3</td><td>15.8 (+1.5)</td></tr><tr><td>en-300h</td><td>19.5</td><td>20.1 (+0.6)</td><td>20.1</td><td>20.2 (+0.1)</td></tr><tr><td>一</td><td>34.1</td><td>36.3 (+2.2)</td><td>34.6</td><td>37.3 (+2.7)</td></tr><tr><td>en-300h</td><td>36.3</td><td>37.9 (+1.6)</td><td>36.4</td><td>37.8 (+1.4)</td></tr></tbody></table></body></html>  \n\n![](images/ca493689f28b6ff9f29d3c911096f0c7e49151fad0a5414f5c4073a4f932c98c.jpg)  \nTable 2: BLEU scores on dev and test sets for models trained with and without data augmentation. We used either 20h of AST training data (top block) or 160h (bottom block), with various pretraining.   \nFig. 3: The AST performance over time (without beam-search) of baseline, pretrained, and pretrained+augmented models.  \n\n# 5. ANALYZING THE MODELS’ REPRESENTATIONS  \n\nFinally, we hope to gain some understanding into why pretraining on ASR helps with AST, and specifically how the neural network representations change during pretraining and fine-tuning. We follow [35] and [10], who built diagnostic classifiers [36] to examine the representation of phonetic information in end-to-end ASR and AST systems, respectively. Unlike [10,35], who used non-linear classifiers, we use a linear classifier to predict phone labels from the internal representations of the trained ASR or AST model.  \n\nUsing a linear classifier allows us to make more precise claims: if the classifier performs better using the representation from a particular layer, we can say that layer represents the phonetic information in a more linearly separable way. Using a nonlinear classifier raises questions about how to choose the complexity of the classifier itself, and therefore makes any results difficult to interpret.  \n\nWe hypothesized that pretraining allows the models to abstract away from nonlinguistic acoustic differences, and to better represent phonetic information: crucially, both in the trained language and in other languages. To test this hypothesis, we used two phone-labelled datasets distinct from all our ASR and AST datasets: the English TIMIT corpus (a language different to all of our trained models, with hand-labeled phones) and the Spanish GlobalPhone corpus (the same language as our AST source language, with phonetic forcedalignments produced using Kaldi). We randomly sampled utterances from these and passed them through the trained encoders, giving us a total of about 600k encoded frames. We used $400\\mathbf{k}$ of these to train logistic regression models to predict the phone labels, and tested on the remaining 200k frames.  \n\n![](images/bbb9d1f44900518388efc4c7dbd75cbccdc03161c05c912a71e39aeba935ecb7.jpg)  \nFig. 4: Phonetic classification accuracy at different layers of our ASR (left) and AST (right) models. Different color bars indicate representations extracted from models (pre)trained on different datasets (pt-gp, $f\\boldsymbol r$ -gp, or zh-ai-large). Results from the baseline AST model (without pretraining) are shown in both panels for comparison. The bars with black edges are results on TIMIT (majority baseline: $12.9\\%$ ); the taller bars are for Spanish GlobalPhone (majority baseline: $15.2\\%$ ).  \n\nSeparate logistic regression models were trained on the representations from each layer of the encoder. Since convolutional layers have a stride of 2, the number of frames decreases at each convolutional layer. To label the frames after a convolutional layer we eliminated every other label (and corresponding frame) from the original label sequence. For example, given label sequence $S_{I}\\,=$ aaaaaaann at input layer, we get sequence $S_{2}=a a a a n$ at the first convolutional layer and sequence $S_{3}=a a n$ at the second convolutional layer and at the following recurrent layers.  \n\nResults for the two classification data sets (Figure 4) show very similar patterns. In both the ASR and the AST models, the pretraining data seems to make little difference to phonetic encoding at the early layers, and classification accuracy peaks at the second CNN layer. However, the RNN layers show a clear trend where phone classification accuracy drops off more slowly for models with better ASR/AST performance (i.e., zh $>$ fr $>1$ pt). That is, the later RNN layers more transparently encode language-universal phonetic information.  \n\nPhone classification accuracy in the RNN layers drops for both English and Spanish after fine-tuning on the AST data. This is slightly surprising for Spanish, since the fine-tuning data (unlike the pretraining data) is actually Spanish speech. However, we hypothesize that for AST, higher layers of the encoder may be recruited more to encode semantic information needed for the translation task, and therefore lose some of the linear separability in the phonetic information. Nevertheless, we still see the same pattern where better end-to-end models have higher classification accuracy in the later layers.  \n\n# 6. CONCLUSIONS  \n\nThis paper explored what factors help pretraining for low-resource AST. We performed careful comparisons to tease apart the effects of language relatedness and data size, ultimately finding that rather than either of these, the WER of the pre-trained ASR model is likely the best direct predictor of AST performance. Given equivalent amounts of data, we did not find multilingual pretraining to help more than monolingual pretraining, but we did find an added benefti from using speed perturbation to augment the AST data. Finally, analysis of the pretrained models suggests that those models with better WER are transparently encoding more language-universal phonetic information in the later RNN layers, and this appears to help with AST.  \n\n# 7. ACKNOWLEDGEMENTS  \n\nThe authors wish to thank Yusheng Tian for her work on her Master’s thesis at the University of Edinburgh which inspired the analysis of the change in neural network representations during pretraining and fine-tuning. Also, thanks to Dr. Yevgen Matusevych and to Ramon Sanabria for useful discussions, proof-reading and providing feedback on the paper. This work was supported in part by a James S. McDonnell Foundation Scholar Award (220020374).", "appendix": ""}, {"title": "Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End Speech Translation", "authors": "Chengyi Wang, Yu Wu, Shujie Liu, Zhenglu Yang, Ming Zhou", "bibkey": "bridging_the_gap_between_pre_training_and_fine_tuning_for_end_to_end_speech_translation", "bibitem": "@article{Wang_aaai2020,\n  url = {https://ojs.aaai.org/index.php/AAAI/article/download/6452/6308},\n  title = {Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End Speech Translation},\n  authors = {Chengyi Wang, Yu Wu, Shujie Liu, Zhenglu Yang, Ming Zhou},\n  journal = {Proceedings of the AAAI Conference on Artificial Intelligence}\n}", "url": "https://ojs.aaai.org/index.php/AAAI/article/download/6452/6308", "latex_url": null, "latex_path": null, "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/6452/6308", "pdf_path": "output/download_papers/6452-Article Text-9677-1-10-20200517/6452-Article Text-9677-1-10-20200517.pdf", "md_url": null, "latex_length": 0, "latex": "", "abstract": "End-to-end speech translation, a hot topic in recent years, aims to translate a segment of audio into a specific language with an end-to-end model. Conventional approaches employ multi-task learning and pre-training methods for this task, but they suffer from the huge gap between pre-training and fine-tuning. To address these issues, we propose a Tandem Connectionist Encoding Network (TCEN) which bridges the gap by reusing all subnets in fine-tuning, keeping the roles of subnets consistent, and pre-training the attention module. Furthermore, we propose two simple but effective methods to guarantee the speech encoder outputs and the MT encoder inputs are consistent in terms of semantic representation and sequence length. Experimental results show that our model leads to significant improvements in En-De and En-Fr translation irrespective of the backbones.", "abstract_length": 866, "abstract_token": 166, "introduction": "Speech-to-Text translation (ST) is essential for a wide range of scenarios: for example in emergency calls, where agents have to respond emergent requests in a foreign language (Munro 2010); or in online courses, where audiences and speakers use different languages (Jan et al. 2018). To tackle this problem, existing approaches can be categorized into cascaded method (Ney 1999; Ma et al. 2019), where a machine translation (MT) model translates outputs of an automatic speech recognition (ASR) system into target language, and end-to-end method (Duong et al. 2016; Weiss et al. 2017), where a single model learns acoustic frames to target word sequence mappings in one step towards the final objective of interest. Although the cascaded model remains the dominant approach due to its better performance, the end-to-end method becomes more and more popular because it has lower latency by avoiding inferences with two models and rectifies the error propagation in theory. Since it is hard to obtain a large-scale ST dataset, multitask learning (Weiss et al. 2017; B´erard et al. 2018) and pretraining techniques (Bansal et al. 2019) have been applied to end-to-end ST model to leverage large-scale datasets of ASR and MT. A common practice is to pre-train two encoderdecoder models for ASR and MT respectively, and then initialize the ST model with the encoder of the ASR model and the decoder of the MT model. Subsequently, the ST model is optimized with the multi-task learning by weighing the losses of ASR, MT, and ST. This approach, however, causes a huge gap between pre-training and fine-tuning, which are summarized into three folds: • Subnet Waste: The ST system just reuses the ASR encoder and the MT decoder, while discards other pretrained subnets, such as the MT encoder. Consequently, valuable semantic information captured by the MT encoder cannot be inherited by the final ST system. • Role Mismatch: The speech encoder plays different roles in pre-training and fine-tuning. The encoder is a pure acoustic model in pre-training, while it has to extract semantic and linguistic features additionally in fine-tuning, which significantly increases the learning difficulty. • Non-pre-trained Attention Module: Previous work (B´erard et al. 2018) trains attention modules for ASR, MT and ST respectively, hence, the attention module of ST does not benefit from the pre-training. To address these issues, we propose a Tandem Connectionist Encoding Network (TCEN), which is able to reuse all subnets in pre-training, keep the roles of subnets consistent, and pre-train the attention module. Concretely, the TCEN consists of three components, a speech encoder, a text encoder, and a target text decoder. Different from the previous work that pre-trains an encoder-decoder based ASR model, we only pre-train an ASR encoder by optimizing the Connectionist Temporal Classification (CTC) (Graves et al. 2006) objective function. In this way, the additional decoder of ASR is not required while keeping the ability to read acoustic features into the source language space by the speech encoder. Besides, the text encoder and decoder can be pre-trained on a large MT dataset. After that, we employ common used multi-task learning method to jointly learn ASR, MT and ST tasks. Compared to prior works, the encoder of TCEN is a concatenation of an ASR encoder and an MT encoder and our model does not have an ASR decoder, so the subnet waste issue is solved. Furthermore, the two encoders work at tandem, disentangling acoustic feature extraction and linguistic feature extraction, ensuring the role consistency between pre-training and fine-tuning. Moreover, we reuse the pretrained MT attention module in ST, so we can leverage the alignment information learned in pre-training. ![](images/c8e33da7cf8ff714044dc9906ab4ef5612f5e092d5e52d87c376968a2a801c9a.jpg) Figure 1: An illustration of multi-task learning for speech translation. Networks inherited from pre-trained models are labeled by rectangles. Since the text encoder consumes word embeddings of plausible texts in MT task but uses speech encoder outputs in ST task, another question is how one guarantees the speech encoder outputs are consistent with the word embeddings. We further modify our model to achieve semantic consistency and length consistency. Specifically, (1) the projection matrix at the CTC classification layer for ASR is shared with the word embedding matrix, ensuring that they are mapped to the same latent space, and (2) the length of the speech encoder output is proportional to the length of the input frame, so it is much longer than a natural sentence. To bridge the length gap, source sentences in MT are lengthened by adding word repetitions and blank tokens to mimic the CTC output sequences. We conduct comprehensive experiments on the IWSLT18 speech translation benchmark (Jan et al. 2018), demonstrating the effectiveness of each component. Our model can lead to significant improvements for both LSTM and Transformer backbone. Our contributions are three-folds: 1) we shed light on why previous ST models cannot sufficiently utilize the knowledge learned from the pre-training process; 2) we propose a new ST model, which alleviates shortcomings in existing methods; and 3) we empirically evaluate the proposed model on a large-scale public dataset.", "introduction_length": 5344, "introduction_token": 1142, "reference": "# References  \n\nAnastasopoulos, A., and Chiang, D. 2018. Tied multitask learning for neural speech translation. In NAACL-HLT 2018, 82–91.   \nBahdanau, D.; Cho, K.; and Bengio, Y. 2015. Neural machine translation by jointly learning to align and translate. In ICLR 2015,.   \nBansal, S.; Kamper, H.; Livescu, K.; Lopez, A.; and Goldwater, S. 2019. Pre-training on high-resource speech recognition improves low-resource speech-to-text translation. In NAACL-HLT 2019, 58–68.   \nBender, O.; Zens, R.; Matusov, E.; and Ney, H. 2004. Alignment templates: the RWTH SMT system. In IWSLT 2004, 79–84.   \nBerard, A.; Pietquin, O.; Servan, C.; and Besacier, L. 2016. Listen and translate: A proof of concept for end-to-end speech-to-text translation. In NeurIPS Workshop on Endto-end Learning for Speech and Audio Processing.   \nB´erard, A.; Besacier, L.; Kocabiyikoglu, A. C.; and Pietquin, O. 2018. End-to-end automatic speech translation of audiobooks. In ICASSP, 2018, 6224–6228.   \nDong, L.; Xu, S.; and Xu, B. 2018. Speech-transformer: A no-recurrence sequence-to-sequence model for speech recognition. In ICASSP 2018, 5884–5888.   \nDuong, L.; Anastasopoulos, A.; Chiang, D.; Bird, S.; and Cohn, T. 2016. An attentional model for speech translation without transcription. In NAACL 2016, 949–959.   \nGraves, A.; Fern´andez, S.; Gomez, F. J.; and Schmidhuber, J. 2006. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In ICML 2006, 369–376.   \nHuang, G.; Liu, Z.; Van Der Maaten, L.; and Weinberger, K. Q. 2017. Densely connected convolutional networks. In CVPR 2017, 4700–4708.   \nInaguma, H.; Zhang, X.; Wang, Z.; Renduchintala, A.; Watanabe, S.; and Duh, K. 2018. The jhu/kyotou speech translation system for iwslt 2018. In IWSLT 2018.   \nJan, N.; Cattoni, R.; Sebastian, S.; Cettolo, M.; Turchi, M.; and Federico, M. 2018. The iwslt 2018 evaluation campaign. In IWSLT, 2–6.   \nJia, Y.; Johnson, M.; Macherey, W.; Weiss, R. J.; Cao, Y.; Chiu, C.; Ari, N.; Laurenzo, S.; and Wu, Y. 2019. Leveraging weakly supervised data to improve end-to-end speechto-text translation. In ICASSP 2019, 7180–7184.   \nJuang, B. H., and Rabiner, L. R. 1991. Hidden markov models for speech recognition. Technometrics 33(3):251– 272.   \nKano, T.; Sakti, S.; and Nakamura, S. 2018. Structuredbased curriculum learning for end-to-end english-japanese speech translation. Interspeech 2017 2630–2634.   \nKocabiyikoglu, A. C.; Besacier, L.; and Kraif, O. 2018. Augmenting librispeech with french translations: A multimodal corpus for direct speech translation evaluation. In LREC 2018. Kudo, T. 2018. Subword regularization: Improving neural network translation models with multiple subword candidates. In ACL 2018, 66–75.   \nLiu, Y.; Xiong, H.; He, Z.; Zhang, J.; Wu, H.; Wang, H.; and Zong, C. 2019. End-to-end speech translation with knowledge distillation. In InterSpeech 2019, volume abs/1904.08075.   \nLuong, M.-T.; Le, Q. V.; Sutskever, I.; Vinyals, O.; and Kaiser, L. 2016. Multi-task sequence to sequence learning. ICLR 2016.   \nMa, M.; Huang, L.; Xiong, H.; Zheng, R.; Liu, K.; Zheng, B.; Zhang, C.; He, Z.; Liu, H.; Li, X.; Wu, H.; and Wang, H. 2019. STACL: simultaneous translation with implicit anticipation and controllable latency using prefix-to-prefix framework. In ACL 2019, 3025–3036.   \nMatusov, E.; Kanthak, S.; and Ney, H. 2005. On the integration of speech recognition and statistical machine translation. In INTERSPEECH 2005, 3177–3180.   \nMeignier, S., and Merlin, T. 2010. Lium spkdiarization: an open source toolkit for diarization. In CMU SPUD Workshot.   \nMunro, R. 2010. Crowdsourced translation for emergency response in haiti: the global collaboration of local knowledge. In AMTA Workshop on Collaborative Crowdsourcing for Translation, 1–4.   \nNey, H. 1999. Speech translation: coupling of recognition and translation. In ICASSP 1999, 517–520.   \nPark, D. S.; Chan, W.; Zhang, Y.; Chiu, C.; Zoph, B.; Cubuk, E. D.; and Le, Q. V. 2019. Specaugment: A simple data augmentation method for automatic speech recognition. CoRR abs/1904.08779.   \nPeddinti, V.; Povey, D.; and Khudanpur, S. 2015. A time delay neural network architecture for efficient modeling of long temporal contexts. In Sixteenth Annual Conference of the International Speech Communication Association. Rousseau, A.; Del´eglise, P.; and Esteve, Y. 2014. Enhancing the ted-lium corpus with selected data for language modeling and more ted talks. In LREC 2014, 3935–3939.   \nSperber, M.; Neubig, G.; Niehues, J.; and Waibel, A. 2019. Attention-passing models for robust and data-efficient endto-end speech translation. TACL 7:313–325.   \nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is all you need. In NeurIPS 2017, 5998–6008.   \nWatanabe, S.; Hori, T.; Karita, S.; Hayashi, T.; Nishitoba, J.; Unno, Y.; Soplin, N. E. Y.; Heymann, J.; Wiesner, M.; Chen, N.; et al. 2018. Espnet: End-to-end speech processing toolkit. arXiv preprint arXiv:1804.00015.   \nWeiss, R. J.; Chorowski, J.; Jaitly, N.; Wu, Y.; and Chen, Z. 2017. Sequence-to-sequence models can directly translate foreign speech. In Interspeech 2017, 2625–2629.", "reference_length": 5222, "reference_token": 1727, "txt_length": 37130, "txt_token": 10670, "txt": "# Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End Speech Translation  \n\nChengyi Wang,1∗Yu Wu,2 Shujie Liu,2 Zhenglu Yang,1 Ming Zhou2 1Nankai University, Tianjin, China 2Microsoft Research Aisa, Beijing, China cywang $@$ mail.nankai.edu.cn, {Wu.Yu, shujliu, mingzhou}@microsoft.com, yangzl $@$ nankai.edu.cn  \n\n# Abstract  \n\nEnd-to-end speech translation, a hot topic in recent years, aims to translate a segment of audio into a specific language with an end-to-end model. Conventional approaches employ multi-task learning and pre-training methods for this task, but they suffer from the huge gap between pre-training and fine-tuning. To address these issues, we propose a Tandem Connectionist Encoding Network (TCEN) which bridges the gap by reusing all subnets in fine-tuning, keeping the roles of subnets consistent, and pre-training the attention module. Furthermore, we propose two simple but effective methods to guarantee the speech encoder outputs and the MT encoder inputs are consistent in terms of semantic representation and sequence length. Experimental results show that our model leads to significant improvements in En-De and En-Fr translation irrespective of the backbones.  \n\n# 1 Introduction  \n\nSpeech-to-Text translation (ST) is essential for a wide range of scenarios: for example in emergency calls, where agents have to respond emergent requests in a foreign language (Munro 2010); or in online courses, where audiences and speakers use different languages (Jan et al. 2018). To tackle this problem, existing approaches can be categorized into cascaded method (Ney 1999; Ma et al. 2019), where a machine translation (MT) model translates outputs of an automatic speech recognition (ASR) system into target language, and end-to-end method (Duong et al. 2016; Weiss et al. 2017), where a single model learns acoustic frames to target word sequence mappings in one step towards the final objective of interest. Although the cascaded model remains the dominant approach due to its better performance, the end-to-end method becomes more and more popular because it has lower latency by avoiding inferences with two models and rectifies the error propagation in theory.  \n\nSince it is hard to obtain a large-scale ST dataset, multitask learning (Weiss et al. 2017; B´erard et al. 2018) and pretraining techniques (Bansal et al. 2019) have been applied to end-to-end ST model to leverage large-scale datasets of ASR and MT. A common practice is to pre-train two encoderdecoder models for ASR and MT respectively, and then initialize the ST model with the encoder of the ASR model and the decoder of the MT model. Subsequently, the ST model is optimized with the multi-task learning by weighing the losses of ASR, MT, and ST. This approach, however, causes a huge gap between pre-training and fine-tuning, which are summarized into three folds:  \n\n• Subnet Waste: The ST system just reuses the ASR encoder and the MT decoder, while discards other pretrained subnets, such as the MT encoder. Consequently, valuable semantic information captured by the MT encoder cannot be inherited by the final ST system. • Role Mismatch: The speech encoder plays different roles in pre-training and fine-tuning. The encoder is a pure acoustic model in pre-training, while it has to extract semantic and linguistic features additionally in fine-tuning, which significantly increases the learning difficulty. • Non-pre-trained Attention Module: Previous work (B´erard et al. 2018) trains attention modules for ASR, MT and ST respectively, hence, the attention module of ST does not benefit from the pre-training.  \n\nTo address these issues, we propose a Tandem Connectionist Encoding Network (TCEN), which is able to reuse all subnets in pre-training, keep the roles of subnets consistent, and pre-train the attention module. Concretely, the TCEN consists of three components, a speech encoder, a text encoder, and a target text decoder. Different from the previous work that pre-trains an encoder-decoder based ASR model, we only pre-train an ASR encoder by optimizing the Connectionist Temporal Classification (CTC) (Graves et al. 2006) objective function. In this way, the additional decoder of ASR is not required while keeping the ability to read acoustic features into the source language space by the speech encoder. Besides, the text encoder and decoder can be pre-trained on a large MT dataset. After that, we employ common used multi-task learning method to jointly learn ASR, MT and ST tasks.  \n\nCompared to prior works, the encoder of TCEN is a concatenation of an ASR encoder and an MT encoder and our model does not have an ASR decoder, so the subnet waste issue is solved. Furthermore, the two encoders work at tandem, disentangling acoustic feature extraction and linguistic feature extraction, ensuring the role consistency between pre-training and fine-tuning. Moreover, we reuse the pretrained MT attention module in ST, so we can leverage the alignment information learned in pre-training.  \n\n![](images/c8e33da7cf8ff714044dc9906ab4ef5612f5e092d5e52d87c376968a2a801c9a.jpg)  \nFigure 1: An illustration of multi-task learning for speech translation. Networks inherited from pre-trained models are labeled by rectangles.  \n\nSince the text encoder consumes word embeddings of plausible texts in MT task but uses speech encoder outputs in ST task, another question is how one guarantees the speech encoder outputs are consistent with the word embeddings. We further modify our model to achieve semantic consistency and length consistency. Specifically, (1) the projection matrix at the CTC classification layer for ASR is shared with the word embedding matrix, ensuring that they are mapped to the same latent space, and (2) the length of the speech encoder output is proportional to the length of the input frame, so it is much longer than a natural sentence. To bridge the length gap, source sentences in MT are lengthened by adding word repetitions and blank tokens to mimic the CTC output sequences.  \n\nWe conduct comprehensive experiments on the IWSLT18 speech translation benchmark (Jan et al. 2018), demonstrating the effectiveness of each component. Our model can lead to significant improvements for both LSTM and Transformer backbone.  \n\nOur contributions are three-folds: 1) we shed light on why previous ST models cannot sufficiently utilize the knowledge learned from the pre-training process; 2) we propose a new ST model, which alleviates shortcomings in existing methods; and 3) we empirically evaluate the proposed model on a large-scale public dataset.  \n\n# 2 Background  \n\nEnd-to-end speech translation aims to translate a piece of audio into a target-language translation in one step. The raw speech signals are usually converted to sequences of acoustic features. Here, we define the speech feature sequence as $\\pmb{x}=(x_{1},\\cdots\\,,x_{T_{x}})$ .  \n\nThe transcription and translation sequences are denoted as $\\pmb{y}^{s}~=~(y_{1}^{s},\\bar{\\bf\\Psi}\\cdot\\cdot\\cdot,y_{T_{s}}^{s})$ , and $y^{t}~=~(\\dot{y}_{1}^{t},\\cdot\\cdot\\cdot\\cdot,y_{T_{t}}^{t})$ repectively. Each symbol in $y^{s}$ or $\\boldsymbol{y}^{t}$ is an integer index of the symbol in a vocabulary $V_{s r c}$ or $V_{t r g}$ respectively (e.g. $y_{i}^{s}=k,k\\in[0,|V_{s r c}|-1])$ . In this work, we suppose that an ASR dataset, an MT dataset, and a ST dataset are available, denoted as $\\pmb{\\mathcal{A}}=\\{(\\pmb{x}_{i},\\pmb{y}_{i}^{s})\\}_{i=0}^{I}$ , $\\mathcal{M}=\\{(\\pmb{y}_{j}^{s},\\pmb{y}_{j}^{t})\\}_{j=0}^{J}$ and ${\\cal{S}}\\,=\\,\\{(x_{l},y_{l}^{t})\\}_{l=0}^{L}$ respectively. Given a new piece of audio $_{\\textbf{\\em x}}$ , our goal is to learn an end to end model to generate a translation sentence $\\boldsymbol{y}^{t}$ without generating an intermediate result $\\boldsymbol{y}^{s}$ .  \n\n# 2.1 Multi-Task Learning and Pre-training for ST  \n\nTo leverage large scale ASR and MT data, multi-task learning and pre-training techniques are widely employed to improve the ST system. As shown in Figure 1, there are three popular multi-task strategies for ST, including 1) one-tomany setting, in which a speech encoder is shared between ASR and ST tasks; 2) many-to-one setting in which a decoder is shared between MT and ST tasks; and 3) many-tomany setting where both the encoder and decoder are shared.  \n\nA many-to-many multi-task model contains two encoders as well as two decoders. It can be jointly trained on ASR, MT, and ST tasks. As the attention module is task-specific, three attentions are defined.  \n\n# 3 Our method  \n\nIn this section, we first introduce the architecture of TCEN, which consists of two encoders connected in tandem, and one decoder with an attention module. Then we give the pre-training and fine-tuning strategy for TCEN. Finally, we propose our solutions for semantic and length inconsistency problems, which are caused by multi-task learning.  \n\n# 3.1 Unified formulation for TCEN Architecture  \n\nFigure 2 sketches the overall architecture of TCEN, including a speech encoder $e n c_{s}$ , a text encoder $e n c_{t}$ and a decoder dec with an attention module att. The $e n c_{s}$ usually contains two modules: EncPre and EncBody. During training, the $e n c_{s}$ acts like an acoustic model which reads the input $_{\\textbf{\\em x}}$ to word or subword representations $h^{s}$ :  \n\n$$\n\\begin{array}{c}{\\tilde{\\pmb{x}}=\\mathrm{EncPre}(\\pmb{x})}\\\\ {\\pmb{h}^{s}=\\mathrm{EncBody}(\\tilde{\\pmb{x}})}\\end{array}\n$$  \n\nThen $e n c_{t}$ learns high-level linguistic knowledge into hidden representations $h^{t}$ :  \n\n$$\nh^{t}=\\operatorname{enc}_{\\mathrm{t}}(\\mathrm{h}^{\\mathrm{s}})\n$$  \n\nFinally, the dec defines a distribution probability over target words through attention mechanism:  \n\n$$\n\\begin{array}{c}{c_{k}=\\operatorname{att}(z_{k-1},\\pmb{h}^{t})}\\\\ {z_{k}=\\operatorname{dec}(z_{k-1},\\pmb{y}_{k-1}^{t},c_{k})}\\\\ {P(\\pmb{y}_{k}^{t}|\\pmb{y}_{<k}^{t},\\pmb{x})=\\operatorname{softmax}(W\\cdot z_{k})}\\end{array}\n$$  \n\nHere, $z_{k}$ is the the hidden state of the deocder at $k$ step and $c_{k}$ is a time-dependent context vector computed by the attention att.  \n\n![](images/789b66cdec729e9322b1be72a79af8ad94847751cd27c00736edd41c2343d6fd.jpg)  \nTable 1: An example of the comparison between the golden transcript and the predicted CTC paths given the corresponding speech. ‘-’ denotes the blank token and the following number represents repeat times.  \n\nThe advantage of our architecture is that two encoders disentangle acoustic feature extraction and linguistic feature extraction, making sure that valuable knowledge learned from ASR and MT tasks can be effectively leveraged for ST training. However, there exists another problem: In ST task, $e n c_{t}$ accepts speech encoder output $h^{s}$ as input. While in MT, $e n c_{t}$ consumes the word embedding representation $e^{s}$ derived from $y^{s}$ , where each element $e_{i}^{s}$ is computed by choosing the $y_{i}^{s}$ -th vector from the source embedding matrix $W_{E^{s}}$ . Since $h^{s}$ and $e^{s}$ belong to different latent space and have different lengths, there remain semantic and length inconsistency problems. We will provide our solutions in Section 3.3. To verify the generalization of our framework, we test on LSTM based setting and Transformer (Vaswani et al. 2017) based setting.  \n\n# 3.2 Training Procedure  \n\nFollowing previous work, we split the training procedure to pre-training and fine-tuning stages. In pre-training stage, the speech encoder $e n c_{s}$ is trained towards CTC objective using dataset $\\mathcal{A}$ , while the text encoder $e n c_{t}$ and the decoder dec are trained on MT dataset $\\mathcal{M}$ . In fine-tuning stage, we jointly train the model on ASR, MT, and ST tasks.  \n\n<html><body><table><thead><tr><td><b>Transcript ys</b></td><td><b>we were not v @en @ge @ful at all</b></td></tr></thead><tbody><tr><td>CTC path π1</td><td>-(11) we we -(3) were -(3) not -(4) v @en @en @ge - @ful -(8) at at -(3) all -(10)</td></tr><tr><td>CTC path π2</td><td>-(9) we -(3) were were -(4) not not -(3) v v @en @en @en @ge - @ful -(7) at -(3) all all -(10)</td></tr></tbody></table></body></html>  \n\nPre-training To sufficiently utilize the large dataset $\\mathcal{A}$ and $\\mathcal{M}$ , the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage.  \n\nFor ASR task, in order to get rid of the requirement for decoder and enable the $e n c_{s}$ to generate subword representation, we leverage connectionist temporal classification (CTC) (Graves et al. 2006) loss to train the speech encoder.  \n\nGiven an input $\\textbf{\\em x}$ , $e n c_{s}$ emits a sequence of hidden vectors $h^{s}$ , then a softmax classification layer predicts a CTC path $\\pi$ , where $\\pi_{t}\\in V_{s r c}\\cup\\{\\ '-\\ '\\}$ is the observing label at particular RNN step $t$ , and $\\bullet\\_,$ is the blank token representing no observed labels:  \n\n$$\nP(\\pi|x)\\approx\\prod_{t=1}^{T}P(\\pi_{t}|x)=\\prod_{t=1}^{T}\\mathrm{softmax}(W_{c t c}\\cdot h_{t}^{s})\n$$  \n\nwhere $W_{c t c}\\in\\mathbb{R}^{d\\times(|V_{s r c}|+1)}$ is the weight matrix in the classification layer and $T$ is the total length of encoder RNN.  \n\nA legal CTC path $\\pi$ is a variation of the source transcription $\\boldsymbol{y}^{s}$ by allowing occurrences of blank tokens and repetitions, as shown in Table 1. For each transcription $\\textit{\\textbf{y}}$ , there exist many legal CTC paths in length $T$ . The CTC objective trains the model to maximize the probability of observing the golden sequence $\\boldsymbol{y}^{s}$ , which is calculated by summing the probabilities of all possible legal paths:  \n\n$$\n\\begin{array}{r l}&{\\displaystyle P(\\pmb{y}|\\pmb{x})=\\sum_{\\pi\\in\\Phi_{T}(\\pmb{y})}P(\\pi|\\pmb{x})}\\\\ &{\\mathcal{L}_{C T C}(\\pmb{\\theta})=-\\displaystyle\\sum_{(\\pmb{x},\\pmb{y}^{s})\\in\\mathcal{A}}\\log P(\\pmb{y}^{s}|\\pmb{x};\\theta_{e n c_{s}},\\theta_{W_{c t c}})}\\end{array}\n$$  \n\nwhere $\\Phi_{T}(y)$ is the set of all legal CTC paths for sequence $\\textit{\\textbf{y}}$ with length $T$ . The loss can be easily computed using forward-backward algorithm. More details about CTC are provided in supplementary material.  \n\nFor MT task, we use the cross-entropy loss as the training objective. During training, $\\boldsymbol{y}^{s}$ is converted to embedding vectors $e^{s}$ through embedding layer $W_{E^{s}}$ , then $e n c_{t}$ consumes $e^{s}$ and pass the output $h^{t}$ to decoder. The objective function is defined as:  \n\n$$\n\\mathcal{L}_{M T}(\\pmb{\\theta})=-\\sum_{(\\boldsymbol{y}^{s},\\boldsymbol{y}^{t})\\in\\mathcal{M}}\\log P(\\pmb{y}^{t}|\\pmb{y}^{s};\\theta_{e n c_{t}},\\theta_{d e c},\\theta_{W_{E^{s}}})\n$$  \n\nFine-tune In fine-tune stage, we jointly update the model on ASR, MT, and ST tasks. The training for ASR and MT follows the same process as it was in pre-training stage.  \n\nFor ST task, the $e n c_{s}$ reads the input $\\textbf{\\em x}$ and generates $h^{s}$ then $e n c_{t}$ learns high-level linguistic knowledge into $h^{t}$ . Finally, the dec predicts the target sentence. The ST loss function is defined as:  \n\n$$\n\\mathcal{L}_{S T}(\\pmb{\\theta})=-\\sum_{(\\pmb{x},\\pmb{y}^{t})\\in\\mathcal{S}}\\log P(\\pmb{y}^{t}|\\pmb{x};\\theta_{e n c s},\\theta_{e n c t},\\theta_{d e c})\n$$  \n\nFollowing the update strategy proposed by Luong et al. (2016), we allocate a different training ratio $\\alpha_{i}$ for each task. When switching between tasks, we select randomly a new task i with probability α iαj .  \n\n# 3.3 Subnet-Consistency  \n\nOur model keeps role consistency between pre-training and fine-tuning by connecting two encoders for ST task. However, this leads to some new problems: 1) The text encoder consumes $e^{s}$ during MT training, while it accepts $h^{s}$ during ST training. However, $e^{s}$ and $h^{s}$ may not follow the same distribution, resulting in the semantic inconsistency. 2) Besides, the length of $h^{s}$ is not the same order of magnitude with the length of $e^{s}$ , resulting in the length inconsistency.  \n\nIn response to the above two challenges, we propose two countermeasures: 1) We share weights between CTC classification layer and source-end word embedding layer during training of ASR and MT, encouraging $e^{s}$ and $h^{s}$ in the same space. 2)We feed the text encoder source sentences in the format of CTC path, which are generated from a seq2seq model, making it more robust toward long inputs.  \n\nSemantic Consistency As shown in Figure 2, during multi-task training, two different hidden features will be fed into the text encoder $e n c_{t}$ : the embedding representation $e^{s}$ in MT task, and the $e n c_{s}$ output $h^{s}$ in ST task. Without any regularization, they may belong to different latent spaces. Due to the space gap, the $e n c_{t}$ has to compromise between two tasks, limiting its performance on individual tasks.  \n\nTo bridge the space gap, our idea is to pull $h^{s}$ into the latent space where $e^{s}$ belong. Specifically, we share the weight $W_{c t c}$ in CTC classification layer with the source embedding weights $W_{E^{s}}$ , which means $W_{c t c}\\,=\\,W_{E^{s}}$ . In this way, when predicting the CTC path $\\pi$ , the probability of observing the particular label $w_{i}\\overset{\\cdot}{\\in}V_{s r c}\\cup\\{\\overset{\\cdot}{-}\\}$ at time step $t$ , $p(\\pi_{t}\\,\\breve{=}\\,w_{i}|\\mathbf{\\dot{x}})_{.}$ , is computed by normalizing the product of hidden vector $h_{t}^{s}$ and the $i$ -th vector in $W_{E^{s}}$ :  \n\n![](images/31a79d87cf5c614d471300c42d8cb2e6097d6156e27012d49d29534be43d71d7.jpg)  \nFigure 3: The architecture of seq2seq model. It predicts the next token and its number of repetition at the same time.  \n\n$$\nP(\\pi_{t}=w_{i}|\\pmb{x})=\\frac{e x p(W_{E^{s}}^{(i)}\\cdot h_{t}^{s})}{\\sum_{j}^{|V_{s r c}|+1}e x p(W_{E^{s}}^{(j)}\\cdot h_{t}^{s})}\n$$  \n\nThe loss function closes the distance between $h_{t}^{s}$ and golden embedding vector, encouraging $h^{s}$ have the same distribution with $e^{s}$ .  \n\nLength Consistency Another existing problem is length inconsistency. The length of the sequence $h^{s}$ is proportional to the length of the input frame $_{\\textbf{\\em x}}$ , which is much longer than the length of $e^{s}$ . To solve this problem, we train an RNNbased seq2seq model to transform normal source sentences to noisy sentences in CTC path format, and replace standard MT with denoising MT for multi-tasking.  \n\nSpecifically, we first train a CTC ASR model based on dataset $\\pmb{\\mathcal{A}}\\,=\\,\\{(\\pmb{x}_{i},\\pmb{y}_{i}^{s})\\}_{i=0}^{I}$ , and generate a CTC-path $\\pi_{i}$ for each audio $\\mathbf{\\Delta}x_{i}$ by greedy decoding. Then we define an operation $S(\\cdot)$ , which converts a CTC path $\\pi$ to a sequence of the unique tokens $\\textbf{\\em u}$ and a sequence of repetition times for each token $\\imath$ , denoted as $S(\\pi)\\;=\\;(u,l)$ . Notably, the operation is reversible, meaning that $S^{-1}({\\dot{\\mathbf{u}}},l)\\,=\\,\\pi$ . We use the example $\\pi_{1}$ in Table 1 and show the corresponding $\\textbf{\\em u}$ and $\\imath$ in Table 2.  \n\nThen we build a dataset $\\mathcal{P}\\;=\\;\\{(\\pmb{y}_{i}^{s},\\pmb{u}_{i},\\pmb{l}_{i})\\}_{i=0}^{I}$ by decoding all the audio pieces in $\\mathcal{A}$ and transform the resulting path by the operation $S(\\cdot)$ . After that, we train a seq2seq model, as shown in Figure 3, which takes $\\boldsymbol{y}_{i}^{s}$ as input and decodes $u_{i},l_{i}$ as outputs. With the seq2seq model, a noisy MT dataset $\\mathcal{M}^{\\prime}=\\{(\\bar{\\pi_{l}},\\pmb{y_{l}^{t}})\\}_{l=0}^{L}$ is obtained by converting every source sentence $\\pmb{y}_{i}^{s}\\in\\mathcal{M}$ to $\\pi_{i}$ , where $\\dot{\\pi}_{i}\\,=\\,S^{-1}(\\bar{\\pmb{u}}_{i},l_{i})$ . We did not use the standard seq2seq model which takes $y^{s}$ as input and generates $\\pi$ directly, since there are too many blank tokens ‘-’ in $\\pi$ and the model tends to generate a long sequence with only blank tokens. During MT training, we randomly sample text pairs from $\\mathcal{M}^{\\prime}$ and $\\mathcal{M}$ according to a hyper-parameter $k$ . After tuning on the validation set, about  \n\n<html><body><table><thead><tr><td><b>CTCpathπ1</b></td><td colspan=\"10\"><b>-(11) we we -(3) were -(3) not -(4) v @en @en @ge - @ful -(8) at at -(3) all -(10)</b></td></tr></thead><tbody><tr><td>u</td><td>-</td><td>we</td><td>-</td><td>were</td><td>not</td><td>-</td><td>-</td><td>V</td><td>@en</td><td>@ge</td><td>3</td><td>@ful</td><td>-</td><td>at</td><td>-</td><td>all</td><td>3</td></tr><tr><td>1</td><td>11</td><td>2</td><td>3</td><td>1</td><td>1</td><td>4</td><td>4</td><td>1</td><td>2</td><td>1</td><td>1</td><td>1</td><td>8</td><td>2</td><td>3</td><td>1</td><td>10</td></tr></tbody></table></body></html>\n\nTable 2: The CTC path $\\pi_{1}$ and corresponding unique tokens $\\textbf{\\em u}$ and repetition times $\\imath$ , where $S(\\pi)=(u,l)$  \n\n$30\\%$ pairs are sampled from $\\mathcal{M}^{\\prime}$ . In this way, the $e n c_{t}$ is more robust toward the longer inputs given by the $e n c_{s}$ .  \n\n# 4 Experiments  \n\n# 4.1 Dataset  \n\nWe conduct experiments on the Speech Translation TED (ST-TED) En-De corpus (Jan et al. 2018) and the augmented Librispeech En-Fr corpus (Kocabiyikoglu, Besacier, and Kraif 2018).  \n\nST-TED En-De The corpus contains 272 hours of English speech with 171k segments. Each example consists of raw English wave, English transcription, and aligned German translation. Aside from ST-TED, we use TED-LIUM2 corpus (Rousseau, Del´eglise, and Esteve 2014) with 207h of speech data for ASR pre-training. For MT model, we use WMT2018 en-de data in pre-training stage and use sentence pairs in the ST-TED corpus as well as $\\mathrm{\\breve{WIT}}3^{1}$ in fine-tune stage. The pre-training data contains 41M sentence pairs and fine-tuning data contains $330\\mathrm{k}$ sentence paris in total. We split $2\\mathbf{k}$ segments from the ST-TED corpus as dev set and tst2010, tst2013, tst2014, tst2015 are used as test sets.  \n\nLibrispeech En-Fr This corpus is colleted by aligning ebooks in French with English utterances, which contains 236 hours of speech in total. The English speech, English transcription, French text translations from alignment and Google Translate references are provided. Following previous work (B´erard et al. 2018), we only use the 100 hours clean train set and double the training size by concatenating the aligned references with Google Translate references. We use the speech-transcription pairs and transcriptiontranslation pairs for ASR and MT pre-training. No additional data is used. The dev set is used as validation set and we report results on the test set.  \n\nData preprocessing Our acoustic features are 80- dimensional log-Mel filterbanks and 3-dimensional pitch features extracted with a step size of 10ms and window size of $25\\mathrm{ms}$ and extended with mean subtraction and variance normalization. The utterances with more than 3000 frames are discarded. All the sentences are in lower-case and the punctuation is removed. To increase the amount of training data, we perform speed perturbation on the raw signals with speed factors 0.9 and 1.1.  \n\nFor the MT pre-training data, sentences longer than 80 words or shorter than 10 words are removed. Besides, we discard pairs whose length ratio between source and target sentence is smaller than 0.5 or larger than 2.0. Word tokenization is performed using the Moses scripts2 and all words are in lower-case.  \n\nFor ST-TED experiments, we apply both subword-level decoding and character-level decoding. For the subword setting, both English and German vocabularies are generated using sentencepiece3 (Kudo 2018) with a fixed size of $5\\mathrm{k}$ tokens. For Librispeech En-Fr experiments, we only apply character-level decoding.  \n\nSince there are no human annotated alignments provided in ST-TED test sets, we segment each audio with the LIUM SpkDiarization tool (Meignier and Merlin 2010) and then perform MWER segmentation with RWTH toolkit (Bender et al. 2004). Case-insensitive BLEU is used as evaluation metric.  \n\n# 4.2 Experimental setups  \n\nModel architecture For LSTM based models, we follow the model structure in Inaguma et al. (2018). The EncPre corresponds to 2-layers of VGG-like max-pooling, resulting 4-fold downsampling of input feature. The EncBody is five bidirectional LSTM layers with cell size of 1024. The decoder is defined as two unidirectional LSTM layers with an additive attention. The decoder has the same dimension as the encoder RNNs.  \n\nFor Transformer based models, we use two-layer CNN with 256 channels, stride size 2 and kernel size 3 as EncPre. The other modules are similar as in paper Dong, Xu, and $\\mathrm{Xu}$ (2018) $(e\\;=\\;12,d\\;=\\;6,d_{m o d e l}\\;=\\;256,d_{f f}\\;=\\;2048$ and $d_{h e a d}=4$ ).  \n\nBaselines We compare our method with the following baselines.  \n\nVanilla ST baseline: The vanilla ST has only a speech encoder and a decoder, which is trained from scratch with only the speech-translation data.   \nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, 2) decoder pre-training, and 3) encoder-decoder pre-training. The pre-trained ASR model has the same architecture with vanilla ST model. The MT model has a $e n c_{t}$ and dec with the same architecture of which in TCEN.   \n• Pre-training $+~\\mathrm{MTL}$ : In this setting, we train a many-tomany multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models.  \n\nImplementation All our models are implemented based on ESPnet (Watanabe et al. 2018). For LSTM based models, we use a dropout of 0.3 for embedding and encoders. The model is trained using Adadelta with initial learning rate of 1.0.  \n\n<html><body><table><thead><tr><td></td><td colspan=\"5\"><b>Subword Level Decoder</b></td><td colspan=\"5\"><b>Char Level Decoder</b></td></tr><tr><td></td><td><b>tst2010</b></td><td><b>tst2013</b></td><td><b>tst2014</b></td><td><b>tst2015</b></td><td><b>Average</b></td><td><b>tst2010</b></td><td><b>tst2013</b></td><td><b>tst2014</b></td><td><b>tst2015</b></td><td><b>Average</b></td></tr></thead><tbody><tr><td>Vanilla</td><td>7.52</td><td>7.04</td><td>6.77</td><td>6.57</td><td>6.98</td><td>13.77</td><td>12.50</td><td>11.50</td><td>12.68</td><td>12.61</td></tr><tr><td> +enc pretrain</td><td>10.70</td><td>10.12</td><td>8.82</td><td>7.76</td><td>9.35</td><td>16.00</td><td>14.49</td><td>12.66</td><td>12.20</td><td>13.76</td></tr><tr><td>+dec pretrain</td><td>9.75</td><td>9.02</td><td>8.34</td><td>8.01</td><td>8.78</td><td>14.44</td><td>12.99</td><td>11.91</td><td>12.87</td><td>13.05</td></tr><tr><td> +enc dec pretrain</td><td>12.14</td><td>11.07</td><td>9.96</td><td>8.77</td><td>10.49</td><td>15.52</td><td>14.62</td><td>13.39</td><td>13.33</td><td>14.22</td></tr><tr><td> pretrain+MTL</td><td>11.92</td><td>11.78</td><td>9.89</td><td>9.27</td><td>10.72</td><td>15.70</td><td>15.42</td><td>13.43</td><td>12.66</td><td>14.30</td></tr><tr><td>Triangle+pretrain</td><td>9.89</td><td>9.91</td><td>7.48</td><td>7.22</td><td>8.63</td><td>11.35</td><td>10.73</td><td>9.43</td><td>9.47</td><td>10.25</td></tr><tr><td>TCEN-LSTM</td><td>15.49</td><td>15.50</td><td>13.21</td><td>13.02</td><td>14.31</td><td>17.61</td><td>17.67</td><td>15.73</td><td>14.94</td><td>16.49</td></tr></tbody></table></body></html>\n\nTable 3: Results of LSTM-based models on ST TED. “Average” denotes it averages the results of all test sets. We copy the numbers of vanilla model from https://github.com/espnet/espnet/blob/master/egs/iwslt18/st1/RESULTS. Since pre-training data is different, we run ESPnet code to obtain the numbers of pre-training and multi-task learning method, which are slightly higher than numbers in their report.  \n\nFor Transformer based model, we use a dropout rate of 0.1 and a gradient clip of 5.0. Following Dong, Xu, and $X\\mathrm{u}(2018)$ , we use Adam optimizer according to the learning rate schedule formula:  \n\n$$\n{\\mathrm{lrate}}=k\\cdot d_{m o d e l}^{-0.5}\\cdot\\operatorname*{min}(n^{-0.5},n\\cdot w a r m u p_{-}n^{-1.5})\n$$  \n\nWe set $k=10$ and warmup $n=25000$ in our experiments. All the models are trained on 4 Tesla P40 GPU for a maximum of 20 epochs.  \n\nFor training of TCEN, we set $\\alpha_{a s r}=0.2$ and $\\alpha_{m t}=0.8$ in the pre-training stage, since the MT dataset is much larger than ASR dataset. For fine-tune, we use $\\alpha_{s t}=0.6,\\alpha_{a s r}=$ 0.2 and $\\alpha_{m t}\\,=\\,0.2$ . At inference time, we use a beam size of 10 and a length normalization weight of 0.2.  \n\n# 4.3 Experimental Results  \n\nReults on ST TED Table 3 shows the LSTM-based results on four test sets as well as the average performance. In this setting, we also re-implement the triangle multi-task strategy (Anastasopoulos and Chiang 2018) as our baseline, denoted as ‘triangle+pretrain’. They concatenate a ST decoder to an ASR encoder-decoder model.  \n\nFrom the table, we can see that our method significantly outperforms the strong ‘pretrain+MTL’ baseline by 3.6 and 2.2 BLEU scores respectively, indicating the proposed method is very effective. Besides, both pre-training and multi-task learning can improve translation quality. We observe a performance degradation in the ‘triangle+pretrain’ baseline. Compared to our method, where the decoder receives higher-level knowledge extracted from text encoder, their ASR decoder can only provide lower word-level linguistic information. Besides, their model cannot utilize the large-scale MT data in all the training stages.  \n\nTable 4 shows the comparison between our best model with the cascaded systems, which combines the ASR model and MT model. In addition to a simple combination system, we also re-segment the ASR outputs before feeding to the MT system, denoted as ‘cascaded+re-seg’. Specifically, we train a seq2seq model (Bahdanau, Cho, and Bengio 2015) on the MT dataset, where the source side is a no punctuation sentence and the target side is a natural sentence. After that, we use the seq2seq model to add sentence boundaries and punctuation on ASR outputs. It can be seen that our end-toend model outperforms the simple cascaded model over 2 BLEU scores, and achieves a comparable performance with the ‘cascaded+re-seg’ system.  \n\n<html><body><table><thead><tr><td></td><td><b>tst2010</b></td><td><b>tst2013</b></td><td><b>tst2014</b></td><td><b>tst2015</b></td></tr></thead><tbody><tr><td>cascaded</td><td>13.38</td><td>15.84</td><td>12.94</td><td>13.79</td></tr><tr><td>cascaded+re-seg our model</td><td>17.12</td><td>17.77</td><td>14.94</td><td>15.01</td></tr></tbody></table></body></html>  \n\nTable 4: BLEU comparison of cascaded results and our best end-to-end results. re-seg denotes the ASR outputs are resegmented before fed into the MT model.   \nTable 5: BLEU of Transformer-based models on tst2013 set. ‘-’: failed training.   \n\n\n<html><body><table><thead><tr><td><b>System</b></td><td><b>tst2013</b></td></tr></thead><tbody><tr><td>Vanilla</td></tr><tr><td>+enc pretrain</td><td>13.41</td></tr><tr><td>+enc dec pretrain</td><td>14.46</td></tr><tr><td>pretrain+MTL</td><td>14.98</td></tr><tr><td>TCEN-Transformer</td></tr></tbody></table></body></html>  \n\nWe list Transformer-based results on tst2013 in Table 5. In this setting, we use character-level decoding strategy due to its better performance. Only in-domain MT data is used during pre-training. It can be seen that our TCEN framework works well on Transformer-based architecture and it outperforms the ‘pretrain+MTL’ baseline by 2.1 BLEU scores.  \n\nResults on Librispeech For this dataset, we only perform LSTM-based experiments and report results in Table 6. Even without utilizing large-scale ASR data or MT data, our method can outperform the pre-training baselines and achieve the same performance with Park et al. (2019), which uses a MT model as a teacher model to guide the ST model.  \n\n# 4.4 Discussion  \n\nAblation Study To better understand the contribution of each component, we perform an ablation study on subwordlevel experiments for ST TED corpus. The results are shown in Table 7. In ‘-MT noise’ setting, we do not add noise to source sentences for MT. In ‘-weight sharing’ setting, we use different parameters in CTC classification layer and source embedding layer. These two experiments prove that both weight sharing and using noisy MT input benefit to the final translation quality. Performance degrades more in ‘-weight sharing’, indicating the semantic consistency contributes more to our model.  \n\n<html><body><table><thead><tr><td colspan=\"2\"><b>Model</b></td><td><b>BLEU</b></td></tr></thead><tbody><tr><td>MT</td><td>Bérard et al.(2018) ESPnet*</td><td>19.2 18.3</td></tr><tr><td>Cascaded</td><td>Bérard et al.(2018) ESPnet*</td><td>14.6 15.8</td></tr><tr><td>E2E</td><td>Berard et al.(2018) +Pretrain+MTL</td><td>12.9 13.4</td></tr></tbody></table></body></html>  \n\nTable 6: BLEU results of LSTM-based models on Librispeech En-Fr. \\*: The ESPnet baseline results are copied from https://github.com/espnet/espnet/blob/master/ egs/libri trans   \n\n\n<html><body><table><thead><tr><td><b>System</b></td><td><b>tst2010</b></td><td><b>tst2013</b></td><td><b>tst2014</b></td><td><b>tst2015</b></td></tr></thead><tbody><tr><td>TCEN</td><td>15.49</td><td>15.50</td><td>13.21</td><td>13.02</td></tr><tr><td>-MT noise</td><td>15.01</td><td>14.95</td><td>13.34</td><td>12.80</td></tr><tr><td>-weight sharing</td><td>13.51</td><td>14.02</td><td>12.25</td><td>11.66</td></tr><tr><td> -pretrain</td><td>8.98</td><td>8.42</td><td>7.94</td><td>8.08</td></tr></tbody></table></body></html>\n\nTable 7: Ablation study for subword-level experiments.  \n\nIn the ‘-pretrain’ experiment, we remove the pre-training stage and directly update the model on three tasks, leading to a dramatic decrease on BLEU score, indicating the pretraining is an indispensable step for end-to-end ST.  \n\nLearning Curve It is interesting to investigate why our method is superior to baselines. We find that TCEN achieves a higher final result owing to a better start-point in finetuning. Figure 4 provides learning curves of subword accuracy on validation set. The $\\mathbf{X}$ -axis denotes the fine-tuning training steps. The vanilla model starts at a low accuracy, because its networks are not pre-trained on the ASR and MT data. The trends of our model and ‘many-to-many+pretrain are similar, but our model outperforms it about five points in the whole fine-tuning process. It indicates that the gain comes from bridging the gap between pre-training and finetuning rather than a better fine-tuning process.  \n\n![](images/e37b09d93eaaa7c38c350e5389e9882b19ed13cb18e7448c36049b2f5ef0036c.jpg)  \nFigure 4: Model learning curves in fine-tuning.  \n\n# 5 Related Work  \n\nEarly works conduct ST in a pipeline manner (Ney 1999; Matusov, Kanthak, and Ney 2005), where the ASR output are fed into an MT system to generate target sentences. HMM (Juang and Rabiner 1991), DenseNet (Huang et al. 2017), TDNN (Peddinti, Povey, and Khudanpur 2015) are commonly used ASR systems, while RNN with attention (Bahdanau, Cho, and Bengio 2015) and Transformer (Vaswani et al. 2017) are top choices for MT.  \n\nTo avoid error propagation and high latency issues, recent works propose translating the acoustic speech into text in target language without yielding the source transcription (Duong et al. 2016; Berard et al. 2016). Since ST data is scarce, pre-training (Bansal et al. 2019), multi-task learning (Duong et al. 2016; B´erard et al. 2018), curriculum learning (Kano, Sakti, and Nakamura 2018), attention-passing (Sperber et al. 2019), and knowledge distillation (Liu et al. 2019; Jia et al. 2019) strategies have been explored to utilize ASR data and MT data. Specifically, Weiss et al. (2017) show improvements of performance by training the ST model jointly with the ASR and the MT model. B´erard et al. (2018) observe faster convergence and better results due to pretraining and multi-task learning on a larger dataset. Bansal et al. (2019) show that pre-training a speech encoder on one language can improve ST quality on a different source language. All of them follow the traditional multi-task training strategies. Kano, Sakti, and Nakamura (2018) propose to use curriculum learning to improve ST performance on syntactically distant language pairs. To effectively leverage transcriptions in ST data, Anastasopoulos and Chiang (2018) augment the multi-task model where the target decoder receives information from the source decoder and they show improvements on low-resource speech translation. Jia et al. (2019) use pre-trained MT and text-to-speech (TTS) synthesis models to convert weakly supervised data into ST pairs and demonstrate that an end-to-end MT model can be trained using only synthesised data.  \n\n# 6 Conclusion  \n\nThis paper has investigated the end-to-end method for ST. We propose a method to reuse every sub-net and keep the role of sub-net consistent between pre-training and finetuning, alleviating the gap between pre-training and finetuning in previous methods. Empirical studies have demonstrated that our model significantly outperforms baselines.  \n\n# 7 Acknowledgements  \n\nThis work was supported in part by the National Natural Science Foundation of China under Grant No.U1636116, 11431006.", "appendix": ""}, {"title": "Curriculum Pre-training for End-to-End Speech Translation", "authors": "Chengyi Wang, Yu Wu, Shujie Liu, Ming Zhou, Zhenglu Yang", "bibkey": "curriculum_pre_training_for_end_to_end_speech_translation", "bibitem": "@article{YangCPFEST,\n  url = {http://arxiv.org/abs/2004.10093v1},\n  title = {Curriculum Pre-training for End-to-End Speech Translation},\n  authors = {Chengyi Wang, Yu Wu, Shujie Liu, Ming Zhou, Zhenglu Yang},\n  abstract = {  End-to-end speech translation poses a heavy burden on the encoder, because it has to transcribe, understand, and learn cross-lingual semantics simultaneously. To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features. However, we argue that pre-training the encoder only through simple speech recognition is not enough and high-level linguistic knowledge should be considered. Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages. The difficulty of these courses is gradually increasing. Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks. },\n  bibkey = {YangCPFEST},\n  arxiv_id = {2004.10093v1},\n  subject = {cs.CL},\n  submission_date = {2020-04-21T15:12:07Z}\n}", "url": "http://arxiv.org/abs/2004.10093v1", "latex_url": "http://arxiv.org/src/2004.10093v1", "latex_path": "output/download_papers/2004.10093v1/2004.10093v1", "pdf_url": "http://arxiv.org/pdf/2004.10093v1", "pdf_path": "output/download_papers/2004.10093v1/2004.10093v1.pdf", "md_url": null, "latex_length": 0, "latex": null, "abstract": "  End-to-end speech translation poses a heavy burden on the encoder, because it has to transcribe, understand, and learn cross-lingual semantics simultaneously. To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features. However, we argue that pre-training the encoder only through simple speech recognition is not enough and high-level linguistic knowledge should be considered. Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages. The difficulty of these courses is gradually increasing. Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks. ", "abstract_length": 838, "abstract_token": 149, "introduction": "Speech-to-Text translation (ST) is essential to breaking the language barrier for communication. It aims to translate a segment of source language speech to the target language text. To perform this task, prior works either employ a cascaded method where an automatic speech recognition (ASR) model and a machine translation (MT) model are chained together or an end-to-end approach where a single model converts the source language audio sequence to the target language text sequence directly (Berard et al., 2016). Due to the alleviation of error propagation and lower latency, the end-to-end ST model has been a hot topic in recent years. However, large paired data of source audios and target sentences is required to train such a model, which is not easy to satisfy for most language pairs. To address this issue, previous works resort to pre-training technique (Berard et al., 2018; Bansal et al., 2019), where they leverage the available ASR and MT data to pre-train an ASR model and an MT model respectively, and then initialize the ST model with the ASR encoder and the MT decoder. This strategy can bring faster convergence and better results. ![](images/560336b467019eb854366e6cb7dfd99f41239b2cec1ccff584e2e5345cec342f.jpg) Figure 1: Comparison between previous encoder pre-training method with our curriculum pre-training method. The end-to-end ST encoder has three inherent roles: transcribe the speech, extract the syntactic and semantic knowledge of the source sentence and then map it to a semantic space, based on which the decoder can generate the correct target sentence. This poses a heavy burden to the encoder, which can be alleviated by pre-training. However, we argue that the current pre-training method restricts the power of pre-trained representations. The encoder pre-trained on the ASR task mainly focuses on transcription, which learns the alignment between the acoustic feature with phonemes or words, and has no ability to capture linguistic knowledge or understand the semantics, which is essential for translation. In order to teach the model to understand the sentence and incorporate the required knowledge, extra courses should be taken before learning translation. Motivated by this, we propose a curriculum pre-training method for end-to-end ST. As shown in Figure 1, we first teach the model transcription through ASR task. After that, we design two tasks, named frame-based masked language model (FMLM) task and frame-based bilingual lexicon translation (FBLT) task, to enable the encoder to understand the meaning of a sentence and map words in different languages. Finally, we fine-tune the model on ST data to obtain the translation ability. For the FMLM task, we mask several segments of the input speech feature, each of which corresponds to a complete word. Then we let the encoder predict the masked word. This task aims to force the encoder to recognize the content of the utterance and understand the inner meaning of the sentence. In FBLT, for each speech segment that aligns with a complete word, whether or not it is masked, we ask the encoder to predict the corresponding target word. In this task, we give the model more explicit and strong cross-lingual training signals. Thus, the encoder has the ability to perform simple word translation and the burden on the ST decoder is greatly reduced. Besides, we adopt a hierarchical manner where different layers are guided to perform different tasks (first 8 layers for ASR and FMLM pre-training, and another 4 layers for FBLT pre-training). This is mainly because the three pre-training tasks have different requirements for language understanding and different output spaces. The hierarchical pre-training method can make the division of labor more clear and separate the incorporation of source semantic knowledge and cross-lingual alignments. We conduct experiments on the LibriSpeech EnFr and IWSLT18 En-De speech translation tasks, demonstrating the effectiveness of our pre-training method. The contributions of our paper are as follows: (1) We propose a novel curriculum pretraining method with three courses: transcription, understanding and mapping, in order to force the encoder to have the ability to generate necessary features for the decoder. (2) We propose two new tasks to learn linguistic features, FMLM and FBLT, which explicitly teach the encoder to do source language understanding and target language meaning mapping. (3) Experiments show that both the proposed courses are helpful for speech translation, and our proposed curriculum pre-training leads to significant improvements.", "introduction_length": 4605, "introduction_token": 926, "reference": "# References  \n\nAntonios Anastasopoulos, David Chiang, and Long Duong. 2016. An unsupervised probability model for speech-to-translation alignment of low-resource languages. In EMNLP 2016, pages 1255–1263. The Association for Computational Linguistics.  \n\nParnia Bahar, Albert Zeyer, Ralf Schlu¨ter, and Hermann Ney. 2019. On using specaugment for endto-end speech translation. CoRR, abs/1911.08876.  \n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In ICLR 2015.  \n\nSameer Bansal, Herman Kamper, Karen Livescu, Adam Lopez, and Sharon Goldwater. 2018. Lowresource speech-to-text translation. In Interspeech 2018, 19th Annual Conference of the International Speech Communication Association, Hyderabad, India, 2-6 September 2018, pages 1298–1302. ISCA.  \n\nSameer Bansal, Herman Kamper, Karen Livescu, Adam Lopez, and Sharon Goldwater. 2019. Pretraining on high-resource speech recognition improves low-resource speech-to-text translation. In NAACL-HLT 2019, pages 58–68. Association for Computational Linguistics.  \n\nLong Duong, Antonios Anastasopoulos, David Chiang, Steven Bird, and Trevor Cohn. 2016. An attentional model for speech translation without transcription. In NAACL HLT 2016, pages 949–959. The Association for Computational Linguistics.  \n\nChen Gong, Dacheng Tao, Stephen J. Maybank, Wei Liu, Guoliang Kang, and Jie Yang. 2016. Multimodal curriculum learning for semi-supervised image classification. IEEE Trans. Image Processing, 25(7):3249–3260.  \n\nAlex Graves, Marc G. Bellemare, Jacob Menick, Re´mi Munos, and Koray Kavukcuoglu. 2017. Automated curriculum learning for neural networks. In ICML 2017, pages 1311–1320.  \n\nHirofumi Inaguma, Kevin Duh, Tatsuya Kawahara, and Shinji Watanabe. 2019. Multilingual end-to-end speech translation. In ASRU 2019, pages 570–577. IEEE.  \n\nYe Jia, Melvin Johnson, Wolfgang Macherey, Ron J. Weiss, Yuan Cao, Chung-Cheng Chiu, Naveen Ari, Stella Laurenzo, and Yonghui Wu. 2019. Leveraging weakly supervised data to improve end-to-end speech-to-text translation. In ICASSP 2019, pages 7180–7184. IEEE.  \n\nDongwei Jiang, Xiaoning Lei, Wubo Li, Ne Luo, Yuxuan Hu, Wei Zou, and Xiangang Li. 2019. Improving transformer-based speech recognition using unsupervised pre-training. CoRR, abs/1910.09932.  \n\nTakatomo Kano, Sakriani Sakti, and Satoshi Nakamura. 2017. Structured-based curriculum learning for endto-end english-japanese speech translation. In Interspeech 2017, pages 2630–2634.  \n\nShigeki Karita, Xiaofei Wang, Shinji Watanabe, Takenori Yoshimura, Wangyou Zhang, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hirofumi Inaguma, Ziyan Jiang, Masao Someki, Nelson Enrique Yalta Soplin, and Ryuichi Yamamoto. 2019. A comparative study on transformer vs RNN in speech applications. In IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2019, Singapore, December 14-18, 2019, pages 449–456. IEEE.  \n\nAli Can Kocabiyikoglu, Laurent Besacier, and Olivier Kraif. 2018. Augmenting librispeech with french translations: A multimodal corpus for direct speech translation evaluation. In LREC 2018.  \n\nTaku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In EMNLP 2018: System Demonstrations, pages 66– 71.  \n\nYuchen Liu, Hao Xiong, Zhongjun He, Jiajun Zhang, Hua Wu, Haifeng Wang, and Chengqing Zong. 2019. End-to-end speech translation with knowledge distillation. In InterSpeech 2019, volume abs/1904.08075.  \n\nThang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. In EMNLP 2015, pages 1412–1421.  \n\nLambert Mathias and William Byrne. 2006. Statistical phrase-based speech translation. In ICASSP 2006, pages 561–564.  \n\nEvgeny Matusov, Stephan Kanthak, and Hermann Ney. 2005. On the integration of speech recognition and statistical machine translation. In INTERSPEECH 2005, pages 3177–3180.  \n\nMichael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger. 2017. Montreal forced aligner: Trainable textspeech alignment using kaldi. In Interspeech 2017, pages 498–502.  \n\nSylvain Meignier and Teva Merlin. 2010. Lium spkdiarization: an open source toolkit for diarization. In CMU SPUD Workshot.  \n\nHermann Ney. 1999. Speech translation: coupling of recognition and translation. In ICASSP ’99, pages 517–520.  \n\nJan Niehues, Ronaldo Cattoni, Sebastian Stu¨ker, Mauro Cettolo, Marco Turchi, and Marcello Federico. 2018. The iwslt 2018 evaluation campaign. In Proceedings of IWSLT.  \n\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An ASR corpus based on public domain audio books. In ICASSP 2015, pages 5206–5210.  \n\nDaniel S. Park, William Chan, Yu Zhang, ChungCheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le. 2019. Specaugment: A simple data augmentation method for automatic speech recognition. CoRR, abs/1904.08779.  \n\nAnthony Rousseau, Paul Dele´glise, and Yannick Este\\`ve. 2014. Enhancing the TED-LIUM corpus with selected data for language modeling and more TED talks. In LREC 2014, pages 3935–3939. European Language Resources Association (ELRA).  \n\nMatthias Sperber, Graham Neubig, Jan Niehues, and Alex Waibel. 2019. Attention-passing models for robust and data-efficient end-to-end speech translation. TACL, 7:313–325.  \n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NeurIPS 2017, pages 5998–6008.  \n\nChengyi Wang, Yu Wu, Yujiao Du, Jinyu Li, Shujie Liu, Liang Lu, Shuo Ren, Guoli Ye, Sheng Zhao, and Ming Zhou. 2019a. Semantic mask for transformer based end-to-end speech recognition. CoRR, abs/1912.03010.  \n\nChengyi Wang, Yu Wu, Shujie Liu, Zhenglu Yang, and Ming Zhou. 2019b. Bridging the gap between pretraining and fine-tuning for end-to-end speech translation. CoRR, abs/1909.07575.  \n\nShinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya Renduchintala, and Tsubasa Ochiai. 2018. Espnet: End-to-end speech processing toolkit. In Interspeech 2018, pages 2207–2211. ISCA.  \n\nRon J. Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, and Zhifeng Chen. 2017. Sequence-tosequence models can directly translate foreign speech. In Interspeech 2017, pages 2625–2629. ISCA.", "reference_length": 6497, "reference_token": 1951, "txt_length": 37271, "txt_token": 9960, "txt": "# Curriculum Pre-training for End-to-End Speech Translation  \n\nChengyi Wang∗1, $\\mathbf{Y}\\mathbf{u}\\;\\mathbf{W}\\mathbf{u}^{2}$ , Shujie $\\mathrm{Liu}^{2}$ , Ming Zhou2, Zhenglu Yang1 1Nankai University, Tianjin, China 2Microsoft Research Asia, Beijing, China cywang@mail.nankai.edu.cn, Wu.Yu $@$ microsoft.com, shujliu@microsoft.com, mingzhou $@$ microsoft.com, yangzl $@$ nankai.edu.cn  \n\n# Abstract  \n\nEnd-to-end speech translation poses a heavy burden on the encoder, because it has to transcribe, understand, and learn cross-lingual semantics simultaneously. To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features. However, we argue that pre-training the encoder only through simple speech recognition is not enough and high-level linguistic knowledge should be considered. Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages. The difficulty of these courses is gradually increasing. Experiments show that our curriculum pre-training method leads to significant improvements on En-De and EnFr speech translation benchmarks.  \n\n# 1 Introduction  \n\nSpeech-to-Text translation (ST) is essential to breaking the language barrier for communication. It aims to translate a segment of source language speech to the target language text. To perform this task, prior works either employ a cascaded method where an automatic speech recognition (ASR) model and a machine translation (MT) model are chained together or an end-to-end approach where a single model converts the source language audio sequence to the target language text sequence directly (Berard et al., 2016).  \n\nDue to the alleviation of error propagation and lower latency, the end-to-end ST model has been a hot topic in recent years. However, large paired data of source audios and target sentences is required to train such a model, which is not easy to satisfy for most language pairs. To address this issue, previous works resort to pre-training technique (Berard et al., 2018; Bansal et al., 2019), where they leverage the available ASR and MT data to pre-train an ASR model and an MT model respectively, and then initialize the ST model with the ASR encoder and the MT decoder. This strategy can bring faster convergence and better results.  \n\n![](images/560336b467019eb854366e6cb7dfd99f41239b2cec1ccff584e2e5345cec342f.jpg)  \nFigure 1: Comparison between previous encoder pre-training method with our curriculum pre-training method.  \n\nThe end-to-end ST encoder has three inherent roles: transcribe the speech, extract the syntactic and semantic knowledge of the source sentence and then map it to a semantic space, based on which the decoder can generate the correct target sentence. This poses a heavy burden to the encoder, which can be alleviated by pre-training. However, we argue that the current pre-training method restricts the power of pre-trained representations. The encoder pre-trained on the ASR task mainly focuses on transcription, which learns the alignment between the acoustic feature with phonemes or words, and has no ability to capture linguistic knowledge or understand the semantics, which is essential for translation.  \n\nIn order to teach the model to understand the sentence and incorporate the required knowledge, extra courses should be taken before learning translation. Motivated by this, we propose a curriculum pre-training method for end-to-end ST. As shown in Figure 1, we first teach the model transcription through ASR task. After that, we design two tasks, named frame-based masked language model (FMLM) task and frame-based bilingual lexicon translation (FBLT) task, to enable the encoder to understand the meaning of a sentence and map words in different languages. Finally, we fine-tune the model on ST data to obtain the translation ability.  \n\nFor the FMLM task, we mask several segments of the input speech feature, each of which corresponds to a complete word. Then we let the encoder predict the masked word. This task aims to force the encoder to recognize the content of the utterance and understand the inner meaning of the sentence. In FBLT, for each speech segment that aligns with a complete word, whether or not it is masked, we ask the encoder to predict the corresponding target word. In this task, we give the model more explicit and strong cross-lingual training signals. Thus, the encoder has the ability to perform simple word translation and the burden on the ST decoder is greatly reduced. Besides, we adopt a hierarchical manner where different layers are guided to perform different tasks (first 8 layers for ASR and FMLM pre-training, and another 4 layers for FBLT pre-training). This is mainly because the three pre-training tasks have different requirements for language understanding and different output spaces. The hierarchical pre-training method can make the division of labor more clear and separate the incorporation of source semantic knowledge and cross-lingual alignments.  \n\nWe conduct experiments on the LibriSpeech EnFr and IWSLT18 En-De speech translation tasks, demonstrating the effectiveness of our pre-training method. The contributions of our paper are as follows: (1) We propose a novel curriculum pretraining method with three courses: transcription, understanding and mapping, in order to force the encoder to have the ability to generate necessary features for the decoder. (2) We propose two new tasks to learn linguistic features, FMLM and FBLT, which explicitly teach the encoder to do source language understanding and target language meaning mapping. (3) Experiments show that both the proposed courses are helpful for speech translation, and our proposed curriculum pre-training leads to significant improvements.  \n\n# 2 Related Work  \n\n# 2.1 Speech Translation  \n\nEarly work on speech translation used a cascade of an ASR model and an MT model (Ney, 1999; Matusov et al., 2005; Mathias and Byrne, 2006), which makes the MT model access to ASR errors. Recent successes of end-to-end models in the MT field (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) and the ASR fields (Chan et al., 2016; Chiu et al., 2018) inspired the research on end-to-end speech-to-text translation system, which avoids error propagation and high latency issues.  \n\nIn this research line, Berard et al. (2016) give the first proof of the potential for an end-to-end ST model. After that, pre-training, multitask learning, attention-passing and knowledge distillation have been applied to improve the ST performance (Anastasopoulos et al., 2016; Duong et al., 2016; Berard et al., 2018; Weiss et al., 2017; Bansal et al., 2018, 2019; Sperber et al., 2019; Liu et al., 2019; Jia et al., 2019). However, none of them attempt to guide the encoder to learn linguistic knowledge explicitly. Recently, Wang et al. (2019b) propose to stack an ASR encoder and an MT encoder as a new ST encoder, which incorporates acoustic and linguistic knowledge respectively. However, the gap between these two encoders is hard to bridge by simply concatenating the encoders. Kano et al. (2017) propose structured-based curriculum learning for English-Japanese speech translation, where they use a new decoder to replace the ASR decoder and to learn the output from the MT decoder (fast track) or encoder (slow track). They formalize learning strategies from easier networks to more difficult network structures. In contrast, we focus on the curriculum learning in pre-training and increase the difficulty of pre-training tasks.  \n\n# 2.2 Curriculum Learning  \n\nCurriculum learning is a learning paradigm that starts from simple patterns and gradually increases to more complex patterns. This idea is inspired by the human learning process and is first applied in the context of machine learning by Bengio et al. (2009). The study shows that this training approach results in better generalization and speeds up the convergence. Its effectiveness has been verified in multiple tasks, including shape recognition (Bengio et al., 2009), object classification (Gong et al., 2016), question answering (Graves et al., 2017), etc. However, most studies focus on how to control the difficulty of the training samples and organize the order of the learning data in the context of single-task learning.  \n\n![](images/2d0a971e87495dc19cd1b1257570b4cecb70437d61ab5741c5e41e0ec10b544b.jpg)  \nFigure 2: Proposed curriculum pre-training process. ${\\mathcal{L}}_{F M L M}$ only predicts the mask word, while $\\mathcal{L}_{F B L T}$ predicts all words in the target language.  \n\nOur method differs from previous works in two ways: (1) We leverage the idea of curriculum learning for pre-training. (2) We do not train the model on the ST task directly with more and more difficult training examples or use more and more difficult structures. Instead, we design a series of tasks with increased difficulty to teach the encoder to incorporate diverse knowledge.  \n\n# 3 Method  \n\n# 3.1 Overview  \n\nThe overview of our training process is shown in Figure 2. It can be divided into three steps: First, we train the model towards the ASR objective $L_{A S R}$ to learn transcription. We note this as the elementary course. Next, we design two advanced courses (tasks) to teach the model understanding a sentence and mapping words in two languages, named Frame-based Masked Language Model (FMLM) task and Frame-based Bilingual Lexicon Translation (FBLT) task. In the FMLM task, we mask some speech segments and ask the encoder to predict the masked words. In the FBLT task, we ask the encoder to predict the target word for each speech segment which corresponds to a complete source word. In this stage, the encoder is updated by $L_{A D V}$ . We adopt a hierarchical training manner where $N$ encoder blocks are used to perform ASR and FMLM tasks, since they both require outputs in source word space, and $N_{e}$ blocks are used in FBLT task. After the two-phases pretraining, the encoder is finally combined with a new decoder or a pre-trained MT decoder to perform the ST task towards $L_{S T}$ .  \n\nProblem Formulation The speech translation corpus usually contains speech-transcriptiontranslation triples, denoted as $\\pmb{S}=\\{(x,y^{s},y^{t})\\}$ . Specially, $\\pmb{x}\\;=\\;(x_{1},\\cdots\\,,x_{T_{x}})$ is a sequence of acoustic features which are extracted from the speech signals. $\\pmb{y}^{s}\\ =\\ (y_{1}^{s},\\cdot\\cdot\\cdot\\ ,y_{T_{s}}^{s})$ and $\\pmb{y}^{t}\\;=$ $(y_{1}^{t},\\cdot\\cdot\\cdot\\,,y_{T_{t}}^{t})$ represent the corresponding transcription in source language and the translation in target language respectively. To pre-train the encoder, an extra ASR dataset ${\\pmb{\\mathcal A}}\\;=\\;\\{({\\pmb x},{\\pmb y}^{s})\\}$ can be leveraged . Finally, the data for encoder pre-training is denoted as $\\{(x,y^{s})|(x,y^{s})\\in\\mathcal{A}\\lor$ $(x,y^{s},y^{t})\\in S\\}$  \n\nAfter the encoder is pre-trained, we fine-tune the model using only $\\mathcal{S}$ , to enable it generate $\\pmb{y}^{t}$ from $\\textbf{\\em x}$ directly. The model is updated using cross-entropy loss $\\mathcal{L}_{S T}=-\\log P(\\pmb{y}^{t}|\\pmb{x})$ .  \n\nModel Architecture In this work, we adopt the architecture of Transformer as in (Karita et al., 2019). The encoder is a stack of two $3\\times3$ 2D CNN layers with stride 2 and $N_{e}$ Transformer encoder blocks. The CNN layers result in downsampling by a factor of 4. The decoder is a stack of $N_{d}$ Transformer decoder blocks.  \n\n# 3.2 Elementary Course: Transcription  \n\nIn the elementary course, we train an end-to-end ASR model, which has the similar architecture as the ST model. The ASR encoder consists of $N$ blocks, and these blocks are used to initialize the bottom $N$ blocks of the ST encoder. For the ASR task, we follow Karita et al. (2019), to employ a multi-task learning strategy, that is, both the E2E decoder and a CTC module predict the source sentence. Offline experiments indicate that the CTC objective is crucial for attentional encoder-decoder based ASR models. The final objective combines the CTC loss $\\mathcal{L}_{c t c}$ and the cross-entropy loss $\\mathcal{L}_{C E}$ :  \n\n$$\n\\begin{array}{r l}&{\\mathcal{L}_{A S R}=\\alpha\\mathcal{L}_{C T C}+(1-\\alpha)\\mathcal{L}_{C E}}\\\\ &{\\qquad\\quad=-\\alpha\\log P_{c t c}(\\pmb{y}^{s}|\\pmb{x})-(1-\\alpha)\\log P_{s2s}(\\pmb{y}^{s}|\\pmb{x}),}\\end{array}\n$$  \n\nIn this work, we set $\\alpha$ to 0.3. The CTC loss works on the encoder output and it pushes the encoder to learn frame-wise alignment between speech with words. The cross-entropy loss works on both the encoder and the ASR decoder.  \n\n# 3.3 Advanced Courses: Understanding and Word Mapping  \n\nWith the ability of transcription, we further propose two new tasks for the advanced courses.  \n\n# 3.3.1 Frame-based Masked Language Model  \n\nThe design of the Frame-based Masked Language Model task is inspired by the Masked Language Model (MLM) objective of BERT (Devlin et al., 2019), and semantic mask for ASR task (Wang et al., 2019a). This task enables the encoder to understand the inner meaning of a segment of speech.  \n\nAs shown in Figure 2, we first perform forcealignment between the speech and the transcript sentence to determine where in time particular words occur in the speech segment. For each word $y_{i}^{s}$ , we obtain its corresponding start position $s_{i}$ and the end position $e_{i}$ in the sequence $\\textbf{\\em x}$ according to force alignment results. At each training iteration, we randomly sample some percentage of the words in the $y^{s}$ and denote the selected word set as $\\tilde{y}^{s}$ Next, for each selected token $\\boldsymbol{y}_{j}^{s}$ in $\\tilde{\\pmb{y}}^{s}$ , we mask the corresponding speech piece $\\left[x_{s_{j}}\\,:\\,x_{e_{j}}\\right]$ . The masked utterance is denoted as $\\tilde{\\pmb{x}}$ and used as input to the encoder:  \n\n$$\n\\pmb{h}=\\operatorname{Enc}(\\tilde{\\pmb{x}})\n$$  \n\nAfter that, for a masked piece $[x_{s_{j}}\\,:\\,x_{e_{j}}]$ , we average the corresponding output hidden states  \n\n[h⌊sj⌋: h⌈ej⌉]1, and compute the distribution probability over source words as shown in follows:  \n\n$$\n\\begin{array}{c}{\\tilde{h}_{j}=\\operatorname{mean}([h_{\\lfloor\\frac{s_{j}}{4}\\rfloor}:h_{\\lceil\\frac{e_{j}}{4}\\rceil}])}\\\\ {p(y_{j}^{s}|\\tilde{\\mathbf{\\pi}})=\\operatorname{softmax}(\\tilde{h}_{j}\\cdot W)}\\end{array}\n$$  \n\nIn practice, the sentence is represented in BPE tokens and $W\\,\\in\\,\\mathcal{R}^{d_{m o d e l}\\,\\times\\,|V_{s}|}$ , where $|V_{s}|$ is the size of source vocabulary. In this way, a speech piece can be aligned with one or more tokens. We compute KL-Divergence loss as:  \n\n$$\n\\mathcal{L}_{F M L M}=-\\sum_{y_{j}^{s}\\in\\tilde{\\pmb{y}}^{s}}\\sum q(y_{j}^{s})\\mathrm{log}\\frac{p(y_{j}^{s}|\\tilde{\\pmb{x}})}{q(y_{j}^{s})}\n$$  \n\n$q(y_{i}^{s})\\in\\mathcal{R}^{|V_{s}|}$ is a distribution over all BPE tokens in source vocabulary $V_{s}$ and defined as:  \n\n$$\nq(y_{j}^{s})_{(p o s)}=\\left\\{\\begin{array}{l l}{1/n_{j},}&{V_{s}[p o s]\\in y_{j}^{s}}\\\\ {0,}&{\\mathrm{otherwise}.}\\end{array}\\right.\n$$  \n\nwhere pos represents the dimension index and $n_{j}$ is the total number of BPE tokens contained in word $\\boldsymbol{y}_{j}^{s}$ .  \n\nIn this work, we use a mask ratio of $15\\%$ following BERT and the masked speech piece is filled with the mean value of the whole utterance following Park et al. (2019). Because FMLM focuses on the understanding of source language, we computes its loss at the $N$ -th layer of encoder (same with ASR loss), in the hope that the bottom $N$ layers are only concerned with source language.  \n\n# 3.3.2 Frame-based Bilingual Lexicon Translation  \n\nAside from predicting masked source words, we go further to leverage cross-lingual information. Specifically, for each segment of speech features $\\left[x_{s_{i}}\\,:\\,x_{e_{i}}\\right]$ which aligned with a source word $y_{i}^{s}$ , we assume we can obtain its target counterpart $\\tilde{y}_{i}^{t}$ . Similar to FMLM, we average the output hidden states from position $\\left\\lfloor{\\frac{s_{i}}{4}}\\right\\rfloor$ to $\\lceil\\frac{e_{i}}{4}\\rceil$ , and then compute the distribution probability over target vocabulary. The alignment between speech segments and target words is a many-to-many correspondence, so there are cases where $\\tilde{y}_{i}^{t}$ contains nothing or contains multiple foreign words. For the former case, we set the loss to zero, and for the latter case, we also compute KL-Divergence loss as:  \n\n$$\n\\mathcal{L}_{F B L T}=-\\sum_{\\tilde{y}_{i}^{t}}\\sum q(\\tilde{y}_{i}^{t})\\mathrm{log}\\frac{p(\\tilde{y}_{i}^{t}|\\tilde{\\pmb{x}})}{q(\\tilde{y}_{i}^{t})}\n$$  \n\nThe definition of $q(\\tilde{y}_{i}^{t})$ is the length normalized distribution over all tokens appear in $\\tilde{y}_{i}^{t}$ . Note that the loss is computed on every speech segments, whether or not it is masked.  \n\nThe only question remaining is how to obtain $\\tilde{y}_{i}^{t}$ for each speech segment. Since there are two types of data for pre-training, $(x,y^{s},y^{t})\\in S$ and $(x,y^{s})\\in A$ , we use two methods to get the alignment:  \n\n$\\forall(\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}}^{s},\\mathbf{\\boldsymbol{y}}^{t})\\in\\mathcal{S}$ , we simply run Moses2 scripts to establish word alignments. It begins from running of $\\mathrm{GLA}++^{3}$ to get source-to-target and targetto-source alignments, and then runs a heuristic grow-diag-final algorithm to get the final results, which means $\\forall y_{i}^{s}\\in y^{s}$ , we choose one word from its translation sentence as the corresponding word $\\exists\\tilde{y}_{i}^{t}\\in\\pmb{y}^{t}$ s.t. $\\tilde{y}_{i}^{t}\\sim{y}^{s}$ .  \n\nThrough the above alignment process, we can calculate a bilingual lexical translation table $\\tau$ with $\\{({\\pmb y}^{s},{\\pmb y}^{t})|({\\pmb x},{\\pmb y}^{s},{\\pmb y}^{t})\\in S\\}$ , which estimates the translation probability between a source word $w_{i}^{s}$ and a target word $w_{j}^{t}$ , denoted as $\\tau\\ =$ $(w_{i}^{s},w_{j}^{t},p(w_{i}^{s},w_{j}^{t}))$ . After that, $\\forall(\\pmb{x},\\pmb{y}^{s})\\ \\in\\ \\mathcal{A}$ we compute a $\\Tilde{y}_{i}^{t}$ for each $y_{i}^{s}$ in $y^{s}$ according to $\\tilde{y}_{i}^{t}=\\operatorname{argmax}_{w_{j}^{s}}p\\big(y_{i}^{s},w_{j}^{s}\\big)$ .  \n\nWe compute the $\\mathcal{L}_{F B L T}$ at the top layer of the encoder, indicating that the top $N_{e}-N$ layers are duty on bilingual word mapping. The final training objective in the advanced course combines FMLM and FBLT losses  \n\n$$\n\\mathcal{L}_{A D V}=\\mathcal{L}_{F M L M}+\\mathcal{L}_{F B L T}\n$$  \n\n# 4 Experiments  \n\n# 4.1 Data and Preprocess  \n\nWe conduct experiments on two publicly available speech translation datasets: the LibriSpeech En-Fr Corpus (Kocabiyikoglu et al., 2018) and the IWSLT En-De Corpus (Niehues et al., 2018).  \n\nLibriSpeech En-Fr: This corpus is a subset of the LibriSpeech ASR corpus (Panayotov et al., 2015) and aligned with French e-books, which contains 236 hours of speech in total. Following previous works, we use the 100 hours clean training set and double the ST size by concatenating the aligned references with the provided Google Translate references, resulting in 90k training instances. We validate on the dev set and report results on the test set (2048 utterances).  \n\nIWSLT En-De: The corpus contains 271 hours of data, with English wave, English transcription, and German translation in each example. We follow Inaguma et al. (2019) to remove utterances of low alignment quality, resulting in $137\\mathrm{k}$ utterances. We sample 2k segments from the ST-TED corpus as dev set and tst2013 is used as the test set (993 utterances).  \n\nData Preprocessing: We run ESPnet4 (Watanabe et al., 2018) recipes to perform data preprocessing. For both tasks, our acoustic features are 80-dimensional log-Mel filterbanks stacked with 3-dimensional pitch features extracted with a step size of $10\\mathrm{ms}$ and window size of $25\\mathrm{ms}$ . The features are normalized by the mean and the standard deviation for each training set. Utterances of more than 3000 frames are discarded. We perform speed perturbation with factors 0.9 and 1.1. The alignment results between speech and transcriptions are obtained by Montreal Forced Aligner (McAuliffe et al., 2017).  \n\nFor references pre-processing, we tokenize and lowercase all the text with the Moses scripts. For pre-training tasks, the vocabulary is generated using sentencepiece (Kudo and Richardson, 2018) with a fixed size of 5k tokens for all languages, and the punctuation is removed. For ST task, we normalize the punctuation using Moses and use the character-level vocabulary due to its better performance (Berard et al., 2018). Since there is no human-annotated segmentation provided in the IWSLT tst2013, we use two methods to segment the audios: 1) Following ESPnet, we segment each audio with the LIUM SpkDiarization tool (Meignier and Merlin, 2010). For evaluation, the hypotheses and references are aligned using the MWER method with RWTH toolkit (Bender et al., 2004). 2) We perform sentence-level forcealignment between audio and transcription using aeneas5 tool and segment the audio according to alignment results.  \n\n# 4.2 Baselines  \n\nExperiments are conducted in two settings: base setting and expanded setting. In base setting, only the corpus described in Section 4.1 is used for each task. In the expanded setting, additional ASR and/or MT data can be used. All results are reported on case-insensitive BLEU with the multibleu.perl script unless noted.  \n\n# 4.2.1 End-to-End ST Baselines  \n\nWe mainly compare our method with the conventional encoder pre-training method which uses only the ASR task to pre-train the encoder. Besides, we also compare with the results of the other works in the literature by copying their numbers.  \n\nLibriSpeech: In the context of base setting, Berard et al. (2018) and ESPnet have reported results on a LSTM-based ST model with pre-training and/or multi-task learning strategy. Liu et al. (2019) use a Transformer ST model and knowledge distillation method. Wang et al. (2019b) stack an ASR encoder and an MT encoder for final ST task, named as TCEN. Regarding the expanded setting, Bahar et al. (2019) apply the SpecAugment on ST task. They use the total 236h of speech for ASR pre-training. Inaguma et al. (2019) combine three ST datasets of 472h training data 6 to train a multilingual ST model. In our work, we use the LibriSpeech ASR corpus as additional pre-training data, including 960h of speech. As the dev and test set of LibriSpeech ST task are extracted from the 960h corpus, we exclude all training utterances with the same speaker that appear in dev or test sets .  \n\nIWSLT: Since previous works use different segmentation methods and BLEU-score scripts, it is unfair to copy their numbers. In our work, we choose the ESPnet results as base setting baseline, the multilingual model and TCEN-LSTM model as expanded baselines. Inaguma et al. (2019) use the same multilingual model as described in LibriSpeech baselines. And Wang et al. (2019b) use an additional 272h TEDLIUM2(Rousseau et al., 2014) ASR corpus and 41M parallel data from WMT18 and WIT37. All of them use ESPnet code, LIUM segmentaion method and multi-bleu.perl script. We follow Wang et al. (2019b) to use another 272h ASR data for encoder pre-training and a subset of $\\mathbf{WMT18^{8}}$ for decoder pre-training. We use the same processing method for MT data, resulting in 4M parallel sentences in total. We also reimplement the CL-fast track of Kano et al. (2017)  \n\nusing our model architecture and data as another baseline.  \n\n# 4.2.2 Cacased Baselines  \n\nFor LibriSpeech ST task, we use results of Berard et al. (2018), Inaguma et al. (2019) and Liu et al. (2019) as base cascaded baselines. The first two use LSTM models for ASR and MT. While the last work trains Transformer ASR and MT models. We build an expanded cascaded system with the pretrained Transformer ASR model and a LSTM MT model with the default setting in ESPnet recipe. For IWSLT ST task, we use Inaguma et al. (2019) as base cascaded baseline, which is based on LSTM architecture. And we implement a Transformerbased baseline using our pre-trained ASR and MT models in the expanded setting.  \n\n# 4.3 Implementation Details  \n\nAll our models are implemented based on ESPnet. We set the model dimension $d_{m o d e l}$ to 256, the head number $H$ to 4, the feed forward layer size $d_{f f}$ to 2048. For LibriSpeech expanded setting, $d_{m o d e l}=512$ and $H=8$ . For all the ST models, we set the number of encoder blocks $N_{e}=12$ and the number of decoder blocks $N_{d}\\,=\\,6$ . Unless noted, we use $N=8$ encoder blocks to perform the ASR and the FMLM pre-training tasks. For MT model used in IWSLT expanded setting, we use the Transformer architecture in Vaswani et al. (2017) with $N_{e}=6,N_{d}=6,H=4,d_{m o d e l}=256$ .  \n\nWe train the model with 4 Tesla P40 GPUs and batch size is set to 64 per GPU. The pre-training takes 50 and 20 epochs for each phase and the final ST task takes another 50 epochs (a total of 120 epochs). We use the Adam optimizer with warmup steps 25000 in each phase. The learning rate decays proportionally to the inverse square root of the step number after 25000 steps. We save checkpoints every epoch and average the last 5 checkpoints as the final model. To avoid overftiting, SpecAugment strategy (Park et al., 2019) is used in ASR pre-training with frequency masking $(\\mathrm{F}=30,\\,\\mathrm{mF}=2)$ and time masking ( $\\mathrm{T}=40$ , $\\scriptstyle\\mathrm{mT}=2$ ). The decoding process uses a beam size of 10 and a length penalty of 0.2.  \n\n# 4.4 Experimental Results  \n\n4.4.1 Comparison with End-to-End Baselines LibriSpeech En-Fr: The results on LibriSpeech En-Fr test set are listed in Table 1. In base setting, our method improves the “Transformer+ASR pre-train” baseline by 1.7 BLEU and beats all the previous works, even though we do not pre-train the decoder. It indicates that through a well-designed learning process, the encoder has a strong potential to incorporate large amount of knowledge. Our method beats a knowledge distillation baseline, where an MT model is utilized to teach the ST model. The reason, we believe, is that our method gives the model more training signals and makes it easier to learn. We also outperform a TCEN baseline which includes two encoders. Compared to them, our method is more flexible and incorporates all information into a single encoder, which avoids the representation gap between the two encoders.  \n\nTable 1: Comparison on LibriSpeech En-Fr test set. The size of ASR data for base setting is 100h unless labeled. Since inputs of the MT models are ground-truth text, the results of MT models can be seen as the upper-bound of ST models. \\*: Unknown BLEU score script.   \n\n\n<html><body><table><thead><tr><td><b>Method</b></td><td><b>Enc pre-train</b></td><td><b>Dec pre-train</b></td><td><b>BLEU</b></td></tr></thead><tbody><tr><td>MT(Berard et al., 2018)*</td><td>√</td><td>12.9</td><td>19.3</td></tr><tr><td>MT(Inaguma et al., 2019)</td><td>√</td><td>12.9</td><td>18.3</td></tr><tr><td>LSTM ST (Berard et al., 2018)* +pre-train+multitask (Berard et al., 2018)* LSTM ST+pre-train (ESPnet) base setting Transformer+pre-train (Liu et al., 2019)</td><td>√ √</td><td>√ √</td><td>16.68 14.30 17.02 17.05 13.4</td></tr><tr><td>Transformer+ASR pre-train Transformer+curriculum pre-train expanded setting LSTM+pre-train+SpecAugment(Bahar et al., 2019)</td><td>V(236h) √(472h)</td><td>15.97</td><td>17.66 17.0</td></tr><tr><td>Multilingual ST+pre-train (Inaguma et al., 2019) Transformer+ASR pre-train Transformer+curriculum pre-train</td><td>(960h) √(960h)</td><td>15.97</td><td>16.90 18.01 17.6</td></tr></tbody></table></body></html>  \n\nAs the ASR data size increases, the model performs better. In the expanded setting, we find the FBLT task performs poorly compared with the base setting. This is because the target word prediction task is dictionary-supervised in expanded setting rather than reference-supervised as in base setting. However, our method still outperforms the simple pre-training method by a large margin. Besides, it is surprising to find that the end-to-end ST model is approaching the performance of an MT model, which is the upper bound of the ST model since it accepts golden source sentence without any ASR errors. This further verifies the effectiveness of our method.  \n\nIWSLT En-De: The results on IWSLT tst2013 are listed in Table 2, showing a similar trend as in LibriSpeech dataset. We find that the segmentation methods have a big influence on the final results. In the base setting, our method can improve the ASR pre-training baseline by 0.9 to 2.2 BLEU scores, depending on the segmentation methods. In the expanded setting, we find when combined with decoder pre-train, the performance is further improved and beats other expanded baselines.  \n\n# 4.4.2 Comparison with Cascaded Baselines  \n\nTable 3 shows comparison with cascaded ST systems. For the base setting of two tasks, our end-toend model can achieve comparable or better results with cascaded methods. This shows the end-toend model has powerful learning capabilities and combines the functions of two models. In the LibriSpeech expanded setting, when more ASR data is available, we also obtain a competitive performance. This indicates our method can make a good use of ASR corpus and learn valuable linguistic knowledge other than simple acoustic information. However, when additional MT data is used, there is still a gap between the end-to-end method and the cascaded method. How to utilize bilingual parallel sentences to improve the E2E ST model is worth further studying.  \n\n# 4.5 Analysis and Discussion  \n\nAblation Study To better understand the contribution of each component, we perform an ablation study on LibriSpeech expanded setting. The results are shown in Table 4. On the one hand, we show that both of our proposed pre-training tasks are beneficial: In “-FMLM task” and “-FBLT task”9, we perform single-task pre-training for advanced course. The performance drops when we remove either one of them. On the other hand, we show the two-phases pre-training paradigm is necessary: The “- phase $2^{\\circ}$ experiment degenerates to the simple ASR pre-training baseline. In “-phase 1” setting, we find that without the ASR pre-training, the training accuracy on FMLM task and FBLT task drops a lot, which further affects the ST performance. This means the ASR task is necessary for both the advanced courses and ST. In “Multi3” setting, we pre-train the model on ASR, FMLM and FBLT tasks in one phase. In this setting, we observe multi-task learning also decrease individual task performances (ASR, FMLM and FBLT) compared to curriculum learning. One reasonable expanation is that it is hard to train on the FMLM and FBLT tasks which takes masked input from randomly initialized parameters, which also leads to performance degradation on the ST task.  \n\n<html><body><table><thead><tr><td rowspan=\"2\"><b>Method</b></td><td><b>Enc pre-train</b></td><td><b>Dec pre-train</b></td><td colspan=\"2\"><b>segment method</b></td></tr><tr><td><b>(speech data)</b></td><td><b>(text data)</b></td><td><b>LIUM</b></td><td><b>aeneas</b></td></tr></thead><tbody><tr><td>base setting ESPnet</td><td>√</td><td>12.50</td><td>13.12 13.54 15.35 16.27</td><td>17.10 19.29</td></tr><tr><td>expanded setting Multilingual ST+pre-train(Inaguma et al., 2019) TCEN-LSTM (Wang et al., 2019b) CL-fast(Kano et al., 2017)(re-implemented) Transformer+curriculum pre-train+dec pre-train</td><td>√(479h) √(479h) √(479h)</td><td>√(40M)</td><td>14.6 17.65 14.33</td><td>16.23</td></tr><tr><td>CL-fast(Kano et al., 2017)(re-implemented) Transformer+curriculum pre-train+dec pre-train</td><td>√(479h)</td><td>√(4M)</td><td>18.15</td><td>20.35</td></tr></tbody></table></body></html>  \n\nTable 2: ST results on IWSLT En-De tst2013 set.   \nTable 3: Comparison with cascaded ST. \\*:we find the LSTM model outperforms Transformer model in our setting since the training data is scarce.   \n\n\n<html><body><table><thead><tr><td><b>Method</b></td><td><b>BLEU</b></td></tr></thead><tbody><tr><td>LibriSpeech base setting LSTM ASR+ MT(Berard et al., 2018)</td><td>14.6 15.8</td></tr><tr><td>IWSLT base setting LSTM ASR+ MT(Inaguma et al., 2019) Ours E2E Transformer ST Ours E2E Transformer ST</td><td>18.05 18.01 17.66</td></tr><tr><td>Ours E2E Transformer ST IWSLT expanded setting Transformer ASR+Transformer ST</td><td>22.16 18.15 16.27</td></tr><tr><td>IWSLT expanded setting Transformer ASR+Transformer ST Ours E2E Transformer ST</td><td>22.16 18.15</td></tr></tbody></table></body></html>  \n\nHyper-parameter N During pre-training, which layer conducts ASR pre-training and FMLM loss is an important hyper-parameter. We conduct experiments on LibriSpeech base setting to explore the influence of different choices. We keep $N_{e}=12$ unchanged and always use the top layer to perform the FBLT task. Then we alter the hyperparameter $N$ . We find if $N=6$ , the model finds it difficult to converge during ST training. That may be because the distance between the decoder and the bottom 6 encoder layers is too far so that the valuable source linguistic knowledge can not be well utilized. Moreover, the model performs undesirable if the choice is 10 or 12, which results in 16.47 and 16.14 BLEU score respectively, since the number of blocks for FBLT task is not enough. The model achieves the best performance when we choose $N=8$ . Thus, we use this strategy in our main experiments.  \n\nTable 4: Ablation study on LibriSpeech expanded setting. ‘-’ indicates removing the task or phase from our method.   \n\n\n<html><body><table><thead><tr><td><b>Method</b></td><td><b>BLEU</b></td></tr></thead><tbody><tr><td>Our method</td><td>18.01</td></tr><tr><td>-FMLM task</td><td>17.62</td></tr><tr><td>-FBLT task</td><td>17.65</td></tr><tr><td>-phase 2</td><td>16.90</td></tr><tr><td>-phase 1</td><td>14.26</td></tr><tr><td>Multi3</td><td>14.82</td></tr></tbody></table></body></html>  \n\nUnlabeled Speech Data In this work, we also explore how to utilize the unlabeled speech data in pre-training, but only get negative results. We conduct exploratory experiments on the LibriSpeech ST task. Assume that the $(x,y^{s})$ from 100h ST corpus as labeled pre-training data and $(x)$ from 960h LibriSpeech ASR corpus as unlabeled data. Following Jiang et al. (2019), we design an unsupervised pre-training task for elementary course, in which we randomly mask $15\\%$ of fbank features and let the bottom 4 encoder layers predict the masked part. We compute the L1 loss between the prediction and groundtruth filterbanks. However, we find that this method is not helpful for the final ST task, which results in 16.85 BLEU score, lower than our base setting model (without extra data pre-training). It is still an open question about how to use unlabeled speech data.  \n\n# 5 Conclusion and Future Work  \n\nThis paper investigates the end-to-end method for ST. We propose a curriculum pre-training method, consisting of an elementary course with an ASR loss, and two advanced courses with a frame-based masked language model loss and a bilingual lexicon translation loss, in order to teach the model syntactic and semantic knowledge in the pre-training stage. Empirical studies have demonstrated that our model significantly outperforms baselines. In the future, we will explore how to leverage unlabeled speech data and large bilingual text data to further improve the performance. Besides, we expect the idea of curriculum pre-training can be adopted on other NLP tasks.  \n\n# Acknowledgements  \n\nThis work was supported in part by the National Natural Science Foundation of China under Grant No.U1636116 and the Ministry of education of Humanities and Social Science project under grant 16YJC790123.  \n\nOliver Bender, Richard Zens, Evgeny Matusov, and Hermann Ney. 2004. Alignment templates: the RWTH SMT system. In IWSLT 2004, pages 79–84.  \n\nYoshua Bengio, Je´roˆme Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In ICML 2009, pages 41–48.  \n\nAlexandre Berard, Laurent Besacier, Ali Can Kocabiyikoglu, and Olivier Pietquin. 2018. End-toend automatic speech translation of audiobooks. In ICASSP 2018, pages 6224–6228. IEEE.  \n\nAlexandre Berard, Olivier Pietquin, Christophe Servan, and Laurent Besacier. 2016. Listen and translate: A proof of concept for end-to-end speech-to-text translation. CoRR, abs/1612.01744.  \n\nWilliam Chan, Navdeep Jaitly, Quoc V. Le, and Oriol Vinyals. 2016. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. In ICASSP 2016, pages 4960–4964.  \n\nChung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Ekaterina Gonina, Navdeep Jaitly, Bo Li, Jan Chorowski, and Michiel Bacchiani. 2018. State-of-the-art speech recognition with sequence-to-sequence models. In ICASSP 2018, pages 4774–4778.  \n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT 2019, pages 4171–4186", "appendix": "."}, {"title": "Self-Supervised Representations Improve End-to-End Speech Translation", "authors": "Anne Wu, Changhan Wang, Juan Pino, Jiatao Gu", "bibkey": "self_supervised_representations_improve_end_to_end_speech_translation", "bibitem": "@article{GuSRIEST,\n  url = {http://arxiv.org/abs/2006.12124v2},\n  title = {Self-Supervised Representations Improve End-to-End Speech Translation},\n  authors = {Anne Wu, Changhan Wang, Juan Pino, Jiatao Gu},\n  abstract = {  End-to-end speech-to-text translation can provide a simpler and smaller system but is facing the challenge of data scarcity. Pre-training methods can leverage unlabeled data and have been shown to be effective on data-scarce settings. In this work, we explore whether self-supervised pre-trained speech representations can benefit the speech translation task in both high- and low-resource settings, whether they can transfer well to other languages, and whether they can be effectively combined with other common methods that help improve low-resource end-to-end speech translation such as using a pre-trained high-resource speech recognition system. We demonstrate that self-supervised pre-trained features can consistently improve the translation performance, and cross-lingual transfer allows to extend to a variety of languages without or with little tuning. },\n  bibkey = {GuSRIEST},\n  arxiv_id = {2006.12124v2},\n  subject = {eess.AS},\n  submission_date = {2020-06-22T10:28:38Z}\n}", "url": "http://arxiv.org/abs/2006.12124v2", "latex_url": "http://arxiv.org/src/2006.12124v2", "latex_path": "output/download_papers/2006.12124v2/2006.12124v2", "pdf_url": "http://arxiv.org/pdf/2006.12124v2", "pdf_path": "output/download_papers/2006.12124v2/2006.12124v2.pdf", "md_url": null, "latex_length": 0, "latex": null, "abstract": "  End-to-end speech-to-text translation can provide a simpler and smaller system but is facing the challenge of data scarcity. Pre-training methods can leverage unlabeled data and have been shown to be effective on data-scarce settings. In this work, we explore whether self-supervised pre-trained speech representations can benefit the speech translation task in both high- and low-resource settings, whether they can transfer well to other languages, and whether they can be effectively combined with other common methods that help improve low-resource end-to-end speech translation such as using a pre-trained high-resource speech recognition system. We demonstrate that self-supervised pre-trained features can consistently improve the translation performance, and cross-lingual transfer allows to extend to a variety of languages without or with little tuning. ", "abstract_length": 866, "abstract_token": 154, "introduction": "Recently, there has been much interest in end-to-end speech translation (ST) models [1, 2, 3, 4, 5, 6, 7], which, compared to traditional cascaded models, are simpler and computationally more efficient, can preserve more acoustic information and can avoid propagating errors from the speech recognition component. Large amounts of annotated data are usually required for achieving a good performance for such systems, but supervised training data for ST remain very limited. On the other hand, unlabeled data are more accessible. Selfsupervised techniques can exploit unlabeled data by learning a representation through, for instance, partial prediction or contrastive methods, and they have been shown effective for natural language [8, 9, 10] and speech processing [11, 12, 13]. In the latter case, several investigations on unsupervised or selfsupervised pre-training have been conducted and applied to English automatic speech recognition (ASR) [12, 13], to multilingual ASR by training multilingual features [14] or transferring contrastive predictive coding (CPC) features to other languages [15]. In this paper, we are interested in whether self-supervised speech pre-training can effectively help speech-to-text translation on both high-resource and low-resource settings. In particular, we focus on the method of wav2vec [12] which makes use of contrastive predictive coding (CPC), the vector-quantized representation vq-wav2vec [13] and BERT features learned on top of the discretized representations [13]. We use speech features pre-trained on English, and first examine a high-resource within-language English-to-X ST setting (X denotes a non-English language), then we transfer the representations to 11 lower-resource X-to-English ST tasks. Transferring the parameters learned on a higher-resource ASR task has been shown to be an effective way to improve the performance and ameliorate the training of low-resource ST [16, 17, 18], thus we also study the interactions with selfsupervised representations and whether we can effectively combine both methods. We first demonstrate that compared to commonly used logmel filterbank features, self-supervised features pre-trained on English can help improve English-to-X ST, but also transfer well to other languages even without requiring additional tuning. However, in the cross-lingual case, training data quantity and linguistic similarity may affect this gain. Further study shows that either fine-tuning the pre-trained input features or using a multilingual ASR model to fine-tune the final ST system can both improve the X-to-English ST. Finally, we show that when using an ASR model to pre-train ST systems, under certain training conditions, the ASR performance may not be a good indicator of the ST performance.", "introduction_length": 2781, "introduction_token": 575, "reference": "# 6. References  \n\n[1] A. Be´rard, O. Pietquin, C. Servan, and L. Besacier, “Listen and translate: A proof of concept for end-to-end speech-to-text translation,” arXiv preprint arXiv:1612.01744, 2016.   \n[2] L. Duong, A. Anastasopoulos, D. Chiang, S. Bird, and T. Cohn, “An attentional model for speech translation without transcription,” in Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2016, pp. 949–959.   \n[3] R. J. Weiss, J. Chorowski, N. Jaitly, Y. Wu, and Z. Chen, “Sequence-to-sequence models can directly translate foreign speech,” arXiv preprint arXiv:1703.08581, 2017.   \n[4] A. Be´rard, L. Besacier, A. C. Kocabiyikoglu, and O. Pietquin, “End-to-end automatic speech translation of audiobooks,” in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 6224–6228.   \n[5] L. C. Vila, C. Escolano, J. A. Fonollosa, and M. R. Costa-jussa\\`, “End-to-end speech translation with the transformer.” in IberSPEECH, 2018, pp. 60–63.   \n[6] M. A. Di Gangi, M. Negri, and M. Turchi, “Adapting transformer to end-to-end spoken language translation,” in INTERSPEECH 2019. International Speech Communication Association (ISCA), 2019, pp. 1133–1137.   \n[7] H. Inaguma, K. Duh, T. Kawahara, and S. Watanabe, “Multilingual end-to-end speech translation,” arXiv preprint arXiv:1910.00254, 2019.   \n[8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pretraining of deep bidirectional transformers for language understanding,” arXiv preprint arXiv:1810.04805, 2018.   \n[9] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language understanding by generative pre-training,” 2018.   \n[10] A. Baevski, S. Edunov, Y. Liu, L. Zettlemoyer, and M. Auli, “Cloze-driven pretraining of self-attention networks,” arXiv preprint arXiv:1903.07785, 2019.   \n[11] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with contrastive predictive coding,” arXiv preprint arXiv:1807.03748, 2018.   \n[12] S. Schneider, A. Baevski, R. Collobert, and M. Auli, “wav2vec: Unsupervised pre-training for speech recognition,” arXiv preprint arXiv:1904.05862, 2019.   \n[13] A. Baevski, S. Schneider, and M. Auli, “vq-wav2vec: Selfsupervised learning of discrete speech representations,” arXiv preprint arXiv:1910.05453, 2019.   \n[14] K. Kawakami, L. Wang, C. Dyer, P. Blunsom, and A. v. d. Oord, “Learning robust and multilingual speech representations,” arXiv preprint arXiv:2001.11128, 2020.   \n[15] M. Rivie\\`re, A. Joulin, P.-E. Mazare´, and E. Dupoux, “Unsupervised pretraining transfers well across languages,” arXiv preprint arXiv:2002.02848, 2020.   \n[16] S. Bansal, H. Kamper, K. Livescu, A. Lopez, and S. Goldwater, “Pre-training on high-resource speech recognition improves low-resource speech-to-text translation,” arXiv preprint arXiv:1809.01431, 2018.   \n[17] M. C. Stoian, S. Bansal, and S. Goldwater, “Analyzing asr pretraining for low-resource speech-to-text translation,” arXiv preprint arXiv:1910.10762, 2019.   \n[18] C. Wang, Y. Wu, S. Liu, Z. Yang, and M. Zhou, “Bridging the gap between pre-training and fine-tuning for end-to-end speech translation,” arXiv preprint arXiv:1909.07575, 2019.   \n[19] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for contrastive learning of visual representations,” arXiv preprint arXiv:2002.05709, 2020.   \n[20] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, “Deep contextualized word representations,” arXiv preprint arXiv:1802.05365, 2018.   \n[21] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,” arXiv preprint arXiv:1910.13461, 2019.   \n[22] M. A. Di Gangi, R. Cattoni, L. Bentivogli, M. Negri, and M. Turchi, “Must-c: a multilingual speech translation corpus,” in 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2019, pp. 2012–2017.   \n[23] C. Wang, J. Pino, A. Wu, and J. Gu, “Covost: A diverse multilingual speech-to-text translation corpus,” arXiv preprint arXiv:2002.01320, 2020.   \n[24] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, “Common voice: A massively-multilingual speech corpus,” arXiv preprint arXiv:1912.06670, 2019.   \n[25] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr corpus based on public domain audio books,” in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 5206–5210.   \n[26] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli, “fairseq: A fast, extensible toolkit for sequence modeling,” arXiv preprint arXiv:1904.01038, 2019.   \n[27] A. D. McCarthy, L. Puzon, and J. Pino, “Skinaugment: Autoencoding speaker conversions for automatic speech translation,” in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7924–7928.   \n[28] T. Kudo and J. Richardson, “Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,” arXiv preprint arXiv:1808.06226, 2018.   \n[29] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014.   \n[30] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, “Specaugment: A simple data augmentation method for automatic speech recognition,” arXiv preprint arXiv:1904.08779, 2019.   \n[31] C. Wang, A. Jain, D. Chen, and J. Gu, “Vizseq: A visual analysis toolkit for text generation tasks,” arXiv preprint arXiv:1909.05424, 2019.   \n[32] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic evaluation of machine translation,” in Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics, 2002, pp. 311–318.   \n[33] M. Post, “A call for clarity in reporting bleu scores,” arXiv preprint arXiv:1804.08771, 2018.", "reference_length": 6294, "reference_token": 2140, "txt_length": 24478, "txt_token": 7478, "txt": "# Self-Supervised Representations Improve End-to-End Speech Translation  \n\nAnne Wu, Changhan Wang, Juan Pino, Jiatao Gu  \n\nFacebook AI, USA {annewu, changhan, juancarabina, jgu}@fb.com  \n\n# Abstract  \n\nEnd-to-end speech-to-text translation can provide a simpler and smaller system but is facing the challenge of data scarcity. Pre-training methods can leverage unlabeled data and have been shown to be effective on data-scarce settings. In this work, we explore whether self-supervised pre-trained speech representations can benefit the speech translation task in both highand low-resource settings, whether they can transfer well to other languages, and whether they can be effectively combined with other common methods that help improve low-resource end-to-end speech translation such as using a pre-trained highresource speech recognition system. We demonstrate that selfsupervised pre-trained features can consistently improve the translation performance, and cross-lingual transfer allows to extend to a variety of languages without or with little tuning. Index Terms: speech recognition, speech translation, pretraining, self-supervised learning, low-resource  \n\n# 1. Introduction  \n\nRecently, there has been much interest in end-to-end speech translation (ST) models [1, 2, 3, 4, 5, 6, 7], which, compared to traditional cascaded models, are simpler and computationally more efficient, can preserve more acoustic information and can avoid propagating errors from the speech recognition component. Large amounts of annotated data are usually required for achieving a good performance for such systems, but supervised training data for ST remain very limited.  \n\nOn the other hand, unlabeled data are more accessible. Selfsupervised techniques can exploit unlabeled data by learning a representation through, for instance, partial prediction or contrastive methods, and they have been shown effective for natural language [8, 9, 10] and speech processing [11, 12, 13]. In the latter case, several investigations on unsupervised or selfsupervised pre-training have been conducted and applied to English automatic speech recognition (ASR) [12, 13], to multilingual ASR by training multilingual features [14] or transferring contrastive predictive coding (CPC) features to other languages [15].  \n\nIn this paper, we are interested in whether self-supervised speech pre-training can effectively help speech-to-text translation on both high-resource and low-resource settings. In particular, we focus on the method of wav2vec [12] which makes use of contrastive predictive coding (CPC), the vector-quantized representation vq-wav2vec [13] and BERT features learned on top of the discretized representations [13].  \n\nWe use speech features pre-trained on English, and first examine a high-resource within-language English-to-X ST setting (X denotes a non-English language), then we transfer the representations to 11 lower-resource X-to-English ST tasks. Transferring the parameters learned on a higher-resource ASR task has been shown to be an effective way to improve the performance and ameliorate the training of low-resource ST [16, 17, 18], thus we also study the interactions with selfsupervised representations and whether we can effectively combine both methods.  \n\nWe first demonstrate that compared to commonly used logmel filterbank features, self-supervised features pre-trained on English can help improve English-to-X ST, but also transfer well to other languages even without requiring additional tuning. However, in the cross-lingual case, training data quantity and linguistic similarity may affect this gain. Further study shows that either fine-tuning the pre-trained input features or using a multilingual ASR model to fine-tune the final ST system can both improve the X-to-English ST. Finally, we show that when using an ASR model to pre-train ST systems, under certain training conditions, the ASR performance may not be a good indicator of the ST performance.  \n\n# 2. Methods  \n\n# 2.1. Self-supervised Learning for Speech Representations  \n\nSelf-supervised learning allows to learn representations [8, 19, 11, 20, 21] through proxy tasks by, for instance, predicting some masked parts of the input, predicting future time-steps, contrasting with negative samples, or generating contextual data. In our case, we focus on three speech feature pre-training techniques which either makes use of CPC or a masked language model.  \n\nIn this work, we explore four self-supervised approaches for learning speech representations in ST. The first and simplest representation is wav2vec[12], which learns speech representations through a future sample prediction task by optimizing a contrastive loss. The model consists of two convolutional neural networks, with an encoder network that takes raw audio as inputs and outputs a low-frequency representation to an aggregator, that creates a contextualized vector representation by combining the latent representation from multiple time steps. As a follow-up, vq-wav2vec[13] has an architecture similar to wav2vec, but with an additional quantization module between the encoder network and the aggregator, which discretizes the encoder’s outputs before feeding them to the aggregator network. The output representation, as discrete tokens, can be consumed by natural language processing algorithms/models such as BERT from which we can extract representations for speech tasks. We also investigate an approach leveraging the pre-trained BERT, described in subsection 2.2.  \n\n# 2.2. Network architecture  \n\nFor both ST and ASR tasks, our experiments are performed with a sequence-to-sequence BiLSTM attention-based encoderdecoder architecture following [4], but with a 3-layers decoder. Speech features are given as inputs to two non-linear (tanh) layers, then passed to a stack of two convolutional layers. The output tensor is flattened and fed into three stacked bidirectional LSTM layers. The decoder is composed of two LSTM layers which output to a linear projection layer.  \n\nTable 1: AST training data statistics. We also use the source language transcripts as the training data for ASR (if used).   \n\n\n<html><body><table><thead><tr><td><b>Pairs</b></td><td><b>Hours</b></td><td><b>Data</b></td><td><b>Pairs</b></td><td><b>Hours</b></td><td><b>Data</b></td></tr></thead><tbody><tr><td>Fr-En</td><td>87h</td><td>CoVoST</td><td>Fa-En</td><td>20h</td><td>CoVoST</td></tr><tr><td>De-En</td><td>71h</td><td>CoVoST</td><td>Sv-En</td><td>1h</td><td>CoVoST</td></tr><tr><td>Es-En</td><td>21h</td><td>CoVoST</td><td>Mn-En</td><td>3h</td><td>CoVoST</td></tr><tr><td>NI-En</td><td>4h</td><td>CoVoST</td><td>Zh-En</td><td>4h</td><td>CoVoST</td></tr><tr><td>Ru-En</td><td>10h</td><td>CoVoST</td><td></td><td></td><td></td></tr><tr><td>It-En</td><td>13h</td><td>CoVoST</td><td>En-Fr</td><td>492h</td><td>MuST-C</td></tr><tr><td>Tr-En</td><td>3h</td><td>CoVoST</td><td>En-Ro</td><td>432h</td><td>MuST-C</td></tr></tbody></table></body></html>  \n\nFor low-resource ST settings, we also investigate a hybrid BERT-backbone architecture, where we reuse the BERT model pre-trained on discretized speech features as the encoder. For the decoder, we keep the same architecture than the BiLSTM. While BERT is commonly used on monolingual tasks since it has been developed at first for natural language understanding, this allows to reuse it for a different goal and avoiding training an important number of parameters from scratch.  \n\n# 3. Experiments  \n\n# 3.1. Datasets  \n\nFor English-to-X ST, we use the MuST-C [22] dataset, a corpus with audio recordings from English TED talks translated into 8 languages. The corpus comprises sentence-level aligned transcriptions and translations.  \n\nFor X-to-English ST, we use the multilingual ST dataset CoVoST [23] from 11 languages (French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese) to English, containing crowd-sourced speech with diverse speakers and accents on a variety of topics, from dialogue to movie scripts. For ASR, we use the English data from the corresponding Common Voice dataset (2019-06-12 release), with approximately 120 hours [24]. For the test set, we use the CoVoST test set for all the languages, and on the Tatoeba test set whenever it is available (i.e. for Fr, De, Nl, Ru and Es-En ST). Dataset statistics can be found in Table 1.  \n\n# 3.2. Self-supervised Pre-trained Models  \n\nIn our experiments, we use the officially open-sourced wav2vec [12], vq-wav2vec (k-means) [13] and BERT models [13] trained on the full 960h of Librispeech corpora [25].  \n\n# 3.3. Experimental Setups  \n\n# 3.3.1. Pipelines  \n\nFor both high-resource and low-resource ST settings, we compute the log-mel filterbank features and extract the frozen learned features for direct ST training. For low-resource ST, we additionally pre-train an English ASR model with the corresponding speech features, then transfer the encoder or both the encoder and decoder parameters for warming-up ST training.  \n\n# 3.3.2. Preprocessing  \n\nFor the preparation of transcript and translation, we normalize the punctuation, tokenize the text with sacreMoses and lowercase to align with previous settings [23] [22]. We remove the punctuations only from the transcripts. On CoVoST, we use a character-level vocabulary, with 54 characters including English alphabet and numerical characters, punctuations and the markers for fairseq [26] dictionary. On MuST-C, we choose a unigram vocabulary of size 10000 as in [27] to better balance the training time, as the sentences in MuST-C are generally longer. The vocabulary is obtained using SentencePiece [28].  \n\nWe convert the raw MP3 files of Common Voice and Tatoeba into monochannel WAV format with a sampling rate of $16000\\,\\textrm{H z}$ . We then extract 80-dimensional log-mel filterbank features, using a $25\\mathrm{ms}$ window size and 10ms window shift. The dimension of the feature has been chosen as the best performing one among several tested. For pre-trained speech features, we use the features extracted respectively from a wav2vec model, a vq-wav2vec (kmeans) model pre-trained on Librispeech, and a BERT model pre-trained on Librispeech quantized with the corresponding vq-wav2vec model. Details of the models are provided in section 2. In the training set, samples with more than 3000 frames or having more than 400 characters are removed for GPU memory efficiency, and samples with less than 5 frames or 1 character are also removed to avoid non-significant or empty inputs.  \n\n# 3.3.3. Training and Inference  \n\nTraining and inference use the fairseq framework [26]. We train using the Adam optimizer [29] with a learning rate of 1e-03 for BiLSTM models, and of 5e-05 for BERT-backbone models. We use a fixed learning schedule for BiLSTM models and a polynomial decay learning schedule for BERT-backbone models. In addition, we use SpecAugment [30] for both ASR and ST with LD policy but without time warping. When training with learned features, we change the policy along the frequency dimension proportional to the embedding size. It can be thought as a kind of dropout applied to the input.  \n\nAt inference time, we use beam search with a beam size of 5. We evaluate using the last 5 checkpoints averaged. For ASR, the reported word error rate (WER) has been obtained using VizSeq [31]. For ST, the BLEU score [32] reported is caseinsensitive and tokenized, obtained using sacreBLEU [33].  \n\n# 4. Results  \n\n# 4.1. English-to-X Speech Translation  \n\nIn this experiment, we compare the baseline log-mel filterbank features (noted as fbank) with wav2vec, vq-wav2vec and BERT features on within-language English-to-X translation, where the source audio matches the language (English) on which the learned features have been pre-trained on.  \n\nTable 2 summarizes the results obtained using different input features with the BiLSTM architecture, on the MuST-C dataset, for the English-French and English-Romanian language pairs. We can see that for both pairs, pre-trained features outperform the baseline log-mel filterbank feature. The largest improvements are obtained using the wav2vec features, with respectively 2 and 1.1 BLEU gains. Note that the MuST-C dataset is composed of TED talks (spoken English), while pre-trained features were learned on Librispeech, without need for domain adaptation. Models using pre-trained features are also found to  \n\n![](images/a1e2feee0caf58148a06a2936c1a146a282711bd0a98c0554872883c543d09d9.jpg)  \nFigure 1: Evolution of the BLEU score across epochs for different speech features on the MuST-C En-Fr dev set. The actual training has been performed until full convergence for all features.  \n\n# 4.2. X-to-English Speech Translation  \n\nWe now investigate whether pre-trained English speech features can be transferred to other languages for the X-to-En ST task.  \n\n# 4.2.1. Main Results  \n\nWe investigate the low-resource X-to-English ST task. We consider both ST training from scratch and using an En ASR model to pre-train the ST components on the CoVoST dataset.  \n\nWe report the ASR and ST results in Table 3. First, we find that while the pre-trained features are not helpful in very-low resource conditions, when there is a good baseline (either with a certain amount of data or combining with the ASR pre-training technique), they can consistently improve over the log-mel filterbank features and transfer well to other languages. On Fr-En ST, without any ASR pre-training, wav2vec features brought an improvement of 4.28/6.37 BLEU on CoVoST/Tatoeba. Second, the gain is cumulative with the ASR pre-training method to help improve low-resource ST performance, for all self-supervised features and almost all language pairs, except for Mongolian on which the systems failed to learn. Also, we observe that while on the ASR task, the most effective pre-trained feature is BERT, in the majority of X-to-En ST tasks, BERT features are outperformed by wav2vec or vq-wav2vec.  \n\nWe plot Fr-En and Zh-En results in Figure 2 and Figure 3 for better visualization (the general trend for most other languages is similar to French). We observe that for French, wav2vec features are consistently outperforming the baseline. In the case of Chinese, log-mel filterbank is slightly worse when we directly train the ST, but outperforms learned representations when combining with ASR pre-training.  \n\nWe also compare the results obtained on the BERTbackbone architecture with the baseline and other selfsupervised approaches, on 5 languages pairs in Table 3. The parameters transferred from the pre-trained BERT encoder can lead to better performance on 4 language pairs compared to the systems trained from scratch, but it is not as effective as using ASR pre-training. What is surprising is that the encoder contains 123.6M parameters and can still be trained effectively on low-resource setting (ex. there are only 4h of training data for Dutch).  \n\n![](images/9fe6b257d0f40c99825585f4d3f7cc1ef75f06990f5a510c4a4c41f42ea36412.jpg)  \nFigure 2: Comparison of BLEU scores for Fr-En ST, with/without ASR pre-training, on CoVoST test set (left) and Tatoeba test set (right)  \n\n![](images/96934cf0c253be85745ba853f1f0ae80373e370f034cb1530a0c0b723785cc5f.jpg)  \nFigure 3: Comparison of BLEU scores for Zh-En ST, with/without ASR pre-training, on CoVoST test set (\\*results averaged over 4 random seeds)  \n\n# 4.2.2. Transferring Features of Language X to English  \n\nWe now study the impact of transferring features of language X to the pre-trained speech representations or systems. We first consider directly fine-tuning a pre-trained representation. Secondly, we consider training an ASR with both source (X) and target (English) data which will then be used to warm-up the ST training.  \n\nIn the first approach, we compare frozen BERT features to the features fine-tuned on Common Voice speech data (2019- 06-12 release) on Fr-En and Zh-EN ST tasks. The advantage of this approach is that no labeled data is required. Table 4 shows that fine-tuning is helpful in all cases, except for Zh-En ST without ASR pre-training. On both language pairs, combining fine-tuned features with ASR pre-training is more helpful when pre-training only the encoder.  \n\nIn the second approach, we leverage ASR data and investigate the impact of mixing source language X with English data to train the ASR model which will then be used to fine-tune the encoder of the ST model. For both English and X, we use the Common Voice ASR training data. Table 5 shows the results for 4 language pairs from higher-resource to low-resource settings. While combining different languages may increase the WER of the ASR, it can still help improve the performance of the resulting ST in all cases. Also, for most languages, pre-trained representations can also improve over the baseline log-mel filterbank in this setting.  \n\nTable 3: Comparison of different speech features for English ASR and X-to-En AST. The first column indicates the WER of EN ASR models used to pre-train the ST. The ST results are on CoVoST/Tatoeba test set (when available). The ST languages are: German (De), French (Fr), Spanish $(E s)$ , Dutch (Nl), Russian (Ru), Italian (It), Turkish (Tr), Persian (Fa), Swedish (Sv), Mongolian (Mn) and Chinese (Zh). The baseline [23] is comparable to the case with ASR encoder pre-training, using log-mel fliterbank features.   \n\n\n<html><body><table><thead><tr><td><b>Language Hours (test)</b></td><td><b>En</b></td><td><b>De 168.3</b></td><td><b>Fr 46.3</b></td><td><b>Es 3.5</b></td><td><b>N1 8.2</b></td><td><b>Ru 8.2</b></td><td><b>It 12.8</b></td><td><b>Tr 3.8</b></td><td><b>Fa 23.9</b></td><td><b>Sv 1.0</b></td><td><b>Mn 2.9</b></td><td><b>Zh 3.7</b></td></tr></thead><tbody><tr><td>Wang et al. [23]  ]</td><td>-</td><td>7.6/7.5</td><td>21.4/10.9</td><td>6.1/1.9</td><td>3.4/5.0</td><td>4.8/1.1</td><td>6.5</td><td>3.1</td><td>2.8</td><td>1.9</td><td>0.3</td><td>5.6</td></tr><tr><td>fbank wav2vec vq-wav2vec BERT-feature</td><td></td><td>3.1/1.5</td><td>17.3/6.4</td><td>0.8/0.5</td><td>0.1/0.1</td><td>1.3/0.1</td><td>0.5</td><td>1.1</td><td>0.3</td><td>0.2</td><td>0.4</td><td>2.4</td></tr><tr><td>wav2vec vq-wav2vec BERT-feature</td><td></td><td>6/5.0</td><td>21.6/12.8</td><td>0.4/0.4</td><td>0.3/0.5</td><td>2.0/0.1</td><td>0.4</td><td>0.9</td><td>1.6</td><td>0.3</td><td>0.2</td><td>3.5</td></tr><tr><td>vq-wav2vec</td><td></td><td>6.1/5.0</td><td>20.8/12.2</td><td>0.7/0.3</td><td>0.2/0.4</td><td>2.0/0.1</td><td>0.5</td><td>0.9</td><td>1.2</td><td>0.4</td><td>0.3</td><td>3</td></tr><tr><td>BERT-feature</td><td></td><td>2.8/1.2</td><td>18.4/7.4</td><td>0.2/0.2</td><td>0.1/0.2</td><td>1.4/0.1</td><td>0.4</td><td>0.6</td><td>0.2</td><td>0.3</td><td>0.1</td><td>2.8</td></tr><tr><td>BERT-backbone</td><td></td><td>6.7</td><td>16.4</td><td>3.4</td><td>2.1</td><td>5.1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan=\"4\">With ASR encoder pre-training</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>fbank wav2vec</td><td>34.3 32.6</td><td>7.2/6.6</td><td>21.9/10.3</td><td>5.5/1.9</td><td>3.3/3.9</td><td>5.1/0.8</td><td>7.0</td><td>3.4</td><td>2.7</td><td>1.8</td><td>0.2</td><td>6.9</td></tr><tr><td>wav2vec</td><td>32.6</td><td>8.6/9.7</td><td>22.7/14.3</td><td>6.5/2.4</td><td>3.8/5.0</td><td>6.1/1.3</td><td>8.2</td><td>3.4</td><td>3.2</td><td>1.9</td><td>0.1</td><td>5.8</td></tr><tr><td>vq-wav2vec</td><td>35</td><td>8.5/9.8</td><td>21.9/12.4</td><td>6.5/2.4</td><td>3.7/5.4</td><td>5.7/1.3</td><td>7.8</td><td>3.1</td><td>3.3</td><td>1.8</td><td>0.3</td><td>5.7</td></tr><tr><td>BERT-feature</td><td>32.1</td><td>7.6/8.3</td><td>19.7/10.4</td><td>5.7/2.4</td><td>4.2/4.2</td><td>5.7/1.0</td><td>6.6</td><td>3.0</td><td>3.1</td><td>1.8</td><td>0.3</td><td>5.7</td></tr><tr><td colspan=\"4\">With ASR encoder+decoder pre-training</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>fbank</td><td></td><td>8.3/7.4</td><td>22.5/11.2</td><td>6.8/2.2</td><td>4.0/5.5</td><td>8.3/1.4</td><td>8.8</td><td>3.2</td><td>3.1</td><td>3.0</td><td>0.2</td><td>8.2</td></tr><tr><td>wav2vec</td><td></td><td>9.7/10.1</td><td>23.0/14.3</td><td>7.2/3.6</td><td>4.9/6.9</td><td>8.8/1.8</td><td>9.7</td><td>3.4</td><td>3.7</td><td>3.7</td><td>0.2</td><td>6.8</td></tr><tr><td>vq-wav2vec</td><td></td><td>9.6/11.2</td><td>22.1/13.1</td><td>6.9/3.3</td><td>5.0/7.0</td><td>9.2/1.7</td><td>9.0</td><td>3.3</td><td>3.7</td><td>3.2</td><td>0.3</td><td>7.0</td></tr><tr><td>BERT-feature</td><td></td><td>8.5/9.4</td><td>19.9/10.5</td><td>6.2/3.2</td><td>4.3/5.8</td><td>8.3/1.3</td><td>7.7</td><td>2.8</td><td>3.3</td><td>2.9</td><td>0.3</td><td>6.4</td></tr></tbody></table></body></html>  \n\nTable 4: BLEU scores using BERT features fine-tuned on language X. The difference compared to the frozen features (row BERT-feature in Table 3) is in parentheses.   \n\n\n<html><body><table><thead><tr><td><b>ASR pre-training</b></td><td><b>Fr</b></td><td><b>Zh</b></td></tr></thead><tbody><tr><td>None</td><td>18.7 (+0.3)</td><td>2.0 (-0.8)</td></tr><tr><td>Encoder</td><td>21.0 (+1.3)</td><td>6.8 (+1.1)</td></tr><tr><td>Encoder+Decoder</td><td>20.9 (+1.0)</td><td>6.7 (+0.3)</td></tr></tbody></table></body></html>  \n\nWe observe that on Fr-En and Es-En ST, for all the 4 features, pre-training only the ST encoder with the $\\mathrm{En+X}$ ASR is performing even better than pre-training both ST encoder and decoder with the En ASR (in Table 3). The largest gaps have been observed on BERT features, with respectively a difference of 1 and 1.6 BLEU for Fr-En and Es-En.  \n\n# 4.2.3. Influence of ASR Performance  \n\nThe experiments in sections 4.2.1 and 4.2.2 suggest that when the training conditions differ, i.e. when comparing ASR models pre-trained on different features and/or on different languages, the ASR WER may not necessarily be correlated with the performance of the final AST.  \n\nTable 3 (column En) shows that while vq-wav2vec led to the worst performance on En ASR, in most cases, the final ST results are better than the systems pre-trained on En ASR with BERT features, whose WER is 2.9 points lower.  \n\nThis effect is even more pronounced in Table 5, where in most cases, ASR models with higher WER can still help improve the translation performances.  \n\nTable 5: WER for $E n\\!+\\!X$ ASR and BLEU for the corresponding ST, using encoder pre-training. Difference with respect to En ASR is in parentheses: for ASR, it is computed against the 1st column of Table 3, for AST against the respective languages of Table 3 for the encoder pre-training case. A, B, C and D refer to fbank, wav2vec, vq-wav2vec and BERT features, respectively.   \n\n\n<html><body><table><thead><tr><td></td><td><b>De</b></td><td><b>Fr</b></td><td><b>Es</b></td><td><b>Zh</b></td></tr></thead><tbody><tr><td colspan=\"5\">ASR</td></tr><tr><td>A</td><td>35.9 (+1.6)</td><td>34.7 (+0.4)</td><td>34.7 (+0.4)</td><td>37.2 (+2.9)</td></tr><tr><td>B</td><td>33.5 (+0.9)</td><td>32.1 (-0.5)</td><td>32.9 (+0.3)</td><td>36.0 (+3.4)</td></tr><tr><td>C</td><td>35.4 (+0.4)</td><td>34.7 (-0.3)</td><td>35.9 (+0.9)</td><td>37.7 (+2.7)</td></tr><tr><td>D</td><td>35.0 (+2.9)</td><td>32.7 (+0.6)</td><td>32.8 (+0.7)</td><td>33.2 (+1.1)</td></tr><tr><td colspan=\"5\">AST</td></tr><tr><td>A</td><td>8.3 (+1.1)</td><td>23.2 (+1.3)</td><td>7.4 (+1.9)</td><td>7.5 (+0.6)</td></tr><tr><td>B</td><td>9.3 (+0.7)</td><td>23.9 (+1.2)</td><td>8.4 (+1.9)</td><td>7.3 (+1.5)</td></tr><tr><td>C</td><td>9.5 (+1.0)</td><td>22.8 (+0.9)</td><td>7.7 (+1.2)</td><td>7.2 (+1.5)</td></tr><tr><td>D</td><td>8.4 (+0.8)</td><td>20.9 (+1.2)</td><td>7.8 (+2.0)</td><td>7.2 (+1.5)</td></tr></tbody></table></body></html>  \n\n# 5. Conclusion  \n\nWe have shown that self-supervised representations can benefit the ST task. The resulting features can be directly transferred to other languages, and can be effectively combined with ASR pretraining for low-resource conditions to boost the performance. To improve the cross-lingual transfer on a given language, an effective way is to leverage ASR data by transferring the parameters learned on an ASR pre-trained on both higher-resource English and X data, or fine-tuning the pre-trained features on language X in an unsupervised way. Further work can include analyzing investigating the robustness of pre-trained features in other data conditions, and exploring multilingual settings.", "appendix": ""}, {"title": "Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained  Models into Speech Translation Encoders", "authors": "Chen Xu, Bojie Hu, Yanyang Li, Yuhao Zhang, shen huang, Qi Ju, Tong Xiao, Jingbo Zhu", "bibkey": "stacked_acoustic_and_textual_encoding_integrating_the_pre_trained_models_into_speech_translation_encoders", "bibitem": "@article{ZhuSAEITPMIST,\n  url = {http://arxiv.org/abs/2105.05752v2},\n  title = {Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained  Models into Speech Translation Encoders},\n  authors = {Chen Xu, Bojie Hu, Yanyang Li, Yuhao Zhang, shen huang, Qi Ju, Tong Xiao, Jingbo Zhu},\n  abstract = {  Encoder pre-training is promising in end-to-end Speech Translation (ST), given the fact that speech-to-translation data is scarce. But ST encoders are not simple instances of Automatic Speech Recognition (ASR) or Machine Translation (MT) encoders. For example, we find that ASR encoders lack the global context representation, which is necessary for translation, whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding (SATE) method for speech translation. Our encoder begins with processing the acoustic sequence as usual, but later behaves more like an MT encoder for a global representation of the input sequence. In this way, it is straightforward to incorporate the pre-trained models into the system. Also, we develop an adaptor module to alleviate the representation inconsistency between the pre-trained ASR encoder and MT encoder, and develop a multi-teacher knowledge distillation method to preserve the pre-training knowledge. Experimental results on the LibriSpeech En-Fr and MuST-C En-De ST tasks show that our method achieves state-of-the-art BLEU scores of 18.3 and 25.2. To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable or even better BLEU performance than the cascaded ST counterpart when large-scale ASR and MT data is available. },\n  bibkey = {ZhuSAEITPMIST},\n  arxiv_id = {2105.05752v2},\n  subject = {cs.CL},\n  submission_date = {2021-05-12T16:09:53Z}\n}", "url": "http://arxiv.org/abs/2105.05752v2", "latex_url": "http://arxiv.org/src/2105.05752v2", "latex_path": "output/download_papers/2105.05752v2/2105.05752v2", "pdf_url": "http://arxiv.org/pdf/2105.05752v2", "pdf_path": "output/download_papers/2105.05752v2/2105.05752v2.pdf", "md_url": null, "latex_length": 0, "latex": null, "abstract": "  Encoder pre-training is promising in end-to-end Speech Translation (ST), given the fact that speech-to-translation data is scarce. But ST encoders are not simple instances of Automatic Speech Recognition (ASR) or Machine Translation (MT) encoders. For example, we find that ASR encoders lack the global context representation, which is necessary for translation, whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding (SATE) method for speech translation. Our encoder begins with processing the acoustic sequence as usual, but later behaves more like an MT encoder for a global representation of the input sequence. In this way, it is straightforward to incorporate the pre-trained models into the system. Also, we develop an adaptor module to alleviate the representation inconsistency between the pre-trained ASR encoder and MT encoder, and develop a multi-teacher knowledge distillation method to preserve the pre-training knowledge. Experimental results on the LibriSpeech En-Fr and MuST-C En-De ST tasks show that our method achieves state-of-the-art BLEU scores of 18.3 and 25.2. To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable or even better BLEU performance than the cascaded ST counterpart when large-scale ASR and MT data is available. ", "abstract_length": 1402, "abstract_token": 286, "introduction": "End-to-end Speech Translation (E2E ST) has become popular recently for its ability to free designers from cascading different systems and shorten ∗Corresponding author 1The source code is available at https://github.com/xuchen neu/SATE Table 1: BLEU scores $[\\%]$ of a cascaded ST model and an end-to-end ST model with pre-training on the MuSTC En-De corpus. Restricted $=$ training is restricted to the ST data, and Unrestricted $=$ additional training data is allowed for ASR and MT. <html><body><table><thead><tr><td><b>Setting</b></td><td><b>Model</b></td><td><b>BLEU</b></td></tr></thead><tbody><tr><td rowspan=\"3\">Restricted</td><td>Cascaded</td><td>23.3</td></tr><tr><td>E2E+Pre-training</td><td>23.1</td></tr><tr><td>E2E+Pre-training</td><td>28.1</td></tr><tr><td rowspan=\"3\">Unrestricted</td><td>Cascaded</td><td>28.1</td></tr><tr><td>E2E+Pre-training</td><td>25.6</td></tr><tr><td>E2E+Pre-training</td><td>25.6</td></tr></tbody></table></body></html> the pipeline of translation (Duong et al., 2016; Berard et al., 2016; Weiss et al., 2017). Promising results on small-scale tasks are generally favorable. However, speech-to-translation paired data is scarce. Researchers typically use pre-trained Automatic Speech Recognition (ASR) and Machine Translation (MT) models to boost ST systems (Berard et al., 2018). For example, one can initialize the ST encoder using a large-scale ASR model (Bansal et al., 2019). But we note that, despite significant development effort, our end-to-end ST system with pre-trained models was not able to outperform the cascaded ST counterpart when the ASR and MT data size was orders of magnitude larger than that of ST (see Table 1). In this paper, we explore reasons why pretraining has been challenging in ST, and how pretrained ASR and MT models might be used together to improve ST. We find that the ST encoder plays both roles of acoustic encoding and textual encoding. This makes it problematic to view an ST encoder as either an individual ASR encoder or an individual MT encoder. More specifically, there are two problems. • Modeling deficiency: the MT encoder tries to capture long-distance dependency structures of language, but the ASR encoder focuses more on local dependencies in the input sequence. Since the ST encoder is initialized by the pre-trained ASR encoder (Berard et al., 2018), it fails to model large contexts in the utterance. But a large scope of representation learning is necessary for translation (Yang et al., 2018). • Representation inconsistency: on the decoder side of ST, the MT decoder is in general used to initialize the model. The assumption here is that the upstream component is an MT-like encoder, whereas the ST encoder actually behaves more like an ASR encoder. We address these problems by marrying the world of ASR encoding with the world of MT encoding. We propose a Stacked Acoustic-andTextual Encoding (SATE) method to cascade the ASR encoder and the MT encoder. It first reads and processes the sequence of acoustic features as a usual ASR encoder. Then an adaptor module passes the acoustic encoding output to an MT encoder with two principles: informative and adaptive. In this way, pre-trained ASR and MT encoders can work for what we would originally design them, and the incorporation of pre-trained models into ST is more straightforward. In addition, we develop a multi-teacher knowledge distillation method to robustly train the ST encoder and preserve the pretrained knowledge during fine-tuning (Yang et al., 2020). We test our method in a Transformer-based endto-end ST system. Experimental results on the LibriSpeech En-Fr and MuST-C En-De speech translation benchmarks show that it achieves the stateof-the-art performance of 18.3 and 25.2 BLEU points. Under a more challenging setup, where the large-scale ASR and MT data is available, SATE achieves comparable or even better performance than the cascaded ST counterpart. We believe that we are the first to present an end-to-end system that can beat the strong cascaded system in unrestricted speech translation tasks.", "introduction_length": 4076, "introduction_token": 973, "reference": "# References  \n\nAshkan Alinejad and Anoop Sarkar. 2020. Effectively pretraining a speech translation decoder with machine translation data. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 8014– 8020. Association for Computational Linguistics.  \n\nAntonios Anastasopoulos and David Chiang. 2018. Tied multitask learning for neural speech translation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 82–91. Association for Computational Linguistics.  \n\nParnia Bahar, Tobias Bieschke, and Hermann Ney. 2019. A comparative study on end-to-end speech to text translation. In IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2019, Singapore, December 14-18, 2019, pages 792–799. IEEE.  \n\nSameer Bansal, Herman Kamper, Karen Livescu, Adam Lopez, and Sharon Goldwater. 2019. Pretraining on high-resource speech recognition improves low-resource speech-to-text translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 58–68. Association for Computational Linguistics.  \n\nAlexandre Berard, Laurent Besacier, Ali Can Kocabiyikoglu, and Olivier Pietquin. 2018. Endto-end automatic speech translation of audiobooks. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2018, Calgary, AB, Canada, April 15- 20, 2018, pages 6224–6228. IEEE.  \n\nAlexandre Berard, Olivier Pietquin, Christophe Servan, and Laurent Besacier. 2016. Listen and translate: A proof of concept for end-to-end speech-to-text translation. CoRR, abs/1612.01744.  \n\nQianqian Dong, Mingxuan Wang, Hao Zhou, Shuang Xu, Bo Xu, and Lei Li. 2021. Consecutive decoding for speech-to-text translation. In The Thirty-ffith AAAI Conference on Artificial Intelligence, AAAI.  \n\nLong Duong, Antonios Anastasopoulos, David Chiang, Steven Bird, and Trevor Cohn. 2016. An attentional model for speech translation without transcription. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016, pages 949–959. The Association for Computational Linguistics.  \n\nMattia Antonino Di Gangi, Roldano Cattoni, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2019. Must-c: a multilingual speech translation corpus. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2012–2017. Association for Computational Linguistics.  \n\nIan J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. 2015. An empirical investigation of catastrophic forgetting in gradient-based neural networks.  \n\nAlex Graves, Santiago Fern´andez, Faustino J. Gomez, and J¨urgen Schmidhuber. 2006. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Machine Learning, Proceedings of the Twenty-Third International Conference (ICML 2006), Pittsburgh, Pennsylvania, USA, June 25-29, 2006, volume 148 of ACM International Conference Proceeding Series, pages 369–376. ACM.  \n\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. 2020. Conformer: Convolution-augmented transformer for speech recognition. In Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020, pages 5036–5040. ISCA.  \n\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the knowledge in a neural network. CoRR, abs/1503.02531.  \n\nHirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, and Shinji Watanabe. 2020. Espnet-st: All-inone speech translation toolkit. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2020, Online, July 5-10, 2020, pages 302–311. Association for Computational Linguistics.  \n\nShigeki Karita, Nelson Enrique Yalta Soplin, Shinji Watanabe, Marc Delcroix, Atsunori Ogawa, and Tomohiro Nakatani. 2019. Improving transformer-based end-to-end speech recognition with connectionist temporal classification and language model integration. In Interspeech 2019, 20th Annual Conference of the International Speech Communication Association, Graz, Austria, 15-19 September 2019, pages 1408– 1412. ISCA.  \n\nAli Can Kocabiyikoglu, Laurent Besacier, and Olivier Kraif. 2018. Augmenting librispeech with french translations: A multimodal corpus for direct speech translation evaluation. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018. European Language Resources Association (ELRA).  \n\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, June 23-30, 2007, Prague, Czech Republic. The Association for Computational Linguistics.  \n\nBei Li, Ziyang Wang, Hui Liu, Quan Du, Tong Xiao, Chunliang Zhang, and Jingbo Zhu. 2020. Learning light-weight translation models from deep transformer. CoRR, abs/2012.13866.  \n\nYuchen Liu, Hao Xiong, Jiajun Zhang, Zhongjun He, Hua Wu, Haifeng Wang, and Chengqing Zong. 2019. End-to-end speech translation with knowledge distillation. In Interspeech 2019, 20th Annual Conference of the International Speech Communication Association, Graz, Austria, 15-19 September 2019, pages 1128–1132. ISCA.  \n\nYuchen Liu, Junnan Zhu, Jiajun Zhang, and Chengqing Zong. 2020. Bridging the modality gap for speech-to-text translation. CoRR, abs/2010.14920.  \n\nLambert Mathias and William Byrne. 2006. Statistical phrase-based speech translation. In 2006 IEEE International Conference on Acoustics Speech and Signal Processing, ICASSP 2006,  \n\nToulouse, France, May 14-19, 2006, pages 561– 564. IEEE.  \n\nHermann Ney. 1999. Speech translation: coupling of recognition and translation. In Proceedings of the 1999 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP ’99, Phoenix, Arizona, USA, March 15- 19, 1999, pages 517–520. IEEE Computer Society.  \n\nDaniel S. Park, William Chan, Yu Zhang, ChungCheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le. 2019. Specaugment: A simple data augmentation method for automatic speech recognition. In Interspeech 2019, 20th Annual Conference of the International Speech Communication Association, Graz, Austria, 15-19 September 2019, pages 2613–2617. ISCA.  \n\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACLHLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 2227– 2237. Association for Computational Linguistics.  \n\nJuan Pino, Liezl Puzon, Jiatao Gu, Xutai Ma, Arya D McCarthy, and Deepak Gopinath. 2019. Harnessing indirect training data for end-to-end automatic speech translation: Tricks of the trade. arXiv preprint arXiv:1909.06515.  \n\nJuan Pino, Qiantong Xu, Xutai Ma, Mohammad Javad Dousti, and Yun Tang. 2020. Selftraining for end-to-end speech translation. In Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020, pages 1476–1480. ISCA.  \n\nMatt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 - November 1, 2018, pages 186–191. Association for Computational Linguistics.  \n\nTanja Schultz, Szu-Chen Jou, Stephan Vogel, and Shirin Saleem. 2004. Using word latice information for a tighter coupling in speech translation systems. In Eighth International Conference on Spoken Language Processing.  \n\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics.  \n\nMatthias Sperber, Graham Neubig, Jan Niehues, and Alex Waibel. 2019. Attention-passing models for robust and data-efficient end-to-end speech translation. Trans. Assoc. Comput. Linguistics, 7:313–325.  \n\nMatthias Sperber, Jan Niehues, Graham Neubig, Sebastian St¨uker, and Alex Waibel. 2018. Selfattentional acoustic models. In Interspeech 2018, 19th Annual Conference of the International Speech Communication Association, Hyderabad, India, 2-6 September 2018, pages 3723–3727. ISCA.  \n\nMihaela C. Stoian, Sameer Bansal, and Sharon Goldwater. 2020. Analyzing ASR pretraining for low-resource speech-to-text translation. In 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020, pages 7909–7913. IEEE.  \n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998–6008.  \n\nChengyi Wang, Yu Wu, Shujie Liu, Zhenglu Yang, and Ming Zhou. 2020a. Bridging the gap between pre-training and fine-tuning for end-toend speech translation. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI  \n\n2020, New York, NY, USA, February 7-12, 2020, pages 9161–9168. AAAI Press.  \n\nChengyi Wang, Yu Wu, Shujie Liu, Ming Zhou, and Zhenglu Yang. 2020b. Curriculum pretraining for end-to-end speech translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 3728–3738. Association for Computational Linguistics.  \n\nShinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya Renduchintala, and Tsubasa Ochiai. 2018. Espnet: End-to-end speech processing toolkit. In Interspeech 2018, 19th Annual Conference of the International Speech Communication Association, Hyderabad, India, 2-6 September 2018, pages 2207–2211. ISCA.  \n\nShinji Watanabe, Takaaki Hori, Suyoun Kim, John R. Hershey, and Tomoki Hayashi. 2017. Hybrid ctc/attention architecture for end-to-end speech recognition. IEEE J. Sel. Top. Signal Process., 11(8):1240–1253.  \n\nRon J. Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, and Zhifeng Chen. 2017. Sequence-to-sequence models can directly translate foreign speech. In Interspeech 2017, 18th Annual Conference of the International Speech Communication Association, Stockholm, Sweden, August 20-24, 2017, pages 2625–2629. ISCA.  \n\nBaosong Yang, Zhaopeng Tu, Derek F. Wong, Fandong Meng, Lidia S. Chao, and Tong Zhang. 2018. Modeling localness for self-attention networks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 4449–4458. Association for Computational Linguistics.  \n\nJiacheng Yang, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Weinan Zhang, Yong Yu, and Lei Li. 2020. Towards making the most of BERT in neural machine translation. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence  \n\nConference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 9378–9385. AAAI Press.", "reference_length": 12778, "reference_token": 3454, "txt_length": 35949, "txt_token": 9628, "txt": "# Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders  \n\nChen $\\mathbf{Xu^{1}}$ , Bojie $\\mathbf{H}\\mathbf{u}^{2}$ , Yanyang $\\mathrm{Li}^{3}$ , Yuhao Zhang1,   \nShen Huang2, Qi $\\mathbf{J}\\mathbf{u}^{2*}$ , Tong $\\mathbf{Xiao^{1,4*}}$ , Jingbo Zhu1,4   \n1NLP Lab, School of Computer Science and Engineering,   \nNortheastern University, Shenyang, China   \n2Tencent Minority-Mandarin Translation, Beijing, China   \n3The Chinese University of Hong Kong, Hong Kong, China   \n4NiuTrans Research, Shenyang, China   \n{xuchenneu,blamedrlee,yoohaozhang}@outlook.com,   \n{bojiehu,springhuang,damonju}@tencent.com,   \n{xiaotong,zhujingbo}@mail.neu.edu.cn  \n\n# Abstract  \n\nEncoder pre-training is promising in end-toend Speech Translation (ST), given the fact that speech-to-translation data is scarce. But ST encoders are not simple instances of Automatic Speech Recognition (ASR) or Machine Translation (MT) encoders. For example, we find that ASR encoders lack the global context representation, which is necessary for translation, whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding (SATE) method for speech translation. Our encoder begins with processing the acoustic sequence as usual, but later behaves more like an MT encoder for a global representation of the input sequence. In this way, it is straightforward to incorporate the pre-trained models into the system. Also, we develop an adaptor module to alleviate the representation inconsistency between the pre-trained ASR encoder and MT encoder, and develop a multi-teacher knowledge distillation method to preserve the pre-training knowledge. Experimental results on the LibriSpeech En-Fr and MuST-C EnDe ST tasks show that our method achieves state-of-the-art BLEU scores of 18.3 and 25.2. To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable or even better BLEU performance than the cascaded ST counterpart when large-scale ASR and MT data is available1.  \n\n# 1 Introduction  \n\nEnd-to-end Speech Translation (E2E ST) has become popular recently for its ability to free designers from cascading different systems and shorten  \n\n∗Corresponding author 1The source code is available at https://github.com/xuchen neu/SATE  \n\nTable 1: BLEU scores $[\\%]$ of a cascaded ST model and an end-to-end ST model with pre-training on the MuSTC En-De corpus. Restricted $=$ training is restricted to the ST data, and Unrestricted $=$ additional training data is allowed for ASR and MT.   \n\n\n<html><body><table><thead><tr><td><b>Setting</b></td><td><b>Model</b></td><td><b>BLEU</b></td></tr></thead><tbody><tr><td rowspan=\"3\">Restricted</td><td>Cascaded</td><td>23.3</td></tr><tr><td>E2E+Pre-training</td><td>23.1</td></tr><tr><td>E2E+Pre-training</td><td>28.1</td></tr><tr><td rowspan=\"3\">Unrestricted</td><td>Cascaded</td><td>28.1</td></tr><tr><td>E2E+Pre-training</td><td>25.6</td></tr><tr><td>E2E+Pre-training</td><td>25.6</td></tr></tbody></table></body></html>  \n\nthe pipeline of translation (Duong et al., 2016; Berard et al., 2016; Weiss et al., 2017). Promising results on small-scale tasks are generally favorable. However, speech-to-translation paired data is scarce. Researchers typically use pre-trained Automatic Speech Recognition (ASR) and Machine Translation (MT) models to boost ST systems (Berard et al., 2018). For example, one can initialize the ST encoder using a large-scale ASR model (Bansal et al., 2019). But we note that, despite significant development effort, our end-to-end ST system with pre-trained models was not able to outperform the cascaded ST counterpart when the ASR and MT data size was orders of magnitude larger than that of ST (see Table 1).  \n\nIn this paper, we explore reasons why pretraining has been challenging in ST, and how pretrained ASR and MT models might be used together to improve ST. We find that the ST encoder plays both roles of acoustic encoding and textual encoding. This makes it problematic to view an ST encoder as either an individual ASR encoder or an individual MT encoder. More specifically, there are two problems.  \n\n• Modeling deficiency: the MT encoder tries to capture long-distance dependency structures of language, but the ASR encoder focuses more on local dependencies in the input sequence. Since the ST encoder is initialized by the pre-trained ASR encoder (Berard et al., 2018), it fails to model large contexts in the utterance. But a large scope of representation learning is necessary for translation (Yang et al., 2018).  \n\n• Representation inconsistency: on the decoder side of ST, the MT decoder is in general used to initialize the model. The assumption here is that the upstream component is an MT-like encoder, whereas the ST encoder actually behaves more like an ASR encoder.  \n\nWe address these problems by marrying the world of ASR encoding with the world of MT encoding. We propose a Stacked Acoustic-andTextual Encoding (SATE) method to cascade the ASR encoder and the MT encoder. It first reads and processes the sequence of acoustic features as a usual ASR encoder. Then an adaptor module passes the acoustic encoding output to an MT encoder with two principles: informative and adaptive. In this way, pre-trained ASR and MT encoders can work for what we would originally design them, and the incorporation of pre-trained models into ST is more straightforward. In addition, we develop a multi-teacher knowledge distillation method to robustly train the ST encoder and preserve the pretrained knowledge during fine-tuning (Yang et al., 2020).  \n\nWe test our method in a Transformer-based endto-end ST system. Experimental results on the LibriSpeech En-Fr and MuST-C En-De speech translation benchmarks show that it achieves the stateof-the-art performance of 18.3 and 25.2 BLEU points. Under a more challenging setup, where the large-scale ASR and MT data is available, SATE achieves comparable or even better performance than the cascaded ST counterpart. We believe that we are the first to present an end-to-end system that can beat the strong cascaded system in unrestricted speech translation tasks.  \n\n# 2 Related Work  \n\nSpeech translation aims at learning models that can predict, given some speech in the source language, the translation into the target language. The earliest of these models were cascaded: they treated ST as a pipeline of running an ASR system and an MT system sequentially (Ney, 1999; Mathias and Byrne, 2006; Schultz et al., 2004). This allows the use of off-the-shelf models, and was (and is) popular in practical ST systems. However, these systems were sensitive to the errors introduced by different component systems and the high latency of the long pipeline.  \n\nAs another stream in the ST area, end-to-end methods have been promising recently (Berard et al., 2016; Weiss et al., 2017; Berard et al., 2018). The rise of end-to-end ST can be traced back to the success of deep neural models (Duong et al., 2016). But, unlike other well-defined tasks in deep learning, annotated speech-to-translation data is scarce, which prevents well-trained ST models. A simple solution to this issue is data augmentation (Pino et al., 2019, 2020). This method is model-free but generating large-scale synthetic data is time consuming. As an alternative, researchers used multi-task learning (MTL) to robustly train the ST model so that it could benefti from additional guide signals (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Berard et al., 2018; Sperber et al., 2019; Dong et al., 2021). Generally, MTL requires a careful design of the loss functions and more complicated architectures.  \n\nIn a similar way, more recent work pre-trains different components of the ST system, and consolidates them into one. For example, one can initialize the encoder with an ASR model, and initialize the decoder with the target-language side of an MT model (Berard et al., 2018; Bansal et al., 2019; Stoian et al., 2020). More sophisticated methods include better training and fine-tuning (Wang et al., 2020a,b), the shrink mechanism (Liu et al., 2020), the adversarial regularizer (Alinejad and Sarkar, 2020), and etc. Although pre-trained models have quickly become dominant in many NLP tasks, they are still found to underperform the cascaded model in ST. This motivates us to explore the reasons why this happens and methods to solve the problems accordingly.  \n\n# 3 Why is ST Encoding Difficult?  \n\nFollowing previous work in end-to-end models (Berard et al., 2016; Weiss et al., 2017), we envision an encoding-decoding process in which an input sequence is encoded into a representation vector, and the vector is then decoded into an output sequence.  \n\n![](images/93cd226c6fd22106b9a73fed665f4dd92d899c174ba17b97b6c9156487bf7df3.jpg)  \nFigure 1: (a) Localness in each layer of the ST, ASR, and MT encoders, (b) the impact of CTC position on localness, and (c) the impact of CTC position on performance of ST and ASR models.  \n\nIn such a scenario, all end-to-end ST, ASR and MT systems can be viewed as instances of the same architecture. Then, components of these systems can be pre-trained and re-used across them.  \n\nAn underlying assumption here is that the ST encoder is doing something quite similar to what the MT (or ASR) encoder is doing. However, Sperber et al. (2018) find that the ASR model beneftis from a small attention window, which is inconsistent with the MT model (Yang et al., 2018). To verify this, we compare the behavior of ST, ASR and MT encoders. We choose Transformer as the base architecture (Vaswani et al., 2017) and run experiments on the MuST-C En-De corpus. We report the results on the MuST-C En-De tst-COMMON test data. For stronger systems, we use Connectionist Temporal Classification (CTC) (Graves et al., 2006) as the auxiliary loss on the encoders when we train the ASR and ST systems (Watanabe et al., 2017; Karita et al., 2019; Bahar et al., 2019). The CTC loss forces the encoders to learn alignments between speech and transcription. It is necessary for the state-of-the-art performance (Watanabe et al., 2018).  \n\nHere we define the localness of a word as the sum of the attention weights to the surrounding words (or features) within a fixed small window2. The window size is $10\\%$ of the sequence length. Figure 1(a) shows the localness of the attention weights for different layers of the encoders. We see that the ST and ASR encoders prefer local attention which indicates a kind of short-distance dependencies in processing acoustics feature sequences. Whereas the MT encoder generates a more global distribution of attention weights for word sequences, especially when we stack more layers. This result arises a new question: Is local attention sufficient for speech translation?  \n\nThen, we design another experiment to examine if the high localness in attention weights of the ASR and ST encoders is due to the bias imposed by CTC. In Figure 1(b), we use the CTC loss in the intermediate layer and show the average localness of the layers above or below CTC. The CTC loss demonstrates strong preference for locally attentive models. The upper-level layers act more like an MT encoder, that is, the layers with no CTC loss generates more global distributions. Taking this further, Figure 1(c) demonstrates a slightly higher BLEU score when we free more upper-level layers from the guide of CTC. Meanwhile, the word error rate (WER) increases because only lower parts of the model are learned in a standard manner of ASR.  \n\nNow we have some hints: the ST encoder is not a simple substitution of the ASR encoder or the MT encoder. Rather, they are complementary to each other, that is, we need the ASR encoder to deal with the acoustic input, and the MT encoder to generate the representation vector that can work better with the decoder.  \n\n# 4 The Method  \n\nIn speech translation, we want the encoder to represent the input speech to some sort of decoderfriendly representations. We also want the encoder to be “natural” for pre-training. In the following, we describe, Stacked Acoustics-and-Textual Encoding (SATE), a new ST encoding method to meet these requirements, and improvements of it.  \n\n![](images/b866c0a7a1821c4a30735c305f999dcf5065e15cfb7d5984a59a8257f00032ca.jpg)  \nFigure 2: The overall architecture of stacked acousticand-textual encoding.  \n\n# 4.1 Stacked Acoustic-and-Textual Encoding  \n\nUnlike previous work, the SATE method does not rely on a single encoder to receive the signal from both the CTC loss and the feedback of the decoder. Instead, it is composed of two encoders: the first does exactly the same thing as the ASR encoder (call it acoustic encoder), and the other generates a higher-level globally-attentive representation on top of the acoustic encoder (call it textual encoder).  \n\nSee Figure 2 for the architecture of SATE. The acoustic encoder is trained by CTC in addition to the supervision signal from the translation loss. Let $(x,y^{s},y^{t})$ be an ST training sample, where $x$ is the input feature sequence of the speech, $y^{s}$ is the transcription of $x$ , and $y^{t}$ is the translation in the target language. We define the output of the acoustic encoder as:  \n\n$$\n\\begin{array}{r l r}{h^{s}}&{{}=}&{E_{s}(x)}\\end{array}\n$$  \n\nwhere $E_{s}(\\cdot)$ is the encoding function. Then, we add a Softmax layer on $h^{s}$ to predict the CTC label path $\\pi=(\\pi_{1},\\cdot\\cdot\\cdot,\\pi_{T})$ , where $T$ is the length of the input sequence. The probability of path $\\mathbf{P}(\\pi|h^{s})$ is the product of the probability $\\mathbf{P}(\\pi_{t}|h_{t}^{s})$ at every time $t$ based on conditionally independent assumption:  \n\n$$\n{\\bf P}(\\pi|h^{s})~~\\approx~~\\prod_{t}^{T}{\\bf P}(\\pi_{t}|h_{t}^{s})\n$$  \n\nCTC works by summing over the probability of all possible alignment paths $\\Phi(y^{s})$ between $x$ and $y^{s}$ , as follows:  \n\n$$\n\\operatorname{P}_{\\mathrm{CTC}}(y^{s}|h^{s})~~=~~\\sum_{\\pi\\in\\Phi(y^{s})}\\operatorname{P}(\\pi|h^{s})\n$$  \n\nThen, the CTC loss is defined as:  \n\n$$\n\\begin{array}{c c l}{{\\mathcal L}_{\\mathrm{CTC}}}&{{=}}&{{-\\log\\mathrm{P}_{\\mathrm{CTC}}(y^{s}|h^{s};\\theta_{\\mathrm{CTC}})}}\\end{array}\n$$  \n\nwhere $\\theta_{\\mathrm{CTC}}$ is the model parameters of the acoustic encoder and the CTC output layer.  \n\nThe acoustic encoder is followed by an adaptor. It receives $h^{s}$ and $\\textstyle P(\\pi|h^{s})$ , and produces a new representation required by the textual encoder. Let $A(\\cdot,\\cdot)$ be the adaptor module. Its output is defined as:  \n\n$$\n\\begin{array}{r l r}{\\hat{h^{s}}}&{{}=}&{A(h^{s},\\mathbb P(\\pi|h^{s}))}\\end{array}\n$$  \n\nWe leave the design of the adaptor to Section 4.2. Furthermore, we stack the textual encoder on the adaptor. The output $h^{t}$ is defined as:  \n\n$$\n\\begin{array}{r l r}{h^{t}}&{{}=}&{E_{t}(\\hat{h^{s}})}\\end{array}\n$$  \n\nwhere $E_{t}(\\cdot)$ is the textual encoder. $h^{t}$ is fed into the decoder for computing the translation probability $\\mathrm{P_{Trans}}(y^{t}|h^{t})$ , as in standard MT systems. We define the translation loss as:  \n\n$$\n\\begin{array}{r l}{\\mathcal{L}_{\\mathrm{Trans}}}&{{}=-\\log\\operatorname{P}_{\\mathrm{Trans}}(y^{t}|h^{t};\\theta_{\\mathrm{ST}})}\\end{array}\n$$  \n\nwhere $\\theta_{\\mathrm{ST}}$ is all model parameters except for the CTC output layer.  \n\nFinally, we interpolate $\\mathcal{L}_{\\mathrm{CTC}}$ and $\\mathcal{L}_{\\mathrm{Trans}}$ (with coefficient $\\alpha$ ) for the loss of the entire model:  \n\n$$\n\\begin{array}{r c l}{{\\mathcal{L}}}&{{=}}&{{\\alpha\\cdot{\\mathcal{L}}_{\\mathrm{CTC}}+(1-\\alpha)\\cdot{\\mathcal{L}}_{\\mathrm{Trans}}}}\\end{array}\n$$  \n\nSince the textual encoder works for the decoder only, it is trained as an MT encoder. In this way, the acoustic and textual encoders can do what we would originally expect them to do: the acoustic encoder deals with the acoustic input (i.e., ASR encoding), and the textual encoder generates a representation for translation (i.e., MT encoding). Also, SATE is friendly to pre-training. One can simply use an ASR encoder as the acoustic encoder, and use an MT encoder as the textual encoder. Note that SATE is in general a cascaded model, in response to the pioneering work in ST (Ney, 1999). It can be seen as cascading the ASR and MT systems in an end-to-end fashion.  \n\n# 4.2 The Adaptor  \n\nNow we turn to the design of the adaptor. Note that the pre-trained MT encoder assumes that the input is a word embedding sequence. Simply stacking the MT encoder and the ASR encoder obviously does not work well. For this reason, the adaptor ftis the output of the ASR encoder (i.e., the acoustic encoder) to what an MT encoder would like to see. We follow two principles in designing the adaptor: adaptive and informative.  \n\nWe need an adaptive representation to make the input of the textual encoder similar to that of the MT encoder. To this end, we generate the soft contextual representation that shares the same latent space with the embedding layer of the MT encoder.  \n\nAs shown in Eq. (2), the CTC output $\\mathbf{P}(\\pi_{t}|h_{t}^{s})$ indicates the alignment probability over the vocabulary at time $t$ . Instead of replacing the representation by the embedding of the most-likely token (Liu et al., 2020), we employ a soft token which is the expectation of the embedding over the distribution from CTC. Let $\\boldsymbol{W}^{e}$ be the embedding matrix of the textual encoder, we define the soft representation $h_{\\mathrm{soft}}^{s}$ as:  \n\n$$\n\\begin{array}{r c l}{h_{\\mathrm{soft}}^{s}}&{=}&{\\mathbf{P}(\\pi|h^{s})\\cdot W^{e}}\\end{array}\n$$  \n\nAlso, an informative representation should contain information in the original input (Peters et al., 2018). The output acoustic representation of the ASR encoder generally involves paralinguistic information, such as emotion, accent, and emphasis. They are not expressed in the form of text explicitly but might be helpful for translation. For example, the generation of the declarative or exclamatory sentences depends on the emotions of the speakers.  \n\nWe introduce a single-layer neural network to learn to map the acoustic representation to the latent space of the textual encoder, which preserves the acoustic information:  \n\n$$\n\\begin{array}{r c l}{h_{\\mathrm{map}}^{s}}&{=}&{\\mathrm{ReLU}(W^{\\mathrm{map}}\\cdot h^{s}+b^{\\mathrm{map}})}\\end{array}\n$$  \n\nwhere $W^{\\mathrm{map}}$ and $b^{\\mathrm{map}}$ are the trainable parameters. The final output of the adaptor is defined to be:  \n\n![](images/4eefecbd16457d52d5ba300c9bb706e8b7a6ea604e4899c5a851dc1b44f97b0b.jpg)  \nFigure 3: The architecture of the adaptor.  \n\nwhere λ is the weight of hsmap and set to 0.5 by default. Figure 3 shows the architecture of the adaptor.  \n\nNote that, in the adaptor, we do not change the sequence length for textual encoding because such a way is simple for implementation and shows satisfactory results in our experiments. Although there is a length inconsistency issue, the sequence representation of the speech should be similar with the  \n\n$$\n\\begin{array}{r c l}{{A(h^{s},P(\\pi|h^{s}))}}&{{=}}&{{\\lambda\\cdot h_{\\mathrm{map}}^{s}+}}\\\\ {{}}&{{}}&{{(1-\\lambda)\\cdot h_{\\mathrm{soft}}^{s}}}\\end{array}\n$$  \n\n# 4.3 Multi-teacher Knowledge Distillation  \n\ncorrespond transcription. Shrinking the sequence simply results in information incompleteness. We will investigate this issue in the future.  \n\nAnother improvement here is that we develop a multi-teacher knowledge distillation (MTKD) method to preserve the pre-trained knowledge during fine-tuning (Hinton et al., 2015).  \n\nThe ST model mimics the teacher distribution by minimizing the cross-entropy loss between the teacher and student (Liu et al., 2019). For a training sample $(x,y^{s},y^{t})$ , we define two loss functions:  \n\n$$\n\\begin{array}{r c l}{{\\mathcal{L}}_{\\mathrm{KD,CTC}}}&{=}&{displaystyle-\\sum_{m=1}^{T}\\sum_{k=1}^{|V|}\\mathrm{Q}(\\pi_{m}=v_{k}|x;\\theta_{\\mathrm{ASR}})}\\\\ &&{\\displaystyle\\times\\log\\mathrm{P}(\\pi_{m}=v_{k}|x;\\theta_{\\mathrm{CTC}})~~(12}\\\\ {{\\mathcal{L}}_{\\mathrm{KD,Trans}}}&{=}&{\\displaystyle-\\sum_{n=1}^{|y^{t}|}\\sum_{k=1}^{|V|}\\mathrm{Q}(y_{n}^{t}=v_{k}|y^{s};\\theta_{\\mathrm{MT}})}\\\\ &&{\\displaystyle~\\times\\log\\mathrm{P}(y_{n}^{t}=v_{k}|x;\\theta_{\\mathrm{ST}})~~~~~(13}\\end{array}\n$$  \n\nwhere $v_{k}$ is the word indexed by $k$ and $V$ is the vocabulary shared among the ST, ASR, and MT models. $\\mathrm{Q}(\\cdot|\\cdot)$ is the teacher distribution and $\\mathbf{P}(\\cdot|\\cdot)$ is the student distribution. $\\theta_{\\mathrm{ASR}},\\,\\theta_{\\mathrm{CTC}},\\,\\theta_{\\mathrm{MT}}$ and $\\theta_{\\mathrm{ST}}$ are the model parameters.  \n\nWe can rewrite Eq. (8) to obtain a new loss:  \n\n$$\n\\begin{array}{r c l}{{\\mathcal{L}}}&{{=}}&{{\\alpha\\cdot\\left(\\beta\\cdot{\\mathcal{L}}_{\\mathrm{CTC}}+(1-\\beta)\\cdot{\\mathcal{L}}_{\\mathrm{KD.CTC}}\\right)}}\\\\ {{}}&{{}}&{{+(1-\\alpha)\\cdot}}\\\\ {{}}&{{}}&{{\\left(\\gamma\\cdot{\\mathcal{L}}_{\\mathrm{Trans}}+(1-\\gamma)\\cdot{\\mathcal{L}}_{\\mathrm{KD.Trans}}\\right)(1}}\\end{array}\n$$  \n\nwhere both $\\beta$ and $\\gamma$ are the hyper-parameters that balance the preference between the teacher distribution and the ground truth.  \n\n# 5 Experiments  \n\n# 5.1 Datasets and Preprocessing  \n\nWe consider restricted and unrestricted settings on speech translation tasks. We run experiments on the LibriSpeech English-French (En-Fr) (Kocabiyikoglu et al., 2018) and MuST-C EnglishGerman (En-De) (Gangi et al., 2019) corpora, which correspond to the low-resource and highresource datasets respectively. Available ASR and MT data is only from the ST data under the restricted setting. For comparison in practical scenarios, the unrestricted setting allows the additional data for ASR and MT models.  \n\nLibriSpeech En-Fr Followed previous work, we use the clean speech translation training set of 100 hours, including 45K utterances and doubled translations of Google Translate. We select the model on the dev set (1,071 utterances) and report results on the test set (2,048 utterances).  \n\nMuST-C En-De MuST-C is a multilingual speech translation corpus extracted from the TED talks. We run the experiments on the English-German speech translation dataset of 400 hours speech with 230K utterances. We select the model on the dev set (1,408 utterances) and report results on the tstCOMMON set (2,641 utterances).  \n\nUnrestricted Setting We use the additional ASR and MT data for pre-training. The 960 hours LibriSpeech ASR corpus is used for the English ASR model. We extract 10M sentences pairs from the WMT14 English-French and 18M sentence pairs from the Opensubtitle $2018^{3}$ English-German translation datasets.  \n\nPreprocessing Followed the preprocessing recipes of ESPnet (Inaguma et al., 2020), we remove the utterances of more than 3,000 frames and augment speech data by speed perturbation with factors of 0.9, 1.0, and 1.1. The 80-channel log-mel fliterbank coefficients with 3-dimensional pitch features are extracted for speech data. We use the lower-cased transcriptions without punctuations. The text is tokenized using the scripts of Moses (Koehn et al., 2007). We learn Byte-Pair Encoding (Sennrich et al., 2016) subword segmentation with 10,000 merge operations based on a shared source and target vocabulary for all datasets.  \n\n# 5.2 Model Settings  \n\nAll experiments are implemented based on the ESPnet toolkit4. We use the Adam optimizer with $\\beta_{1}=0.9$ , $\\beta_{2}=0.997$ and adopt the default learning schedule in ESPnet. We apply dropout with a rate of 0.1 and label smoothing $\\epsilon_{l s}\\;=\\;0.1$ for regularization.  \n\nFor reducing the computational cost, the input speech features are processed by two convolutional layers, which have a stride of $2\\times2$ and downsample the sequence by a factor of 4 (Weiss et al., 2017). The encoder consists of 12 layers for both the ASR and vanilla ST models, and 6 layers for the MT model. The encoder of SATE includes an acoustic encoder of 12 layers and a textual encoder of 6 layers. The decoder consists of 6 layers for all models. The weight of CTC objective $\\alpha$ for multitask learning is set to 0.3 for all ASR and ST models. The coefficients $\\beta$ and $\\gamma$ are set to 0.5 in Eq. (14) for the MTKD method.  \n\nUnder the restricted setting, we employ the Transformer architecture, where each layer comprises 256 hidden units, 4 attention heads, and 2048 feed-forward size. For the unrestricted setting, we use the superior architecture Conformer (Gulati et al., 2020) on the ASR and ST tasks and widen the model by increasing the hidden size to 512 and attention heads to 8. The $\\mathrm{ASR}^{5}$ and MT models pre-train with the additional data and fine-tune the model parameters with the task-specific data.  \n\nDuring inference, we average the model parameters on the best 5 checkpoints based on the performance of the development set. We use beam search with a beam size of 4 for all models. Different from previous work, we report the case-sensitive SacreBLEU6 (Post, 2018) for future standardization comparison across papers.  \n\n# 5.3 Results  \n\nResults on MuST-C En-De Table 2 summaries the experimental results on the MuST-C En-De task. Under the restricted setting, the cascaded ST model translates the output of the ASR model, which degrades the performance compared with the MT model that translates from the reference transcription. The performance of the E2E ST baseline with pre-training is only slightly lower than the cascaded counterpart. SATE outperforms the baseline model significantly. This demonstrates the superiority of stacked acoustic and textual encoding for the speech translation task. Incorporating the pretrained ASR and MT models into SATE releases the encoding burden of the model and achieves a remarkable improvement. The MTKD method provides a strong supervised signal and forces the model to preserve the pre-trained knowledge. Furthermore, we utilize the SpecAugment (Park et al., 2019) which is applied in the input speech features for better generalization and robustness7. It yields a remarkable improvement of 1.9 BLEU points over the cascaded baseline and achieves a new state-ofthe-art performance.  \n\n<html><body><table><thead><tr><td><b>Method</b></td><td><b>Restricted</b></td><td><b>Unrestricted</b></td></tr></thead><tbody><tr><td>ESPnet MT*</td><td>27.63</td></tr><tr><td>ESPnet Cascaded*</td><td>23.65</td></tr><tr><td>MT</td><td>26.9</td><td>31.1</td></tr><tr><td>Cascaded ST</td><td>23.3</td><td>28.1</td></tr><tr><td>ESPnet E2E ST*</td><td>22.33</td></tr><tr><td>E2E ST</td><td>22.1</td><td>23.6</td></tr><tr><td>+Pre-training</td><td>23.1</td><td>25.6</td></tr><tr><td>SATE</td><td>23.3</td><td>23.6</td></tr><tr><td>+Pre-training</td><td>24.1</td><td>27.3</td></tr><tr><td>+MTKD</td><td>24.7</td><td>27.9</td></tr><tr></tr></tbody></table></body></html>\n\nTable 2: BLEU scores $[\\%]$ on the test set of MuST-C En-De corpus. $^*$ : results reported in the ESPnet toolkit.  \n\nUnder the unrestricted setting, the large-scale ASR and MT data is available, whereas the ST data is scarce. This leads to the cascaded method outperforms the vanilla E2E method with a huge margin of 4.5 BLEU points. The pre-training only slightly closes the gap due to the modeling deficiency and representation inconsistency. SATE incorporates the pre-trained models fully, which achieves a significant improvement of 3.7 BLEU points. With the MTKD and SpecAugment methods, we achieve a comparable performance of 28.1 BLEU points. To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable performance with the cascaded counterpart when large-scale ASR and MT data is available.  \n\nResults on LibriSpeech En-Fr Table 3 summaries the experimental results on the LibriSpeech En-Fr task. Different from the MuST-C corpus, it is of small magnitude with clean speech data. This results in that the performance of the vanilla E2E baseline is even better than the cascaded counterpart under the restricted setting. Furthermore, pre-training helps the model achieve an improvement of 0.8 BLEU points over the cascaded baseline. More interestingly, SATE without pre-training outperforms the above methods significantly, even achieves a slight improvement than the MT model. A possible reason is that the diverse acoustic representation is fed to the textual encoder, which improves the robustness of the model. This demonstrates the superiority of our method.  \n\n<html><body><table><thead><tr><td><b>Method</b></td><td><b>Restricted</b></td><td><b>Unrestricted</b></td></tr></thead><tbody><tr><td>ESPnet MT*</td><td>18.09</td><td>一</td></tr><tr><td>ESPnet Cascaded*</td><td>16.96</td><td>一</td></tr><tr><td>MT</td><td>17.5</td><td>21.3</td></tr><tr><td>Cascaded ST</td><td>16.3</td><td>20.6</td></tr><tr><td>ESPnet E2E ST*</td><td>16.22</td><td>一</td></tr><tr><td>E2E ST +Pre-training</td><td>16.7</td><td>17.7</td></tr><tr><td>SATE</td><td>17.1</td><td>20.0</td></tr><tr><td>SATE</td><td>17.6</td><td>18.1</td></tr><tr><td>+Pre-training</td><td>17.4</td><td>20.8</td></tr><tr><td>+MTKD</td><td>17.7</td><td>20.8</td></tr><tr><td>+SpecAug</td><td>18.3</td><td>20.8</td></tr></tbody></table></body></html>\n\nTable 3: BLEU scores $[\\%]$ on the test set of LibriSpeech En-Fr corpus. $^*$ : results reported in the ESPnet toolkit.  \n\nCombining our proposed methods yields a substantial improvement of 2.0 BLEU points over the cascaded baseline. It is a new state-of-the-art result of 18.3 BLEU points. Also, we outperform the cascaded counterpart by 0.2 BLEU points on the unrestricted task.  \n\n# 6 Analysis  \n\n# 6.1 Model Performance vs. Speedup  \n\nIn Table 4, we summarize the performance and inference speedup based on the real time factor (RTF). The vanilla E2E ST model yields an inference speedup of $1.91\\times$ than the cascaded counterpart and demonstrates the low latency of the end-to-end methods. We increase the encoder layers for comparison with SATE under the similar model parameters. However, there is a remarkable gap of 0.5 or 0.6 BLEU points, with or without pre-training.  \n\n<html><body><table><thead><tr><td><b>Method</b></td><td><b>BLEU</b></td><td><b>RTF/Speedup</b></td></tr></thead><tbody><tr><td>Cascaded ST</td><td>23.3</td><td>0.0286/1.00×</td></tr><tr><td>E2E ST +Pre-training</td><td>22.1 23.1</td><td rowspan=\"2\">0.0150/1.91x</td></tr><tr><td>+Pre-training</td><td>23.1</td></tr><tr><td>E2E ST (Enc 18) +Pre-training</td><td>22.8 23.5</td><td rowspan=\"2\">0.0155/1.85x</td></tr><tr><td>23.5</td></tr><tr><td>SATE</td><td>23.3</td><td rowspan=\"4\">0.0169/1.69x</td></tr><tr><td>+Pre-training</td><td>24.1</td></tr><tr><td>+All</td><td>25.2</td></tr><tr><td>+All</td></tr></tbody></table></body></html>  \n\nTable 4: BLEU scores $[\\%]$ and speedup on the test set (2641 utterances) of the MuST-C En-De corpus under the restricted setting. We evaluate the RTF on the NVIDIA V100 GPU with a batch size of 4 for all models.   \n\n\n<html><body><table><thead><tr><td><b>Pre-trained Module</b></td><td><b>MuST-C</b></td><td><b>LibriSpeech</b></td></tr></thead><tbody><tr><td>All</td><td>27.3</td><td>20.8</td></tr><tr><td>-ASR Enc</td><td>24.7</td><td>19.9</td></tr><tr><td>-MT</td><td>25.1</td><td>19.4</td></tr><tr><td>-MT Enc</td><td>25.7</td><td>20.7</td></tr><tr><td>-MT Dec</td><td>25.3</td><td>19.9</td></tr></tbody></table></body></html>\n\nTable 5: Effects of the pre-trained modules on BLEU scores $[\\%]$ under the unrestricted setting. We only remove one pre-trained module in each experiment.  \n\nOur method not only improves the performance of 1.9 BLEU points but also reaches up to $1.69\\times$ speedup than the cascaded baseline. This encourages the application of the end-to-end ST model in practical scenarios.  \n\n# 6.2 Effects of Pre-trained Modules  \n\nThe effects of the pre-trained modules are shown in Table 5. The model performance drops significantly without the pre-trained ASR encoder, especially on the MuST-C corpus that contains noisy speech. The model parameters of pre-trained MT model are updated for adapting the output representation of the random initialized acoustic encoder. This results in the catastrophic forgetting problem (Goodfellow et al., 2015). The effect of the pretrained MT model is more remarkable on the LibriSpeech corpus due to the modeling burden on the translation. The benefit of the pre-trained MT decoder is larger than the MT encoder. This is contrary to the previous conclusions that the MT encoder helps the performance significantly (Li et al., 2020). A possible reason is that the pre-trained  \n\nTable 6: BLEU scores $[\\%]$ of different adaptor setups on the development set under the unrestricted setting.   \n\n\n<html><body><table><thead><tr><td><b>Design</b></td><td><b>MuST-C</b></td><td><b>LibriSpeech</b></td></tr></thead><tbody><tr><td>None</td><td>25.7</td><td>21.7</td></tr><tr><td>Soft</td><td>25.7</td><td>21.9</td></tr><tr><td>Mapping</td><td>26.0</td><td>21.8</td></tr><tr><td>Fusion</td><td>26.4</td><td>21.9</td></tr></tbody></table></body></html>  \n\n![](images/ca901fe626e433f7f0d9cb969dad6744790b0b468dd01c5f4dca014b46da0a6d.jpg)  \nFigure 4: The localness of the vanilla E2E ST model and SATE model with pre-training.  \n\nASR encoder provides a rich representation and acts as part of the MT encoder, this leads to lower performance degradation when the textual encoder trains from scratch.  \n\nEach pre-trained module has a great effect on the final performance. With the complete integration of the pre-trained modules, the model parameters are updated slightly, which preserves the pre-trained knowledge.  \n\n# 6.3 Effects of The Adaptor  \n\nWe show the effects of the adaptor in Table 6. The straight connection which omits the representation inconsistency issue results in the lower benefit of pre-training. Although the soft representation aims at generating the adaptive representation, there is no obvious improvement on the MuST-C corpus. A possible reason is that the noisy speech inputs produce the misalignment probabilities, which disturbs the textual encoding. The mapping method achieves a slight improvement by transforming the acoustic representation to the textual representation. Fusing the soft and mapping representation enriches the information and avoids the representation inconsistency issue, which achieves the best performances.  \n\n# 6.4 Impact on Localness  \n\nWe show the encoder localness of the vanilla E2E ST model and SATE model with pre-training in  \n\nFigure 4. As mentioned above, the vanilla ST model inherits the preference of ASR, which focuses on short-distance dependencies. SATE initializes with the pre-trained ASR and MT encoders, which stacks acoustic and textual encoding. The complementary behaviors of the pre-trained models benefit the translation, that is, the lower layers act like an ASR encoder while the upper layers capture global representation like an MT encoder.  \n\n# 7 Conclusion  \n\nIn this paper, we investigate the difficulty of speech translation and shed light on the reasons why pretraining has been challenging in ST. This inspires us to propose a Stacked Acoustic-and-Textual Encoding method, which is straightforward to incorporate the pre-trained models into ST. We also introduce an adaptor module and a multi-teacher knowledge distillation method for bridging the gap between pre-training and fine-tuning.  \n\nResults on the LibriSpeech and MuST-C corpora demonstrate the superiority of our method. Furthermore, we achieve comparable or even better performance than the cascaded counterpart when large-scale ASR and MT data is available.  \n\n# 8 Acknowledgement  \n\nThis work was supported in part by the National Science Foundation of China (Nos. 61876035 and 61732005), the National Key R&D Program of China (No. 2019QY1801), and the Ministry of Science and Technology of the PRC (Nos. 2019YFF0303002 and 2020AAA0107900). The authors would like to thank anonymous reviewers for their comments.", "appendix": ""}, {"title": "GigaST: A 10,000-hour Pseudo Speech Translation Corpus", "authors": "Rong Ye, Chengqi Zhao, Tom Ko, Chutong Meng, Tao Wang, Mingxuan Wang, Jun Cao", "bibkey": "gigast_a_10000_hour_pseudo_speech_translation_corpus", "bibitem": "@article{ye2022gigast,\n  url = {https://arxiv.org/pdf/2204.03939},\n  title = {GigaST: A 10,000-hour Pseudo Speech Translation Corpus},\n  authors = {Rong Ye, Chengqi Zhao, Tom Ko, Chutong Meng, Tao Wang, Mingxuan Wang, Jun Cao},\n  journal = {INTERSPEECH 2023}\n}", "url": "https://arxiv.org/pdf/2204.03939", "latex_url": null, "latex_path": null, "pdf_url": "https://arxiv.org/pdf/2204.03939", "pdf_path": "output/download_papers/2204.03939v2/2204.03939v2.pdf", "md_url": null, "latex_length": 0, "latex": "", "abstract": "This paper introduces GigaST, a large-scale pseudo speechto-text translation (ST) corpus. We create the corpus by translating the transcript in GigaSpeech, an English ASR corpus, into German and Chinese. The training set is translated by a strong machine translation system and the test set is translated by human. ST models trained with an addition of our corpus obtain new state-of-the-art results on the MuST-C English-German benchmark test set. We provide a detailed description of the translation process and verify its quality. We make the translated text data public and hope to facilitate research in speech translation. Additionally, we also release the training scripts on NeurST1 to make it easy to replicate our systems. GigaST dataset is available at https://st-benchmark.github. io/resources/GigaST.", "abstract_length": 813, "abstract_token": 164, "introduction": "End-to-end speech-to-text translation (ST) directly translates the speech in the source language into sentences in the target language, without outputting the source language transcription [1]. With the success of attention-based models for speech and text-related tasks, a typical and effective baseline model for ST is speech-transformer [2, 3, 4, 5], which has much of the same model structure as the commonly used MT model Transformer [6], except for the pre-processing down-sampling module for speech signals. To train such an end-to-end ST model, a high-quality dataset is important. In general, the more data available, the better the model can be trained. For example, in the MT task, as the bilingual parallel data increases, so does the translation performance. As for the speech translation benchmark dataset MuSTC English-German (En-De), which is currently most widely compared by various models, for example, there are only 234k samples (408 hours), while there are more than 4M parallel text training samples for En-De text translation, and in comparison, they are not even in the same order of magnitude. If the ST training data is also upgraded to the same order of magnitude as the MT data, what will be the translation performance of various end-to-end models? Therefore, in this paper, we try to build a large-scale speechto-text translation dataset. Fortunately, GigaSpeech [7] provided 10,000 hours of English speech-transcription parallel data, which served as the speech source for our datasets. We extend the GigaSpeech ASR dataset to a massive ST corpus – GigaST, up to 25 times as large as the existing open-source dataset, such as MuST-C [8] and TEDx [9]. Specifically, the targetside translations of the training set are obtained by translating the transcription using high-quality MT models. The translations in the test sets are manually annotated and verified one by one, which avoided the alignment errors in the previous MuSTC dataset. The GigaST dataset contains both English-Chinese (En-Zh) and English-German (En-De) translation directions. Using this dataset, we first evaluate the performance of the standard Speech-Transformer models. Then we evaluate the performance of SSL-Transformer models (SSL stands for self-supervised learning) by incorporating pre-trained speech encoders, Wav2vec2 [10] and HuBERT [11]. Results show that increasing the size of the training dataset with GigaST improves the BLEU scores in all test sets.", "introduction_length": 2468, "introduction_token": 523, "reference": "# 5. References  \n\n[1] A. Be´rard, O. Pietquin, C. Servan, and L. Besacier, “Listen and translate: A proof of concept for end-to-end speech-to-text translation,” in NIPS workshop on End-to-end Learning for Speech and Audio Processing, 2016. [2] L. Dong, S. Xu, and B. Xu, “Speech-transformer: A norecurrence sequence-to-sequence model for speech recognition,” in Proc. of ICASSP, 2018, pp. 5884–5888. [3] H. Inaguma, S. Kiyono, K. Duh, S. Karita, N. Yalta, T. Hayashi, and S. Watanabe, “ESPnet-ST: All-in-one speech translation toolkit,” in Proc. of ACL, 2020, pp. 302–311.   \n[4] C. Wang, Y. Tang, X. Ma, A. Wu, D. Okhonko, and J. Pino, “Fairseq s2t: Fast speech-to-text modeling with fairseq,” in Proc. of AACL, 2020, pp. 33–39.   \n[5] C. Zhao, M. Wang, Q. Dong, R. Ye, and L. Li, “NeurST: Neural speech translation toolkit,” in Proc. of ACL - System Demonstrations, Aug. 2021. [6] L. Duong, A. Anastasopoulos, D. Chiang, S. Bird, and T. Cohn, “An attentional model for speech translation without transcription,” in Proc. of NAACL-HLT, 2016, pp. 949–959.   \n[7] G. Chen, S. Chai, G.-B. Wang, J. Du, W.-Q. Zhang, C. Weng, D. Su, D. Povey, J. Trmal, J. Zhang, M. Jin, S. Khudanpur, S. Watanabe, S. Zhao, W. Zou, X. Li, X. Yao, Y. Wang, Z. You, and Z. Yan, “GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio,” in Proc. Interspeech 2021, 2021, pp. 3670–3674. [8] M. A. Di Gangi, R. Cattoni, L. Bentivogli, M. Negri, and M. Turchi, “MuST-C: a Multilingual Speech Translation Corpus,” in Proc. of NAACL-HLT, 2019, pp. 2012–2017.   \n[9] E. Salesky, M. Wiesner, J. Bremerman, R. Cattoni, M. Negri, M. Turchi, D. W. Oard, and M. Post, “The multilingual tedx corpus for speech recognition and translation,” in Proc. of Interspeech, 2021.   \n[10] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A framework for self-supervised learning of speech representations,” in Proc. of NeurIPS, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., 2020.   \n[11] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, “Hubert: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021.   \n[12] F. Akhbardeh, A. Arkhangorodsky, M. Biesialska, O. Bojar, R. Chatterjee, V. Chaudhary, M. R. Costa-jussa, C. Espan˜a-Bonet, A. Fan, C. Federmann, M. Freitag, Y. Graham, R. Grundkiewicz, B. Haddow, L. Harter, K. Heafield, C. Homan, M. Huck, K. Amponsah-Kaakyire, J. Kasai, D. Khashabi, K. Knight, T. Kocmi, P. Koehn, N. Lourie, C. Monz, M. Morishita, M. Nagata, A. Nagesh, T. Nakazawa, M. Negri, S. Pal, A. A. Tapo, M. Turchi, V. Vydrin, and M. Zampieri, “Findings of the 2021 conference on machine translation (WMT21),” in Proceedings of the Sixth Conference on Machine Translation. Association for Computational Linguistics, Nov. 2021, pp. 1–88. [Online]. Available: https://aclanthology.org/2021.wmt-1.1   \n[13] Y. Jia, M. Johnson, W. Macherey, R. J. Weiss, Y. Cao, C.-C. Chiu, N. Ari, S. Laurenzo, and Y. Wu, “Leveraging weakly supervised data to improve end-to-end speech-to-text translation,” in Proc. of ICASSP. IEEE, 2019.   \n[14] K. Kuligowska and B. Kowalczuk, “Pseudo-labeling with transformers for improving question answering systems,” Procedia Computer Science, vol. 192, pp. 1162–1169, 2021, knowledgeBased and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021. [Online]. Available: https://www.sciencedirect.com/science/article/ pii/S1877050921016082   \n[15] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proc. of NeurIPS, I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, Eds., 2017, pp. 5998–6008.   \n[16] B. Li, Y. Li, C. Xu, Y. Lin, J. Liu, H. Liu, Z. Wang, Y. Zhang, N. Xu, Z. Wang, K. Feng, H. Chen, T. Liu, Y. Li, Q. Wang, T. Xiao, and J. Zhu, “The NiuTrans machine translation systems for WMT19,” in Proceedings of the Fourth Conference on Machine Translation, 2019.   \n[17] L. Wu, X. Pan, Z. Lin, Y. Zhu, M. Wang, and L. Li, “The volctrans machine translation system for wmt20,” in Proceedings of the Fifth Conference on Machine Translation (Volume 2: Shared Task Papers), Nov. 2020.   \n[18] Y. Kim and A. M. Rush, “Sequence-level knowledge distillation,” in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, 2016.   \n[19] M. Freitag, Y. Al-Onaizan, and B. Sankaran, “Ensemble distillation for neural machine translation,” CoRR, vol. abs/1702.01802, 2017.   \n[20] R. Sennrich, B. Haddow, and A. Birch, “Improving neural machine translation models with monolingual data,” in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, 2016.   \n[21] R. Bawden, R. Sennrich, A. Birch, and B. Haddow, “Evaluating discourse phenomena in neural machine translation,” in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). New Orleans, Louisiana: Association for Computational Linguistics, Jun. 2018, pp. 1304– 1313. [Online]. Available: https://aclanthology.org/N18-1118   \n[22] S. La¨ubli, R. Sennrich, and M. Volk, “Has machine translation achieved human parity? a case for document-level evaluation,” in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Brussels, Belgium: Association for Computational Linguistics, 2018, pp. 4791–4796. [Online]. Available: https://aclanthology.org/D18-1512   \n[23] Z. Sun, M. Wang, H. Zhou, C. Zhao, S. Huang, J. Chen, and L. Li, “Capturing longer context for document-level neural machine translation: A multi-resolutional approach,” CoRR, vol. abs/2010.08961, 2020.   \n[24] F. Akhbardeh, A. Arkhangorodsky, M. Biesialska, O. Bojar, R. Chatterjee, V. Chaudhary, M. R. Costa-jussa, C. Espan˜a-Bonet, A. Fan, C. Federmann, M. Freitag, Y. Graham, R. Grundkiewicz, B. Haddow, L. Harter, K. Heafield, C. Homan, M. Huck, K. Amponsah-Kaakyire, J. Kasai, D. Khashabi, K. Knight, T. Kocmi, P. Koehn, N. Lourie, C. Monz, M. Morishita, M. Nagata, A. Nagesh, T. Nakazawa, M. Negri, S. Pal, A. A. Tapo, M. Turchi, V. Vydrin, and M. Zampieri, “Findings of the 2021 conference on machine translation (WMT21),” in Proceedings of the Sixth Conference on Machine Translation. Online: Association for Computational Linguistics, Nov. 2021, pp. 1–88. [Online]. Available: https://aclanthology.org/2021.wmt-1.1   \n[25] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of rare words with subword units,” in Proc. of ACL, 2016, pp. 1715–1725.   \n[26] Y. Liu, H. Xiong, J. Zhang, Z. He, H. Wu, H. Wang, and C. Zong, “End-to-end speech translation with knowledge distillation,” in Proc. of INTERSPEECH, G. Kubin and Z. Kacic, Eds., 2019, pp. 1128–1132.   \n[27] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, “Specaugment: A simple data augmentation method for automatic speech recognition,” in Proc. of INTERSPEECH, 2019.   \n[28] S.-w. Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin et al., “Superb: Speech processing universal performance benchmark,” in Proc. of INTERSPEECH, 2021.", "reference_length": 7449, "reference_token": 2633, "txt_length": 24221, "txt_token": 7078, "txt": "# GigaST: A 10,000-hour Pseudo Speech Translation Corpus  \n\ne\\*, Chengqi Zhao\\* , Tom Ko, Chutong Meng† , Tao Wang, Mingxuan Wang, Jun Ca  \n\nByteDance  \n\n{yerong, zhaochengqi.d, tom.ko, wangtao.960826, wangmingxuan.89, caojun.sh}@bytedance.com, mengct00@gmail.com  \n\n# Abstract  \n\nThis paper introduces GigaST, a large-scale pseudo speechto-text translation (ST) corpus. We create the corpus by translating the transcript in GigaSpeech, an English ASR corpus, into German and Chinese. The training set is translated by a strong machine translation system and the test set is translated by human. ST models trained with an addition of our corpus obtain new state-of-the-art results on the MuST-C English-German benchmark test set. We provide a detailed description of the translation process and verify its quality. We make the translated text data public and hope to facilitate research in speech translation. Additionally, we also release the training scripts on NeurST1 to make it easy to replicate our systems. GigaST dataset is available at https://st-benchmark.github. io/resources/GigaST.  \n\n# 1. Introduction  \n\nEnd-to-end speech-to-text translation (ST) directly translates the speech in the source language into sentences in the target language, without outputting the source language transcription [1]. With the success of attention-based models for speech and text-related tasks, a typical and effective baseline model for ST is speech-transformer [2, 3, 4, 5], which has much of the same model structure as the commonly used MT model Transformer [6], except for the pre-processing down-sampling module for speech signals.  \n\nTo train such an end-to-end ST model, a high-quality dataset is important. In general, the more data available, the better the model can be trained. For example, in the MT task, as the bilingual parallel data increases, so does the translation performance. As for the speech translation benchmark dataset MuSTC English-German (En-De), which is currently most widely compared by various models, for example, there are only 234k samples (408 hours), while there are more than 4M parallel text training samples for En-De text translation, and in comparison, they are not even in the same order of magnitude. If the ST training data is also upgraded to the same order of magnitude as the MT data, what will be the translation performance of various end-to-end models?  \n\nTherefore, in this paper, we try to build a large-scale speechto-text translation dataset. Fortunately, GigaSpeech [7] provided 10,000 hours of English speech-transcription parallel data, which served as the speech source for our datasets. We extend the GigaSpeech ASR dataset to a massive ST corpus – GigaST, up to 25 times as large as the existing open-source dataset, such as MuST-C [8] and TEDx [9]. Specifically, the targetside translations of the training set are obtained by translating the transcription using high-quality MT models. The translations in the test sets are manually annotated and verified one by one, which avoided the alignment errors in the previous MuSTC dataset. The GigaST dataset contains both English-Chinese (En-Zh) and English-German (En-De) translation directions.  \n\nUsing this dataset, we first evaluate the performance of the standard Speech-Transformer models. Then we evaluate the performance of SSL-Transformer models (SSL stands for self-supervised learning) by incorporating pre-trained speech encoders, Wav2vec2 [10] and HuBERT [11]. Results show that increasing the size of the training dataset with GigaST improves the BLEU scores in all test sets.  \n\n# 2. Dataset Creation  \n\nThis section describes our method of creating a pseudo ST corpus from an existing ASR corpus. Pseudo-labeling has been proven effective in various machine learning tasks [12, 13, 14].  \n\n# 2.1. Training Set  \n\nWe start from GigaSpeech [7], a multi-domain English speech recognition corpus with 10,000 hours of labeled audio, and create paired text-to-text translation with an MT model. To obtain high-quality pseudo labels, we train deep transformer-based machine translation models [15], with 24 layers of the encoder and 6 layers of the decoder. The training data for MT consists of WMT2021 2 and CCMatrix, CCAlign and OpenSubtitles portions from OPUS 3. We follow the data filtering and pre-processing methods described in [16, 17], and utilize iterative sequence-level knowledge distillation [18, 19] and back translation [20] techniques to improve the performance of MT.  \n\nThe utterances of the original GigaSpeech corpus are segmented from long audio recordings. The discourse phenomena, such as pronominal anaphora and lexical consistency will be neglected by sentence-level MT systems, which makes it far worse than human translations [21, 22]. Therefore we apply multi-resolution training [23] to build a context-aware MT system. Specifically, we first concatenate each segment with its previous one and insert a special token [SEP] as the segment separator. Then the generated translation is split according to the [SEP] token.  \n\nSince our training dataset is artifacted from GigaSpeech ASR corpus based on the MT system described above, we want to verify how good the MT system is and whether it is close to the real translations of translators. To this end, we perform a comparison in terms of both automated metrics evaluation and human evaluation.  \n\nTable 1 illustrates the automated metrics evaluation (in BLEU) of our MT models for En-Zh and En-De directions on newstest2021 set, where Online-B/W and Online-W/A are top2 online systems individually for En-Zh and En-De reported by [24]. It shows that our MT model can obtain state-of-the-art performance and is comparable with online systems.  \n\nTable 1: BLEU scores of our MT models versus online MT systems on newstest2021 test set.   \n\n\n<html><body><table><thead><tr><td><b>Direction</b></td><td><b>System</b></td><td><b>BLEU</b></td></tr></thead><tbody><tr><td rowspan=\"3\">En-Zh</td><td>Online-B</td><td>48.5</td></tr><tr><td>Online-W</td><td>44.8</td></tr><tr><td>Our model</td><td>47.3</td></tr><tr><td rowspan=\"3\">En-De</td><td>Online-W</td><td>51.0</td></tr><tr><td>Online-A</td><td>47.6</td></tr><tr><td>Our model</td><td>49.8</td></tr></tbody></table></body></html>  \n\nFurthermore, we conduct the human evaluation on the pseudo labels to verify the quality. First, we sample 30 audios from the training set and randomly select 1 to 20 continuous segments from each audio, with a total of 320 unique segments. Then 2 professional translators separately produce Chinese and German translations for this evaluation set, which we take as the ground truth. And another 6 evaluators (3 for En-Zh and 3 for En-De) are asked to rate the translations of both human and MT systems from 0 to 6. A rating of 6 indicates that the expression is fluent and the meaning of the translation is faithful to the source without any grammar errors, while a rating of 0 means the translation is incomprehensible and full of errors. Table 2 lists the averaged scores from the evaluators. The En-Zh MT system gets a rating of 4.14 and for En-De it is 4.82. The above 4 rating means our generated translations are semantically consistent with the source texts. The weakness mainly comes from fluency problems, such as unidiomatic word translations, informal expressions, and function word errors. Moreover, the rating of En-De MT (4.82) is close to that of human translations (5.06), which further verifies the quality of our produced training set.  \n\nTable 2: Human evaluation results on the translations by human translators and machine translation models.   \n\n\n<html><body><table><thead><tr><td><b>Direction</b></td><td><b>Human</b></td><td><b>MT</b></td></tr></thead><tbody><tr><td>En-Zh</td><td>4.92</td><td>4.14</td></tr><tr><td>En-De</td><td>5.06</td><td>4.82</td></tr></tbody></table></body></html>  \n\n# 2.2. Test Sets  \n\nWe provide En-Zh and En-De test sets in GigaST. The En-Zh test set contains the translation of all GigaSpeech test utterances while the En-De test set contains a subset of it for this current release. The test sets are produced by human translators looking at the transcriptions. A small number of transcriptions are difficult to understand due to the lack of context and are ignored in our test sets.  \n\nThe statistics of GigaST are listed in Table 3. Note that nonspeech segments, such as music and noise, are not included in our statistics. The #tokens is counted in character and word for En-Zh and En-De respectively.  \n\nTable 3: The statistics of GigaST   \n\n\n<html><body><table><thead><tr><td><b>Direction</b></td><td><b>|Subset</b></td><td><b>#seg.</b></td><td><b>#hours</b></td><td><b>#tokens</b></td></tr></thead><tbody><tr><td rowspan=\"5\">En-Zh</td><td>S</td><td>210,012</td><td>243.1</td><td>4.1M</td></tr><tr><td>M</td><td>835,846</td><td>974.3</td><td>16.7M</td></tr><tr><td>L</td><td>2,084,274</td><td>2,337.7</td><td>41.8M</td></tr><tr><td>XL</td><td>7,650,889</td><td>9,780.8</td><td>168.3M</td></tr><tr><td>Test</td><td>19,888</td><td>35.3</td><td>0.6M</td></tr><tr><td rowspan=\"5\">En-De</td><td>S</td><td>221,572</td><td>256.2</td><td>2.5M</td></tr><tr><td>M</td><td>868,316</td><td>1,013.1</td><td>10.2M</td></tr><tr><td>L</td><td>2,147,471</td><td>2,510.9</td><td>25.3M</td></tr><tr><td>XL</td><td>7,815,436</td><td>9,997.9</td><td>101.6M</td></tr><tr><td>Test</td><td>4,163</td><td>7.1</td><td>0.7M</td></tr></tbody></table></body></html>  \n\n# 3. Experiment  \n\nIn this section, we conduct ST experiments with speech transformer models and SSL-Transformer models. All models are implemented using NeurST [5].  \n\n# 3.1. Setups  \n\nPreprocessing and Filtering For speech transformer models, we extract 80-channel log-Mel filterbank coefficients (fbank) of the audio, with windows of $25\\mathrm{ms}$ and steps of 10ms, and then apply CMVN (cepstral mean and variance normalization). The SSL-Transformer models use raw wave signals as the input. For the text side, words are encoded in subword-level. In detail, we lowercase the English transcriptions, remove all punctuations and use SentencePiece4 with a vocabulary of 15,000. For Chinese text, we first segment sentences by Jieba5, and then apply Byte-Pair Encoding (BPE)6[25] with 32,000 merge operations. German texts are first tokenized using Moses tokenizer, followed by BPE with 32,000 merge operations. During training, we truncate the audio to 30 seconds for GPU memory efficiency, that is, 480,000 for raw wave signals or 3,000 fbank frames. We remove training samples whose translation text longer than 120 tokens or the percentage of aligned words to the original English transcription is less than $40\\%$ (produced by fast-align toolkit).  \n\nTraining For En-Zh, we use GigaST as the training set and use TED dev2010 and $\\mathsf{t}_{\\mathsf{S}}\\mathsf{t}_{2}\\mathsf{0}\\,\\mathsf{1}\\,\\mathsf{5}$ [26] as the validation set. For En-De, MuST-C is added to the training set and we use MuST-C dev set for validation.  \n\nEvaluation Apart from the GigaST test set described in Section 2.22.2, for both language directions, we add two additional test sets: an in-house test set containing a total of 8.5 hours of news and tech talks (3,917 sentences) for En-Zh and MuSTC tst-COMMON set for En-De. The metric we use is casesensitive detokenized BLEU8.  \n\n# 3.2. End-to-end Models  \n\nWe compare various end-to-end ST models of different sizes.  \n\nTable 4: En-Zh BLEU scores. S, M, L and XL stand for training data of various scales. The test set includes GigaST test set as created in Section 2.2 and the in-house test set.   \n\n\n<html><body><table><thead><tr><td rowspan=\"2\" colspan=\"2\"><b></b></td><td rowspan=\"2\"><b>|# params</b></td><td colspan=\"4\"><b>GigaST Test</b></td><td colspan=\"4\"><b>In-house Test</b></td></tr><tr><td><b>S</b></td><td><b>M</b></td><td><b>L</b></td><td><b>XL</b></td><td><b>S</b></td><td><b>M</b></td><td><b>L</b></td><td><b>XL</b></td></tr></thead><tbody><tr><td rowspan=\"3\">Speech- Transformer</td><td>S-Transf-S</td><td>37 M</td><td>24.2</td><td>28.8</td><td>29.8</td><td>29.9</td><td>20.1</td><td>24.6</td><td>25.5</td><td>25.9</td></tr><tr><td>S-Transf-M</td><td>90 M</td><td>23.3</td><td>31.4</td><td>33</td><td>33.6</td><td>19.4</td><td>26.6</td><td>28.5</td><td>29.4</td></tr><tr><td>S-Transf-L</td><td>322 M</td><td>21.2</td><td>31.4</td><td>35</td><td>36.3</td><td>17.8</td><td>26.7</td><td>30.2</td><td>31.5</td></tr><tr><td rowspan=\"4\">SSL- Transformer</td><td>w2v2-base</td><td>359 M</td><td>27.2</td><td>32.1</td><td>34.2</td><td>37.4</td><td>22.1</td><td>27.0</td><td>28.8</td><td>32.1</td></tr><tr><td>w2v2-large</td><td>581 M</td><td>27.0</td><td>31.6</td><td>33.9</td><td>36.9</td><td>19.8</td><td>24.5</td><td>26.9</td><td>30.4</td></tr><tr><td>hubert-base</td><td>359 M</td><td>27.6</td><td>31.8</td><td>34.0</td><td>37.2</td><td>22.5</td><td>25.9</td><td>29.3</td><td>32.1</td></tr><tr><td>hubert-large</td><td>581 M</td><td>30.1</td><td>33.4</td><td>35.6</td><td>38.0</td><td>24.4</td><td>27.9</td><td>30.3</td><td>32.5</td></tr></tbody></table></body></html>  \n\n<html><body><table><thead><tr><td rowspan=\"3\" colspan=\"2\"><b></b></td><td rowspan=\"3\"><b>|# params</b></td><td colspan=\"4\"><b>GigaST Test</b></td><td colspan=\"4\"><b>MuST-C tst-COM</b></td></tr><tr><td><b>S</b></td><td><b>M</b></td><td><b>L</b></td><td><b>XL</b></td><td><b>S</b></td><td><b>M</b></td><td><b>L</b></td><td><b>XL</b></td></tr></thead><tbody><tr><td rowspan=\"3\"> Speech- Transformer</td><td>S-Transf-S</td><td>35 M</td><td>21.5</td><td>24.1</td><td>25.1</td><td>25.6</td><td>24.4</td><td>25.7</td><td>25.8</td><td>25.1</td></tr><tr><td>S-TransfM</td><td>87M</td><td>22.7</td><td>27.3</td><td>28.8</td><td>29.6</td><td>24.9</td><td>27.6</td><td>28.2</td><td>28.4</td></tr><tr><td>S-Transf-L</td><td>315 M</td><td>21.3</td><td>27.5</td><td>31.1</td><td>32.6</td><td>23.2</td><td>27.8</td><td>29.6</td><td>30.1</td></tr><tr><td rowspan=\"4\">SSL- Transformer</td><td>w2v2-base</td><td>359 M</td><td>24.3</td><td>28.0</td><td>32.1</td><td>33.4</td><td>26.5</td><td>28.0</td><td>29.9</td><td>30.3</td></tr><tr><td>w2v2-large</td><td>581 M</td><td>23.6</td><td>28.5</td><td>28.5</td><td>33.0</td><td>23.4</td><td>26.9</td><td>27.0</td><td>30.2</td></tr><tr><td>hubert-base</td><td>359 M</td><td>24.1</td><td>28.3</td><td>30.2</td><td>33.7</td><td>24.9</td><td>27.7</td><td>29.2</td><td>30.5</td></tr><tr><td>hubert-large</td><td>581 M</td><td>27.1</td><td>30.6</td><td>31.8</td><td>33.5</td><td>27.7</td><td>29.6</td><td>30.1</td><td>30.6</td></tr></tbody></table></body></html>  \n\nSpeech-Transformer [2] is our benchmark model. The model uses fbank features as the input, and stacks two $3\\times3$ CNN layers with stride size 2 and a transformer encoder-decoder module. Specifically, we implement three different model sizes, namely S-Transf S, S-Transf M, S-Transf L, with model hyper-parameters listed in Table 6. We follow the setup of the optimizer and the learning rate schedule as in [5], as well as using ASR pre-training and SpecAugment [27].  \n\nTable 6: Hyper-parameters for Speech-Transformer models   \n\n\n<html><body><table><thead><tr><td></td><td><b>S</b></td><td><b>M</b></td><td><b>L</b></td></tr></thead><tbody><tr><td>Hidden Size</td><td>256</td><td>512</td><td>1024</td></tr><tr><td>Filter Size</td><td>2048</td><td>2048</td><td>4096</td></tr><tr><td>Attention Heads</td><td>4</td><td>8</td><td>16</td></tr><tr><td>Encoder Layers</td><td>12</td><td>12</td><td>12</td></tr><tr><td>Decoder Layers</td><td>6</td><td>6</td><td>6</td></tr></tbody></table></body></html>  \n\nSSL-Transformer As recent research on self-supervised learning (SSL) in speech has intensified, pre-trained speech encoders, such as Wav2vec2 [10] and HuBERT [11], have been applied to downstream tasks instead of spectral features [28]. We can adopt a similar idea for the ST task. We evaluate SSL-Transformer, which replace Fbank features in SpeechTransformer with representations extracted from pre-trained speech encoders. To reduce the sequence length, we add two layers of convolutional subsampler with stride $^{=2}$ after the SSL module, and apply the Transformer encoder-decoder as the downstream module. For the SSL speech encoders, we try four of them which performed well on the SUPERB leaderboard9, namely w2v2-base, w2v2-large, hubert-base, and hubert-large. For downstream Transformer, we use the same hyperparameter as S-Trans L, with 6 layers of encoder and decoder, $d_{\\mathrm{model}}=1024$ , $d_{\\mathrm{ff}}=4096$ , $d_{\\mathrm{head}}=8$ . Combining the above modules, we get four models with model parameter sizes of 359M, 581M, 359M, and 581M, respectively. For the subsequent model notations, since the structures of CNN and Transformer modules are the same, for simplicity, we only use the name of the speech encoders as the name of the whole model. The raw waveform of the entire speech is fed into the model, and SSL modules are NOT frozen during training. We set warmup steps at 25, 000 and peak learning rate at $2e^{-4}$ .  \n\nResults The BLEU scores of the En-Zh and En-De test sets are shown in Tables 4 and 5 for different models trained with varying training sets. It is obvious that, for every models, the performance improves as the training data size increases. And model capacity often determines how good the translation is. In general, with the same amount of training data, the larger the model size, the better the performance. It is interesting to note that S-Transf S, with only 35M parameters, fails to improve substantially as the data size increases from 1k to 10k hours. On the other hand, large models tend to underfit when the training set is small. For example, when training with the GigaST S subset, S-transf L performs worse than S-transf S.  \n\n# 3.3. Cascade Systems  \n\nBy concatenating the ASR models and the MT models, we obtain cascade systems. Specifically, our ASR model has the same structure as S-transf L and we get 11.8 WER on the GigaSpeech test set, which is on par with other ASR systems on the GigaSpeech leaderboard10. For the MT model, we provide two models, one is Transformer-large trained using GigaSpeech transcription-translation bilingual text, noted as constrained, and the other is the MT model introduced in Section 2.1, noted as unconstrained. The former is for a fair comparison between the cascade systems and the end-to-end models with the same training data, while the latter has a stronger translation performance with BLEU scores higher than 40.  \n\nTable 7 shows the performances of the cascade systems. When the models are trained with the same amount of data, the performance of different end-to-end models are better than those of the cascade systems. However, the unconstrained cascade models, boosted by the larger amount of text training data, have a higher quality of text translation, which leads to better speech translation than the end-to-end models. Now with a powerful MT model, the gap in BLEU between end-to-end and cascade models is around 2. We still have room to improve the end-to-end performance through pre-training of decoder, multitask training and other techniques. We leave these for future work.  \n\nTable 7: The BLEU scores of cascade models on GigaST test sets   \n\n\n<html><body><table><thead><tr><td rowspan=\"2\"><b>Direction</b></td><td colspan=\"2\"><b>MT</b></td><td rowspan=\"2\"><b>ST BLEU</b></td></tr><tr><td><b>Condition</b></td><td><b>BLEU</b></td></tr></thead><tbody><tr><td rowspan=\"2\">En-Zh</td><td> constrained</td><td>24.9</td><td>22.3</td></tr><tr><td>unconstrained</td><td>44.3</td><td>39.8</td></tr><tr><td rowspan=\"2\">En-De</td><td> constrained</td><td>37.6</td><td>33.4</td></tr><tr><td>unconstrained</td><td>42.2</td><td>35.7</td></tr></tbody></table></body></html>  \n\n# 3.4. Analysis on Speech Representations  \n\nIn addition to the results of the baseline Speech-Transformers, we also investigate the performance of pre-trained speech encoders under large-scale speech translation. Are these pretrained encoders still effective and helpful under large amounts of training data?  \n\nBefore answering this question, we need to figure out: how should these pre-trained speech encoders be incorporated into the training? Should they be frozen with parameters not updated, or should the whole model be fine-tuned based on ST supervised data? Taking two pre-trained encoders, hubert-base and hubert-large, as examples, Table 8 shows the results of En-Zh translation with and without the encoder frozen. It shows that with both base and large models, the frozen case perform much worse than the unfrozen case, with an average difference of as much as 2.6 BLEU. This differs from the practice of freezing speech encoders in the downstream ASR task, where freezing these encoders can still yield a word error rate of as low as 3.4 on LibriSpeech [28], but in ST tasks, we recommend fine-tuning the speech encoders together with other components, which can greatly improve ST performance. Therefore, we conduct the rest of our experiments without freezing the speech encoders. Detailed setups are introduced in Section 3.2.  \n\nThe results of En-Zh and En-De experiments are summarized in Tables 4 and 5. It can be seen that SSL-Transformer models are generally better than the Speech-Transformer models. Even though S-Transf L and w2v2-base have roughly the same order of magnitude of parameters ( $300\\mathrm{m}$ parameters), w2v2-base outperforms S-Transf L. When the training data size is increased to 10,000 hours, according to the BLEU scores throughout the XL columns, the pre-trained speech module continues to play an essential role in improving ST performance. In addition, looking at the performances of the models trained with the S subset (only 250 hours), pre-trained speech encoders are particularly useful. Take En-Zh translation as an example (Table 4), using hubert-Large to train 250 hours of speech, the translation performance can reach or even exceed S-Transf S training on 10,000 hours. Meanwhile, as the data size increases, we find that the gain from the pre-trained speech encoders for the downstream ST task becomes smaller. Despite the reduced benefit, we observe that pre-trained speech encoders are still useful in our setup.  \n\nTable 8: To freeze or not to freeze the SSL speech encoders? hubert-base and hubert-large were finetuned with GigaST XL subset and tested on the En-Zh test sets.   \n\n\n<html><body><table><thead><tr><td><b>SSL repr.</b></td><td><b>freeze?</b></td><td><b>GigaST</b></td><td><b>In-house</b></td><td><b>Avg.</b></td></tr></thead><tbody><tr><td rowspan=\"2\">hubert-base</td><td>√</td><td>34.4</td><td>29.7</td><td>32.1</td></tr><tr><td>×</td><td>37.2</td><td>32.1</td><td>34.7</td></tr><tr><td rowspan=\"2\">hubert-large</td><td>√</td><td>35.3</td><td>30.0</td><td>32.7</td></tr><tr><td>×</td><td>38.0</td><td>32.5</td><td>35.3</td></tr></tbody></table></body></html>  \n\nOn the other hand, there are performance differences between the four pre-trained speech encoders. Analyzing the performance of En-Zh and En-De translations comprehensively, the rank among the four is hubert-large, hubert-base, w2v2-base, and $\\mathtt{w2v2-1a r g e}$ . It is surprising to see that w2v2-large (581M parameters) performs worse than the base models (359M parameters), where it violates the common belief that larger model capacity means more representation capability and with better performance. Overall, HuBERT models empirically perform better than Wav2vec2 models in our setup.  \n\n# 4. Conclusion  \n\nThis paper presents GigaST, a new speech-to-text corpus suitable for training and evaluating ST systems. Our corpus is created by translating the transcript in GigaSpeech, which is one of the largest open-source English ASR corpora. We have demonstrated that models trained with an addition of our corpus can obtain new state-of-the-art results on the MuST-C EnglishGerman benchmark test set. We also establish new benchmark test sets for the two language directions. We release all the training data, human-translated test sets and our training scripts so that others can easily replicate our results. We believe our released dataset will open new avenues in speech translation research.", "appendix": ""}, {"title": "Revisiting End-to-End Speech-to-Text Translation From Scratch", "authors": "Biao Zhang, Barry Haddow, Rico Sennrich", "bibkey": "revisiting_end_to_end_speech_to_text_translation_from_scratch", "bibitem": "@article{Zhang_ICML2022,\n  url = {http://arxiv.org/abs/2206.04571v1},\n  title = {Revisiting End-to-End Speech-to-Text Translation From Scratch},\n  authors = {Biao Zhang, Barry Haddow, Rico Sennrich},\n  abstract = {End-to-end (E2E) speech-to-text translation (ST) often depends on pretraining its encoder and/or decoder using source transcripts via speech recognition or text translation tasks, without which translation performance drops substantially. However, transcripts are not always available, and how significant such pretraining is for E2E ST has rarely been studied in the literature. In this paper, we revisit this question and explore the extent to which the quality of E2E ST trained on speech-translation pairs alone can be improved. We reexamine several techniques proven beneficial to ST previously, and offer a set of best practices that biases a Transformer-based E2E ST system toward training from scratch. Besides, we propose parameterized distance penalty to facilitate the modeling of locality in the self-attention model for speech. On four benchmarks covering 23 languages, our experiments show that, without using any transcripts or pretraining, the proposed system reaches and even outperforms previous studies adopting pretraining, although the gap remains in (extremely) low-resource settings. Finally, we discuss neural acoustic feature modeling, where a neural model is designed to extract acoustic features from raw speech signals directly, with the goal to simplify inductive biases and add freedom to the model in describing speech. For the first time, we demonstrate its feasibility and show encouraging results on ST tasks.},\n  arxiv_id = {2206.04571v1},\n  subject = {cs.CL},\n  submission_date = {2022-06-09T15:39:19Z}\n}", "url": "http://arxiv.org/abs/2206.04571v1", "latex_url": "http://arxiv.org/src/2206.04571v1", "latex_path": "output/download_papers/2206.04571v1/2206.04571v1", "pdf_url": "http://arxiv.org/pdf/2206.04571v1", "pdf_path": "output/download_papers/2206.04571v1/2206.04571v1.pdf", "md_url": null, "latex_length": 0, "latex": "", "abstract": "End-to-end (E2E) speech-to-text translation (ST) often depends on pretraining its encoder and/or decoder using source transcripts via speech recognition or text translation tasks, without which translation performance drops substantially. However, transcripts are not always available, and how significant such pretraining is for E2E ST has rarely been studied in the literature. In this paper, we revisit this question and explore the extent to which the quality of E2E ST trained on speech-translation pairs alone can be improved. We reexamine several techniques proven beneficial to ST previously, and offer a set of best practices that biases a Transformer-based E2E ST system toward training from scratch. Besides, we propose parameterized distance penalty to facilitate the modeling of locality in the self-attention model for speech. On four benchmarks covering 23 languages, our experiments show that, without using any transcripts or pretraining, the proposed system reaches and even outperforms previous studies adopting pretraining, although the gap remains in (extremely) low-resource settings. Finally, we discuss neural acoustic feature modeling, where a neural model is designed to extract acoustic features from raw speech signals directly, with the goal to simplify inductive biases and add freedom to the model in describing speech. For the first time, we demonstrate its feasibility and show encouraging results on ST tasks.", "abstract_length": 1443, "abstract_token": 269, "introduction": "End-to-end (E2E) speech-to-text translation (ST) is the task of translating a source-language audio directly to a foreign text without any intermediate outputs (Duong et al., 2016; B´erard et al., 2016), which has gained increasing popularity and obtained great success recently (Sung et al., 2019; Salesky et al., 2019; Zhang et al., 2020; Chen et al., 2020; Han et al., 2021; Zheng et al., 2021; Anastasopoulos et al., 2021). Different from the traditional cascading method which decomposes ST into two sub-tasks – automatic speech recognition (ASR) for transcription and machine translation (MT) for translation, E2E ST jointly handles them in a single, large neural network. This endows E2E ST with special advantages on reducing translation latency and bypassing transcription mistakes made by ASR models, making it theoretically attractive. However, directly modeling speech-to-text mapping is nontrivial. The translation alignment between speech and text is no longer subject to the monotonic assumption. Also, the high variation of speech increases the modeling difficulty. Therefore, rather than training E2E ST models from scratch, researchers often resort to pipeline-based training with auxiliary tasks utilizing source transcripts, which first pretrains the speech encoder on ASR data and/or the text decoder on MT data followed by a finetuning on ST data. Such pretraining was reported to greatly improve translation quality (Di Gangi et al., 2019; Wang et al., 2019a; Zhang et al., 2020; Xu et al., 2021), and has become the de-facto standard in recent ST studies and toolkits (Inaguma et al., 2020; Wang et al., 2020a; Zhao et al., 2021; Zheng et al., 2021). Despite these successes, nevertheless, how significant the pretraining is for E2E ST and how far we can go using speech-translation pairs alone are still open questions. In this paper, we aim at exploring the extent to which the quality of ST models trained from scratch can be improved, whether the performance gap against pretraining-based ST can be narrowed, and also when the pretraining really matters.23 We argue that the inferior performance of ST from scratch is mainly a result of the dominance of pretraining, and consequent lack of focus on optimizing E2E ST models trained from scratch. To test this hypothesis, we investigate methods to bias a Transformer-based E2E ST model (Vaswani et al., 2017) towards training from scratch. We summarize a set of best practices for our setup by revisiting several existing techniques that have been proven useful to ST previously. We further introduce two proposals to add freedom to Transformer to model speech with the hope of gaining translation quality: 1) a parameterized distance penalty that facilitates self-attention to capture local dependencies of speech; and 2) neural acoustic feature modeling providing a trainable alternative to the heuristic rule-based acoustic feature extraction. To examine the generality of our methods, we conducted (bilingual) experiments on four speech translation benchmarks, including MuST-C, Covost2, LibriSpeech, and Kosp2e, which cover 23 languages of different families with varying training data sizes. Experimental results show that the significance of pretraining has been over-estimated in prior work, and integrating techniques to improve E2E ST from scratch is feasible and promising. Our main findings: • With proper adaptation, E2E ST trained from scratch only on speech-translation pairs can match or even surpass previous studies using ASR/MT pretraining on source transcripts. • Pretraining still matters, mainly in (extremely) lowresource regimes and when large-scale external ASR or MT corpora are available. • We present a set of best practices for E2E ST from scratch, including smaller vocabulary size, wider feedforward layer, deep speech encoder with the post-LN (layer normalization) structure, Connectionist Temporal Classification (CTC)-based regularization using translation as the target, and a novel parameterized distance penalty. • We demonstrate that dropping heuristic rule-based acoustic features is feasible, and that neural acoustic features can be learned in an end-to-end ST framework.", "introduction_length": 4189, "introduction_token": 895, "reference": "# References  \n\nAnastasopoulos, A. and Chiang, D. Tied multitask learning for neural speech translation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 82–91, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/ v1/N18-1008. URL https://www.aclweb.org/ anthology/N18-1008.  \n\nAnastasopoulos, A., Bojar, O., Bremerman, J., Cattoni, R., Elbayad, M., Federico, M., Ma, X., Nakamura, S., Negri, M., Niehues, J., Pino, J., Salesky, E., St¨uker, S., Sudoh, K., Turchi, M., Waibel, A., Wang, C., and Wiesner, M. FINDINGS OF THE IWSLT 2021 EVALUATION CAMPAIGN. In Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021), pp. 1–29, Bangkok, Thailand (online), August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.iwslt-1.1. URL https: //aclanthology.org/2021.iwslt-1.1.  \n\nArdila, R., Branson, M., Davis, K., Kohler, M., Meyer, J., Henretty, M., Morais, R., Saunders, L., Tyers, F., and Weber, G. Common voice: A massively-multilingual speech corpus. In Proceedings of the 12th Language Resources and Evaluation Conference, pp. 4218–4222, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https: //aclanthology.org/2020.lrec-1.520.  \n\nBahar, P., Bieschke, T., and Ney, H. A comparative study on end-to-end speech to text translation. In 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 792–799, 2019. doi: 10.1109/ASRU46091. 2019.9003774.  \n\nBahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7- 9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1409.0473.  \n\nBansal, S., Kamper, H., Livescu, K., Lopez, A., and Goldwater, S. Pre-training on high-resource speech recognition improves low-resource speech-to-text translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 58–68, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1006. URL https: //www.aclweb.org/anthology/N19-1006.  \n\nBe´rard, A., Pietquin, O., Servan, C., and Besacier, L. Listen and translate: A proof of concept for end-to-end speechto-text translation. In NIPS Workshop on End-to-end Learning for Speech and Audio Processing, Barcelona, Spain, 2016.   \nChen, J., Ma, M., Zheng, R., and Huang, L. Mam: Masked acoustic modeling for end-to-end speech-to-text translation. arXiv preprint arXiv:2010.11445, 2020.   \nCho, W. I., Kim, S. M., Cho, H., and Kim, N. S. kosp2e: Korean Speech to English Translation Corpus. In Proc. Interspeech 2021, pp. 3705–3709, 2021. doi: 10.21437/ Interspeech.2021-1040.   \nChuang, S.-P., Chuang, Y.-S., Chang, C.-C., and Lee, H.-y. Investigating the reordering capability in CTCbased non-autoregressive end-to-end speech translation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 1068– 1077, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl. 92. URL https://aclanthology.org/2021. findings-acl.92.   \nDi Gangi, M. A., Cattoni, R., Bentivogli, L., Negri, M., and Turchi, M. MuST-C: a Multilingual Speech Translation Corpus. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2012–2017, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/ v1/N19-1202. URL https://www.aclweb.org/ anthology/N19-1202.   \nDi Gangi, M. A., Negri, M., and Turchi, M. Adapting Transformer to End-to-End Spoken Language Translation. In Proc. Interspeech 2019, pp. 1133–1137, 2019. doi: 10. 21437/Interspeech.2019-3045. URL http://dx.doi. org/10.21437/Interspeech.2019-3045.   \nDong, Q., Ye, R., Wang, M., Zhou, H., Xu, S., Xu, B., and Li, L. Listen, understand and translate: Triple supervision decouples end-to-end speechto-text translation. Proceedings of the AAAI Conference on Artificial Intelligence, 35(14):12749–12759, May 2021. URL https://ojs.aaai.org/index. php/AAAI/article/view/17509.   \nDu, Y., Zhang, Z., Wang, W., Chen, B., Xie, J., and Xu, T. Regularizing end-to-end speech translation with triangular decomposition agreement. arXiv preprint arXiv:2112.10991, 2021.   \nDuong, L., Anastasopoulos, A., Chiang, D., Bird, S., and Cohn, T. An attentional model for speech translation without transcription. In Proceedings of the 2016  \n\nConference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 949–959, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1109. URL https: //www.aclweb.org/anthology/N16-1109.  \n\nGaido, M., Cettolo, M., Negri, M., and Turchi, M. CTCbased compression for direct speech translation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 690–696, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.eacl-main.57. URL https://aclanthology. org/2021.eacl-main.57.  \n\nGraves, A., Fern´andez, S., and Gomez, F. Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In In Proceedings of the International Conference on Machine Learning, ICML 2006, pp. 369–376, 2006.  \n\nGu, J. and Kong, X. Fully non-autoregressive neural machine translation: Tricks of the trade. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 120–133, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. findings-acl.11. URL https://aclanthology. org/2021.findings-acl.11.  \n\nHan, C., Wang, M., Ji, H., and Li, L. Learning shared semantic space for speech-to-text translation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 2214–2225, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. findings-acl.195. URL https://aclanthology. org/2021.findings-acl.195.  \n\nHaviv, A., Vassertail, L., and Levy, O. Can latent alignments improve autoregressive machine translation? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2637– 2641, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main. 209. URL https://aclanthology.org/2021. naacl-main.209.  \n\nHoshen, Y., Weiss, R. J., and Wilson, K. W. Speech acoustic modeling from raw multichannel waveforms. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4624–4628, 2015. doi: 10.1109/ICASSP.2015.7178847.  \n\nInaguma, H., Duh, K., Kawahara, T., and Watanabe, S. Multilingual end-to-end speech translation. In 2019 IEEE  \n\nAutomatic Speech Recognition and Understanding Workshop (ASRU), pp. 570–577. IEEE, 2019.  \n\nInaguma, H., Kiyono, S., Duh, K., Karita, S., Yalta, N., Hayashi, T., and Watanabe, S. ESPnet-ST: All-in-one speech translation toolkit. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 302–311, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-demos.34. URL https: //aclanthology.org/2020.acl-demos.34.  \n\nIndurthi, S., Zaidi, M. A., Kumar Lakumarapu, N., Lee, B., Han, H., Ahn, S., Kim, S., Kim, C., and Hwang, I. Task aware multi-task learning for speech to text tasks. In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7723–7727, 2021. doi: 10.1109/ICASSP39728.2021. 9414703.  \n\nKarita, S., Chen, N., Hayashi, T., Hori, T., Inaguma, H., Jiang, Z., Someki, M., Soplin, N. E. Y., Yamamoto, R., Wang, X., Watanabe, S., Yoshimura, T., and Zhang, W. A comparative study on transformer vs rnn in speech applications. In 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 449–456, 2019.  \n\nKingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.  \n\nKocabiyikoglu, A. C., Besacier, L., and Kraif, O. Augmenting librispeech with French translations: A multimodal corpus for direct speech translation evaluation. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, May 2018. European Language Resources Association (ELRA). URL https://www.aclweb.org/ anthology/L18-1001.  \n\nKoehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst, E. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pp. 177–180, Prague, Czech Republic, June 2007. Association for Computational Linguistics. URL https: //www.aclweb.org/anthology/P07-2045.  \n\nKudo, T. and Richardson, J. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language  \n\nProcessing: System Demonstrations, pp. 66–71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL https://aclanthology.org/D18-2012.   \nLam, M. W., Wang, J., Weng, C., Su, D., and Yu, D. Raw Waveform Encoder with Multi-Scale Globally Attentive Locally Recurrent Networks for End-to-End Speech Recognition. In Proc. Interspeech 2021, pp. 316–320, 2021. doi: 10.21437/Interspeech.2021-2084.   \nLe, H., Pino, J., Wang, C., Gu, J., Schwab, D., and Besacier, L. Lightweight adapter tuning for multilingual speech translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 817–824, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-short. 103. URL https://aclanthology.org/2021. acl-short.103.   \nLibovick´y, J. and Helcl, J. End-to-end non-autoregressive neural machine translation with connectionist temporal classification. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3016–3021, Brussels, Belgium, OctoberNovember 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1336. URL https: //aclanthology.org/D18-1336.   \nLiu, Y., Xiong, H., Zhang, J., He, Z., Wu, H., Wang, H., and Zong, C. End-to-End Speech Translation with Knowledge Distillation. In Proc. Interspeech 2019, pp. 1128–1132, 2019. doi: 10.21437/ Interspeech.2019-2582. URL http://dx.doi.org/ 10.21437/Interspeech.2019-2582.   \nLiu, Y., Zhu, J., Zhang, J., and Zong, C. Bridging the modality gap for speech-to-text translation. ArXiv, abs/2010.14920, 2020.   \nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311– 318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/ 1073083.1073135. URL https://www.aclweb. org/anthology/P02-1040.   \nPark, D. S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E. D., and Le, Q. V. Specaugment: A simple data augmentation method for automatic speech recognition. 2019.   \nPost, M. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 186–191, Belgium, Brussels, October 2018. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/W18-6319.   \nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr. org/papers/v21/20-074.html.   \nSaharia, C., Chan, W., Saxena, S., and Norouzi, M. Nonautoregressive machine translation with latent alignments. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1098–1108, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. emnlp-main.83. URL https://aclanthology. org/2020.emnlp-main.83.   \nSainath, T. N., Kingsbury, B., Mohamed, A.-r., and Ramabhadran, B. Learning fliter banks within a deep neural network framework. In 2013 IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 297–302, 2013. doi: 10.1109/ASRU.2013.6707746.   \nSalesky, E., Sperber, M., and Black, A. W. Exploring phoneme-level speech representations for end-to-end speech translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 1835–1841, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/ v1/P19-1179. URL https://www.aclweb.org/ anthology/P19-1179.   \nSchneider, S., Baevski, A., Collobert, R., and Auli, M. wav2vec: Unsupervised pre-training for speech recognition. Apr 2019. doi: http://doi.org/10.21437/ Interspeech.2019-1873. URL https://arxiv.org/ abs/1904.05862.   \nSeki, H., Yamamoto, K., and Nakagawa, S. A deep neural network integrated with filterbank learning for speech recognition. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5480–5484, 2017. doi: 10.1109/ICASSP.2017.7953204.   \nSennrich, R. and Zhang, B. Revisiting low-resource neural machine translation: A case study. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 211–221, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1021. URL https: //aclanthology.org/P19-1021.   \nSennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1715–1725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/ v1/P16-1162. URL https://www.aclweb.org/ anthology/P16-1162.   \nShaw, P., Uszkoreit, J., and Vaswani, A. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 464– 468, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2074. URL https://aclanthology.org/N18-2074.   \nSung, T.-W., Liu, J.-Y., Lee, H.-y., and Lee, L.-s. Towards end-to-end speech-to-text translation with two-pass decoding. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7175–7179. IEEE, 2019.   \nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30, pp. 5998–6008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 7181-attention-is-all-you-need.pdf.   \nWang, C., Wu, Y., Liu, S., Yang, Z., and Zhou, M. Bridging the gap between pre-training and fine-tuning for end-toend speech translation. arXiv preprint arXiv:1909.07575, 2019a.   \nWang, C., Tang, Y., Ma, X., Wu, A., Okhonko, D., and Pino, J. Fairseq S2T: Fast speech-to-text modeling with fairseq. In Proceedings of the 1st Conference of the AsiaPacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations, pp. 33–39, Suzhou, China, December 2020a. Association for Computational Linguistics. URL https: //aclanthology.org/2020.aacl-demo.6.   \nWang, C., Wu, A., and Pino, J. Covost 2 and massively multilingual speech-to-text translation. arXiv preprint arXiv:2007.10310, 2020b.   \nWang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F., and Chao, L. S. Learning deep transformer models for machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 1810–1822, Florence, Italy, July 2019b. Association for Computational Linguistics. doi: 10.18653/v1/P19-1176.   \nURL https://aclanthology.org/P19-1176.  \n\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Łukasz Kaiser, Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O., Corrado, G., Hughes, M., and Dean, J. Google’s neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144, 2016. URL http://arxiv.org/abs/1609.08144.  \n\nXu, C., Hu, B., Li, Y., Zhang, Y., Huang, S., Ju, Q., Xiao, T., and Zhu, J. Stacked acoustic-and-textual encoding: Integrating the pre-trained models into speech translation encoders. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 2619–2630, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.204. URL https: //aclanthology.org/2021.acl-long.204.  \n\nZhang, B., Titov, I., and Sennrich, R. Improving deep transformer with depth-scaled initialization and merged attention. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 898–909, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1083. URL https://aclanthology.org/D19-1083.  \n\nZhang, B., Titov, I., Haddow, B., and Sennrich, R. Adaptive feature selection for end-to-end speech translation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 2533–2544, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp. 230. URL https://aclanthology.org/2020. findings-emnlp.230.  \n\nZhang, B., Titov, I., Haddow, B., and Sennrich, R. Beyond sentence-level end-to-end speech translation: Context helps. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 2566–2578, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.200. URL https: //aclanthology.org/2021.acl-long.200.  \n\nZhao, C., Wang, M., Dong, Q., Ye, R., and Li, L. NeurST: Neural speech translation toolkit. In Proceedings of the  \n\n59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pp. 55–62, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. acl-demo.7. URL https://aclanthology.org/ 2021.acl-demo.7.  \n\nZheng, R., Chen, J., Ma, M., and Huang, L. Fused acoustic and text encoding for multimodal bilingual pretraining and speech translation. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 12736–12746. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/ v139/zheng21a.html.", "reference_length": 20307, "reference_token": 6073, "txt_length": 43038, "txt_token": 12483, "txt": "# evisiting End-to-End Speech-to-Text Translation From Scratc  \n\nBiao Zhang 1 Barry Haddow 1 Rico Sennrich 2 1  \n\n# Abstract  \n\nEnd-to-end (E2E) speech-to-text translation (ST) often depends on pretraining its encoder and/or decoder using source transcripts via speech recognition or text translation tasks, without which translation performance drops substantially. However, transcripts are not always available, and how significant such pretraining is for E2E ST has rarely been studied in the literature. In this paper, we revisit this question and explore the extent to which the quality of E2E ST trained on speechtranslation pairs alone can be improved. We reexamine several techniques proven beneficial to ST previously, and offer a set of best practices that biases a Transformer-based E2E ST system toward training from scratch. Besides, we propose parameterized distance penalty to facilitate the modeling of locality in the self-attention model for speech. On four benchmarks covering 23 languages, our experiments show that, without using any transcripts or pretraining, the proposed system reaches and even outperforms previous studies adopting pretraining, although the gap remains in (extremely) low-resource settings. Finally, we discuss neural acoustic feature modeling, where a neural model is designed to extract acoustic features from raw speech signals directly, with the goal to simplify inductive biases and add freedom to the model in describing speech. For the first time, we demonstrate its feasibility and show encouraging results on ST tasks.1  \n\n# 1. Introduction  \n\nEnd-to-end (E2E) speech-to-text translation (ST) is the task of translating a source-language audio directly to a foreign text without any intermediate outputs (Duong et al., 2016; B´erard et al., 2016), which has gained increasing popularity and obtained great success recently (Sung et al., 2019; Salesky et al., 2019; Zhang et al., 2020; Chen et al., 2020; Han et al., 2021; Zheng et al., 2021; Anastasopoulos et al., 2021). Different from the traditional cascading method which decomposes ST into two sub-tasks – automatic speech recognition (ASR) for transcription and machine translation (MT) for translation, E2E ST jointly handles them in a single, large neural network. This endows E2E ST with special advantages on reducing translation latency and bypassing transcription mistakes made by ASR models, making it theoretically attractive.  \n\nHowever, directly modeling speech-to-text mapping is nontrivial. The translation alignment between speech and text is no longer subject to the monotonic assumption. Also, the high variation of speech increases the modeling difficulty. Therefore, rather than training E2E ST models from scratch, researchers often resort to pipeline-based training with auxiliary tasks utilizing source transcripts, which first pretrains the speech encoder on ASR data and/or the text decoder on MT data followed by a finetuning on ST data. Such pretraining was reported to greatly improve translation quality (Di Gangi et al., 2019; Wang et al., 2019a; Zhang et al., 2020; Xu et al., 2021), and has become the de-facto standard in recent ST studies and toolkits (Inaguma et al., 2020; Wang et al., 2020a; Zhao et al., 2021; Zheng et al., 2021). Despite these successes, nevertheless, how significant the pretraining is for E2E ST and how far we can go using speech-translation pairs alone are still open questions.  \n\nIn this paper, we aim at exploring the extent to which the quality of ST models trained from scratch can be improved, whether the performance gap against pretraining-based ST can be narrowed, and also when the pretraining really matters.23 We argue that the inferior performance of ST from scratch is mainly a result of the dominance of pretraining, and consequent lack of focus on optimizing E2E ST models trained from scratch. To test this hypothesis, we investigate methods to bias a Transformer-based E2E ST model (Vaswani et al., 2017) towards training from scratch. We summarize a set of best practices for our setup by revisiting several existing techniques that have been proven useful to ST previously. We further introduce two proposals to add freedom to Transformer to model speech with the hope of gaining translation quality: 1) a parameterized distance penalty that facilitates self-attention to capture local dependencies of speech; and 2) neural acoustic feature modeling providing a trainable alternative to the heuristic rule-based acoustic feature extraction.  \n\nTo examine the generality of our methods, we conducted (bilingual) experiments on four speech translation benchmarks, including MuST-C, Covost2, LibriSpeech, and Kosp2e, which cover 23 languages of different families with varying training data sizes. Experimental results show that the significance of pretraining has been over-estimated in prior work, and integrating techniques to improve E2E ST from scratch is feasible and promising. Our main findings:  \n\n• With proper adaptation, E2E ST trained from scratch only on speech-translation pairs can match or even surpass previous studies using ASR/MT pretraining on source transcripts.   \n• Pretraining still matters, mainly in (extremely) lowresource regimes and when large-scale external ASR or MT corpora are available.   \n• We present a set of best practices for E2E ST from scratch, including smaller vocabulary size, wider feedforward layer, deep speech encoder with the post-LN (layer normalization) structure, Connectionist Temporal Classification (CTC)-based regularization using translation as the target, and a novel parameterized distance penalty.   \n• We demonstrate that dropping heuristic rule-based acoustic features is feasible, and that neural acoustic features can be learned in an end-to-end ST framework.  \n\n# 2. Why Revisiting ST From Scratch?  \n\nIn our view, there are several reasons making E2E ST from scratch intriguing.  \n\nFirst of all, our study does not preclude pretraining (or more generally, multi-task learning) for ST. We believe that leveraging knowledge from auxiliary tasks via pretraining to improve ST is a remarkable research direction. But rather, our study contributes to a better understanding of the genuine role of pretraining in E2E ST. Re-assessing the importance of pretraining is a useful signal to inform future research projects and practical deployments of ST.  \n\n![](images/1522e33f30b12ab008a28b9da6a9f8651343c195b022779345c56d18f9318fc8.jpg)  \nFigure 1: Overview of the proposed ST system. The example is for En-De translation. During inference, the CTC layer is dropped and only the autoregressive decoder is used.  \n\nSecondly, focusing on ST from scratch has an even higher relevance in settings where ASR/MT data is scarce. By only requiring speech-translation training pairs, ST from scratch reduces data requirements and the associated costs. This is especially important for the estimated 3000 languages in the world that have no written form at all, for which it would be impractical to collect large amounts of phonetically transcribed data.  \n\nThirdly, removing pretraining eases model analysis and simplifies the training pipeline, which also offers a testbed to identify inductive biases that support ST with better data efficiency. Pretraining often takes extra training time and computing resources. As pretraining itself affects the final results, it becomes more difficult to figure out the source of the improved performance when new algorithms or architectures are incorporated. In contrast, ST from scratch simplifies model development, and lets us efficiently reexamine recently proposed techniques for ST, and explore novel techniques. This allows us to build strong models for future research to build on or compare to.  \n\n# 3. Methods for ST From Scratch  \n\nWe argue that the inferior performance of ST from scratch as reported in the literature is due to a lack of system adaptation with respect to training and modeling. In this section, starting with a brief overview of our baseline system, we discuss several potential directions that could strengthen E2E ST trained from scratch. The overall framework of the proposed system is shown in Figure 1.  \n\n# 3.1. Baseline  \n\nOur baseline follows the encoder-decoder paradigm (Bahdanau et al., 2015) and uses Transformer (Vaswani et al., 2017) as its backbone. Except for (speech, translation) pairs denoted as $(X,Y)$ , respectively, we assume that there is no ccess to other data at training for ST from scrat  \n\nThe encoder stacks $N_{e n c}$ identical layers, each of which has a multi-head self-attention sublayer and a feed-forward sublayer. To enhance its short-range dependence modeling, we apply the logarithmic distance penalty (Di Gangi et al., 2019) to each head of its self-attention:  \n\n$$\n\\operatorname{Head}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{d_{h e a d}}}-\\pi\\left(\\mathbf{D}\\right)\\right)\\mathbf{V},\n$$  \n\nwhere $\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{|X|\\times d_{h e a d}}$ are the query, key and value inputs, respectively. $d_{h e a d}$ is the attention head dimension. $|\\cdot|$ denotes sequence length. $\\mathbf{D}\\;\\in\\;\\mathbb{R}^{|X|\\;\\times\\;|X|}$ stores the position distance, i.e. $D_{i,j}=|i-j|+1$ , and $\\pi(\\cdot)=\\log(\\cdot)$  \n\nAnalogous to the encoder, the decoder stacks $N_{d e c}$ identical layers. We reuse the standard Transformer decoder for our baseline, and optimize all model parameters using the traditional maximum likelihood objective (MLE), or $\\mathcal{L}^{\\mathrm{MLE}}$ .  \n\n# 3.2. Hyperparameter Tuning  \n\nHyperparameters often highly affect ST from scratch, but exhaustively searching for optimal settings is impractical. Instead, we take inspiration from past studies and re-examine several configurations that have been proven beneficial to ST with pretraining. We hypothesize that such configurations also have a high chance to generalize to ST from scratch. For example, since ST is generally a low-resource task, using smaller vocabulary (Inaguma et al., 2020), larger dropout rate (Sennrich & Zhang, 2019), reduced attention heads and model dimension (Inaguma et al., 2020; Zhao et al., 2021) might help to avoid overfitting. We also test different settings for acoustic feature extraction, deep encoder (Zhang et al., 2019) and wide feed-forward layer (Inaguma et al., 2020), apart from tuning the length penalty at inference (Wu et al., 2016).  \n\n# 3.3. CTC-based Regularization  \n\nCTC, or Connectionist Temporal Classification, is a latent alignment objective that models probabilistic distribution by marginalizing over all valid mappings between the input and output sequence (Graves et al., 2006). Under a strong conditional independence assumption, it can be computed efficiently and tractably via dynamic programming. We refer readers to Graves et al. (2006) for more algorithmic details. CTC with source transcripts as output labels has been found to be an effective auxiliary task for E2E ST (Bahar et al., 2019). Another application of CTC is to use translations as the output labels, thus modelling the translation task. So far, this idea has been applied to nonautoregressive MT and ST successfully (Libovicky´ & Helcl, 2018; Chuang et al., 2021).  \n\nIn this paper, we regard CTC with translations as output labels as a regularizer and stack it onto the encoder for ST modeling as shown in Figure 1. The overall training objective becomes as below:  \n\n$$\n\\begin{array}{r}{\\mathcal{L}(X,Y)=(1-\\lambda)\\mathcal{L}^{\\mathrm{MLE}}(Y|X)+\\lambda\\mathcal{L}^{\\mathrm{CTC}}(Y|X),}\\end{array}\n$$  \n\nwhere $\\lambda$ is a hyperparameter controlling the degree of the regularization. Chuang et al. (2021) showed that CTC improves the reordering tendency of the self-attention in nonautoregressive ST, although it assumes monotonicity. We expect that such reordering could reduce the learning difficulty of ST and ease the decoder’s job, delivering better translation quality. One problem of applying CTC to ST is that the input speech sequence might be shorter than its translation sequence, which violates CTC’s presumption. We simply ignore these samples during training. Note that the CTC layer will be abandoned after training.  \n\n# 3.4. Parameterized Distance Penalty  \n\nThe distance penalty in Eq. 1 penalizes attention logits logarithmically with distance based on a hard-coded function, reaching a certain degree of balance in modeling local and global dependencies. However, such a function lacks flexibility and inevitably suffers from insufficient capacity when characterizing data-specific locality. To solve this problem, we propose parameterized distance penalty (PDP) which includes a learnable parameter for each distance. PDP is inspired by the relative position representation (Shaw et al., 2018; Raffel et al., 2020) and is formulated as below:  \n\n$$\n\\begin{array}{r}{\\pi^{\\mathrm{PDP}}({\\bf D})=\\log({\\bf D})f({\\bf D}),}\\end{array}\n$$  \n\n$$\nf(D_{i,j})=\\left\\{\\!\\!\\begin{array}{l l}{\\mathbf{w}_{D_{i,j}},}&{\\mathrm{if}~D_{i,j}<R}\\\\ {\\mathbf{w}_{R},}&{\\mathrm{otherwise}}\\end{array}\\!\\!\\right.\n$$  \n\nwhere $\\mathbf{w}\\in\\mathbb{R}^{R}$ is a trainable vector, $R$ is a hyperparameter, and $\\mathbf{w}_{i}$ denotes its $i$ -th element. PDP is easily parallelizable, adding little computational overhead. We initialize each $\\mathbf{w}_{i}$ to 1 so that PDP starts from $\\pi(\\cdot)$ and then gradually adjusts itself during training. Besides, w is attention head-specific, i.e. each head has its own parameterization. By doing so, we enable different heads capturing varying degree of locality, which further increases modeling freedom.  \n\n# 4. Experimental Setup  \n\nDataset We work on four benchmarks covering different domains and 23 languages from diverse language families.  \n\nMuST-C MuST-C is extracted from TED talks (Di Gangi et al., 2019), offering translations from English (En) to 8 languages: German (De), Spanish (Es), French (Fr), Italian (It), Dutch (Nl), Portuguese $({\\bf P}{\\bf\\hat{t}})$ , Romanian (Ro) and Russian (Ru). The training sets of each language are at a similar scale, roughly 452 hours with 252K utterances on average.  \n\nTable 1: Ablation results on MuST-C En-De test set. #Params: number of model parameters. BLEU: higher is better, SacreBLEU. Numbers in bold denote top scores.   \n\n\n<html><body><table><thead><tr><td><b>ID</b></td><td><b> System</b></td><td><b>#Params</b></td><td><b>BLEU↑</b></td></tr></thead><tbody><tr><td>1</td><td>Baseline</td><td>51M</td><td>18.1</td></tr><tr><td colspan=\"4\">Tune beam search, dropout and batch size</td></tr><tr><td>2</td><td>1 + adjust length penalty at inference</td><td>51M</td><td>18.8</td></tr><tr><td>3</td><td>1 + higher dropout (0.2→0.4)</td><td>51M</td><td>17.4</td></tr><tr><td>4</td><td>1 + apply dropout to raw waveform signals (rate 0.1)</td><td>51M</td><td>14.6</td></tr><tr><td>5</td><td>1 + reduce batch size by half</td><td>51M</td><td>17.6</td></tr><tr><td colspan=\"4\">Tune model dimension and depth</td></tr><tr><td>6</td><td> 2 + reduce model dimension and attention heads (H : 8 →→ 4, dmodel : 512 → 256)</td><td>20M</td><td>19.0</td></tr><tr><td>7</td><td>6 + enlarge feed-forward layer (df f : 2048 → 4096)</td><td>33M</td><td>19.3</td></tr><tr><td>8</td><td>6 + enlarge encoder depth with DS-Init (Nenc : 6 -→ 12)</td><td>28M</td><td>20.4</td></tr><tr><td>9</td><td> 8 + enlarge feed-forward layer (Nenc = 12, dff : 2048 → 4096)</td><td>47M</td><td>21.1</td></tr><tr><td>10</td><td> 2 + enlarge encoder depth with DS-Init (Nenc : 6 →> 12)</td><td>70M</td><td>20.3</td></tr><tr><td colspan=\"4\">Add parameterized distance penalty (PDP)</td></tr><tr><td>11</td><td>2 + PDP (R = 512)</td><td>51M</td><td>19.5</td></tr><tr><td>12</td><td>11 + initialize w in PDP randomly</td><td>51M</td><td>18.3</td></tr><tr><td>13</td><td>11 + use 80-dimensional log mel-scale filterbank (F : 40 →→ 80)</td><td>51M</td><td>19.3</td></tr><tr><td>14</td><td>11 + remove delta and delta-delta features (dspeech : 120 -→ 40)</td><td>50M</td><td>18.8</td></tr><tr><td colspan=\"4\">Tune vocabulary size and LN</td></tr><tr><td>15</td><td>9 + PDP</td><td>47M</td><td>21.8</td></tr><tr><td>16</td><td>15 + small BPE vocabulary (V : 16K → 8K)</td><td>46M</td><td>21.8</td></tr><tr><td>17</td><td>16 + change post-LN to pre-LN</td><td>46M</td><td>20.6</td></tr><tr><td colspan=\"4\">Final system: add CTC</td></tr><tr><td>18</td><td>16 + CTC regularization (X = 0.3) (also, the proposed system)</td><td>48M</td><td>22.7</td></tr><tr><td colspan=\"4\">Compare to ST with ASR pretraining</td></tr><tr><td>19</td><td>for comparison: 16 + ASR pretraining</td><td>46M</td><td>22.9</td></tr><tr><td>20</td><td>for comparison: 1 + ASR pretraining</td><td>51M</td><td>20.7</td></tr></tbody></table></body></html>  \n\nLibriSpeech En-Fr The Augmented LibriSpeech dataset is collected by aligning e-books in French with English utterances of LibriSpeech (Kocabiyikoglu et al., 2018). We only use the 100 hours clean training set and its augmented references offered by Google Translate for training, totalling 94K utterances.  \n\nKosp2e Ko-En Kosp2e is constructed from a mix of four domains (textbook, news, AI agent and diary) for Korean-to-English (Ko-En) speech translation (Cho et al., 2021). The training set has about 190 hours with 106K utterances.  \n\nCoVoST CoVoST (version 2) is a large-scale multilingual ST corpus collected from Common Voice (Ardila et al., 2020), providing translations from En to 15 languages – Arabic (Ar), Catalan (Ca), Welsh (Cy), De, Estonian (Et), Persian (Fa), Indonesian (Id), Japanese (Ja), Latvian (Lv), Mongolian (Mn), Slovenian (Sl), Swedish (Sv), Tamil (Ta), Turkish (Tr), Chinese (Zh) – and from 21 languages to En, including the 15 target languages as well as Es, Fr, It, Nl, Pt and $\\mathrm{Ru}$ (Wang et al., 2020b). The training set for $\\mathrm{En\\mathrm{\\rightarrow}X x}$ translation is of similar scale, roughly 427 hours with 289K utterances. In contrast, the training data size for $\\mathrm{Xx\\mathrm{\\rightarrow\\mathrm{En}}}$ translation varies greatly, from about 1.2 hours/1.2K utterances (Id) to 263 hours/207K utterances $(\\mathrm{Fr})$ . We mainly work on Fr, De, Es, Ca, It, Ru, and Zh for $\\mathrm{Xx\\mathrm{\\rightarrow\\mathrm{En}}}$ .  \n\nFor each benchmark, we use the official train/dev/test split for experiments. We convert all audios to a sampling rate of 16KHz and truncate segments to 3000 frames. We extract 40- dimensional log mel-scale fliterbank features ( $F=40)$ ) with a step size of 10ms and window size of $25\\mathrm{ms}$ , which are then expanded with their delta and delta-delta features followed by mean subtraction and variance normalization, resulting in the final 120-dimensional acoustic features $(d_{s p e e c h}=120)$ ). We tokenize and truecase all texts via Moses (Zh and Ja excluded) (Koehn et al., 2007), and handle infrequent words via subwords (Sennrich et al., 2016; Kudo & Richardson, 2018) with a vocabulary size of 16K $V=16K)$ ).  \n\nModel Setting On top of the acoustic input, we concatenate three consecutive frames without overlapping as a way of downsampling (Zhang et al., 2020), as in Figure 1. We then add a linear layer to get the encoder input of dimension $d_{m o d e l}$ . We use the sinusoidal encoding to distinguish different positions, and employ the post-LN (layer normalization) structure for Transformer (Vaswani et al., 2017).  \n\nRegarding Baseline, we set $d_{m o d e l}=512$ , $d_{h e a d}=64$ , the number of attention head $H=8$ , the feed-forward layer size $d_{f f}\\,=\\,2048$ and $N_{e n c}=N_{d e c}=6$ . Note $d_{m o d e l}=$ $H\\cdot d_{h e a d}$ . By default, we set $R=512$ and $\\lambda=0.3$ .  \n\nWe employ Adam (Kingma & Ba, 2015, $\\beta_{1}=0.9,\\beta_{2}=$ 0.98) for parameter update using adaptive learning rate schedule as in (Vaswani et al., 2017) with a warmup step of 4K and label smoothing of 0.1. Dropout of rate 0.2 is applied to residual connections and ReLU activations. We organize training samples of around 20K target subwords into one batch, and train models up to 50K steps.  \n\nEvaluation We average the best 10 checkpoints according to dev set performance for evaluation. For decoding, we adopt beam search, where we set the beam size and length penalty to 8 and 0.6, respectively. We will examine the impact of the length penalty on translation later. Unless otherwise stated, we measure translation quality with detokenized case-sensitive BLEU (Papineni et al., 2002) offered by SacreBLEU (Post, 2018).4Note that we did not perform any filtering to the test set at evaluation time.  \n\n![](images/8cd4d6edfc961a3a5c18ce3d5d80be510cf7322012963fafa738de17a173e6b0.jpg)  \nFigure 2: Dev SacreBLEU scores as a function of length penalty $(0.5\\rightarrow1.5)$ ) for Baseline on MuST-C En-De. Trade-off exists.  \n\n![](images/bf0f35ecd04a52ded42b5438c9cfe3773abeea97e65dab09f58f2fb2fb9079fb.jpg)  \nFigure 3: Dev SacreBLEU on MuST-C En-De when changing $R$ in PDP for system 11. Setting $R=512$ yields the best result.  \n\n# 5. Results and Analysis  \n\nWe test different hyperparameters and our proposals mainly on MuST-C En-De. Table 1 summarizes the results.  \n\nApart from architecture, length penalty in beam search also matters. Length penalty is used to bias beam search generating longer or shorter outputs, which often largely affects translation quality as shown in Figure 2.5 Tuning this setting alone results in $+0.7$ BLEU gains $\\left.\\right\\vert1\\rightarrow2$ ).  \n\nApplying more dropout and smaller batch size helps little. Dropout is a popular regularizer to avoid overfitting. We tried using larger dropout rate and adding dropout to raw waveforms, but ended up with significantly slower convergence and worse performance $\\mathrm{[1\\rightarrow3,4]}$ ). Also, reducing training batch deteriorates ST ( $\\mathrm{1\\rightarrow5}$ ).  \n\nDeepening speech encoder, widening feed-forward layer, and reducing model dimension benefit ST. Halving model dimension greatly reduces the number of model parameters but still retains translation quality $\\mathrm{~2~}\\rightarrow\\mathrm{~6~}$ .  \n\nEnlarging the encoder depth (from 6 to 12) and the feedforward dimension (from 2048 to 4096) leads to substantial quality improvement, $+2.1$ BLEU $\\left(6\\right.\\rightarrow\\left.9\\right)$ ). After varying dimensions, we could achieve a BLEU score of 21.1. Note, we employed the depth-scaled initialization to smooth model gradients for deep Transformer (Zhang et al., 2019, DS-Init) and set $\\alpha\\,=\\,0.5$ . Besides, deep speech encoder improves ST from scratch with the Baseline dimensions $2\\rightarrow10$ ).  \n\nThe proposed parameterized distance penalty improves ST. The hyperparameter $R$ in Eq. 3 affects the flexibility of PDP in modeling local context. Figure 3 shows its impact on ST. In general, setting $R=512$ achieves good performance. Note, its optimal setting might (and is likely to) be dataset-dependent.  \n\nApplying PDP to ST gains BLEU $2\\rightarrow11\\$ ) and is complementary to model dimension manipulation $\\left(9\\right.\\rightarrow\\left.15\\right)$ ), reaching a test BLEU score of 21.8. We also tested the effectiveness of initializing all $\\mathbf{w}_{i}$ to 1. Using the vanilla random initialization instead delivers inferior quality, -1.2 BLEU $(11\\rightarrow12)$ ).  \n\nInadequate acoustic feature extraction hurts ST. In previous ST systems (Inaguma et al., 2020; Zhao et al.,  \n\n![](images/cbbcaed39ba516594d00a4605b5976bbe2eca06f84614d1af27e781923a06e49.jpg)  \nFigure 4: Dev SacreBLEU as a function of $\\lambda$ on MuST-C En-De for system 18. We set $\\lambda=0.3$ in our experiments.  \n\n![](images/e5a21139d639e4b9b2ec05bbabcb1071590dd392533ec180e30a7bc7271091e0.jpg)  \nFigure 5: Impact of the amount of training data on MuST-C En-De translation. Results are for test SacreBLEU.  \n\n2021), acoustic feature extraction often uses 80-dimensional filterbanks without delta and delta-delta features. We checked this in our setup. Using more filterbanks does not help much $11\\rightarrow13)$ ), and delta features benefit ST a lot $11\\rightarrow14$ ).  \n\nReducing vocabulary size affects En-De translation little. Previous studies also suggest to use smaller vocabularies in low-resource settings (Karita et al., 2019; Sennrich & Zhang, 2019). Reducing vocabulary size by half yields little impact on En-De translation $\\mathrm{15\\rightarrow16)}$ ). We adopt smaller vocabularies due to three reasons: 1) it reduces the number of parameters; 2) we observed that it has much greater influence on other languages; and 3) CTC with smaller vocabulary is more computationally efficient.  \n\nPost-LN vs. Pre-LN Another way to train deep Transformer is to use the pre-LN structure (Wang et al., 2019b). It has been shown that the post-LN, once successfully optimized, often outperforms its pre-LN counterpart (Zhang et al., 2019). We reconfirmed this observation, and found that the post-LN ST with DS-Init shows clear superiority in performance, $+1.2$ BLEU $17\\to16$ ).  \n\nCTC greatly improves ST from scratch. Finally, we integrate the CTC regularization into our best system. The hyperparameter $\\lambda$ in Eq. 2 controls the trade-off between two different objectives. Figure 4 shows that $\\lambda$ directly affects ST, and setting $\\lambda=0.3$ achieves the best result. Under this setting, CTC beneftis ST with another significant quality gain, $+0.9$ BLEU, reaching a test BLEU of 22.7 (18).  \n\nLarge combined effect, with and without pretraining From Baseline to system 18, we improve ST by 4.6 BLEU. Note that this system also outperforms the baseline with ASR pretraining (20). Comparing systems 19 and 20, we can also see that our proposals benefit models with pretraining, although the improvement (2.2 BLEU) is smaller than for models trained from scratch. Consequently, the gap between our best system trained from scratch and its pretrained counterpart has become very narrow (18 vs. 19).6  \n\nFor all follow-up experiments, we focus on models trained from scratch, and use system 18 as our proposed system.  \n\nPretraining matters in low-resource regime. Pretraining might not be crucial when rich training data is given, but it matters as the amount of training data decreases. Figure 5 demonstrates this. ASR pretraining helps low-resource ST.  \n\nResults On Other Languages Putting all together, we obtain a set of best practices, involving $N_{e n c}=12,N_{d e c}=$ $6,d_{m o d e l}\\,=\\,256,H\\,=\\,4,d_{f f}\\,=\\,4096,V\\,=\\,8K,$ , using PDP with $R=512$ and applying CTC with $\\lambda=0.3$ . We then keep this configuration and train models for other language pairs. Tables 2-5 list the results.  \n\nOur revisiting of ST from scratch shows that its performance gap to ST with pretraining has generally been overestimated in the literature. This gap can be largely reduced and even fully closed after biasing E2E ST towards training from scratch. Our system achieves an average BLEU of 25.1 and 17.3 on MuST-C and CoVoST $\\mathrm{En\\mathrm{{\\rightarrow}X\\mathrm{{x}}}}$ , respectively, which surpasses many popular neural systems, such as the ones supported by Fairseq (Wang et al., 2020a) and NeurST (Zhao et al., 2021). Similarly, our system achieves very promising performance on LibriSpeech En-Fr and Kosp2e Ko-En, delivering 18.9 and 5.8 BLEU, respectively. Note Cho et al. (2021) employed extra large-scale ASR data for pretraining, which is merely 0.1 BLEU higher than ours. While this is beyond the scope of our work, our results suggest that it is worthwhile to revisit large-scale pretraining based on our stronger baseline, which will lead to either new state-of-the-art results or a re-evaluation of the effectiveness of large-scale pretraining.  \n\nTable 2: Results of different systems on MuST-C tst-COMMON. Avg: average score over different languages. $^\\dagger$ : systems that might perform filtering to the test set, so comparison could be unfair. ‡: systems using large-scale external ASR and/or MT data.   \n\n\n<html><body><table><thead><tr><td rowspan=\"2\"><b>System</b></td><td colspan=\"2\"><b>Aux. Data</b></td><td rowspan=\"2\"><b>De</b></td><td rowspan=\"2\"><b>Es</b></td><td rowspan=\"2\"><b>Fr</b></td><td rowspan=\"2\"><b>It</b></td><td rowspan=\"2\"><b>N1</b></td><td rowspan=\"2\"><b>Pt</b></td><td rowspan=\"2\"><b>Ro</b></td><td rowspan=\"2\"><b>Ru</b></td><td rowspan=\"2\"><b>Avg</b></td></tr><tr><td><b>ASR</b></td><td><b>MT</b></td></tr></thead><tbody><tr><td> Adapted Transformer (Di Gangi et al., 2019)</td><td>√</td><td></td><td>17.3</td><td>20.8</td><td>26.9</td><td>16.8</td><td>18.8</td><td>20.1</td><td>16.5</td><td>10.5</td><td>18.5</td></tr><tr><td> ESPnet-ST (Inaguma et al., 2020)t</td><td>√</td><td>√</td><td>22.9</td><td>28.0</td><td>32.8</td><td>23.8</td><td>27.4</td><td>28.0</td><td>21.9</td><td>15.8</td><td>25.1</td></tr><tr><td>AFS (Zhang et al., 2020)</td><td>√</td><td></td><td>22.4</td><td>26.9</td><td>31.6</td><td>23.0</td><td>24.9</td><td>26.3</td><td>21.0</td><td>14.7</td><td>23.9</td></tr><tr><td> Contextual Modeling (Zhang et al., 2021)</td><td>√</td><td></td><td>22.9</td><td>27.3</td><td>32.5</td><td>23.1</td><td>26.0</td><td>27.1</td><td>23.6</td><td>15.8</td><td>24.8</td></tr><tr><td> Fairseq-ST (Wang et al., 2020a)t</td><td>√</td><td></td><td>22.7</td><td>27.2</td><td>32.9</td><td>22.7</td><td>27.3</td><td>28.1</td><td>21.9</td><td>15.3</td><td>24.8</td></tr><tr><td>NeurST (Zhao et al., 2021)</td><td>√</td><td></td><td>22.8</td><td>27.4</td><td>33.3</td><td>22.9</td><td>27.2</td><td>28.7</td><td>22.2</td><td>15.1</td><td>24.9</td></tr><tr><td>E2E-ST-JT (Du et al., 2021)t</td><td>√</td><td></td><td>23.1</td><td>27.5</td><td>32.8</td><td>23.6</td><td>27.8</td><td>28.7</td><td>22.1</td><td>14.9</td><td>25.1</td></tr><tr><td> Chimera (Han et al., 2021)</td><td></td><td>√</td><td>27.1</td><td>30.6</td><td>35.6</td><td>25.0</td><td>29.2</td><td>30.2</td><td>24.0</td><td>17.4</td><td>27.4</td></tr><tr><td> our system</td><td></td><td></td><td>22.7</td><td>28.1</td><td>33.4</td><td>23.2</td><td>26.9</td><td>28.3</td><td>22.6</td><td>15.4</td><td>25.1</td></tr><tr><td> our system + neural acoustic feature modeling</td><td></td><td></td><td>23.0</td><td>28.0</td><td>33.5</td><td>23.5</td><td>27.1</td><td>28.2</td><td>23.0</td><td>15.6</td><td>25.2</td></tr></tbody></table></body></html>  \n\nTable 3: Results of different systems for $\\mathrm{En\\mathrm{{\\rightarrow}X\\mathrm{{x}}}}$ and $\\mathrm{Xx\\mathrm{{\\rightarrow}\\mathrm{{En}}}}$ on CoVoST. We report character-level BLEU for Chinese and Japanese following Wang et al. (2020b). Languages underlined have training data fewer than 100K samples.   \n\n\n<html><body><table><thead><tr><td colspan=\"7\"><b>System</b></td><td colspan=\"9\"><b>Aux. Data</b></td></tr><tr><td colspan=\"5\"></td><td><b>ASR</b></td><td><b>MT</b></td><td><b>Fr</b></td><td><b>Ca</b></td><td><b>De</b></td><td><b>Es</b></td><td><b>Ca</b></td><td><b>It</b></td><td><b>Ru</b></td><td><b>Zh</b></td><td><b>Avg</b></td></tr></thead><tbody><tr><td colspan=\"7\">ST from scratch (Wang et al., 2020b) ST + ASR Pretraining (Wang et al., 2020b)</td><td>√</td><td>24.3 26.3</td><td>24.3 26.3</td><td>8.4 17.1</td><td>12.0 23.0</td><td>0.2 11.3</td><td>0.2 11.3</td><td>1.2 14.8</td><td>1.4 5.8</td></tr><tr><td colspan=\"7\">our system</td><td>26.9</td><td>14.1</td><td>15.7</td><td>17.2</td><td>2.4</td><td>2.4</td><td>2.4</td></tr><tr><td colspan=\"7\">En→Xx</td><td colspan=\"8\">En→Xx</td></tr><tr><td>Ar</td><td>Ca</td><td>Cy</td><td>De</td><td>Et</td><td>Fa</td><td>Id</td><td>Ja</td><td>Lv</td><td>Mn</td><td>S1</td><td>Ta</td><td>Tr</td><td>Zh</td><td>Avg</td></tr><tr><td>8.7 12.1</td><td>20.2</td><td>22.2</td><td>13.6</td><td>11.1</td><td>11.5</td><td>18.9</td><td>26.9</td><td>11.5</td><td>6.6</td><td>11.5</td><td>9.9</td><td>8.9</td><td>20.6</td><td>14.8</td></tr><tr><td>12.1</td><td>21.8</td><td>23.9</td><td>16.3</td><td>13.2</td><td>13.1</td><td>20.4</td><td>29.6</td><td>13.0</td><td>9.2</td><td>16.0 21.8</td><td>10.9</td><td>10.0</td><td>25.4</td><td>17.1</td></tr><tr><td>12.3</td><td>22.9</td><td>24.5</td><td>17.5</td><td>13.6</td><td>12.7</td><td>21.4</td><td>28.8</td><td>13.6</td><td>9.9</td><td>15.2</td><td>10.8</td><td>10.8</td><td>23.3</td><td>17.3</td></tr></tbody></table></body></html>  \n\nTable 4: Results of different systems on LibriSpeech En-Fr test set. For comparison to previous work, we report both case-insensitive tokenized BLEU (tok) and SacreBLEU.   \n\n\n<html><body><table><thead><tr><td rowspan=\"2\"><b>System</b></td><td colspan=\"2\"><b>Aux. Data</b></td><td colspan=\"2\"><b>BLEU</b></td></tr><tr><td><b>ASR</b></td><td><b>MT</b></td><td><b>tok</b></td><td><b>Sacre</b></td></tr></thead><tbody><tr><td>ST + KD (Liu et al., 2019)</td><td></td><td><</td><td>17.02</td><td></td></tr><tr><td>TCEN (Wang et al., 2019a)</td><td><</td><td></td><td>17.05</td><td></td></tr><tr><td>AFS (Zhang et al., 2020)</td><td><</td><td></td><td>18.56</td><td></td></tr><tr><td>LUT (Dong et al., 2021)</td><td>√</td><td>←</td><td>18.34</td><td></td></tr><tr><td>Chimera (Han et al., 2021)t</td><td>√</td><td>√</td><td></td><td>19.4</td></tr><tr><td>our system</td><td></td><td></td><td>18.90</td><td>16.5</td></tr></tbody></table></body></html>  \n\nOur results also show that pretraining matters mainly in two aspects: 1) low-resource scenarios, where our system still lags far behind pretraining-enhanced ST, -5.0 BLEU on CoVoST $\\mathrm{Xx\\mathrm{\\rightarrow}E n}$ in Table 3; and 2) large-scale external ASR and/or MT data is available, where pretraining or joint modeling can largely improve ST, $+2.3$ BLEU on MuST-C in Table 2 yielded by Chimera (Han et al., 2021).  \n\nTable 5: Results of different systems on Kosp2e Ko-En test set.   \n\n\n<html><body><table><thead><tr><td rowspan=\"2\"><b>System</b></td><td colspan=\"2\"><b>Aux. Data</b></td><td rowspan=\"2\"><b>BLEU</b></td></tr><tr><td><b>ASR</b></td><td><b>MT</b></td></tr></thead><tbody><tr><td>ST from scratch (Cho et al., 2021)</td><td></td><td></td><td>2.6</td></tr><tr><td>ST + pretraining (Cho et al., 2021)*</td><td></td><td>5.9</td></tr><tr><td>our system</td><td></td><td></td></tr></tbody></table></body></html>  \n\nNotice that our system should be regarded as a lower-bound for ST from scratch, since many outstanding optimization techniques for E2E ST, e.g. SpecAugment (Park et al., 2019), are not considered here due to resource limitations. In addition, we did not aggressively optimize our system towards very low-resource scenarios, so there should still be room for quality improvement on CoVoSt $\\mathrm{Xx\\mathrm{{\\rightarrow}\\mathrm{{En}}}}$ . Also note that comparison to ST models powered by ESPnet (Inaguma et al., 2020) and Fairseq (Wang et al., 2020a) might not be fair because both toolkits perform data filtering to the test set, although SacreBLEU is also used.  \n\nTable 6: Results of applying NAFM to ST on MuST-C En-De.   \n\n\n<html><body><table><thead><tr><td><b>System</b></td><td><b># Params</b></td><td><b>BLEU</b></td></tr></thead><tbody><tr><td> our system</td><td>48M</td><td>22.7</td></tr><tr><td> our system + NAFM</td><td>54M</td><td>23.0</td></tr><tr><td> our system + two FFN blocks alone</td><td>54M</td><td>22.7</td></tr></tbody></table></body></html>  \n\n# 6. Neural Acoustic Feature Modeling  \n\nA general trend in deep learning is to replace handcrafted features with neural networks to let the model automatically capture or learn the underlying pattern behind data. In E2E ST, one heuristic is the adoption of log mel-scale fliterbanks for acoustic modeling. Despite its success, fliterbankbased modeling prevents us from accessing full acoustic details and its transformation might suffer from information loss (Lam et al., 2021), making it sub-optimal for ST. Inspired by recent speech studies on modeling raw waveforms (Lam et al., 2021), we propose neural acoustic feature modeling (NAFM) to remove such heuristic and increase the freedom of E2E ST in describing speech.  \n\nThe extraction of filterbanks often involves a sequence of two specifically designed linear transformations. To simulate such structure, we employ two feed-forward neural blocks for NAFM as follows:  \n\n$$\n\\begin{array}{r l}&{\\mathbf{x}^{(1)}=\\mathrm{LN}\\left(\\mathrm{FFN}\\left(\\mathbf{x}^{(0)}\\right)+\\mathbf{x}^{(0)}\\right),}\\\\ &{\\mathbf{x}^{(2)}=\\mathrm{LN}\\left(\\mathrm{FFN}\\left(\\mathbf{x}^{(1)}\\right)+\\mathbf{x}^{(1)}\\right),}\\end{array}\n$$  \n\nwhere $\\mathbf{x}^{(0)}\\in\\mathbb{R}^{d_{s p e e c h}}$ is the raw speech frame, and $\\mathrm{{FFN}\\mathrm{(\\cdot)}}$ is the feed-forward layer as in Transformer (Vaswani et al., 2017) with $d_{f f}=4096$ . We expect that, by adding trainable parameters tuned with translation losses, NAFM could induce ST-oriented acoustic features that improves ST.  \n\nHowever, directly using $\\mathbf{x}^{(2)}$ as an alternative to the filterbank features $\\mathbf{x}^{f}$ results in poor convergence. We argue that fliterbanks offer helpful inductive biases to ST, and propose to leverage such information to regularize NAFM. Formally, we add the following $L_{2}$ objective into training:  \n\n$$\n\\mathcal{L}^{\\mathrm{NAFM}}(X,Y)=\\mathcal{L}(X,Y)+\\gamma\\frac{1}{\\lvert X\\rvert}(\\lvert\\lvert\\mathbf{X}^{(2)}-\\mathbf{X}^{f}\\rvert\\rvert^{2}),\n$$  \n\nwhere $\\gamma$ is a hyperparameter and set to 0.05 in experiments.  \n\nResults in Table 6 show that training E2E ST from scratch on raw waveforms is feasible. NAFM improves ST by 0.3 BLEU on MuST-C En-De, and such improvement is not a trivial result of simply adding parameters. The last row of Table 2 shows the effectiveness of NAFM on other languages. Overall, the performance of NAFM matches and even outperforms its filterbank-based counterpart across different languages. Although NAFM does not deliver significant gains, we believe that optimizing ST with raw waveforms has great potential and deserves more effort.  \n\n# 7. Related Work  \n\nMethods to improve E2E ST are many. Apart from developing novel model architectures (Di Gangi et al., 2019; Karita et al., 2019; Zhang et al., 2020), one promising way is to leverage knowledge transfer from auxiliary tasks. Multilingual or cross-lingual ST improves translation by adding translation supervisions from other languages (Inaguma et al., 2019; Bansal et al., 2019; Liu et al., 2019). Multi-task learning beneftis ST by jointly modeling ASR and ST tasks within a single model (Anastasopoulos & Chiang, 2018; Zheng et al., 2021; Dong et al., 2021; Indurthi et al., 2021). Pretraining methods, including large-scale self-supervised pretraining (Schneider et al., 2019) and ASR/MT-based supervised pretraining, offer a warm-up initialization for E2E ST to improve its data efficiency (Le et al., 2021; Salesky et al., 2019; Xu et al., 2021). However, all these studies assume that (bilingual) ST from scratch is poor, while spending little effort on optimizing it. We challenge this assumption and demonstrate that ST from scratch can also yield decent performance after optimization.  \n\nWe adopt the CTC objective as a regularizer to improve E2E ST. CTC was proposed for ASR tasks to handle the latent alignment between speech and transcript (Graves et al., 2006), which has been widely used to train ASR models. Based on source transcripts, CTC also improves autoregressive E2E ST via ASR pretraining (Zhao et al., 2021), encoder representation compression using the learned latent alignment (Liu et al., 2020; Gaido et al., 2021), and encoder regularization (Bahar et al., 2019). Particularly, Bahar et al. (2019) applied CTC in a similar way to ours but focused on a multi-task setup where source transcripts are used as CTC labels. Instead, we explore target translations as CTC labels. Besides, CTC contributes to non-autoregressive translation. Libovick´y & Helcl (2018) and Saharia et al. (2020) applied the CTC loss to non-autoregressive MT and obtained improved translation performance. Gu & Kong (2021) observed that CTC is essential to achieve fully or one-step non-autoregressive MT. In addition, Chuang et al. (2021) showed that CTC enhances the reordering behavior of nonautoregressive ST. Different from these studies, we apply CTC to improve autoregressive ST, although Haviv et al. (2021) showed that CTC helps autoregressive MT little.  \n\nThere are several pioneering studies trying to relax the heuristics in acoustic features to improve speech representation. Sainath et al. (2013) and Seki et al. (2017) explored a neural filter bank layers as an alternative to the handengineered filterbanks. Hoshen et al. (2015) proposed a convolutional neural acoustic model that operates directly on raw waveforms, aiming at capturing the fine-grained time structure. Lam et al. (2021) further proposed a globally attentive locally recurrent network, gaining quality and robustness for ASR. These studies mainly focus on ASR. To the best of our knowledge, applying NAFM to ST has never been investigated before, and we showed its feasibility.  \n\n# 8. Conclusion and Discussion  \n\nHow much can we achieve for E2E ST from scratch without relying on transcripts or any pretraining? We answer this question by reexamining several techniques and devising two novel proposals, namely parameterized distance penalty (PDP) and neural acoustic feature modeling (NAFM). Via extensive experiments, we present a set of best practices for ST from scratch, including smaller vocabulary, deep post-LN encoder, wider feed-forward layer, ST-based CTC regularization and PDP. We show that ST models trained from scratch, when properly optimized, can match and even outperform previous work relying on pretraining.  \n\nOur study does not preclude pretraining (with source transcripts) for ST. Instead, we provide an improved understanding of its role on E2E ST. Our results show that pretraining matters mainly in two settings: (extremely) low-resource setup and scenarios where large-scale external ASR and MT data is available. The performance gap in such settings remains. From our perspective, how to leverage other types of data to improve pretraining for ST is a promising yet challenging research topic. We invite researchers to build upon our models to re-examine the importance of pretraining in various settings.  \n\nIn addition, we examined and demonstrated the feasibility of performing E2E ST on raw waveforms through NAFM. Although we did not obtain consistent and substantial quality gains, NAFM still has the potential of fully leveraging all acoustic signals and yielding improved acoustic features for ST, achieving better results with more suitable architectures.  \n\nIn the future, we are interested in exploring how the proposed techniques can advance the state-of-the-art when coupled with large-scale pretraining.  \n\n# Acknowledgements  \n\nWe thank the reviewers for their insightful comments. This work has received funding from the European Union’s Horizon 2020 Research and Innovation Programme under Grant Agreements No 825460 (ELITR) and 825299 (GoURMET). RS acknowledges funding from the Swiss National Science Foundation (project MUTAMUR; no. 176727)", "appendix": "."}, {"title": "Fused Acoustic and Text Encoding for Multimodal Bilingual Pretraining  and Speech Translation", "authors": "Renjie Zheng, Junkun Chen, Mingbo Ma, Liang Huang", "bibkey": "fused_acoustic_and_text_encoding_for_multimodal_bilingual_pretraining_and_speech_translation", "bibitem": "@article{HuangFAATEFMBPA,\n  url = {http://arxiv.org/abs/2102.05766v2},\n  title = {Fused Acoustic and Text Encoding for Multimodal Bilingual Pretraining  and Speech Translation},\n  authors = {Renjie Zheng, Junkun Chen, Mingbo Ma, Liang Huang},\n  abstract = {  Recently, representation learning for text and speech has successfully improved many language related tasks. However, all existing methods suffer from two limitations: (a) they only learn from one input modality, while a unified representation for both speech and text is needed by tasks such as end-to-end speech translation, and as a result,(b) they can not exploit various large-scale text and speech data and their performance is limited by the scarcity of parallel speech translation data.To address these problems, we propose a Fused Acoustic and Text Masked Language Model (FAT-MLM) which jointly learns a unified representation for both acoustic and text input from various types of corpora including parallel data for speech recognition and machine translation, and even pure speech and text data. Within this cross-modal representation learning framework, we further present an end-to-end model for Fused Acoustic and Text Speech Translation (FAT-ST). Experiments on three translation directions show that by fine-tuning from FAT-MLM, our proposed speech translation models substantially improve translation quality by up to +5.9 BLEU. },\n  bibkey = {HuangFAATEFMBPA},\n  arxiv_id = {2102.05766v2},\n  subject = {cs.CL},\n  submission_date = {2021-02-10T22:53:40Z}\n}", "url": "http://arxiv.org/abs/2102.05766v2", "latex_url": "http://arxiv.org/src/2102.05766v2", "latex_path": "output/download_papers/2102.05766v2/2102.05766v2", "pdf_url": "http://arxiv.org/pdf/2102.05766v2", "pdf_path": "output/download_papers/2102.05766v2/2102.05766v2.pdf", "md_url": null, "latex_length": 0, "latex": null, "abstract": "  Recently, representation learning for text and speech has successfully improved many language related tasks. However, all existing methods suffer from two limitations: (a) they only learn from one input modality, while a unified representation for both speech and text is needed by tasks such as end-to-end speech translation, and as a result,(b) they can not exploit various large-scale text and speech data and their performance is limited by the scarcity of parallel speech translation data.To address these problems, we propose a Fused Acoustic and Text Masked Language Model (FAT-MLM) which jointly learns a unified representation for both acoustic and text input from various types of corpora including parallel data for speech recognition and machine translation, and even pure speech and text data. Within this cross-modal representation learning framework, we further present an end-to-end model for Fused Acoustic and Text Speech Translation (FAT-ST). Experiments on three translation directions show that by fine-tuning from FAT-MLM, our proposed speech translation models substantially improve translation quality by up to +5.9 BLEU. ", "abstract_length": 1148, "abstract_token": 220, "introduction": "In recent years, task-agnostic text representation learning (Peters et al., 2018; Devlin et al., 2019; Sun et al., 2019) has attracted much attention in the NLP community due to its strong performance to many downstream tasks. More recently, unsupervised speech representation learning (Baevski et al., 2020; Chen et al., 2020; Liu et al., 2020a) also successfully improved many speech related tasks, such as speech recognition and speech translation. ![](images/4167ab68f24acbdead59c2e19ed18f3c2d33551e957a3c65a25e266ce5308f0e.jpg) Figure 1. The quality of end-to-end speech translation models has been limited by the scarcity of speech translation datasets. However, there is an abundance of datasets for speech, text, speech recognition, and machine translation data that can be leveraged. However all these existing methods can only handle one modality, either text or speech, while joint acoustic and text representation is desired for many end-to-end spoken language processing tasks, such as spoken question answering (Chuang et al., 2019) and end-to-end speech-to-text translation (Liu et al., 2020b). For example, end-to-end speech translation (ST) is desired due to its advantages over the pipeline paradigm, such as low latency, alleviation of error propagation, and fewer parameters (Weiss et al., 2017; B´erard et al., 2018; Jia et al., 2019; Sperber et al., 2017; Zheng et al., 2020; Chen et al., 2021). However, its translation quality is limited by the scarcity of large-scale parallel speech translation data while there exists sufficient data for speech recognition and text machine translation (Fig. 1). It would be helpful if source speech and bilingual text can be encoded into a unified representation via abundant speech recognition and text machine translation data. Liu et al. (2020b) show that jointly training a multi-modal ST encoder can largely improve the translation quality. However, their proposed representation learning method is constrained to the sequence-to-sequence framework and there is no experiment showing whether their proposed method can benefit from extra speech recognition and machine translation data. Inspired by recent cross-lingual language model pre-training work (Lample & Conneau, 2019) which shows the potential to unify the representations of different languages into one encoder, we propose a Fused Acoustic and Text Masked Language Model (FAT-MLM). This model jointly learns a unified representation for both acoustic and text input. In this way, we extend the masked language model’s input from only acoustic or text data to multimodal corpora containing both acoustic and text data, such as speech recognition and speech translation for the first time (Fig. 1). ![](images/4b197a5b52de1f3be85aa52ae43ef5b110b3d3b04fb1a1e83c3f1888882ce085.jpg) Figure 2. Previous work for speech or text monomodal representation learning. We further extend this Fused Acoustic and Text encoder to a sequence-to-sequence framework and present an end-to-end Speech Translation model (FAT-ST). This enables the model to be trained from both speech and text machine translation data into one single encoder-decoder model. Meanwhile, this model can also learn from speech recognition data using an extra FAT-MLM loss. This resolves the limitation of existing single encoder and decoder speech translation models, which can only learn from scarce parallel speech translation data, but neglects much larger scale speech recognition and text machine translation data (Fig. 1). We make the following contributions: • We propose the Fused Acoustic and Text Masked Language Model (FAT-MLM), which can learn a unified acoustic and text representation. • Based on FAT-MLM, we propose the Fused Acoustic and Text Speech Translation model (FAT-ST), which can do speech recognition and machine translation in a single encoder-decoder framework. • Spontaneous speech translation experiments on three language pairs show that by finetuning FAT-MLM, the accuracy of FAT-ST improves end-to-end speech translation model by $+4.65$ BLEU in average and achieves state-of-the-art. This is the first time that an end-to-end speech translation model achieves similar performance with the strong cascaded system in these three translation directions of this dataset, while still maintaining a smaller model size and faster decoding time. • We show that FAT-MLM trained with additional speech recognition, machine translation, and monolingual text data can improve FAT-ST by $+1.25$ BLEU. FAT-ST can be further improved by using additional speech recognition and machine translation data.", "introduction_length": 4604, "introduction_token": 1015, "reference": "# References  \n\nBaevski, A., Zhou, H., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. NeurIPS 2020, 2020.   \nBe´rard, A., Besacier, L., Kocabiyikoglu, A. C., and Pietquin, O. End-to-end automatic speech translation of audiobooks. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6224– 6228. IEEE, 2018.   \nChen, J., Ma, M., Zheng, R., and Huang, L. Mam: Masked acoustic modeling for end-to-end speech-to-text translation. arXiv preprint arXiv:2010.11445, 2020.   \nChen, J., Ma, M., Zheng, R., and Huang, L. Direct simultaneous speech-to-text translation assisted by synchronized streaming asr. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics: Findings, 2021.   \nChuang, Y.-S., Liu, C.-L., Lee, H.-y., and Lee, L.-s. Speechbert: An audio-and-text jointly learned language model for end-to-end spoken question answering. arXiv preprint arXiv:1910.11559, 2019.   \nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2019.   \nDi Gangi, M. A., Cattoni, R., Bentivogli, L., Negri, M., and Turchi, M. MuST-C: a Multilingual Speech Translation Corpus. In NAACL, 2019.   \nDong, Q., Ye, R., Wang, M., Zhou, H., Xu, S., Xu, B., and Li, L. ” listen, understand and translate”: Triple supervision decouples end-to-end speech-to-text translation. arXiv preprint arXiv:2009.09704, 2020.   \nDong, Q., Wang, M., Zhou, H., Xu, S., Xu, B., and Li, L. Consecutive decoding for speech-to-text translation. In The Thirty-fifth AAAI Conference on Artificial Intelligence, AAAI, 2021.   \nGraves, A., Fern´andez, S., Gomez, F., and Schmidhuber, J. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd international conference on Machine learning, pp. 369–376, 2006.   \nHoward, J. and Ruder, S. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146, 2018.   \nInaguma, H., Kiyono, S., Duh, K., Karita, S., Soplin, N. E. Y., Hayashi, T., and Watanabe, S. Espnet-st: All-in-one speech translation toolkit. arXiv preprint arXiv:2004.10234, 2020.   \nJia, Y., Johnson, M., Macherey, W., Weiss, R. J., Cao, Y., Chiu, C.-C., Ari, N., Laurenzo, S., and Wu, Y. Leveraging weakly supervised data to improve end-to-end speechto-text translation. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7180–7184. IEEE, 2019.   \nKahn, J., Rivi\\`ere, M., Zheng, W., Kharitonov, E., Xu, Q., Mazar´e, P. E., Karadayi, J., Liptchinsky, V., Collobert, R., Fuegen, C., Likhomanenko, T., Synnaeve, G., Joulin, A., Mohamed, A., and Dupoux, E. Librilight: A benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7669–7673, 2020. https://github. com/facebookresearch/libri-light.   \nKocabiyikoglu, A. C., Besacier, L., and Kraif, O. Augmenting librispeech with french translations: A multimodal corpus for direct speech translation evaluation. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), 2018.   \nKoehn, P. Europarl: A parallel corpus for statistical machine translation. In MT summit, volume 5, pp. 79–86. Citeseer, 2005.   \nKudo, T. and Richardson, J. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66–71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/ v1/D18-2012. URL https://www.aclweb.org/ anthology/D18-2012.   \nLample, G. and Conneau, A. Cross-lingual language model pretraining. arXiv preprint arXiv:1901.07291, 2019.   \nLe, H., Pino, J., Wang, C., Gu, J., Schwab, D., and Besacier, L. Dual-decoder transformer for joint automatic speech recognition and multilingual speech translation. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 3520–3533, 2020.   \nLiu, A. T., Li, S.-W., and Lee, H.-y. Tera: Self-supervised learning of transformer encoder representation for speech. arXiv preprint arXiv:2007.06028, 2020a.   \nLiu, Y., Xiong, H., He, Z., Zhang, J., Wu, H., Wang, H., and Zong, C. End-to-end speech translation with knowledge distillation. arXiv preprint arXiv:1904.08075, 2019.   \nLiu, Y., Zhu, J., Zhang, J., and Zong, C. Bridging the modality gap for speech-to-text translation. arXiv preprint arXiv:2010.14920, 2020b.   \nPanayotov, V., Chen, G., Povey, D., and Khudanpur, S. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5206–5210. IEEE, 2015.   \nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.   \nPino, J., Xu, Q., Ma, X., Dousti, M. J., and Tang, Y. Selftraining for end-to-end speech translation. Proc. Interspeech 2020, pp. 1476–1480, 2020.   \nPovey, D., Ghoshal, A., Boulianne, G., Goel, N., Hannemann, M., Qian, Y., Schwarz, P., and Stemmer, G. The kaldi speech recognition toolkit. In In IEEE 2011 workshop, 2011.   \nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by generative pretraining. 2018.   \nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad: $100{,}000{+}$ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.   \nSperber, M., Neubig, G., Niehues, J., and Waibel, A. Neural lattice-to-sequence models for uncertain inputs. arXiv preprint arXiv:1704.00559, 2017.   \nSun, Y., Wang, S., Li, Y., Feng, S., Chen, X., Zhang, H., Tian, X., Zhu, D., Tian, H., and Wu, H. Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223, 2019.   \nTaylor, W. L. “cloze procedure”: A new tool for measuring readability. Journalism quarterly, 30(4):415–433, 1953.   \nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.   \nWeiss, R. J., Chorowski, J., Jaitly, N., Wu, Y., and Chen, Z. Sequence-to-sequence models can directly translate foreign speech. Proc. Interspeech 2017, pp. 2625–2629, 2017.   \nZheng, R., Ma, M., Zheng, B., Liu, K., Yuan, J., Church, K., and Huang, L. Fluent and low-latency simultaneous speech-to-speech translation with self-adaptive training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pp. 3928–3937, 2020.", "reference_length": 6911, "reference_token": 2195, "txt_length": 42491, "txt_token": 12799, "txt": "# Fused Acoustic and Text Encoding for Multimodal Bilingual Pretraining and Speech Translation  \n\nRenjie Zheng \\* 1 Junkun Chen \\* 2 Mingbo Ma 1 Liang Huang  \n\n# Abstract  \n\nRecently, representation learning for text and speech has successfully improved many language related tasks. However, all existing methods suffer from two limitations: (a) they only learn from one input modality, while a unified representation for both speech and text is needed by tasks such as end-to-end speech translation, and as a result, (b) they can not exploit various large-scale text and speech data and their performance is limited by the scarcity of parallel speech translation data. To address these problems, we propose a Fused Acoustic and Text Masked Language Model (FATMLM) which jointly learns a unified representation for both acoustic and text input from various types of corpora including parallel data for speech recognition and machine translation, and even pure speech and text data. Within this crossmodal representation learning framework, we further present an end-to-end model for Fused Acoustic and Text Speech Translation (FAT-ST). Experiments on three translation directions show that by fine-tuning from FAT-MLM, our proposed speech translation models substantially improve translation quality by up to $+5.9$ BLEU.  \n\n# 1. Introduction  \n\nIn recent years, task-agnostic text representation learning (Peters et al., 2018; Devlin et al., 2019; Sun et al., 2019) has attracted much attention in the NLP community due to its strong performance to many downstream tasks. More recently, unsupervised speech representation learning (Baevski et al., 2020; Chen et al., 2020; Liu et al., 2020a) also successfully improved many speech related tasks, such as speech recognition and speech translation.  \n\n![](images/4167ab68f24acbdead59c2e19ed18f3c2d33551e957a3c65a25e266ce5308f0e.jpg)  \nFigure 1. The quality of end-to-end speech translation models has been limited by the scarcity of speech translation datasets. However, there is an abundance of datasets for speech, text, speech recognition, and machine translation data that can be leveraged.  \n\nHowever all these existing methods can only handle one modality, either text or speech, while joint acoustic and text representation is desired for many end-to-end spoken language processing tasks, such as spoken question answering (Chuang et al., 2019) and end-to-end speech-to-text translation (Liu et al., 2020b). For example, end-to-end speech translation (ST) is desired due to its advantages over the pipeline paradigm, such as low latency, alleviation of error propagation, and fewer parameters (Weiss et al., 2017; B´erard et al., 2018; Jia et al., 2019; Sperber et al., 2017; Zheng et al., 2020; Chen et al., 2021). However, its translation quality is limited by the scarcity of large-scale parallel speech translation data while there exists sufficient data for speech recognition and text machine translation (Fig. 1). It would be helpful if source speech and bilingual text can be encoded into a unified representation via abundant speech recognition and text machine translation data. Liu et al. (2020b) show that jointly training a multi-modal ST encoder can largely improve the translation quality. However, their proposed representation learning method is constrained to the sequence-to-sequence framework and there is no experiment showing whether their proposed method can benefit from extra speech recognition and machine translation data.  \n\nInspired by recent cross-lingual language model pre-training work (Lample & Conneau, 2019) which shows the potential to unify the representations of different languages into one encoder, we propose a Fused Acoustic and Text Masked Language Model (FAT-MLM). This model jointly learns a unified representation for both acoustic and text input. In this way, we extend the masked language model’s input from only acoustic or text data to multimodal corpora containing both acoustic and text data, such as speech recognition and speech translation for the first time (Fig. 1).  \n\n![](images/4b197a5b52de1f3be85aa52ae43ef5b110b3d3b04fb1a1e83c3f1888882ce085.jpg)  \nFigure 2. Previous work for speech or text monomodal representation learning.  \n\nWe further extend this Fused Acoustic and Text encoder to a sequence-to-sequence framework and present an end-to-end Speech Translation model (FAT-ST). This enables the model to be trained from both speech and text machine translation data into one single encoder-decoder model. Meanwhile, this model can also learn from speech recognition data using an extra FAT-MLM loss. This resolves the limitation of existing single encoder and decoder speech translation models, which can only learn from scarce parallel speech translation data, but neglects much larger scale speech recognition and text machine translation data (Fig. 1).  \n\nWe make the following contributions:  \n\n• We propose the Fused Acoustic and Text Masked Language Model (FAT-MLM), which can learn a unified acoustic and text representation. • Based on FAT-MLM, we propose the Fused Acoustic and Text Speech Translation model (FAT-ST), which can do speech recognition and machine translation in a single encoder-decoder framework. • Spontaneous speech translation experiments on three language pairs show that by finetuning FAT-MLM, the accuracy of FAT-ST improves end-to-end speech translation model by $+4.65$ BLEU in average and achieves state-of-the-art. This is the first time that an end-to-end speech translation model achieves similar performance with the strong cascaded system in these three translation directions of this dataset, while still maintaining a smaller model size and faster decoding time.  \n\n• We show that FAT-MLM trained with additional speech recognition, machine translation, and monolingual text data can improve FAT-ST by $+1.25$ BLEU. FAT-ST can be further improved by using additional speech recognition and machine translation data.  \n\n# 2. Previous Work  \n\n# 2.1. Masked Language Modeling  \n\nRadford et al. (2018), Howard & Ruder (2018) and Devlin et al. (2019) investigate language modeling for pretraining Transformer encoders. Unlike Radford et al. (2018) using unidirectional language models for pretraining, Devlin et al. (2019) proposes BERT which enables deep bidirectional representation pretraining by a masked language modeling (MLM) objective inspired by the Cloze task (Taylor, 1953) which randomly masks some of the tokens from the input, with an objective to recover the masked word based only on its context. Their approaches lead to drastic improvements on several natural language understanding tasks including text classification (Wang et al., 2018),and question answering (Rajpurkar et al., 2016).  \n\n# 2.2. Translation Language Modeling  \n\nLample & Conneau (2019) extend MLM to cross-lingual pretraining by proposing two methods: one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective which is called Translation Language Model (TLM). As shown in Fig. 2(b), TLM encodes both source and target sentences from a parallel data after masking several tokens with [MASK], and then learn to recover the masked tokens. Experiments show that TLM achieves state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation.  \n\n![](images/a76ac0169dacd9ec1c3c75c607f6e4e69d5a24abc8ca265221e36566b3b38913.jpg)  \nFigure 3. Fused Acoustic and Text-Masked Language Model (FAT-MLM).  \n\n# 2.3. Masked Acoustic Model  \n\nRecently, Chen et al. (2020) propose to learn a speech encoder in a self-supervised fashion on the speech side, which can utilize speech data without transcription. This technique termed Masked Acoustic Modeling (MAM), can also perform pretraining on any acoustic signals (including nonspeech ones) without annotation. Fig. 2(c) demonstrate the architecture of MAM. Similar with MLM, MAM replaces a span of speech spectrogram with mask tokens [MASK]. After a 2D Convolution layer and a Transformer Encoder, MAM learns to recover the masked spectrogram via a 2D De-convolution layer during training. Chen et al. (2020) shows that MAM can improve end-to-end speech translation as either an additional loss or a pretraining model. Parallel to MAM, Baevski et al. (2020) proposes the wav2vec 2.0 pretraining model, which masks the speech input in the latent space and pretrains the model via a contrastive task defined over a quantization of the latent representations.  \n\n# 3. Fused Acoustic and Text Masked Language Model (FAT-MLM)  \n\nAlthough existing pretraining models show a strong representation learning ability and significantly improve upon many down-streaming tasks, they all can only learn the representation for either text or speech. However, a unified speech and text multi-modal representation is useful for many end-to-end spoken language processing tasks.  \n\nTo address this problem, we propose the Fused Acoustic and Text Masked Language Model (FAT-MLM), a multimodal pretraining model which encodes acoustic, text into a unified representation. The idea is similar with Lample & Conneau (2019) who propose to learn a unified representation of different languages. They first propose a method relying on the shared sub-word vocabulary to align different languages’ representation. However this is unapplicable in our case because of the modality difference. Thus we propose a method similar to their second approach TLM which uses parallel speech recognition data. In the following sections, we first introduce the monolingual FAT-MLM and then show how to extend it to translation scenario.  \n\n# 3.1. Monolingual FAT-MLM  \n\nThe monolingual FAT-MLM takes speech and transcription tuples as input, denotes as $D_{\\mathbf{s},\\mathbf{x}}\\;=\\;\\{(\\mathbf{s},\\mathbf{x})\\}$ , where $\\mathbf s=(s_{1},...,s_{|s|})$ is a sequence of acoustic features $s_{i}\\in\\mathbb{R}^{d_{s}}$ which can be the spectrogram or mel-spectrogram of the speech audio, and each $s_{i}$ represents the frame-level speech feature, and $\\mathbf{x}\\,=\\,(x_{1},...,x_{|\\mathbf{x}|})$ is the sequence of corresponding transcription.  \n\nAs shown in Fig. 3(c), we first randomly mask several spans of s by a random masking function over the input s:  \n\n$$\n\\hat{\\mathbf{s}}\\sim\\mathbf{Mask}_{\\mathrm{span}}(\\mathbf{s},\\lambda)\n$$  \n\nwhere $\\mathrm{Mask}_{\\mathrm{span}}(\\cdot)$ replaces several random spans of s by probability of $\\lambda$ ( $30\\%$ in our work) with a random initialized vector $\\epsilon_{\\mathbf{s}}\\ \\in\\ \\mathbb{R}^{d_{\\mathbf{s}}}$ . Then we encode $\\hat{\\bf s}$ with Convolutions and a Transformer encoder for acoustic embeddings $e_{\\hat{\\mathbf{s}}}$ Similarly, we randomly mask tokens in $\\mathbf{x}$ by a random masking function over the input $\\mathbf{s},\\mathbf{x}$ :  \n\n$$\n\\hat{\\mathbf{x}}\\sim\\mathbf{Mask}_{\\mathrm{token}}(\\mathbf{x},\\lambda)\n$$  \n\nwhere $\\mathrm{Mask}_{\\mathrm{token}}(\\cdot)$ replaces several tokens of $\\mathbf{x}$ by probability of $\\lambda$ with a random initialized vector $\\epsilon_{\\mathrm{{token}}}\\in\\mathbb{R}^{d_{\\mathbf{x}}}$ . Then we concatenate acoustic embeddings and source text embeddings $\\left[\\hat{e}_{\\mathbf{s}};\\hat{\\mathbf{x}}\\right]$ , and obtain the latent representation $f([e_{\\hat{\\mathbf{s}}};\\hat{\\mathbf{x}}])$ using another Transformer encoder, denoted as $f$ . Same with Lample & Conneau (2019), we reset the positional embeddings for different types of input.  \n\nThe training objective of monolingual FAT-MLM includes a speech reconstruction loss $\\ell_{\\mathbf{s}}(D_{\\mathbf{s},\\mathbf{x}})$ and a text reconstruction loss $\\ell_{\\mathbf{x}}(D_{\\mathbf{s},\\mathbf{x}})$ . For speech input s, we have the following training objective to reconstruct the original speech signal with the surrounding context information1:  \n\n$$\n\\begin{array}{r}{\\ell_{\\mathbf{s}}(D_{\\mathbf{s},\\mathbf{x}})=\\sum_{(\\mathbf{s},\\mathbf{x})\\in D_{\\mathbf{s},\\mathbf{x}}}||\\mathbf{s}-g(f([e_{\\hat{\\mathbf{s}}};\\hat{\\mathbf{x}}])||_{2}^{2}}\\end{array}\n$$  \n\nwhere $g$ is a reconstruction function (we use 2D deconvolution in this work) which tries to recover the original signal from encoded representation $f([e_{\\hat{\\mathbf{s}}};\\hat{\\mathbf{x}}])$ . We use mean squared error for measuring the difference between $s$ and the reconstructed spectrogram. For transcription input $\\mathbf{x}$ , following Devlin et al. (2019) we use cross entropy loss , denoted as  \n\n$$\n\\begin{array}{r}{\\ell_{\\mathbf{x}}(D_{\\mathbf{s},\\mathbf{x}})=-\\sum_{(\\mathbf{s},\\mathbf{x})\\in D_{\\mathbf{s},\\mathbf{x}}}\\log p(\\mathbf{x}\\mid[e_{\\hat{\\mathbf{s}}};\\hat{\\mathbf{x}}])}\\end{array}\n$$  \n\nto reconstruct the masked token. The final loss for monolingual FAT-MLM is:  \n\n$$\n\\ell_{\\mathrm{FAT-MLM}}(D_{\\mathbf{s},\\mathbf{x}})=\\ell_{\\mathbf{s}}(D_{\\mathbf{s},\\mathbf{x}})+\\ell_{\\mathbf{x}}(D_{\\mathbf{s},\\mathbf{x}})\n$$  \n\n# 3.2. Translation FAT-MLM  \n\nTo support multimodal crosslingual tasks such as speech translation, We propose Translation FAT-MLM which extends Monolingual FAT-MLM by using additional target language translation of the source language transcription as input. Formally Translation FAT-MLM takes $\\cal D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}}=$ $\\{(\\mathbf{s},\\mathbf{x},\\mathbf{y})\\}$ as input, where $\\mathbf{y}\\,=\\,[y_{1},...,y_{|y|}]$ denotes the sequence of target language translation. This kind of triplet input is very common in speech translation corpus.  \n\nAs shown in Fig. 3(d), we incorporate source language embedding $e_{\\mathrm{src}}$ and target language embedding $e_{\\mathrm{tgt}}$ for different languages to show the language difference. Similar to Monolingual FAT-MLM, Translation FAT-MLM randomly masks the translation input $\\hat{\\mathbf{y}}\\sim\\mathrm{Mask}_{\\mathrm{token}}(\\mathbf{y},\\lambda)$ and concatenate it with another two embeddings:  \n\n$$\n\\mathbf{h_{s,x,y}}=[e_{\\hat{\\mathbf{s}}}+e_{\\mathrm{src}};\\hat{\\mathbf{x}}+e_{\\mathrm{src}};\\hat{\\mathbf{y}}+e_{\\mathrm{tgt}}]\n$$  \n\nThen we reconstruct masked input from concatenated embeddings $\\mathbf{h}_{\\mathbf{s},\\mathbf{x},\\mathbf{y}}$ via a Transformer encoder. The reconstruction loss for different masked input is:  \n\n$$\n\\begin{array}{r l}&{\\ell_{\\mathbf{s}}(D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})=\\sum_{(\\mathbf{s},\\mathbf{x},\\mathbf{y})\\in D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}}}||\\mathbf{s}-g(f(\\mathbf{h}_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})||_{2}^{2})}\\\\ &{\\ell_{\\mathbf{x}}(D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})=-\\sum_{(\\mathbf{s},\\mathbf{x},\\mathbf{y})\\in D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}}}\\log p(\\mathbf{x}\\mid\\mathbf{h}_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})}\\\\ &{\\ell_{\\mathbf{y}}(D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})=-\\sum_{(\\mathbf{s},\\mathbf{x},\\mathbf{y})\\in D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}}}\\log p(\\mathbf{y}\\mid\\mathbf{h}_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})}\\end{array}\n$$  \n\nWe sum these loss functions for the final loss function of Translation FAT-MLM:  \n\n$$\n\\ell_{\\mathrm{FAT-MLM}}(D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})=\\ell_{\\mathbf{s}}(D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})+\\ell_{\\mathbf{x}}(D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})+\\ell_{\\mathbf{y}}(D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})\n$$  \n\nTo fully utilize the corpora for different tasks, FAT-MLM can take any combination of speech, transcription, translation triplets $D_{2^{\\{\\mathbf{s},\\mathbf{x},\\mathbf{y}\\}}}$ as input.2 Specifically, these combinations include speech only data $\\{\\mathbf{s}\\}$ , monolingual text data, $\\{{\\bf x}\\}$ or $\\{\\mathbf{y}\\}$ , speech and transcription tuple $\\{(\\mathbf{s},\\mathbf{x})\\}$ for speech recognition, transcription and translation tuple $\\{(\\mathbf{x},\\mathbf{y})\\}$ for machine translation, speech and translation tuple $\\{(\\mathbf{s},\\mathbf{y})\\}$ for direct speech translation and speech transcription translation triplets $\\{(\\mathbf{s},\\mathbf{x},\\mathbf{y})\\}$ . For different combinations of input, FAT-MLM encodes the full concatenation of their embeddings and recover the masked portion. The loss function is:  \n\n$$\n\\ell_{\\mathrm{FAT-MLM}}(D_{2\\{\\mathbf{s},\\mathbf{x},\\mathbf{y}\\}})=\\ell_{\\mathbf{s}}(D_{\\mathbf{s}\\star})\\,{+}\\ell_{\\mathbf{x}}(D_{\\mathbf{x}\\star})\\,{+}\\ell_{\\mathbf{y}}(D_{\\mathbf{y}\\star})\n$$  \n\nwhere $D_{\\mathbf{s}\\star},\\,D_{\\mathbf{x}\\star},\\,D_{\\mathbf{y}\\star}$ means any input including speech, source language text and target language text respectively. Note that in this framework, we can denote MLM as $\\ell_{\\mathbf{x}}(D_{\\mathbf{x}})$ , TLM as $\\ell_{\\mathbf{x},\\mathbf{y}}(D_{\\mathbf{x},\\mathbf{y}})$ , MAM as $\\ell_{\\mathbf{s}}(\\mathbf{s})$ .  \n\n# 3.3. Attention Visualization  \n\nTo demonstrate FAT-MLM’s ability to unify the representation of different modality and language, we show the self-attention layers of a translation FAT-MLM in Fig. 4 and 5. The clear monotonic attention in Fig. 4 shows that our proposed method can learn good representation for speech (Chen et al., 2020). Fig. 5(a) shows that FAT-MLM can learn a good crosslingual alignment between two languages, such as and to Und and you to Sie. Fig. 5(b) shows that FAT-MLM is able to learn a clear monotonic speech-to-text crossmodal attention like many speech recognition models.  \n\n![](images/cfaa952246284d7f242aaecfbd660f948b96b4930a9b1f846a86de5f68e65833.jpg)  \nFigure 4. One speech self-attention head’s output at the first transformer layer in acoustic embedding module and its corresponding spectrogram. This is a Translation FAT-MLM model trained with Must-C En $\\rightarrow\\mathrm{De}$ dataset.  \n\n# 4. Fused Acoustic and Text Speech Translation (FAT-ST)  \n\nIn this section, we present how to adapt FAT-MLM to speech translation and enable speech translation models to learn from speech recognition and text machine translation.  \n\n# 4.1. From Text Translation to Speech Translation  \n\nRegardless of the particular design of different seq-to-seq models, the text machine translation encoder always takes the input sequence ${\\bf x}=(x_{1},...,x_{n})$ where each $x_{i}\\in\\mathbb{R}^{d_{x}}$ is a word embedding of $d_{x}$ dimensions, and produces a new sequence of hidden states $\\mathbf{h}=f(\\mathbf{x})=(h_{1},...,h_{n})$ . On the other hand, a decoder predicts the next output word $y_{t}$ given the source sequence (actually its representation $\\mathbf{h}$ ) and previously generated words, denoted $\\mathbf{y}_{<t}=(y_{1},...,y_{t-1})$ . The decoder stops when it emits $<\\ominus\\lor S>$ , and the final hypothesis $\\mathbf{y}=(y_{1},...,..\\mathbf{e}_{0\\,\\mathtt{S}}.)$ has probability  \n\n$$\n\\begin{array}{r}{p(\\mathbf{y}\\mid\\mathbf{x})_{\\mathrm{MT}}=\\prod_{t=1}^{|\\mathbf{y}|}p(y_{t}\\mid\\mathbf{x},\\,\\mathbf{y}_{<t})}\\end{array}\n$$  \n\nAt training time, we maximize the conditional probability of each ground-truth target sentence $\\mathbf{y}^{\\star}$ given input $\\mathbf{x}$ over the whole training data $D_{\\mathbf{x},\\mathbf{y}}$ , or equivalently minimizing the following loss:  \n\n$$\n\\begin{array}{r}{\\ell_{\\mathrm{MT}}(D_{\\mathbf{x},\\mathbf{y}})=-\\sum_{(\\mathbf{x},\\mathbf{y})\\in D_{\\mathbf{x},\\mathbf{y}}}\\log p(\\mathbf{y}\\mid\\mathbf{x})}\\end{array}\n$$  \n\nDifferent from text machine translation, speech translation takes speech features $\\mathbf{s}=(s_{1},...,s_{|\\mathbf{s}|})$ as input. Same as the speech input portion of FAT-MLM, these speech features are converted from the speech signals (e.g. spectrogram).  \n\n![](images/2248df14f9311f4b7c58a745dc73202212f3606fed22784f5123d9b1d8f48cc0.jpg)  \n(a) This self-attention head shows bilingual alignment between “and’‘ and “Und”, “you’‘ and “Sie”, “what” and “?” in transcription and translation respectively.  \n\n![](images/080f63b9571078323aaf0d1d901460abe1802a7e45ed3b476d15c2555d634757.jpg)  \n(b) Left side spectrogram shows gold speech-transcription alignment. This self-attention head shows monotonic crossmodal attention in red box. Meanwhile, the speech-totranslation attention (in blue box) clearly show the alignment between “you’‘ and “Sie”, “know” and “wissen” in speech and translation respectively. Note that in this speech, the pronounciation of “and” is very weak.  \n\nFigure 5. Two self-attention heads’ output at the first layer of acoustic and text shared transformerfrom a Translation FAT-MLM model trained with Must-C En $\\rightarrow$ De dataset, annotated with corresponding spectrogram, transcription (red) and translation (blue).  \n\nFormally, the decoding and training of speech translation models can be defined as follows:  \n\n$$\n\\begin{array}{r}{p(\\mathbf{y}\\mid\\mathbf{s})_{\\mathrm{ST}}=\\prod_{t=1}^{|\\mathbf{y}|}p(y_{t}\\mid\\mathbf{s},\\,\\mathbf{y}_{<t})}\\end{array}\n$$  \n\n$$\n\\begin{array}{r}{\\ell_{\\mathrm{ST}}(D_{\\mathbf{s},\\mathbf{y}})=-\\sum_{(\\mathbf{s},\\mathbf{y})\\in D_{\\mathbf{s},\\mathbf{y}}}\\log p(\\mathbf{y}\\mid\\mathbf{s})}\\end{array}\n$$  \n\n![](images/1ddd32eb4a7fadd2b5e3fe36927f52d0df905551b9239249e15615c6813c2243.jpg)  \nFigure 6. Fused Acoustic and Text-Speech Translation (FAT-ST).  \n\n# 4.2. FAT-ST  \n\nTo boost the performance of end-to-end speech translation, we propose to enable speech translation to encode both acoustic and text features as input by simply adapting the architecture of monolingual FAT-MLM to a Fused Acoustic and Text Speech Translation model (FAT-ST).  \n\nFAT-ST’s encoder shares identical architecture with monolingual FAT-MLM. In this way, we can simply encode either acoustic or text features by this encoder and the FAT-ST model can be optimized by speech translation loss $\\ell_{\\mathrm{ST}}$ , machine translation loss $\\ell_{\\mathrm{MT}}$ and FAT-MLM loss $\\ell_{\\mathrm{FAT-MLM}}$ . For a speech translation dataset $D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}}$ , we decouple the triplets into three part $D_{\\mathbf{s},\\mathbf{y}}$ for $\\ell_{\\mathrm{ST}}$ , $D_{\\mathbf{s},\\mathbf{x}}$ for $\\ell_{\\mathrm{FAT-MLM}}$ and $D_{\\mathbf{x},\\mathbf{y}}$ for $\\ell_{\\mathrm{MT}}$ . The loss function of FAT-ST is:  \n\n$$\n\\begin{array}{r}{\\ell_{\\mathrm{FAT-ST}}(D_{\\mathbf{s},\\mathbf{y}}\\cup D_{\\mathbf{s},\\mathbf{x}}\\cup D_{\\mathbf{x},\\mathbf{y}})=\\ell_{\\mathrm{ST}}(D_{\\mathbf{s},\\mathbf{y}})+\\ell_{\\mathrm{MT}}(D_{\\mathbf{x},\\mathbf{y}})}\\\\ {+\\ell_{\\mathrm{FAT-MLM}}(D_{\\mathbf{s},\\mathbf{x}})}\\end{array}\n$$  \n\nPlease note that the speech recognition and machine translation data can either be included in speech translation data or additional datasets. Meanwhile, in practice, we find that CTC loss (Graves et al., 2006) is useful to improve the translation quality so that we include it in all the experiments.  \n\n# 4.3. Finetuning FAT-ST from Translation FAT-MLM  \n\nSimilar to Lample & Conneau (2019) we can further improve FAT-ST by finetuning from FAT-MLM. Since the FAT-ST decoder predicts text only, we initialize it from the acoustic and text shared Transformer encoder. Although Transformer decoder is unidirectional which is different from bidirectional FAT-MLM, it can still benefit from FATMLM in our experiments, This is also observed by Lample & Conneau (2019) and Devlin et al. (2019).  \n\n# 5. Experiments  \n\nWe conducted speech translation experiments in 3 directions: English to German $\\scriptstyle{\\mathrm{En}}\\to{\\mathrm{De}}$ ), English to Spanish $(\\mathrm{En}{\\rightarrow}\\mathrm{Es})$ ), and English to Dutch $(\\mathrm{En}{\\rightarrow}\\mathrm{Nl})$ ) to show the translation quality of baselines and our proposed methods.  \n\n5.1. Dataset   \n(a) Bilingual Dataset   \n\n\n<html><body><table><thead><tr><td rowspan=\"2\"><b>Type</b></td><td rowspan=\"2\"><b>Name</b></td><td colspan=\"2\"><b>En → De</b></td><td colspan=\"2\"><b>En →Es</b></td><td colspan=\"2\"><b>En → NI</b></td></tr><tr><td><b>Hours</b></td><td><b>#Sent</b></td><td><b>Hours</b></td><td><b>#Sent</b></td><td><b>Hours</b></td><td><b>#Sent</b></td></tr></thead><tbody><tr><td>Ds,x,y</td><td>Must-C ST</td><td>408</td><td>226K</td><td>504</td><td>262K</td><td>442</td><td>245K</td></tr><tr><td>Dx.y</td><td>Europarl MT</td><td>-</td><td>1.9M</td><td>-</td><td>2.0M</td><td>-</td><td>2.0M</td></tr></tbody></table></body></html>  \n\n<html><body><table><thead><tr><td rowspan=\"2\"><b>Type</b></td><td rowspan=\"2\"><b>Name</b></td><td colspan=\"2\"><b>En</b></td><td rowspan=\"2\"><b>De #Sent</b></td><td rowspan=\"2\"><b>Es #Sent</b></td><td rowspan=\"2\"><b>N1 #Sent</b></td></tr><tr><td><b>Hours</b></td><td><b>#Sent</b></td></tr></thead><tbody><tr><td>Ds,x</td><td>Librispeech ASR</td><td>960</td><td>281K</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Ds</td><td>Libri-light Speech</td><td>3,748</td><td>579K</td><td>-</td></tr><tr><td>Dx/Dy</td><td>Europarl / Wiki Text</td><td>-</td><td>2.3M</td><td>2.1M</td></tr></tbody></table></body></html>  \n\nTable 1. Statistics of all datasets used in our experiments. Note that we use Europarl for En, De, Es monolingual text and Wiki Text for Nl because there is no monolingual Nl portion in Europarl. #Sent means the number of sentences.  \n\nWe use 5 corpora with different modalities and languages: speech translation data $D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}}$ Must-C (Di Gangi et al., 2019), speech recognition data $D_{\\mathbf{s},\\mathbf{x}}$ Librispeech (Panayotov et al., 2015), machine translation and monolingual text data $D_{\\mathbf{x},\\mathbf{y}},D_{\\mathbf{x}},D_{\\mathbf{y}}$ Europarl V7 (Koehn, 2005), speech only data $D_{\\mathbf{s}}$ Libri-Light (medium version) (Kahn et al., 2020) and monolingual text data Wiki Text (only for Nl). The statistical results of the dataset are shown in Table. 1. We evaluate our models on Must-C dev and test set. Note that Must-C is collected based on spontaneous speeches (TED) which are very different from other audiobook speech dataset used in our experiments. Spontaneous speeches are much harder for speech translation than audiobook dataset such as Libri-trans (Kocabiyikoglu et al., 2018). That is one of the reasons why the translation accuracy of end-to-end speech translation is much worse than cascaded systems on Must-C than other speech translation corpus.  \n\n# 5.2. Training Detail  \n\nRaw audio flies are processed by Kaldi (Povey et al., 2011) to extract 80-dimensional log-Mel filterbanks stacked with 3-dimensional pitch features using a window size of $25~\\mathrm{ms}$ and step size of $10~\\mathrm{{ms}}$ . We train sentencepiece (Kudo & Richardson, 2018) models with a joint vocabulary size of 8K for text in each dataset. Training samples that have more than 3000 frames have been ignored for GPU efficiency. Our basic Transformer-based E2E-ST framework has similar settings with ESPnet-ST(Inaguma et al., 2020). the speech input is first down-sampled the speech input with 2 layers of 2D convolution of size 3 with stride size of 2. Then there is a standard 12-layers Transformer with feed-forward layer of 2048 hidden size to bridge the source and target side. We only use 4 attention heads on each side of the transformer and each of them has a dimensionality of 256. We also show the results of FAT-ST big model with 4096 hidden  \n\nFused Acoustic and Text Encoding for Multimodal Bilingual Pretraining and Speech Translation   \n\n\n<html><body><table><thead><tr><td><b>Pretrain Method</b></td><td><b>Models</b></td><td><b>En→De</b></td><td><b>En→Es</b></td><td><b>En→N1</b></td><td><b>Avg.</b></td><td><b>Model Size</b></td></tr></thead><tbody><tr><td rowspan=\"9\">No  Pretraining</td><td>ST</td><td>19.64</td><td>23.68</td><td>23.01</td><td>22.11</td><td>31.25M</td></tr><tr><td>ST+ASR</td><td>21.70</td><td>26.83</td><td>25.44</td><td>24.66 (+2.55)</td><td>44.82M</td></tr><tr><td>ST + ASR & MT</td><td>21.58</td><td>26.37</td><td>26.17</td><td>24.71 (+2.60)</td><td>56.81M</td></tr><tr><td>ST + MAM</td><td>20.78</td><td>25.34</td><td>24.46</td><td>23.53 (+1.42)</td><td>33.15M</td></tr><tr><td>ST + MAM + ASR</td><td>22.41</td><td>26.89</td><td>26.49</td><td>25.26 (+3.15)</td><td>46.72M</td></tr><tr><td>Liu et al. (2020b)</td><td>22.55</td><td>26.83</td><td>26.03</td><td>24.94 (+2.83)</td><td>31.25M</td></tr><tr><td>Le et al. (2020)</td><td>23.63</td><td>28.12</td><td>27.55</td><td>26.43 (+4.32)</td><td>51.20M</td></tr><tr><td>Cascades</td><td>23.65</td><td>28.68</td><td>27.91</td><td>26.75 (+4.64)</td><td>83.79M</td></tr><tr><td>FAT-ST (base).</td><td>22.70</td><td>27.86</td><td>27.03</td><td>25.86 (+3.75)</td><td>39.34M</td></tr><tr><td>MAM</td><td>FAT-ST (base)</td><td>22.29</td><td>27.21</td><td>26.26</td><td>25.25 (+3.14)</td><td>39.34M</td></tr><tr><td>FAT-MLM</td><td>FAT-ST (base) FAT-ST (big)</td><td>23.68 23.64</td><td>28.61 29.00</td><td>27.84 27.64</td><td>26.71 (+4.60)</td><td>39.34M</td></tr></tbody></table></body></html>  \n\nTable 2. BLEU comparisons on Must-C test set between our proposed methods and other baselines over 3 translation directions using MuST-C $(D_{\\mathbf{s},\\mathbf{x},\\mathbf{y}})$ only (including pretraining methods). § are reported in Inaguma et al. (2020).   \n\n\n<html><body><table><thead><tr><td><b> Pretrain Data</b></td><td><b> Pretrain Method</b></td><td><b>Train Data</b></td><td><b>Models</b></td><td><b>En→De</b></td><td><b>En→Es</b></td><td><b>En→N1</b></td><td><b>Avg.</b></td></tr></thead><tbody><tr><td></td><td></td><td rowspan=\"4\">Ds,x,y</td><td>ST Cascades</td><td>19.64 23.65</td><td>23.68 28.68</td><td>23.01 27.91</td><td>22.11 26.75 (+4.64)</td></tr><tr><td rowspan=\"2\">Ds,x,y U Ds,x U Dx.y</td><td>ASR & MT</td><td>ST ST + ASR & MT</td><td>22.20 22.73</td><td>27.16 27.99</td><td>26.15 27.12</td><td>25.17 (+3.06) 25.95 (+3.84)</td></tr><tr><td>FAT-MLM</td><td>FAT-ST (base) FAT-ST (big)</td><td>23.98 24.34</td><td>28.95 29.41</td><td>28.08 28.86</td><td>27.00 (+4.89) 27.54 (+5.43)</td></tr><tr><td>Ds,x,y U Ds,x U Dx.y UDs U Dx U Dy</td><td>FAT-MLM</td><td>FAT-ST (base) FAT-ST (big)</td><td>24.02 24.58</td><td>29.25 30.10</td><td>28.28 29.36</td><td>27.18 (+5.07) 28.01 (+5.90)</td></tr><tr><td>Ds,x,y U Ds,x U Dx,y UDs U Dx U Dy</td><td>FAT-MLM</td><td>Ds,xy Ds,x U Dx,y</td><td>FAT-ST (base) FAT-ST (big)</td><td>23.91 25.47</td><td>29.01 30.75</td><td>28.18 30.08</td><td>27.03 (+4.92) 28.77 (+6.66)</td></tr><tr><td></td><td></td><td>Ds,x.y + Ds,y</td><td>Pino et al. (2020)</td><td>25.2</td><td>-</td><td>-</td><td>-</td></tr></tbody></table></body></html>  \n\nTable 3. BLEU comparisons on Must-C test set between our proposed methods using additional data. $D_{\\mathbf{s},\\mathbf{x}}$ : Librispeech, $D_{\\mathbf{x},\\mathbf{y}}$ : Europarl MT, $D_{\\mathbf{s}}$ : libri-light, $D_{\\mathbf{x}},D_{\\mathbf{y}}$ : monolingual data from Europarl or Wiki Text. § are reported in Inaguma et al. (2020). Pino et al. (2020) use extra $D_{\\mathbf{s},\\mathbf{y}}^{\\prime}$ which includes Librispeech $(D_{\\mathbf{s},\\mathbf{x}})$ and 35,217 hour version of Libri-light speech data (almost $10\\times$ of our $D_{\\mathbf{s}}$ ) paired with their corresponding pseudo-translations generated by ASR and MT models. Their model size is $435.0\\mathrm{M}$ .  \n\nsize for feed-forward layers of all transformer layer. For speech reconstruction module, we simply linearly project the outputs of the Transformer encoder to another latent space, then upsample the latent representation with 2-layers deconvolution to match the size of the original input signal. We choose $30\\%$ for the random masking ratio $\\lambda$ across all the experiments including pre-training. During inference, we do not perform any masking over the speech input. We average the last 5 checkpoints for testing. For decoding, we use a beam search with beam-size 5 and length penalty 0.6 for German, 0.0 for Spanish and 0.3 for Dutch.  \n\n# 5.3. Translation Quality Comparisons  \n\nWe showcase the translation accuracy of FAT-ST comparing against to the baselines in Table 2 and Table 3:  \n\n• ST: this is the vanilla speech translation system which does not use transcriptions.   \n• ST $^+$ ASR MTL: ST model with an additional ASR decoder and is trained with ASR multi-task learning using the transcriptions.   \n• $\\mathbf{S}\\mathbf{T}+\\mathbf{A}\\mathbf{S}\\mathbf{R}$ & MT MTL: ST model with an additional ASR decoder and a MT encoder. It is trained with ASR and MT multi-task learning.   \n• ST $^+$ MAM: ST trained with additional MAM loss (Chen et al., 2020) which can be formalized as $\\ell_{\\mathbf{s}}(D_{\\mathbf{s}})$ (See Fig. 2(c)).   \n• $\\mathbf{S}\\mathbf{T}+\\mathbf{M}\\mathbf{A}\\mathbf{M}+\\mathbf{A}\\mathbf{S}\\mathbf{R}$ MTL: ST trained with MAM loss and ASR multi-task learning.   \n• Liu et al. (2020b): An end-to-end ST system with a multimodal encoder.  \n\nFused Acoustic and Text Encoding for Multimodal Bilingual Pretraining and Speech Translation   \nTable 4. Models sizes of different models.   \n\n\n<html><body><table><thead><tr><td><b>Model</b></td><td><b># Parameters</b></td></tr></thead><tbody><tr><td>MAM</td><td>23.69 M</td></tr><tr><td>FAT-MLM (base)</td><td>25.76 M</td></tr><tr><td>FAT-MLM (big)</td><td>38.36 M</td></tr></tbody></table></body></html>  \n\n• Le et al. (2020): The state-of-the-art end-to-end ST model with an extra ASR decoder.  \n\n• Cascade: cascaded model which first transcribes the speech into transcription then passes the results to a machines translation system.  \n\n• $\\mathbf{S}\\mathbf{T}+\\mathbf{A}\\mathbf{S}\\mathbf{R}$ & MT pretraining: the encoder of ST is initialized by a pretrained ASR encoder and decoder initialized by a pretrained MT decoder  \n\n• Pino et al. (2020): They propose to leverage additional speech data by generating pseudo-translations using a cascaded or an end-to-end speech translation model.  \n\n# 5.3.1. MODEL SIZE OF PRETRAINING MODELS  \n\nTable 4 shows the number of parameters of different pretraining models. We can see that our FAT-MLM base model is a little bit larger than the MAM pretraining model, and the FAT-MLM big model is much larger than the base model.  \n\n# 5.3.2. TRAINING WITH Ds,x,y  \n\nIn Table 2, with no pretraining, we can see that our proposed FAT-ST base model achieves the best results except Le et al. (2020) and the cascaded model. However, our base model has much less parameters than both of them. Models with ASR or MT MTL and Liu et al. (2020b) all use the transcription data in Must-C dataset but show worse performance, thus our model can use transcription data more efficiently. Similar to other open source ST implementation results on Must-C 3, our implementation of $\\mathrm{ST}+$ ASR & MT MTL is worse than $\\mathrm{ST}+\\mathrm{ASR}$ .  \n\nWe also compare the performance of models pretrained from different pretraining models. With pretrained on MustC, FAT-ST (base) is improved by 0.85 BLEU by being finetuned from FAT-MLM, while it’s performance drops by finetuning from MAM. Meanwhile, our proposed methods achieve much better performance compared with ASR & MT pretraining baselines. We also note that our FAT-ST base model for the first time achieves similar performances compared with Cascade baselines in these three translation directions of Must-C, while comparing with the cascaded model, our our base model is much smaller in size and faster in inference (see Fig. 7).  \n\nTable 5. Comparisons of the auxiliary MT task between MT baselines and our proposed methods. § are reported in Inaguma et al. (2020).   \n\n\n<html><body><table><thead><tr><td><b>Train Data</b></td><td><b>Pretrain Data</b></td><td><b>Models</b></td><td><b>→De</b></td><td><b>→Es</b></td><td><b>→NI</b></td></tr></thead><tbody><tr><td rowspan=\"9\">Ds,x.y</td><td rowspan=\"2\">No  pretraining</td><td>MTS</td><td>27.63</td><td>32.61</td><td>32.08</td></tr><tr><td>FAT-ST (base)</td><td>24.41</td><td>30.81</td><td>29.18</td></tr><tr><td rowspan=\"2\">Ds,x,y</td><td>FAT-ST (base)</td><td>27.24</td><td>31.98</td><td>31.27</td></tr><tr><td>FAT-ST (big)</td><td>26.92</td><td>32.29</td><td>31.48</td></tr><tr><td rowspan=\"2\">Ds,x.y UDs,xU Dx.y</td><td>FAT-ST (base)</td><td>27.43</td><td>32.38</td><td>32.44</td></tr><tr><td>FAT-ST (big)</td><td>27.60</td><td>32.95</td><td>32.37</td></tr><tr><td rowspan=\"2\">Ds.x.yUDsxU Dx.y UDs U Dx U Dy</td><td>FAT-ST (base)</td><td>27.63</td><td>32.75</td><td>32.52</td></tr><tr><td>FAT-ST (big)</td><td>28.13</td><td>33.39</td><td>32.72</td></tr><tr><td rowspan=\"2\">Ds,x.y UDsxUDxy</td><td rowspan=\"2\">Ds.x.yU DsxU Dx.y UDs U DxU Dy</td><td>FAT-ST (base)</td><td>27.89</td><td>32.96</td><td>32.43</td></tr><tr><td>FAT-ST (big)</td><td>28.80</td><td>34.28</td><td>34.22</td></tr></tbody></table></body></html>  \n\n# 5.3.3. PRETRAINING WITH ADDITIONAL DATA  \n\nTable 3 shows that FAT-MLM can further improve FATST by simply adding speech recognition data $D_{\\mathbf{s},\\mathbf{x}}$ (Librispeech) text machine translation data $D_{\\mathbf{x},\\mathbf{y}}$ (Europarl) and even speech only data $D_{\\mathbf{s}}$ (Libri-light) and monolingual text data $D_{\\mathbf{x}}\\cup D_{\\mathbf{y}}$ . This shows good representation learning ability of our proposed FAT-MLM models. We can see that using larger data, the performance of our big model is increased much faster than the base model. That’s because the number of parameters of the base model is too limited to learn from such big data.  \n\n# 5.3.4. FINETUNING WITH ADDITIONAL DATA  \n\nThe last part of Table 2 show that FAT-ST can be improved by learning from extra speech recognition and machine translation data. This is promising because speech translation data is very limited compared with much more abundant speech recognition and machine translation data. Different from Pino et al. (2020) who propose to leverage additional speech data by generating pseudo-translations, our method doesn’t use any pseudo-labels. Our best model outperforms their result on $\\mathrm{En\\mathrm{\\rightarrow}D e}$ by using much $7\\times$ smaller model size and almost $10\\times$ smaller speech data.  \n\n<html><body><table><thead><tr><td><b>Model</b></td><td><b>En→De</b></td></tr></thead><tbody><tr><td>FAT-ST with FAT-MLM (base)</td><td>23.68</td></tr><tr><td>- FAT-MLM decoder init.</td><td>23.20</td></tr><tr><td>- FAT-MLM encoder init.</td><td>22.70</td></tr><tr><td>- CTC loss</td><td>22.30</td></tr><tr><td>- Hierarchical Transformer</td><td>22.07</td></tr><tr><td>- FAT-MLM loss</td><td>20.64</td></tr><tr><td>- MT loss</td><td>19.64</td></tr></tbody></table></body></html>  \n\nTable 6. Ablation study. Here, hierarchical transformer means the model only shares the 6 layers of the transformer encoder for acoustic feature input and text feature input.  \n\nFused Acoustic and Text Encoding for Multimodal Bilingual Pretraining and Speech Translation   \n\n\n<html><body><table><thead><tr><td><b>Speech transcription Target reference</b></td><td><b>those are their expectations of who you are not yours</b></td><td><b>that is they expected your appearance notyourself expectation</b></td><td><b>J。</b></td></tr></thead><tbody><tr><td>Cascade-ASR</td><td>those are there expectations to do you are not yours</td><td>that is they expected your appearance notyourself expectation</td><td>J。</td></tr><tr><td>Cascade-Translation</td><td>那些都是希望做到的，你不是你的 those are expect achievement you not yours</td><td>that is they expected your appearance notyourself expectation</td><td>J。</td></tr><tr><td>FAT-ST</td><td>这些是他们对你的期望，而不是你的期望。</td><td>these are they to your expectation  noty</td><td>your  expectation</td></tr></tbody></table></body></html>  \n\nTable 7. English-to-Chinese speech translation example. The cascaded system is our implementation using the TED training data. The errors of cascaded model is highlighted in red.  \n\nTable 8. BLEU comparisons on English-to-Chinese speech translation. \\* is our implementation. Cascaded model is implemented by Dong et al. (2020).   \n\n\n<html><body><table><thead><tr><td><b>Models</b></td><td><b>En→Zh</b></td></tr></thead><tbody><tr><td>KD (Liu et al., 2019)</td><td>19.55</td></tr><tr><td>LUT (Dong et al., 2020)</td><td>20.84</td></tr><tr><td>COSTT (Dong et al., 2021)</td><td>21.12</td></tr><tr><td>Cascade (Dong et al., 2020)</td><td>21.36</td></tr><tr><td>ST*</td><td>22.07</td></tr><tr><td>FAT-ST</td><td>23.73</td></tr><tr><td>FAT-MLM + FAT-ST</td><td>25.49</td></tr></tbody></table></body></html>  \n\n# 5.3.5. PERFORMANCE OF AUXILIARY MT TASK  \n\nTable 5 shows the translation quality of auxiliary MT task of FAT-ST. Although our models trained with Must-C are worse than the MT baseline, by using FAT-MLM trained with more data, our proposed methods can easily outperform the MT baseline. Note that these models’ parameters are tuned to optimize speech translation task and MT is just an auxiliary task.  \n\n# 5.3.6. ABLATION STUDY  \n\nTable 6 shows an ablation study of our proposed method. we can see that all the components contribute to the final performance.  \n\n# 5.3.7. ENGLISH $\\rightarrow$ CHINESE SPEECH TRANSLATION  \n\nWe also compare several models in TED English $\\rightarrow$ Chinese speech translation task (Liu et al., 2019) with 524 hours speech in training set, 1.5 hours validation set (dev2010) and 2.5 hours test set (tst2015). We follow our previous experiments to preprocess the data. Same with previous work, we evaluate the performance with character-level BLEU. Table 8 shows that our proposed model can largely outperform other baselines. Table 7 shows one example in this dataset. The translation of the cascaded model is wrong because of the errors in the its ASR (their $\\rightarrow$ their, of who $\\rightarrow$ to do), while our FAT-ST produces the right translation.  \n\n![](images/66afa48e1dc8a31d3b4b5350e7fda6490bee60a9c0e52c6294be6149fbc9ac04.jpg)  \nFigure 7. Decoding time comparison between Cascaded model (including its ASR) and FAT-ST.  \n\n# 5.3.8. DECODING SPEED  \n\nFig. 7 shows the decoding speed comparison between the Cascade model and our proposed FAT-ST. Our proposed FAT-ST model is almost $2\\times$ faster than the Cascade system which needs to wait for the speech recognition module to finish before starting to translate. The decoding time of FAT-ST (big) is almost the same as FAT-ST (base) because we only increase the feedforward network in Transformers.  \n\n# Conclusion  \n\nIn this paper, we propose Fused Acoustic and Text Masked Language Model (FAT-MLM) which learns a unified representation for text and speech from any data that combines speech and text. We further extend this framework to a sequence-to-sequence speech translation model which enables learning from speech recognition and text-based machine translation data at the first time. Our results show significant improvement on three translation directions of the Must-C dataset and outperform the cascaded baseline.  \n\n# Acknowledgements  \n\nWe thank Kenneth Church and Jiahong Yuan for discussions, and Juneki Hong for proofreading, and the anonymous reviewers for suggestions", "appendix": "."}, {"title": "A Comprehensive Survey on Transfer Learning", "authors": "Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, Qing He", "bibkey": "a_comprehensive_survey_on_transfer_learning", "bibitem": "@article{Zhuang_IEEE2021,\n  url = {http://arxiv.org/abs/1911.02685v3},\n  title = {A Comprehensive Survey on Transfer Learning},\n  authors = {Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, Qing He},\n  abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},\n  arxiv_id = {1911.02685v3},\n  subject = {cs.LG},\n  submission_date = {2019-11-07T00:15:02Z}\n}", "url": "http://arxiv.org/abs/1911.02685v3", "latex_url": "http://arxiv.org/src/1911.02685v3", "latex_path": "output/download_papers/1911.02685v3/1911.02685v3", "pdf_url": "http://arxiv.org/pdf/1911.02685v3", "pdf_path": "output/download_papers/1911.02685v3/1911.02685v3.pdf", "md_url": null, "latex_length": 0, "latex": "", "abstract": "Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.", "abstract_length": 1724, "abstract_token": 282, "introduction": "LTHOUGH traditional machine learning technology has achieved great success and has been successfully applied in many practical applications, it still has some limitations for certain real-world scenarios. The ideal scenario of machine learning is that there are abundant labeled training instances, which have the same distribution as the test data. However, collecting sufficient training data is often expensive, time-consuming, or even unrealistic in many scenarios. Semi-supervised learning can partly solve this problem by relaxing the need of mass labeled data. Typically, a semisupervised approach only requires a limited number of labeled data, and it utilizes a large amount of unlabeled data to improve the learning accuracy. But in many cases, unlabeled instances are also difficult to collect, which usually makes the resultant traditional models unsatisfactory. Transfer learning, which focuses on transferring the knowledge across domains, is a promising machine learning methodology for solving the above problem. The concept about transfer learning may initially come from educational psychology. According to the generalization theory of transfer, as proposed by psychologist C.H. Judd, learning to transfer is the result of the generalization of experience. It is possible to realize the transfer from one situation to another, as long as a person generalizes his experience. According to this theory, the prerequisite of transfer is that there needs to be a connection between two learning activities. In practice, a person who has learned the violin can learn the piano faster than others, since both the violin and the piano are musical instruments and may share some common knowledge. Fig. 1 shows some intuitive examples about transfer learning. Inspired by human beings’ capabilities to transfer knowledge across domains, transfer learning aims to leverage knowledge from a related domain (called source domain) to improve the learning performance or minimize the number of labeled examples required in a target domain. It is worth mentioning that the transferred knowledge does not always bring a positive impact on new tasks. If there is little in common between domains, knowledge transfer could be unsuccessful. For example, learning to ride a bicycle cannot help us learn to play the piano faster. Besides, the similarities between domains do not always facilitate learning, because sometimes the similarities may be misleading. For example, although Spanish and French have a close relationship with each other and both belong to the Romance group of languages, people who learn Spanish may experience difficulties in learning French, such as using the wrong vocabulary or conjugation. This occurs because previous successful experience in Spanish can interfere with learning the word formation, usage, pronunciation, conjugation, etc., in French. In the field of psychology, the phenomenon that previous experience has a negative effect on learning new tasks is called negative transfer [1]. Similarly, in the transfer learning area, if the target learner is negatively affected by the transferred knowledge, the phenomenon is also termed as negative transfer [2], [3]. Whether negative transfer will occur may depend on several factors, such as the relevance between the source and the target domains and the learners capacity of finding the transferable and beneficial part of the knowledge across domains. In [3], a formal definition and some analyses of negative transfer are given. ![](images/f1a01b913b07d81010da2cc2fcc94d1897b409da668d7271790724d824100411.jpg) Fig. 1. Intuitive examples about transfer learning. Roughly speaking, according to the discrepancy between domains, transfer learning can be further divided into two categories, i.e., homogeneous and heterogeneous transfer learning [4]. Homogeneous transfer learning approaches are developed and proposed for handling the situations where the domains are of the same feature space. In homogeneous transfer learning, some studies assume that domains differ only in marginal distributions. Therefore, they adapt the domains by correcting the sample selection bias [5] or covariate shift [6]. However, this assumption does not hold in many cases. For example, in sentiment classification problem, a word may have different meaning tendencies in different domains. This phenomenon is also called context feature bias [7]. To solve this problem, some studies further adapt the conditional distributions. Heterogeneous transfer learning refers to the knowledge transfer process in the situations where the domains have different feature spaces. In addition to distribution adaptation, heterogeneous transfer learning requires feature space adaptation [7], which makes it more complicated than homogeneous transfer learning. The survey aims to give readers a comprehensive understanding about transfer learning from the perspectives of data and model. The mechanisms and the strategies of transfer learning approaches are introduced to allow readers grasp how the approaches work. And a number of the existing transfer learning researches are connected and systematized. Specifically, over forty representative transfer learning approaches are introduced. Besides, we conduct experiments to demonstrate on which dataset a transfer learning model performs well. In this survey, we focus more on homogeneous transfer learning. Some interesting transfer learning topics are not covered in this survey, such as reinforcement transfer learning [8], lifelong transfer learning [9], and online transfer learning [10]. The rest of this survey are organized into seven sections. Section 2 clarifies the difference between transfer learning and other related machine learning techniques. Section 3 introduces the notations used in this survey and the definitions about transfer learning. Sections 4 and 5 interpret transfer learning approaches from the data and the model perspectives, respectively. Section 6 introduces some applications of transfer learning. Experiments are conducted and the results are provided in Section 7. The last section concludes this survey. The main contributions of this survey are summarized below. • Over forty representative transfer learning approaches are introduced and summarized, which can give readers a comprehensive overview about transfer learning. • Experiments are conducted to compare different transfer learning approaches. The performance of over twenty different approaches is displayed intuitively and then analyzed, which may be instructive for the readers to select the appropriate ones in practice.", "introduction_length": 6632, "introduction_token": 1167, "reference": "# REFERENCES  \n\n[1] D.N. Perkins and G. Salomon, Transfer of Learning. Oxford, England: Pergamon, 1992.   \n[2] S.J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Trans. Knowl. Data Eng., vol. 22, no. 10, pp. 1345–1359, Oct. 2010.   \n[3] Z. Wang, Z. Dai, B. ´Poczos, and J. Carbonell, “Characterizing and avoiding negative transfer,” in Proc. IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, Jun. 2019, pp. 11293– 11302.   \n[4] K. Weiss, T.M. Khoshgoftaar, and D. Wang, “A survey of transfer learning,” J. Big Data, vol. 3, no. 1, Dec. 2016.   \n[5] J. Huang, A.J. Smola, A. Gretton, K.M. Borgwardt, and B. Sch¨olkopf, “Correcting sample selection bias by unlabeled data,” in Proc. 20th Annual Conference on Neural Information Processing Systems, Vancouver, Dec. 2006, pp. 601–608.   \n[6] M. Sugiyama, T. Suzuki, S. Nakajima, H. Kashima, P. Bnau, and M. Kawanabe, “Direct importance estimation for covariate shift adaptation,” Ann. Inst. Stat. Math., vol. 60, no. 4, pp. 699–746, Dec. 2008.   \n[7] O. Day and T.M. Khoshgoftaar, “A survey on heterogeneous transfer learning,” J. Big Data, vol. 4, no. 1, Dec. 2017.   \n[8] M.E. Taylor and P. Stone, “Transfer learning for reinforcement learning domains: A survey,” J. Mach. Learn. Res., vol. 10, pp. 1633– 1685, Sep. 2009.   \n[9] H.B. Ammar, E. Eaton, J.M. Luna, and P. Ruvolo, “Autonomous cross-domain knowledge transfer in lifelong policy gradient reinforcement learning,” in Proc. 24th International Joint Conference on Artificial Intelligence, Buenos Aires, Jul. 2015, pp. 3345–3351.   \n[10] P. Zhao and S.C.H. Hoi, “OTL: A framework of online transfer learning,” in Proc. 27th International Conference on Machine Learning, Haifa, Jun. 2010, pp. 1231–1238.   \n[11] O. Chapelle, B. Schlkopf, and A. Zien, Semi-supervised Learning. Cambridge: MIT Press, 2010.   \n[12] S. Sun, “A survey of multi-view machine learning,” Neural Comput. Appl., vol. 23, no. 7–8, pp. 2031–2038, Dec. 2013.   \n[13] C. Xu, D. Tao, and C. Xu, “A survey on multi-view learning,” 2013, arXiv:1304.5634v1.   \n[14] J. Zhao, X. Xie, X. Xu, and S. Sun, “Multi-view learning overview: Recent progress and new challenges,” Inf. Fusion, vol. 38, pp. 43–54, Nov. 2017.   \n[15] D. Zhang, J. He, Y. Liu, L. Si, and R. Lawrence, “Multi-view transfer learning with a large margin approach,” in Proc. 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Diego, Aug. 2011, pp. 1208–1216.   \n[16] P. Yang and W. Gao, “Multi-view discriminant transfer learning,” in Proc. 23rd International Joint Conference on Artificial Intelligence, Beijing, Aug. 2013, pp. 1848–1854.   \n[17] K.D. Feuz and D.J. Cook, “Collegial activity learning between heterogeneous sensors,” Knowl. Inf. Syst., vol. 53, pp. 337–364, Mar. 2017.   \n[18] Y. Zhang and Q. Yang, “An overview of multi-task learning,” Natl. Sci. Rev., vol. 5, no. 1, pp. 30–43, Jan. 2018.   \n[19] W. Zhang, R. Li, T. Zeng, Q. Sun, S. Kumar, J. Ye, and S. Ji, “Deep model based transfer and multi-task learning for biological image analysis,” in Proc. 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Sydney, Aug. 2015, pp. 1475– 1484.   \n[20] A. Liu, N. Xu, W. Nie, Y. Su, and Y. Zhang, “Multi-domain and multi-task learning for human action recognition,” IEEE Trans. Image Process., vol. 28, no. 2, pp. 853–867, Feb. 2019.   \n[21] X. Peng, Z. Huang, X. Sun, and K. Saenko, “Domain agnostic learning with disentangled representations,” in Proc. 36th Internat5i1o0n2al– 5C1o1n2f.erence on Machine Learning, Long Beach, Jun. 2019, pp.   \n[22] J. Lu, V. Behbood, P. Hao, H. Zuo, S. Xue, and G. Zhang, “Transfer learning using computational intelligence: A survey,” KnowledgeBased Syst., vol. 80, pp. 14–23, May 2015.   \n[23] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu, “A Survey on deep transfer learning,” in Proc. 27th International Conference on Artificial Neural Networks, Rhodes, Oct. 2018, pp. 270–279.   \n[24] M. Wang and W. Deng, “Deep visual domain adaptation: A survey,” Neurocomputing, vol. 312, pp. 135–153, Oct. 2018.   \n[25] D. Cook, K.D. Feuz, and N.C. Krishnan, “Transfer learning for activity recognition: A survey,” Knowl. Inf. Syst., vol. 36, no. 3, pp. 537–556, Sep. 2013.   \n[26] L. Shao, F. Zhu, and X. Li, “Transfer learning for visual categorization: A survey,” IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 5, pp. 1019–1034, May 2015.   \n[27] W. Pan, $^{\\prime}\\mathrm{A}$ survey of transfer learning for collaborative recommendation with auxiliary data,” Neurocomputing, vol. 177, pp. 447– 453, Feb. 2016.   \n[28] R. Liu, Y. Shi, C. Ji, and M. Jia, “A Survey of sentiment analysis based on transfer learning,” IEEE Access, vol. 7, pp. 85401–85412, Jun. 2019.   \n[29] Q. Sun, R. Chattopadhyay, S. Panchanathan, and J. Ye, “A twostage weighting framework for multi-source domain adaptation,” in Proc. 25th Annual Conference on Neural Information Processing Systems, Granada, Dec. 2011, pp. 505–513.   \n[30] M. Belkin, P. Niyogi, and V. Sindhwani, “Manifold regularization: A geometric framework for learning from labeled and unlabeled examples,” J. Mach. Learn. Res., vol. 7, pp. 2399–2434, Nov. 2006.   \n[31] W. Dai, Q. Yang, G. Xue, and Y. Yu, “Boosting for transfer learning,” in Proc. 24th International Conference on Machine Learning, Corvalis, Jun. 2007, pp. 193–200.   \n[32] Y. Freund and R.E. Schapire, $^{\\prime}\\mathrm{A}$ decision-theoretic generalization of on-line learning and an application to boosting,” J. Comput. Syst. Sci., vol. 55, no. 1, pp. 119–139, Aug. 1997.   \n[33] Y. Yao and G. Doretto, “Boosting for transfer learning with multiple sources,” in Proc. IEEE Conference on Computer Vision and Pattern Recognition, San Francisco, Jun. 2010, pp. 1855–1862.   \n[34] J. Jiang and C. Zhai, “Instance weighting for domain adaptation in NLP,” in Proc. 45th Annual Meeting of the Association of Computational Linguistics, Prague, Jun. 2007, pp. 264–271.   \n[35] K.M. Borgwardt, A. Gretton, M.J. Rasch, H.-P. Kriegel, B. Scholkopf, and A.J. Smola, “Integrating structured biological data by kernel maximum mean discrepancy,” Bioinformatics, vol. 22, no. 14, pp. 49–57, Jul. 2006.   \n[36] S.J. Pan, I.W. Tsang, J.T. Kwok, and Q. Yang, “Domain adaptation via transfer component analysis,”IEEE Trans. Neural Netw., vol. 22, no. 2, pp. 199–210, Feb. 2011.   \n[37] M. Ghifary, W.B. Kleijn, and M. Zhang, “Domain adaptive neural networks for object recognition,” in Proc. Pacific Rim International Conference on Artificial Intelligence, Gold Coast, Dec. 2014, pp. 898– 904.   \n[38] M. Long, J. Wang, G. Ding, J. Sun, and P.S. Yu, “Transfer feature learning with joint distribution adaptation,”in Proc. IEEE International Conference on Computer Vision, Sydney, Dec. 2013, pp. 2200– 2207.   \n[39] M. Long, J. Wang, G. Ding, S.J. Pan, and P.S. Yu, “Adaptation regularization: A general framework for transfer learning,” IEEE Trans. Knowl. Data Eng., vol. 26, no. 5, pp. 1076-1089, May 2014.   \n[40] S. Kullback and R.A. Leibler, “On information and sufficiency,” Ann. Math. Statist., vol. 22, no. 1, pp. 79–86, 1951.   \n[41] W. Dai, G.-R. Xue, Q. Yang, and Y. Yu, “Co-clustering based classification for out-of-domain documents,” in Proc. 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Jose, Aug. 2007, pp. 210–219.   \n[42] W. Dai, Q. Yang, G. Xue, and Y. Yu, “Self-taught clustering,” in Proc. 25th International Conference of Machine Learning, Helsinki, Jul. 2008, pp. 200–207.   \n[43] J. Davis and P. Domingos, “Deep transfer via second-order Markov logic,” in Proc. 26th International Conference on Machine Learning, Montreal, Jun. 2009, pp. 217–224.   \n[44] F. Zhuang, X. Cheng, P. Luo, S.J. Pan, and Q. He, “Supervised representation learning: Transfer learning with deep autoencoders,” in Proc. 24th International Joint Conference on Artificial Intelligence, Buenos Aires, Jul. 2015, pp. 4119–4125.   \n[45] I. Dagan, L. Lee, and F. Pereira, “Similarity-based methods for word sense disambiguation,” in Proc. 35th Annual Meeting of the Association of Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics (ACL/EACL), Madrid, Jul. 1997, pp. 56–63.   \n[46] B. Chen, W. Lam, I. Tsang, and T. Wong, “Location and scatter matching for dataset shift in text mining,” in Proc. 10th IEEE International Conference on Data Mining, Sydney, Dec. 2010, pp. 773– 778.   \n[47] S. Dey, S. Madikeri, and P. Motlicek, “Information theoretic clustering for unsupervised domain-adaptation,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing, Shanghai, Mar. 2016, pp. 5580–5584.   \n[48] W.-H. Chen, P.-C. Cho, and Y.-L. Jiang, “Activity recognition using transfer learning,” Sens. Mater., vol. 29, no. 7, pp. 897–904, Jul. 2017.   \n[49] J. Giles, K.K. Ang, L.S. Mihaylova, and M. Arvaneh, “A subjectto-subject transfer learning framework based on Jensen-Shannon divergence for improving brain-computer interface,” in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing, Brighton, May 2019, pp. 3087–3091.   \n[50] L.M. Bregman, “The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming,” USSR Comput. Math. Math. Phys., vol. 7, no. 3, pp. 200–217, 1967.   \n[51] S. Si, D. Tao, and B. Geng, “Bregman divergence-based regularization for transfer subspace learning,”IEEE Trans. Knowl. Data Eng., vol. 22, no. 7, pp. 929–942, Jul. 2010.   \n[52] H. Sun, S. Liu, S. Zhou, and H. Zou, “Unsupervised cross-view semantic transfer for remote sensing image classification,” IEEE Geosci. Remote Sens. Lett., vol. 13, no. 1, pp. 13–17, Jan. 2016.   \n[53] H. Sun, S. Liu, and S. Zhou, “Discriminative subspace alignment for unsupervised visual domain adaptation,” Neural Process. Lett., vol. 44, no. 3, pp. 779–793, Dec. 2016.   \n[54] Q. Shi, Y. Zhang, X. Liu, and K. Zhao, “Regularised transfer learning for hyperspectral image classification,” IET Comput. Vis., vol. 13, no. 2, pp. 188–193, Feb. 2019.   \n[55] A. Gretton, O. Bousquet, A.J. Smola, and B. Schlkopf, “Measuring statistical dependence with Hilbert-Schmidt norms,” in Proc. 18th International Conference on Algorithmic Learning Theory, Singapore, Oct. 2005, pp. 63–77.   \n[56] H. Wang and Q. Yang, “Transfer learning by structural analogy,” in Proc. 25th AAAI Conference on Artificial Intelligence, San Francisco, Aug. 2011, pp. 513–518.   \n[57] M. Xiao and Y. Guo, “Feature space independent semi-supervised domain adaptation via kernel matching,”IEEE Trans. Pattern Anal. Mach. Intell., vol. 37, no. 1, pp. 54–66, Jan. 2015.   \n[58] K. Yan, L. Kou, and D. Zhang, “Learning domain-invariant subspace using domain features and independence maximization,” IEEE T. Cybern., vol. 48, no. 1, pp. 288–299, Jan. 2018.   \n[59] J. Shen, Y. Qu, W. Zhang, and Y. Yu, “Wasserstein distance guided representation learning for domain adaptation,” in Proc. 32nd AAAI Conference on Artificial Intelligence, New Orleans, Feb. 2018, pp. 4058–4065.   \n[60] C.-Y. Lee, T. Batra, M.H. Baig, and D. Ulbricht, “Sliced Wasserstein discrepancy for unsupervised domain adaptation,” in Proc. IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, Jun. 2019, pp. 10285–10295.   \n[61] W. Zellinger, T. Grubinger, E. Lughofer, T. Natschlger, and S. Saminger-Platz, “Central moment discrepancy (CMD) for domaininvariant representation learning,” in Proc. 5th International Conference on Learning Representations, Toulon, Apr. 2017, pp. 1–13.   \n[62] A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, K. Fukumizu, and B.K. Sriperumbudur, “Optimal kernel choice for large-scale two-sample tests,” in Proc. 26th Annual Conference on Neural Information Processing Systems, Lake Tahoe, Dec. 2012, pp. 1205–1213.   \n[63] H. Yan, Y. Ding, P. Li, Q. Wang, Y. Xu, and W. Zuo, “Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation,” in Proc. IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, Jul. 2017, pp. 2272–2281.   \n[64] H. Daume´ III, “Frustratingly easy domain adaptation,” in Proc. 45th Annual Meeting of the Association for Computational Linguistics, Prague, Jun. 2007, pp. 256–263.   \n[65] H. Daume´ III, A. Kumar, and A. Saha, “Co-regularization based semi-supervised domain adaptation,” in Proc. 24th Annual Conference on Neural Information Processing Systems, Vancouver, Dec. 2010, pp. 478–486.   \n[66] L. Duan, D. Xu, and I.W. Tsang, “Learning with augmented features for heterogeneous domain adaptation,” in Proc. 29th International Conference on Machine Learning, Edinburgh, Jun. 2012, pp. 1–8.   \n[67] W. Li, L. Duan, D. Xu, and I.W. Tsang, “Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 36, no. 6, pp. 1134–1148, Jun. 2014.   \n[68] K.I. Diamantaras and S.Y. Kung, Principal Component Neural Networks. New York: Wiley, 1996.   \n[69] B. Schlkopf, A. Smola, and K. Mller, “Nonlinear component analysis as a kernel eigenvalue problem,” Neural Comput., vol. 10, no. 5, pp. 1299–1319, Jul. 1998.   \n[70] J. Wang, Y. Chen, S. Hao, W. Feng, and Z. Shen, “Balanced distribution adaptation for transfer learning,” in Proc. 17th IEEE International Conference on Data Mining, New Orleans, Nov. 2017, pp. 1129–1134.   \n[71] A. Blum and T. Mitchell, “Combining labeled and unlabeled data with co-training,” in Proc. 11th Annual Conference on Computational Learning Theory, Madison, Jul. 1998, pp. 92–100.   \n[72] M. Chen, K.Q. Weinberger, and J.C. Blitzer, “Co-training for domain adaptation,” in Proc. 25th Annual Conference on Neural Information Processing Systems, Granada, Dec. 2011, pp. 2456–2464.   \n[73] Z.-H. Zhou and M. Li, “Tri-training: Exploiting unlabeled data using three classifiers,” IEEE Trans. Knowl. Data Eng., vol. 17, no. 11, pp. 1529–1541, Nov. 2005.   \n[74] K. Saito, Y. Ushiku, and T. Harada, “Asymmetric tri-training for unsupervised domain adaptation,” in Proc. 34th International Conference on Machine Learning, Sydney, Aug. 2017, pp. 2988–2997.   \n[75] S.J. Pan, J.T. Kwok, and Q. Yang, “Transfer learning via dimensionality reduction,” in Proc. 23rd AAAI Conference on Artificial Intelligence, Chicago, Jul. 2008, pp. 677–682.   \n[76] K.Q. Weinberger, F. Sha, and L.K. Saul, “Learning a kernel matrix for nonlinear dimensionality reduction,” in Proc. 21st International Conference on Machine Learning, Banff, Jul. 2004, pp. 106–113.   \n[77] L. Vandenberghe and S. Boyd, “Semidefinite programming,” SIAM Rev., vol. 38, no. 1, pp. 49–95, Mar. 1996.   \n[78] S.J. Pan, I.W. Tsang, J.T. Kwok, and Q. Yang, “Domain adaptation via transfer component analysis,” in Proc. 21st International Joint Conference on Artificial Intelligence, Pasadena, Jul. 2009, pp. 1187– 1192.   \n[79] C. Hou, Y.H. Tsai, Y. Yeh, and Y.F. Wang, “Unsupervised domain adaptation with label and structural consistency,” IEEE Trans. Image Process., vol. 25, no. 12, pp. 5552–5562, Dec. 2016.   \n[80] J. Tahmoresnezhad and S. Hashemi, “Visual domain adaptation via transfer feature learning,” Knowl. Inf. Syst., vol. 50, no. 2, pp. 585–605, Feb. 2017.   \n[81] J. Zhang, W. Li, and P. Ogunbona, “Joint geometrical and statistical alignment for visual domain adaptation,” in Proc. IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, Jul. 2017, pp. 5150–5158.   \n[82] B. Schlkopf, R. Herbrich, and A.J. Smola, “A generalized representer theorem,” in Proc. International Conference on Computational Learning Theory, Amsterdam, Jul. 2001, pp. 416–426.   \n[83] L. Duan, I.W. Tsang, and D. Xu, “Domain transfer multiple kernel learning,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 3, pp. 465–479, Mar. 2012.   \n[84] A. Rakotomamonjy, F.R. Bach, S. Canu, and Y. Grandvalet, “SimpleMKL,” J. Mach. Learn. Res., vol. 9, pp. 2491-2521, Nov. 2008.   \n[85] I.S. Dhillon, S. Mallela, and D.S. Modha, “Information-theoretic co-clustering,” in Proc. 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Washington, Aug. 2003, pp. 89–98.   \n[86] S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman, “Indexing by latent semantic analysis,” J. Am. Soc. Inf. Sci., vol. 41, pp. 391–407, Sep. 1990.   \n[87] T. Hofmann, “Probabilistic latent semantic analysis,” in Proc. 15th Conference on Uncertainty in Artificial Intelligence, Stockholm, Jul. 1999, pp. 289–296.   \n[88] J. Yoo and S. Choi, “Probabilistic matrix tri-factorization,” in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, Taipei, Apr. 2009, pp. 1553–1556.   \n[89] A. Dempster, N. Laird, and D. Rubin, “Maximum likelihood from incomplete data via the EM algorithm,” J. R. Stat. Soc. - Ser. B, vol. 39, no. 1, pp. 1–38, 1977.   \n[90] G.-R. Xue, W. Dai, Q. Yang, and Y. Yu, “Topic-bridged PLSA for cross-domain text classification,” in Proc. 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Singapore, Jul. 2008, pp. 627–634.   \n[91] F. Zhuang, P. Luo, Z. Shen, Q. He, Y. Xiong, Z. Shi, and H. Xiong, “Collaborative Dual-PLSA: Mining distinction and commonality across multiple domains for text classification,” in Proc. 19th ACM International Conference on Information and Knowledge Management, Toronto, Oct. 2010, pp. 359–368.   \n[92] F. Zhuang, P. Luo, Z. Shen, Q. He, Y. Xiong, Z. Shi, and H. Xiong, “Mining distinction and commonality across multiple domains using generative model for text classification,” IEEE Trans. Knowl. Data Eng., vol. 24, no. 11, pp. 2025–2039, Nov. 2012.   \n[93] F. Zhuang, P. Luo, P. Yin, Q. He, and Z. Shi, “Concept learning for cross-domain text classification: A general probabilistic framework,” in Proc. 23rd International Joint Conference on Artificial Intelligence, Beijing, Aug. 2013, pp. 1960–1966.   \n[94] J. Blitzer, R. McDonald, and F. Pereira, “Domain adaptation with structural correspondence learning,” in Proc. Conference on Empirical Methods in Natural Language Processing, Sydney, Jul. 2006, pp. 120– 128.   \n[95] R.K. Ando and T. Zhang, “A framework for learning predictive structures from multiple tasks and unlabeled data,” J. Mach. Learn. Res., vol. 6, pp. 1817–1853, Dec. 2005.   \n[96] X. Glorot, A. Bordes, and Y. Bengio, “Domain adaptation for large-scale sentiment classification: A deep learning approach,” in Proc. 28th International Conference on Machine Learning, Bellevue, Jun. 2011, pp. 513–520.   \n[97] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, “Extracting and composing robust features with denoising autoencoders,” in Proc. 25th International Conference on Machine Learning, Helsinki, Jul. 2008, pp. 1096–1103.   \n[98] M. Chen, Z. Xu, K. Weinberger, and F. Sha, “Marginalized denoising autoencoders for domain adaptation,” in Proc. 29th International Conference on Machine Learning, Edinburgh, Jun. 2012, pp. 767–774.   \n[99] M. Chen, K.Q. Weinberger, Z. Xu, and F. Sha, “Marginalizing stacked linear denoising autoencoders,” J. Mach. Learn. Res., vol. 16, no. 1, pp. 3849–3875, Jan. 2015.   \n[100] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars, “Unsupervised visual domain adaptation using subspace alignment,” in Proc. IEEE International Conference on Computer Vision, Sydney, Dec. 2013, pp. 2960–2967.   \n[101] B. Sun and K. Saenko, “Subspace distribution alignment for unsupervised domain adaptation,” in Proc. British Machine Vision Conference, Swansea, Sep. 2015, pp. 24.1–24.10.   \n[102] B. Gong, Y. Shi, F. Sha, and K. Grauman, “Geodesic flow kernel for unsupervised domain adaptation,” in Proc. IEEE Conference on Computer Vision and Pattern Recognition, Providence, Jun. 2012, pp. 2066–2073.   \n[103] R. Gopalan, Ruonan Li, and R. Chellappa, “Domain adaptation for object recognition: An unsupervised approach,” in Proc. IEEE International Conference on Computer Vision, Barcelona, Jun. 2011, pp. 999-1006.   \n[104] M.I. Zelikin, Control Theory and Optimization I in Encyclopaedia of Mathematical Sciences, vol. 86, Berlin: Springer, 2000.   \n[105] B. Sun, J. Feng, and K. Saenko, “Return of frustratingly easy domain adaptation,” in Proc. 30th AAAI Conference on Artificial Intelligence, Phoenix, Feb. 2016, pp. 2058–2065.   \n[106] S.J. Pan, X. Ni, J.-T. Sun, Q. Yang, and Z. Chen, “Cross-domain sentiment classification via spectral feature alignment,” in Proc. 19th International Conference on World Wide Web, Raleigh, Apr. 2010, pp. 751–760.   \n[107] J. Blitzer, M. Dredze, and F. Pereira, “Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification,” in Proc. 45th Annual Meeting of the Association of Computational Linguistics, Prague, Jun. 2007, pp. 440–447.   \n[108] F.R.K. Chung, Spectral Graph Theory. Providence: American Mathematical Society, 1997.   \n[109] A.Y. Ng, M.I. Jordan, and Y. Weiss, “On spectral clustering: Analysis and an algorithm,” in Proc. 15th Annual Conference on Neural Information Processing Systems, Vancouver, Dec. 2001, pp. 849- 856.   \n[110] X. Ling, W. Dai, G.-R. Xue, Q. Yang, and Y. Yu, “Spectral domaintransfer learning,” in Proc. 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, Aug. 2008, pp. 488–496.   \n[111] S.D. Kamvar, D. Klein, and C.D. Manning, “Spectral learning,” in Proc. 18th International Joint Conference on Artificial Intelligence, Acapulco, Aug. 2003, pp. 561–566.   \n[112] J. Shi and J. Malik, “Normalized cuts and image segmentation,” IEEE Trans. Pattern Anal. Mach. Intell., vol .22, no. 8, pp. 888–905, Aug. 2000.   \n[113] L. Duan, I.W. Tsang, D. Xu, and T.-S. Chua, “Domain adaptation from multiple sources via auxiliary classifiers,” in Proc. 26th International Conference on Machine Learning, Montreal, Jun. 2009, pp. 289–296.   \n[114] L. Duan, D. Xu, and I.W. Tsang, “Domain adaptation from multiple sources: A domain-dependent regularization approach,” IEEE Trans. Neural Netw. Learn. Syst., vol. 23, no. 3, pp. 504–518, Mar. 2012.   \n[115] P. Luo, F. Zhuang, H. Xiong, Y. Xiong, and Q. He, “Transfer learning from multiple source domains via consensus regularization,” in Proc. 17th ACM Conference on Information and Knowledge Management, Napa Valley, Oct. 2008, pp. 103–112.   \n[116] F. Zhuang, P. Luo, H. Xiong, Y. Xiong, Q. He, and Z. Shi, “Crossdomain learning from multiple sources: A consensus regularization perspective,” IEEE Trans. Knowl. Data Eng., vol. 22, no. 12, pp. 1664– 1678, Dec. 2010.   \n[117] T. Evgeniou, C.A. Micchelli, and M. Pontil, “Learning multiple tasks with kernel methods,” J. Mach. Learn. Res., vol. 6, pp. 615-637, Apr. 2005.   \n[118] T. Kato, H. Kashima, M. Sugiyama, and K. Asai, “Multi-task learning via conic programming,” in Proc. 21st Annual Conference o7n3 7N–7e4u4r.al Information Processing Systems, Vancouver, Dec. 2007, pp.   \n[119] A.J. Smola and B. Schlkopf, “A tutorial on support vector regression,” Stat. Comput., vol. 14, no. 3, pp. 199–222, Aug. 2004.   \n[120] J. Weston, R. Collobert, F. Sinz, L. Bottou, and V. Vapnik, “Inference with the universum,” in Proc. 23rd International Conference on Machine Learning, Pittsburgh, Jun. 2006, pp. 1009–1016.   \n[121] X. Yu and Y. Aloimonos, “Attribute-based transfer learning for object categorization with zero/one training example,” in Proc. European Conference on Computer Vision, Heraklion, Sep. 2010, pp. 127–140.   \n[122] F. Zhuang, P. Luo, H. Xiong, Q. He, Y. Xiong, and Z. Shi, “Exploiting associations between word clusters and document classes for cross-domain text categorization,” Stat. Anal. Data Min., vol. 4, no. 1, pp. 100–114, Feb. 2011.   \n[123] F. Zhuang, P. Luo, C. Du, Q. He, Z. Shi, and H. Xiong, “Triplex transfer learning: Exploiting both shared and distinct concepts for text classification,” IEEE T. Cybern., vol. 44, no. 7, pp. 1191–1203, Jul. 2014.   \n[124] M. Long, J. Wang, G. Ding, W. Cheng, X. Zhang, and W. Wang, “Dual transfer learning,” in Proc. 12th SIAM International Conference on Data Mining, Anaheim, Apr. 2012, pp. 540–551.   \n[125] H. Wang, F. Nie, H. Huang, and C. Ding, “Dyadic transfer learning for cross-domain image classification,” in Proc. International Conference on Computer Vision, Barcelona, Nov. 2011, pp. 551–556.   \n[126] D. Wang, C. Lu, J. Wu, H. Liu, W. Zhang, F. Zhuang, and H. Zhang, “Softly associative transfer learning for crossdomain classification,” IEEE T. Cybern., to be published. doi: 10.1109/TCYB.2019.2891577.   \n[127] Q. Do, W. Liu, J. Fan, and D. Tao, “Unveiling hidden implicit similarities for cross-domain recommendation,” IEEE Trans. Knowl. Data Eng., to be published. doi: 10.1109/TKDE.2019.2923904.   \n[128] T. Tommasi and B. Caputo, “The more you know, the less you learn: from knowledge transfer to one-shot learning of object categories” in Proc. British Machine Vision Conference, London, Sep. 2009, pp. 80.1–80.11.   \n[129] G.C. Cawley, “Leave-one-out cross-validation based model selection criteria for weighted LS-SVMs,” in Proc. IEEE International Joint Conference on Neural Network, Vancouver, Jul. 2006, pp. 1661–1668.   \n[130] T. Tommasi, F. Orabona, and B. Caputo, “Safety in numbers: Learning categories from few examples with multi model knowledge transfer,” in Proc. IEEE Conference on Computer Vision and Pattern Recognition, San Francisco, Jun. 2010, pp. 3081–3088.   \n[131] C.-K. Lin, Y.-Y. Lee, C.-H. Yu, and H.-H. Chen, “Exploring ensemble of models in taxonomy-based cross-domain sentiment classification,” in Proc. 23rd ACM International on Conference on Information and Knowledge Management, Shanghai, Nov. 2014, pp. 1279–1288.   \n[132] J. Gao, W. Fan, J. Jiang, and J. Han, “Knowledge transfer via multiple model local structure mapping,” in Proc. 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, Aug. 2008, pp. 283–291.   \n[133] F. Zhuang, P. Luo, S.J. Pan, H. Xiong, and Q. He. “Ensemble of anchor adapters for transfer learning,” in Proc. 25th ACM International on Conference on Information and Knowledge Management, Indianapolis, Oct. 2016, pp. 2335–2340.   \n[134] F. Zhuang, X. Cheng, P. Luo, S.J. Pan, and Q. He, “Supervised representation learning with double encoding-layer autoencoder for transfer learning,” ACM Trans. Intell. Syst. Technol., vol. 9, no. 2, pp. 1–17, Jan. 2018.   \n[135] M. Ghifary, W.B. Kleijn, and M. Zhang, “Domain adaptive neural networks for object recognition,” in Proc. 13th Pacific Rim International Conference on Artificial Intelligence, Gold Coast, Dec. 2014, pp. 898–904.   \n[136] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell, “Deep domain confusion: Maximizing for domain invariance,” 2014, arXiv:1412.3474v1.   \n[137] M. Long, Y. Cao, J. Wang, and M.I. Jordan, “Learning transferable features with deep adaptation networks,” in Proc. 32nd International Conference on Machine Learning, Lille, Jul. 2015, pp. 97–105.   \n[138] A. Krizhevsky, I. Sutskever, and G.E. Hinton, “Imagenet classification with deep convolutional neural networks,” in Proc. 26th Annual Conference on Neural Information Processing Systems, Lake Tahoe, Dec. 2012, pp. 1097–1105.   \n[139] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in Proc. 28th Annual Conference on Neural Information Processing Systems, Montreal, Dec. 2014, pp. 2672–2680.   \n[140] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable are features in deep neural networks?” in Proc. 28th Annual Conference on Neural Information Processing Systems, Montreal, Dec. 2014, pp. 3320–3328.   \n[141] M. Long, Y. Cao, Z. Cao, J. Wang, and M.I. Jordan, “Transferable representation learning with deep adaptation networks,” IEEE Trans. Pattern Anal. Mach. Intell., to be published. doi: 10.1109/TPAMI.2018.2868685.   \n[142] Y. Grandvalet and Y. Bengio, “Semi-supervised learning by entropy minimization,” in Proc. 18th Annual Conference on Neural Information Processing Systems, Vancouver, Dec. 2004, pp. 529–536.   \n[143] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in Proc. IEEE Conference on Computer Vision and Pattern Recognition, Boston, Jun. 2015, pp. 1–9.   \n[144] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proc. IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, Jun. 2016, pp. 770–778.   \n[145] K.P. Chwialkowski, A. Ramdas, D. Sejdinovic, and A. Gretton, “Fast two-sample testing with analytic representations of probability measures,” in Proc. 29th Annual Conference on Neural Information Processing Systems, Montreal, Dec. 2015, pp. 1981–1989.   \n[146] M. Long, H. Zhu, J. Wang, and M.I. Jordan, “Unsupervised domain adaptation with residual transfer networks,” in Proc. 30th Annual Conference on Neural Information Processing Systems, Barcelona, Dec. 2016, pp. 136–144.   \n[147] M. Long, H. Zhu, J. Wang, and M.I. Jordan, “Deep transfer learning with joint adaptation networks,” in Proc. 34th International Conference on Machine Learning, Sydney, Aug. 2017, pp. 2208–2217.   \n[148] B. Sun and K. Saenko, “Deep CORAL: Correlation alignment for deep domain adaptation,” in Proc. European Conference on Computer Vision Workshops, Amsterdam, Oct. 2016, pp. 443–450.   \n[149] C. Chen, Z. Chen, B. Jiang, and X. Jin, “Joint domain alignment and discriminative feature learning for unsupervised deep domain adaptation,” in Proc. 33rd AAAI Conference on Artificial Intelligence, Honolulu, Jan. 2019, pp. 3296–3303.   \n[150] Y. Pan, T. Yao, Y. Li, Y. Wang, C.-W. Ngo, and T. Mei, “Transferrable prototypical networks for unsupervised domain adaptation,” in Proc. IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, Jun. 2019, pp. 2239–2247.   \n[151] G. Kang, L. Jiang, Y. Yang, and A.G. Hauptmann, “Contrastive adaptation network for unsupervised domain adaptation,” in Proc. IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, Jun. 2019, pp. 4893–4902.   \n[152] Y. Zhu, F. Zhuang, J. Wang, J. Chen, Z. Shi, W. Wu, and Q. He, “Multi-representation adaptation network for cross-domain image classification,” Neural Netw., vol. 119. pp. 214–221, Nov. 2019.   \n[153] Y. Zhu, F. Zhuang, and D. Wang, “Aligning domain-specific distribution and classifier for cross-domain classification from multiple sources,” in Proc. 33rd AAAI Conference on Artificial Intelligence, Honolulu, Jan. 2019, pp. 5989–5996.   \n[154] Y. Ganin and V. Lempitsky, “Unsupervised domain adaptation by backpropagation,” in Proc. 32nd International Conference on Machine Learning, Lille, Jul. 2015, pp. 1180–1189.   \n[155] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F.Laviolette, M. Marchand, and V. Lempitsky, “Domain-adversarial training of neural networks,” J. Mach. Learn. Res., vol. 17, pp. 1–35, Apr. 2016.   \n[156] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discriminative domain adaptation,” in Proc. IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, Jul. 2017, pp. 2962–2971.   \n[157] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A.A. Efros, and T. Darrell, “CyCADA: Cycle-consistent adversarial domain adaptation,” in Proc. 35th International Conference on Machine Learning, Stockholm, Jul. 2018, pp. 1994–2003.   \n[158] M. Long, Z. Cao, J. Wang, and M.I. Jordan, “Conditional adversarial domain adaptation,” in Proc. 32nd Annual Conference on Neural Information Processing Systems, Montreal, Dec. 2018, pp. 1640–1650.   \n[159] Y. Zhang, H. Tang, K. Jia, and M. Tan, “Domain-symmetric networks for adversarial domain adaptation,” in Proc. IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, Jun. 2019, pp. 5031–5040.   \n[160] H. Zhao, S. Zhang, G. Wu, J.M.F. Moura, J.P. Costeira, and G.J. Gordon, “Adversarial multiple source domain adaptation,” in Proc. 32nd Annual Conference on Neural Information Processing Systems, Montreal, Dec. 2018, pp. 8559–8570.   \n[161] C. Yu, J. Wang, Y. Chen, and M. Huang, “Transfer learning with dynamic adversarial adaptation network,” in Proc. 19th IEEE International Conference on Data Mining, Beijing, Nov. 2019, pp. 1–9.   \n[162] J. Zhang, Z. Ding, W. Li, and P. Ogunbona, “Importance weighted adversarial nets for partial domain adaptation,” in Proc. IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, Jun. 2018, pp. 8156–8163.   \n[163] Z. Cao, M. Long, J. Wang, and M.I. Jordan, “Partial transfer learning with selective adversarial networks,” in Proc. IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, Jun. 2018, pp. 2724–2732.   \n[164] B. Wang, M. Qiu, X. Wang, Y. Li, Y. Gong, X. Zeng, J. Huang, B. Zheng, D. Cai, and J. Zhou, “A minimax game for instance based selective transfer learning,” in Proc. 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Anchorage, Aug. 2019, pp. 34–43.   \n[165] X. Chen, S. Wang, M. Long, and J. Wang, “Transferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation,” in Proc. 36th International Conference on Machine Learning, Long Beach, Jun. 2019, pp. 1081–1090.   \n[166] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet: A large-scale hierarchical image database,” in Proc. IEEE Conference on Computer Vision and Pattern Recognition, Miami, Jun. 2009, pp. 248–255.   \n[167] M. Maqsood, F. Nazir, U. Khan, F. Aadil, H. Jamal, I. Mehmood, and O. Song, “Transfer learning assisted classification and detection of Alzheimer’s disease stages using 3D MRI scans,” Sensors, vol. 19, no. 11, pp. 1–19, Jun. 2019.   \n[168] D.S. Marcus, A.F. Fotenos, J.G. Csernansky, J.C. Morris, and R.L. Buckner, “Open access series of imaging studies: Longitudinal MRI data in nondemented and demented older adults,”J. Cogn. Neurosci., vol. 22, no. 12, pp. 2677–2684, Dec. 2010.   \n[169] H.-C. Shin, H.R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao, D. Mollura, and R.M. Summers, “Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer Learning,” IEEE Trans. Med. Imaging, vol. 35, no. 5, pp. 1285–1298, May 2016.   \n[170] M. Byra, M. Wu, X. Zhang, H. Jang, Y.-J. Ma, E.Y. Chang, S. Shah, and Jiang Du, “Knee menisci segmentation and relaxometry of 3D ultrashort echo time cones MR imaging using attention UNet with transfer learning,” Magn. Reson. Med., Sep. 2019, doi: 10.1002/mrm.27969.   \n[171] X. Tang, B. Du, J. Huang, Z. Wang, and L. Zhang, “On combining active and transfer learning for medical data classification,” IET Comput. Vis., vol. 13, no. 2, pp. 194–205, Feb. 2019.   \n[172] M. Zeng, M. Li, Z. Fei, Y. Yu, Y. Pan, and J. Wang, “Automatic ICD-9 coding via deep transfer learning,” Neurocomputing, vol. 324, pp. 43–50, Jan. 2019.   \n[173] G. Schweikert, G. Ratsch, C. Widmer, and B. Scholkopf, “An empirical analysis of domain adaptation algorithms for genomic sequence analysis,” in Proc. 22nd Annual Conference on Neural Information Processing Systems, Vancouver, Dec. 2008, pp. 1433–1440.   \n[174] R. Petegrosso, S. Park, T.H. Hwang, and R. Kuang, “Transfer learning across ontologies for phenome-genome association prediction,” Bioinformatics, vol. 33, no. 4, pp. 529–536, Feb. 2017.   \n[175] T. Hwang and R. Kuang, “A heterogeneous label propagation algorithm for disease gene discovery,” in Proc. 10th SIAM International Conference on Data Mining, Columbus, Apr. 2010, pp. 583–594.   \n[176] Q. Xu, E.W. Xiang, and Q. Yang, “Protein-protein interaction prediction via collective matrix factorization,” in Proc. IEEE International Conference on Bioinformatics and Biomedicine, Hong Kong, Dec. 2010, pp. 62–67.   \n[177] A.P. Singh and G.J. Gordon, “Relational learning via collective matrix factorization,” in Proc. 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, Aug. 2008, pp. 650–658.   \n[178] S. Di, H. Zhang, C. Li, X. Mei, D. Prokhorov, and H. Ling, “Crossdomain traffic scene understanding: A dense correspondence-based transfer learning approach,” IEEE Trans. Intell. Transp. Syst., vol. 19, no. 3, pp. 745–757, Mar. 2018.   \n[179] H. Abdi, “Partial least squares regression and projection on latent structure regression (PLS Regression),” Wiley Interdiscip. Rev.- Comput. Statist., vol. 2, no. 1, pp. 97106, Jan. 2010.   \n[180] S.D. Pietra, V.D. Pietra, and J. Lafferty, “Inducing features of random fields,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 19, no. 4, pp. 380–393, Apr. 1997.   \n[181] C. Liu, J. Yuen, and A. Torralba, “Nonparametric scene parsing via label transfer,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 12, pp. 2368–2382, Dec. 2011.   \n[182] C. Lu, F. Hu, D. Cao, J. Gong, Y. Xing, and Z. Li, “Transfer learning for driver model adaptation in lane-changing scenarios using manifold alignment,” IEEE Trans. Intell. Transp. Syst., to be published. doi: 10.1109/TITS.2019.2925510.   \n[183] D.J. Berndt and J. Clifford, “Using dynamic time warping to find patterns in time series,” in Proc. Knowledge Discovery in Databases Workshop, Seattle, Jul. 1994, pp. 359–370.   \n[184] N. Makondo, M. Hiratsuka, B. Rosman, and O. Hasegawa, “A non-linear manifold alignment approach to robot learning from demonstrations,” J. Robot. Mechatron., vol. 30, no. 2, pp. 265–281, Apr. 2018.   \n[185] P. Angkititrakul, C. Miyajima, and K. Takeda, “Modeling and adaptation of stochastic driver-behavior model with application to car following,” in Proc. IEEE Intelligent Vehicles Symposium (IV), Baden-Baden, Jun. 2011, pp. 814–819.   \n[186] Y. Liu, P. Lasang, S. Pranata, S. Shen, and W. Zhang, “Driver pose estimation using recurrent lightweight network and virtual data augmented transfer learning,” IEEE Trans. Intell. Transp. Syst., vol. 20, no. 10, pp. 3818–3831, Oct. 2019.   \n[187] J. Wang, H. Zheng, Y. Huang, and X. Ding, “Vehicle type recognition in surveillance images from labeled web-nature data using deep transfer learning,” IEEE Trans. Intell. Transp. Syst., vol. 19, no. 9, pp. 2913–2922, Sep. 2018.   \n[188] K. Gopalakrishnan, S.K. Khaitan, A. Choudhary, and A. Agrawal, “Deep convolutional neural networks with transfer learning for computer vision-based data-driven pavement distress detection,” Constr. Build. Mater., vol. 157, pp. 322–330, Dec. 2017.   \n[189] S. Bansod and A. Nandedkar, “Transfer learning for video anomaly detection,” J. Intell. Fuzzy Syst., vol. 36, no. 3, pp. 1967– 1975, Mar. 2019.   \n[190] G. Rosario, T. Sonderman, and X. Zhu, “Deep transfer learning for traffic sign recognition,” in Proc. IEEE International Conference on Information Reuse and Integration, Salt Lake City, Jul. 2018, pp. 178–185.   \n[191] W. Pan, E.W. Xiang, and Q. Yang, “Transfer learning in collaborative filtering with uncertain ratings,” in Proc. 26th AAAI Conference on Artificial Intelligence, Toronto, Jul. 2012, pp. 662–668.   \n[192] G. Hu, Y. Zhang, and Q. Yang, “Transfer meets hybrid: A synthetic approach for cross-domain collaborative filtering with text,” in Proc. 28th International Conference on World Wide Web, San Francisco, May 2019, pp. 2822–2829.   \n[193] W. Pan, E.W. Xiang, N.N. Liu, and Q. Yang, “Transfer learning in collaborative filtering for sparsity reduction,” in Proc. 24th AAAI Conference on Artificial Intelligence, Atlanta, Jul. 2010, pp. 230–235.   \n[194] W. Pan and Q. Yang, “Transfer learning in heterogeneous collaborative filtering domains,” Artif. Intell., vol. 197, pp. 39–55, Apr. 2013.   \n[195] F. Zhuang, Y. Zhou, F. Zhang, X. Ao, X. Xie, and Q. He, “Sequential transfer learning: Cross-domain novelty seeking trait mining for recommendation,” in Proc. 26th International Conference on World Wide Web Companion, Perth, Apr. 2017, pp. 881–882.   \n[196] J. Zheng, F. Zhuang, and C. Shi, “Local ensemble across multiple sources for collaborative filtering,” in Proc. 26th ACM International on Conference on Information and Knowledge Management, Singapore, Nov. 2017, pp. 2431–2434.   \n[197] F. Zhuang, J. Zheng, J. Chen, X. Zhang, C. Shi, and Q. He, “Transfer collaborative filtering from multiple sources via consensus regularization,” Neural Netw., vol. 108, pp. 287–295, Dec. 2018.   \n[198] J. He, R. Liu, F. Zhuang, F. Lin, C. Niu, and Q. He, “A general cross-domain recommendation framework via Bayesian neural network,” in Proc. 18th IEEE International Conference on Data Mining, Singapore, Nov. 2018, pp. 1001–1006.   \n[199] F. Zhu, Y. Wang, C. Chen, G. Liu, M.A. Orgun, and J. Wu, “A deep framework for cross-domain and cross-system recommendations,” in Proc. 27th International Joint Conference on Artificial Intelligence, Stockholm, Jul. 2018, pp. 3711–3717.   \n[200] F. Yuan, L. Yao, and B. Benatallah, “DARec: Deep domain adaptation for cross-domain recommendation via transferring rating patterns,” in Proc. 29th International Joint Conference on Artificial Intelligence, Macao, Aug. 2019, pp. 4227–4233.   \n[201] E. Bastug, M. Bennis, and M. Debbah, “A transfer learning approach for cache-enabled wireless networks,” in Proc. 13th International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks, Mumbai, May 2015, pp. 161–166.   \n[202] R. Li, Z. Zhao, X. Chen, J. Palicot, and H. Zhang, “TACT: A transfer actor-critic learning framework for energy saving in cellular radio access networks,” IEEE Trans. Wirel. Commun., vol. 13, no. 4, pp. 2000–2011, Apr. 2014.   \n[203] Q. Zhao and D. Grace, “Transfer learning for QoS aware topology management in energy efficient 5G cognitive radio networks,” in Proc. 1st International Conference on 5G for Ubiquitous Connectivity, Akaslompolo, Nov. 2014, pp. 152–157.   \n[204] B. Guo, J. Li, V.W. Zheng, Z. Wang, and Z. Yu, “Citytransfer: Transferring inter- and intra-city knowledge for chain store site recommendation based on multi-source urban data,” in Proc. ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Jan. 2018, pp. 1–23.   \n[205] Y. Wei, Y. Zheng, and Q. Yang, “Transfer knowledge between cities,” in Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, Aug. 2016, pp. 1905–1914.   \n[206] U. Cote-Allard, C.L. Fall, A. Drouin, A. Campeau-Lecours, C. Gosselin, K. Glette, F. Laviolette, and B. Gosselin, “Deep learning for electromyographic hand gesture signal classification using transfer learning,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 27, no. 4, pp. 760–771, Apr. 2019.   \n[207] C. Ren, D. Dai, K. Huang, and Z. Lai, “Transfer learning of structured representation for face recognition,” IEEE Trans. Image Process., vol. 23, no. 12, pp. 5440–5454, Dec. 2014.   \n[208] J. Wang, Y. Chen, L. Hu, X. Peng, and P.S. Yu, “Stratified transfer learning for cross-domain activity recognition,” in Proc. IEEE International Conference on Pervasive Computing and Communications, Athens, Mar. 2018, pp. 1–10.   \n[209] J. Deng, Z. Zhang, E. Marchi, and B. Schuller, “Sparse autoencoder-based feature transfer learning for speech emotion recognition,” in Proc. Humaine Association Conference on Affective Computing and Intelligent Interaction, Geneva, Sep. 2013, pp. 511– 516.   \n[210] D. Xi, F. Zhuang, G. Zhou, X. Cheng, F. Lin, and Q. He, “Domain adaptation with category attention network for deep sentiment analysis,” in Proc. The Web Conference, Taipei, Apr. 2020, pp. 3133– 3139.   \n[211] Y. Zhu, D. Xi, B. Song, F. Zhuang, S. Chen, X. Gu, and Q. He, “Modeling users’ behavior sequences with hierarchical explainable network for cross-domain fraud detection,” in Proc. The Web Conference, Taipei, Apr. 2020, pp. 928–938.   \n[212] J. Tang, T. Lou, J. Kleinberg, and S. Wu, “Transfer learning to infer social ties across heterogeneous networks,” ACM Trans. Inf. Syst., vol. 34, no. 2, pp. 1–43, Apr. 2016.   \n[213] L. Zhang, L. Zhang, D. Tao, and X. Huang, “Sparse transfer manifold embedding for hyperspectral target detection,” IEEE Trans. Geosci. Remote Sensing, vol. 52, no. 2, pp. 1030–1043, Feb. 2014.   \n[214] F. Zhuang, K. Duan, T. Guo, Y. Zhu, D. Xi, Z. Qi, and Q. He, “Transfer learning toolkit: Primers and benchmarks,” 2019, arXiv:1911.08967v1.   \n[215] K. Saenko, B. Kulis, M. Fritz, and T. Darrell, “Adapting visual category models to new domains,” in Proc. 11th European Conference on Computer Vision, Heraklion, Sep. 2010, pp. 213–226.   \n[216] Z.C. Lipton, “The mythos of model interpretability,” ACM Que., vol. 16, no. 3, May 2018, pp. 1–27.", "reference_length": 44610, "reference_token": 14072, "txt_length": 159154, "txt_token": 38165, "txt": "# A Comprehensive Survey on Transfer Learning  \n\nFuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Senior Member, IEEE, Hui Xiong, Fellow, IEEE, and Qing He  \n\nAbstract—Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.  \n\nndex Terms—Transfer learning, machine learning, domain adaptation, interpretat  \n\n# 1 INTRODUCTION  \n\nLTHOUGH traditional machine learning technology has achieved great success and has been successfully applied in many practical applications, it still has some limitations for certain real-world scenarios. The ideal scenario of machine learning is that there are abundant labeled training instances, which have the same distribution as the test data. However, collecting sufficient training data is often expensive, time-consuming, or even unrealistic in many scenarios. Semi-supervised learning can partly solve this problem by relaxing the need of mass labeled data. Typically, a semisupervised approach only requires a limited number of labeled data, and it utilizes a large amount of unlabeled data to improve the learning accuracy. But in many cases, unlabeled instances are also difficult to collect, which usually makes the resultant traditional models unsatisfactory.  \n\nTransfer learning, which focuses on transferring the knowledge across domains, is a promising machine learning methodology for solving the above problem. The concept about transfer learning may initially come from educational psychology. According to the generalization theory of transfer, as proposed by psychologist C.H. Judd, learning to transfer is the result of the generalization of experience. It is possible to realize the transfer from one situation to another, as long as a person generalizes his experience. According to this theory, the prerequisite of transfer is that there needs to be a connection between two learning activities. In practice, a person who has learned the violin can learn the piano faster than others, since both the violin and the piano are musical instruments and may share some common knowledge. Fig. 1 shows some intuitive examples about transfer learning. Inspired by human beings’ capabilities to transfer knowledge across domains, transfer learning aims to leverage knowledge from a related domain (called source domain) to improve the learning performance or minimize the number of labeled examples required in a target domain. It is worth mentioning that the transferred knowledge does not always bring a positive impact on new tasks. If there is little in common between domains, knowledge transfer could be unsuccessful. For example, learning to ride a bicycle cannot help us learn to play the piano faster. Besides, the similarities between domains do not always facilitate learning, because sometimes the similarities may be misleading. For example, although Spanish and French have a close relationship with each other and both belong to the Romance group of languages, people who learn Spanish may experience difficulties in learning French, such as using the wrong vocabulary or conjugation. This occurs because previous successful experience in Spanish can interfere with learning the word formation, usage, pronunciation, conjugation, etc., in French. In the field of psychology, the phenomenon that previous experience has a negative effect on learning new tasks is called negative transfer [1]. Similarly, in the transfer learning area, if the target learner is negatively affected by the transferred knowledge, the phenomenon is also termed as negative transfer [2], [3]. Whether negative transfer will occur may depend on several factors, such as the relevance between the source and the target domains and the learners capacity of finding the transferable and beneficial part of the knowledge across domains. In [3], a formal definition and some analyses of negative transfer are given.  \n\n![](images/f1a01b913b07d81010da2cc2fcc94d1897b409da668d7271790724d824100411.jpg)  \nFig. 1. Intuitive examples about transfer learning.  \n\nRoughly speaking, according to the discrepancy between domains, transfer learning can be further divided into two categories, i.e., homogeneous and heterogeneous transfer learning [4]. Homogeneous transfer learning approaches are developed and proposed for handling the situations where the domains are of the same feature space. In homogeneous transfer learning, some studies assume that domains differ only in marginal distributions. Therefore, they adapt the domains by correcting the sample selection bias [5] or covariate shift [6]. However, this assumption does not hold in many cases. For example, in sentiment classification problem, a word may have different meaning tendencies in different domains. This phenomenon is also called context feature bias [7]. To solve this problem, some studies further adapt the conditional distributions. Heterogeneous transfer learning refers to the knowledge transfer process in the situations where the domains have different feature spaces. In addition to distribution adaptation, heterogeneous transfer learning requires feature space adaptation [7], which makes it more complicated than homogeneous transfer learning.  \n\nThe survey aims to give readers a comprehensive understanding about transfer learning from the perspectives of data and model. The mechanisms and the strategies of transfer learning approaches are introduced to allow readers grasp how the approaches work. And a number of the existing transfer learning researches are connected and systematized. Specifically, over forty representative transfer learning approaches are introduced. Besides, we conduct experiments to demonstrate on which dataset a transfer learning model performs well.  \n\nIn this survey, we focus more on homogeneous transfer learning. Some interesting transfer learning topics are not covered in this survey, such as reinforcement transfer learning [8], lifelong transfer learning [9], and online transfer learning [10]. The rest of this survey are organized into seven sections. Section 2 clarifies the difference between transfer learning and other related machine learning techniques. Section 3 introduces the notations used in this survey and the definitions about transfer learning. Sections 4 and 5 interpret transfer learning approaches from the data and the model perspectives, respectively. Section 6 introduces some applications of transfer learning. Experiments are conducted and the results are provided in Section 7. The last section concludes this survey. The main contributions of this survey are summarized below.  \n\n• Over forty representative transfer learning approaches are introduced and summarized, which can give readers a comprehensive overview about transfer learning. • Experiments are conducted to compare different transfer learning approaches. The performance of over twenty different approaches is displayed intuitively and then analyzed, which may be instructive for the readers to select the appropriate ones in practice.  \n\n# 2 RELATED WORK  \n\nSome areas related to transfer learning are introduced. The connections and difference between them and transfer learning are clarified.  \n\nSemi-Supervised Learning [11]: Semi-supervised learning is a machine learning task and method that lies between supervised learning (with completely labeled instances) and unsupervised learning (without any labeled instances). Typically, a semi-supervised method utilizes abundant unlabeled instances combined with a limited number of labeled instances to train a learner. Semi-supervised learning relaxes the dependence on labeled instances, and thus reduces the expensive labeling costs. Note that, in semisupervised learning, both the labeled and unlabeled instances are drawn from the same distribution. In contrast, in transfer learning, the data distributions of the source and the target domains are usually different. Many transfer learning approaches absorb the technology of semi-supervised learning. The key assumptions in semi-supervised learning, i.e., smoothness, cluster, and manifold assumptions, are also made use of in transfer learning. It is worth mentioning that semi-supervised transfer learning is a controversial term. The reason is that the concept of whether the label information is available in transfer learning is ambiguous because both the source and the target domains can be involved.  \n\nMulti-View Learning [12]: Multi-view learning focuses on the machine learning problems with multi-view data. A view represents a distinct feature set. An intuitive example about multiple views is that a video object can be described from two different viewpoints, i.e., the image signal and the audio signal. Briefly, multi-view learning describes an object from multiple views, which results in abundant information. By properly considering the information from all views, the learner’s performance can be improved. There are several strategies adopted in multi-view learning such as subspace learning, multi-kernel learning, and co-training [13], [14]. Multi-view techniques are also adopted in some transfer learning approaches. For example, Zhang et al. proposed a multi-view transfer learning framework, which imposes the consistency among multiple views [15]. Yang and Gao incorporated multi-view information across different domains for knowledge transfer [16]. The work by Feuz and Cook introduces a multi-view transfer learning approach for activity learning, which transfers activity knowledge between heterogeneous sensor platforms [17].  \n\nMulti-Task Learning [18]: The thought of multi-task learning is to jointly learn a group of related tasks. To be more specific, multi-task learning reinforces each task by taking advantage of the interconnections between task, i.e., considering both the inter-task relevance and the inter-task difference. In this way, the generalization of each task is enhanced. The main difference between transfer learning and multi-task learning is that the former transfer the knowledge contained in the related domains, while the latter transfer the knowledge via simultaneously learning some related tasks. In other words, multi-task learning pays equal attention to each task, while transfer learning pays more attention to the target task than to the source task. There are some commons and associations between transfer learning and multi-task learning. Both of them aim to improve the performance of learners via knowledge transfer. Besides, they adopt some similar strategies for constructing models, such as feature transformation and parameter sharing. Note that some existing studies utilize both the transfer learning and the multi-task learning technologies. For example, the work by Zhang et al. employs multi-task and transfer learning techniques for biological image analysis [19]. The work by Liu et al. proposes a framework for human action recognition based on multi-task learning and multi-source transfer learning [20].  \n\n# 3 OVERVIEW  \n\nIn this section, the notations used in this survey are listed for convenience. Besides, some definitions and categorizations about transfer learning are introduced, and some related surveys are also provided.  \n\n# 3.1 Notation  \n\nFor convenience, a list of symbols and their definitions are shown in Table 1. Besides, we use $||\\cdot||$ to represent the norm and superscript T to denote the transpose of a vector/matrix.  \n\n# 3.2 Definition  \n\nIn this subsection, some definitions about transfer learning are given. Before giving the definition of transfer learning, let us review the definitions of a domain and a task.  \n\nDefinition 1. (Domain) $A$ domain $\\mathcal{D}$ is composed of two parts, i.e., a feature space $\\mathcal{X}$ and a marginal distribution $P(X)$ . In other words, $\\boldsymbol{\\mathcal{D}}\\,=\\,\\{\\boldsymbol{\\mathcal{X}},\\boldsymbol{P}(\\boldsymbol{X})\\}$ . And the symbol $X$ denotes an instance set, which is defined as $X\\,=\\,\\dot{\\{\\mathbf{x}}|\\mathbf{x}_{i}}\\,\\in\\,\\mathcal{X}$ , $i=$ $1,\\cdot\\cdot\\cdot\\,,n\\}$ .  \n\nDefinition 2. (Task) $A$ task $\\tau$ consists of a label space $\\boldsymbol{\\wp}$ and $^{a}$ decision function $f,$ i.e., $\\mathcal{T}=\\{\\mathcal{V},f\\}$ . The decision function $f$ is an implicit one, which is expected to be learned from the sample data.  \n\nSome machine learning models actually output the predicted conditional distributions of instances. In this case, $f(\\mathbf{x}_{j})=\\{P(y_{k}|\\mathbf{x}_{j})|y_{k}\\in\\mathcal{Y},k=1,\\cdots,|\\mathcal{Y}|\\}$ .  \n\nIn practice, a domain is often observed by a number of instances with/without the label information. For example, a source domain $\\mathcal{D}_{S}$ corresponding to a source task $\\tau_{S}$ is usually observed via the instance-label pairs, i.e., ${\\cal D}_{S}\\ =\\ \\{({\\bf x},\\bar{y})|{\\bf x}_{i}\\ \\in\\ \\chi^{S},y_{i}\\ \\in\\ \\y^{S},i\\ =\\ 1,\\ldots\\stackrel{\\cdot}{,}n^{S}\\};$ an observation of the target domain usually consists of a number of unlabeled instances and/or limited number of labeled instances.  \n\nDefinition 3. (Transfer Learning) Given some/an observation(s) corresponding to $m^{S}~\\in~\\mathbb{N}^{+}$ source domain(s) and task(s)  \n\n<html><body><table><thead><tr><td><b>Symbol</b></td><td><b>Definition</b></td></tr></thead><tbody><tr><td>n</td><td>Number of instances</td></tr><tr><td>m</td><td>Number of domains</td></tr><tr><td>D</td><td> Domain</td></tr><tr><td>T</td><td>Task</td></tr><tr><td>七</td><td> Feature space</td></tr><tr><td>J</td><td>Label space</td></tr><tr><td>X</td><td>Feature vector</td></tr><tr><td>y</td><td>Label</td></tr><tr><td>X</td><td> Instance set</td></tr><tr><td>Y</td><td> Label set corresponding to X</td></tr><tr><td>S</td><td> Source domain</td></tr><tr><td>T</td><td>Target domain</td></tr><tr><td>L</td><td> Labeled instances</td></tr><tr><td>U</td><td> Unlabeled instances</td></tr><tr><td>H</td><td> Reproducing kernel Hilbert space</td></tr><tr><td>θ</td><td>Mapping/Coefficient vector</td></tr><tr><td>α</td><td>Weighting coefficient</td></tr><tr><td>β</td><td>Weighting coeficient</td></tr><tr><td>入</td><td> Tradeoff parameter</td></tr><tr><td>8</td><td>Parameter/Error</td></tr><tr><td>b</td><td>Bias</td></tr><tr><td>B</td><td> Boundary parameter</td></tr><tr><td>N</td><td>Iteration/Kernel number</td></tr><tr><td>f</td><td>Decision function</td></tr><tr><td>C</td><td>Loss function</td></tr><tr><td>n</td><td> Scale parameter</td></tr><tr><td>G</td><td>Graph</td></tr><tr><td>重</td><td> Nonlinear mapping</td></tr><tr><td>。</td><td>Monotonically increasing function</td></tr><tr><td>S</td><td>Structural risk</td></tr><tr><td>K</td><td>Kernel function</td></tr><tr><td>K</td><td>Kernel matrix</td></tr><tr><td>H</td><td> Centering matrix</td></tr><tr><td>C</td><td>Covariance matrix</td></tr><tr><td>d</td><td>Document</td></tr><tr><td>W</td><td>Word</td></tr><tr><td>2</td><td> Class variable</td></tr><tr><td>之</td><td>Noise</td></tr><tr><td>D</td><td> Discriminator</td></tr><tr><td>G</td><td>Generator</td></tr><tr><td>S</td><td>Function</td></tr><tr><td>M</td><td> Orthonormal bases</td></tr><tr><td>θ</td><td> Model parameters</td></tr><tr><td>P</td><td>Probability</td></tr><tr><td>E</td><td>Expectation</td></tr><tr><td>Q</td><td>Matrix variable</td></tr><tr><td>R</td><td>Matrix variable</td></tr><tr><td>W</td><td> Mapping matrix</td></tr></tbody></table></body></html>  \n\n(i.e., $\\{(T_{S_{i}},T_{S_{i}})|i\\;=\\;1,\\cdot\\cdot\\cdot\\;,m^{S}\\})$ , and some/an observation(s) about $m^{T}\\,\\in\\,\\mathbb{N}^{+}$ target domain(s) and task(s) (i.e., $\\{({\\mathcal D}_{T_{j}},{\\mathcal T}_{T_{j}})|j\\ =\\ 1,\\cdot\\cdot\\cdot,m^{\\tilde{T}}\\}),$ , transfer learning utilizes the knowledge implied in the source domain(s) to improve the performance of the learned decision functions $f^{T_{j}}$ $(j\\,=$ $1,\\cdot\\cdot\\dot{,m}^{T})$ on the target domain(s).  \n\nThe above definition, which covers the situation of multisource transfer learning, is an extension of the one presented in the survey [2]. If $m^{S}$ equals 1, the scenario is called singlesource transfer learning. Otherwise, it is called multi-source transfer learning. Besides, $m^{T}$ represents the number of the transfer learning tasks. A few studies focus on the setting that $m^{T}\\geq2$ [21]. The existing transfer learning studies pay more attention to the scenarios where $m^{T}={\\bar{1}}$ (especially where $m^{S}\\ =\\ m^{T}\\ =\\ 1)$ ). It is worth mentioning that the observation of a domain or a task is a concept with broad sense, which is often cemented into a labeled/unlabeled instance set or a pre-learned model. A common scenario is that we have abundant labeled instances or have a welltrained model on the source domain, and we only have limited labeled target-domain instances. In this case, the resources such as the instances and the model are actually the observations, and the goal of transfer learning is to learn a more accurate decision function on the target domain.  \n\n![](images/7f83b2aee841a4c9b803bc8c3e849d44e78ab07002a3534927277ec477819243.jpg)  \nFig. 2. Categorizations of transfer learning.  \n\nAnother term commonly used in the transfer learning area is domain adaptation. Domain adaptation refers to the process that adapting one or more source domains to transfer knowledge and improve the performance of the target learner [4]. Transfer learning often relies on the domain adaptation process, which attempts to reduce the difference between domains.  \n\n# 3.3 Categorization of Transfer Learning  \n\nThere are several categorization criteria of transfer learning. For example, transfer learning problems can be divided into three categories, i.e., transductive, inductive, and unsupervised transfer learning [2]. The complete definitions of these three categories are presented in [2]. These three categories can be interpreted from a label-setting aspect. Roughly speaking, transductive transfer learning refers to the situations where the label information only comes from the source domain. If the label information of the targetdomain instances is available, the scenario can be categorized into inductive transfer learning. If the label information is unknown for both the source and the target domains, the situation is known as unsupervised transfer learning. Another categorization is based on the consistency between the source and the target feature spaces and label spaces. If $\\mathcal{X}^{S}\\ =\\ \\mathcal{X}^{T}$ and $y^{S^{\\breve{\\mathbf{\\alpha}}}}=\\ y^{T}$ , the scenario is termed as homogeneous transfer learning. Otherwise, if $\\mathcal{X}^{S}\\ \\ne\\ \\mathcal{X}^{T}$ or/and $\\mathcal{V}^{S}\\ne\\mathcal{V}^{T}.$ , the scenario is referred to as heterogeneous transfer learning.  \n\nAccording to the survey [2], the transfer learning approaches can be categorized into four groups: instancebased, feature-based, parameter-based, and relational-based approaches. Instance-based transfer learning approaches are mainly based on the instance weighting strategy. Featurebased approaches transform the original features to create a new feature representation; they can be further divided into two subcategories, i.e., asymmetric and symmetric featurebased transfer learning. Asymmetric approaches transform the source features to match the target ones. In contrast, symmetric approaches attempt to find a common latent feature space and then transform both the source and the target features into a new feature representation. The parameter-based transfer learning approaches transfer the knowledge at the model/parameter level. Relational-based transfer learning approaches mainly focus on the problems in relational domains. Such approaches transfer the logical relationship or rules learned in the source domain to the target domain. For better understanding, Fig. 2 shows the above-mentioned categorizations of transfer learning.  \n\nSome surveys are provided for the readers who want to have a more complete understanding of this field. The survey by Pan and Yang [2], which is a pioneering work, categorizes transfer learning and reviews the research progress before 2010. The survey by Weiss et al. introduces and summarizes a number of homogeneous and heterogeneous transfer learning approaches [4]. Heterogeneous transfer learning is specially reviewed in the survey by Day and Khoshgoftaar [7]. Some surveys review the literatures related to specific themes such as reinforcement learning [8], computational intelligence [22], and deep learning [23], [24]. Besides, a number of surveys focus on specific application scenarios including activity recognition [25], visual categorization [26], collaborative recommendation [27], computer vision [24], and sentiment analysis [28].  \n\nNote that the organization of this survey does not strictly follow the above-mentioned categorizations. In the next two sections, transfer learning approaches are interpreted from the data and the model perspectives. Roughly speaking, data-based interpretation covers the above-mentioned instance-based and feature-based transfer learning approaches, but from a broader perspective. Model-based interpretation covers the above-mentioned parameter-based approaches. Since there are relatively few studies concerning relational-based transfer learning and the representative approaches are well introduced in [2], [4], this survey does not focus on relational-based approaches.  \n\n![](images/907d9899b6c7ec1973c81516b0002ed3dbcf6c460bcfa4961450759a7683b20f.jpg)  \nFig. 3. Strategies and the objectives of the transfer learning approaches from the data perspective.  \n\n# 4 DATA-BASED INTERPRETATION  \n\nMany transfer learning approaches, especially the databased approaches, focus on transferring the knowledge via the adjustment and the transformation of data. Fig. 3 shows the strategies and the objectives of the approaches from the data perspective. As shown in Fig. 3, space adaptation is one of the objectives. This objective is required to be satisfied mostly in heterogeneous transfer learning scenarios. In this survey, we focus more on homogeneous transfer learning, and the main objective in this scenario is to reduce the distribution difference between the source-domain and the target-domain instances. Furthermore, some advanced approaches may attempt to preserve the data properties in the adaptation process. There are generally two strategies for realizing the objectives from the data perspective, i.e., instance weighting and feature transformation. In this section, some related transfer learning approaches are introduced in proper order according to the strategies shown in Fig. 3.  \n\n# 4.1 Instance Weighting Strategy  \n\nLet us first consider a simple scenario in which a large number of labeled source-domain and a limited number of target-domain instances are available and domains differ only in marginal distributions (i.e., $P^{S}(X)\\,\\neq\\,P^{T}(X)$ and $P^{S}\\!(Y|X)\\ =\\ P^{T}(Y|X))$ . For example, suppose we need to build a model to diagnose cancer in a specific region where the elderly are the majority. Limited target-domain instances are given, and relevant data are available from another region where young people are the majority. Directly transferring all the data from another region may be unsuccessful, since the marginal distribution difference exists, and the elderly have a higher risk of cancer than younger people. In this scenario, it is natural to consider adapting the marginal distributions. A simple idea is to assign weights to the source-domain instances in the loss function. The weighting strategy is based on the following equation [5]:  \n\n$$\n\\begin{array}{r}{\\mathbb{E}_{(\\mathbf{x},y)\\sim P^{T}}\\left[\\mathcal{L}(\\mathbf{x},y;f)\\right]=\\mathbb{E}_{(\\mathbf{x},y)\\sim P^{S}}\\left[\\frac{P^{T}(\\mathbf{x},y)}{P^{S}(\\mathbf{x},y)}\\mathcal{L}(\\mathbf{x},y;f)\\right]}\\\\ {=\\mathbb{E}_{(\\mathbf{x},y)\\sim P^{S}}\\left[\\frac{P^{T}(\\mathbf{x})}{P^{S}(\\mathbf{x})}\\mathcal{L}(\\mathbf{x},y;f)\\right].}\\end{array}\n$$  \n\nTherefore, the general objective function of a learning task can be written as [5]:  \n\n$$\n\\underset{f}{\\operatorname*{min}}\\frac{1}{n^{S}}\\sum_{i=1}^{n^{S}}\\beta_{i}\\mathcal{L}\\left(f(\\mathbf{x}_{i}^{S}),y_{i}^{S}\\right)+\\Omega(f),\n$$  \n\nwhere $\\beta_{i}$ $\\mathrm{~\\boldmath~\\chi~}_{(i}~=~1,2,\\cdots,n^{S})$ is the weighting parameter. The theoretical value of $\\beta_{i}$ is equal to $P^{T}\\overline{{(\\mathbf{x}_{i})}}/P^{S}(\\mathbf{x}_{i})$ . However, this ratio is generally unknown and is difficult to be obtained by using the traditional methods.  \n\nKernel Mean Matching (KMM) [5], which is proposed by Huang et al., resolves the estimation problem of the above unknown ratios by matching the means between the sourcedomain and the target-domain instances in a Reproducing Kernel Hilbert Space (RKHS), i.e.,  \n\n$$\n\\begin{array}{r l}&{\\displaystyle\\arg\\operatorname*{min}_{\\beta_{i}\\in[0,B]}\\left\\|\\frac{1}{n^{S}}\\sum_{i=1}^{n^{S}}\\beta_{i}\\Phi(\\mathbf{x}_{i}^{S})-\\frac{1}{n^{T}}\\sum_{j=1}^{n^{T}}\\Phi(\\mathbf{x}_{j}^{T})\\right\\|_{\\mathbb{H}}^{2}}\\\\ &{\\displaystyle s.t.\\ |\\frac{1}{n^{S}}\\sum_{i=1}^{n^{S}}\\beta_{i}-1|\\leq\\delta,}\\end{array}\n$$  \n\nwhere $\\delta$ is a small parameter, and $B$ is a parameter for constraint. The above optimization problem can be converted into a quadratic programming problem by expanding and using the kernel trick. This approach to estimating the ratios of distributions can be easily incorporated into many existing algorithms. Once the weight $\\beta_{i}$ is obtained, a learner can be trained on the weighted source-domain instances.  \n\nThere are some other studies attempting to estimate the weights. For example, Sugiyama et al. proposed an approach termed Kullback-Leibler Importance Estimation Procedure (KLIEP) [6]. KLIEP depends on the minimization of the Kullback-Leibler (KL) divergence and incorporates a built-in model selection procedure. Based on the studies of weight estimation, some instance-based transfer learning frameworks or algorithms are proposed. For example, Sun et al. proposed a multi-source framework termed 2-Stage Weighting Framework for Multi-Source Domain Adaptation (2SW-MDA) with the following two stages [29].  \n\n1. Instance Weighting: The source-domain instances are assigned with weights to reduce the marginal distribution difference, which is similar to KMM.   \n2. Domain Weighting: Weights are assigned to each source domain for reducing the conditional distribution difference based on the smoothness assumption [30].  \n\nThen, the source-domain instances are reweighted based on the instance weights and the domain weights. These reweighted instances and the labeled target-domain instances are used to train the target classifier.  \n\nIn addition to directly estimating the weighting parameters, adjusting weights iteratively is also effective. The key is to design a mechanism to decrease the weights of the instances that have negative effects on the target learner. A representative work is TrAdaBoost [31], which is a framework proposed by Dai et al. This framework is an extension of AdaBoost [32]. AdaBoost is an effective boosting algorithm designed for traditional machine learning tasks. In each iteration of AdaBoost, a learner is trained on the instances with updated weights, which results in a weak classifier. The weighting mechanism of instances ensures that the instances with incorrect classification are given more attention. Finally, the resultant weak classifiers are combined to form a strong classifier. TrAdaBoost extends the AdaBoost to the transfer learning scenario; a new weighting mechanism is designed to reduce the impact of the distribution difference. Specifically, in TrAdaBoost, the labeled source-domain and labeled target-domain instances are combined as a whole, i.e., a training set, to train the weak classifier. The weighting operations are different for the source-domain and the target-domain instances. In each iteration, a temporary variable $\\overline{{\\delta}}_{.}$ , which measures the classification error rate on the labeled target-domain instances, is calculated. Then, the weights of the target-domain instances are updated based on $\\boldsymbol{\\vec{\\delta}}$ and the individual classification results, while the weights of the source-domain instances are updated based on a designed constant and the individual classification results. For better understanding, the formulas used in the $k$ -th iteration $(k\\,=\\,1,\\cdot\\cdot\\,,N)$ for updating the weights are presented repeatedly as follows [31]:  \n\n$$\n\\begin{array}{r l}&{\\beta_{k,i}^{S}=\\beta_{k-1,i}^{S}(1+\\sqrt{2\\ln n^{S}/N})^{-\\left|f_{k}(\\mathbf{x}_{i}^{S})-y_{i}^{S}\\right|}\\,\\big(i=1,\\cdots,n^{S}\\big),}\\\\ &{\\beta_{k,j}^{T}=\\beta_{k-1,j}^{T}\\big(\\bar{\\delta}_{k}/(1-\\bar{\\delta}_{k}))^{-\\left|f_{k}(\\mathbf{x}_{j}^{T})-y_{j}^{T}\\right|}\\,\\big(j=1,\\cdots,n^{T}\\big).}\\end{array}\n$$  \n\nNote that each iteration forms a new weak classifier. The final classifier is constructed by combining and ensembling half the number of the newly resultant weak classifiers through voting scheme.  \n\nSome studies further extend TrAdaBoost. The work by Yao and Doretto [33] proposes a Multi-Source TrAdaBoost (MsTrAdaBoost) algorithm, which mainly has the following two steps in each iteration.  \n\n1. Candidate Classifier Construction: A group of candidate weak classifiers are respectively trained on the weighted instances in the pairs of each source domain and the target domain, i.e., $\\mathcal{D}_{S_{i}}\\cup\\mathcal{D}_{T}$ $(i=1,\\cdots,m^{S})$ .   \n2. Instance Weighting: A classifier (denoted by $j$ and trained on $\\mathcal{D}_{S_{j}}\\cup\\underline{{\\mathcal{D}}}_{T})$ which has the minimal classification error rate $\\bar{\\delta}$ on the target domain instances is selected, and then is used for updating the weights of the instances in $\\mathcal{D}_{S_{j}}$ and $\\mathcal{D}_{T}$ .  \n\nFinally, the selected classifiers from each iteration are combined to form the final classifier. Another parameter-based algorithm, i.e., TaskTrAdaBoost, is also proposed in the work [33], which is introduced in Section 5.3.  \n\nSome approaches realize instance weighting strategy in a heuristic way. For example, Jiang and Zhai proposed a general weighting framework [34]. There are three terms in the framework’s objective function, which are designed to minimize the cross-entropy loss of three types of instances. The following types of instances are used to construct the target classifier.  \n\n• Labeled Target-domain Instance: The classifier should minimize the cross-entropy loss on them, which is actually a standard supervised learning task. • Unlabeled Target-domain Instance: These instances’ true conditional distributions $P(y|\\mathbf{x}_{i}^{T,U})$ are unknown and should be estimated. A possible solution is to train an auxiliary classifier on the labeled source-domain and target-domain instances to help estimate the conditional distributions or assign pseudo labels to these instances. • Labeled Source-domain Instance: The authors define the weight of xiS,L as the product of two parts, i.e., $\\alpha_{i}$ and $\\beta_{i}$ . The weight $\\beta_{i}$ is ideally equal to $P^{T}(\\mathbf{x}_{i})/P^{S}(\\mathbf{x}_{i}),$ which can be estimated by non-parametric methods such as KMM or can be set uniformly in the worst case. The weight $\\alpha_{i}$ is used to filter out the source-domain instances that differ greatly from the target domain.  \n\nA heuristic method can be used to produce the value of $\\alpha_{i},$ which contains the following three steps.  \n\n1. Auxiliary Classifier Construction: An auxiliary classifier trained on the labeled target-domain instances are used to classify the unlabeled source-domain instances.   \n2. Instance Ranking: The source-domain instances are ranked based on the probabilistic prediction results.   \n3. Heuristic Weighting $(\\beta_{i})$ : The weights of the top- $k$ source-domain instances with wrong predictions are set to zero, and the weights of others are set to one.  \n\n# 4.2 Feature Transformation Strategy  \n\nFeature transformation strategy is often adopted in featurebased approaches. For example, consider a cross-domain text classification problem. The task is to construct a target classifier by using the labeled text data from a related domain. In this scenario, a feasible solution is to find the common latent features (e.g., latent topics) through feature transformation and use them as a bridge to transfer knowledge. Feature-based approaches transform each original feature into a new feature representation for knowledge transfer. The objectives of constructing a new feature representation include minimizing the marginal and the conditional distribution difference, preserving the properties or the potential structures of the data, and finding the correspondence between features. The operations of feature transformation can be divided into three types, i.e., feature augmentation, feature reduction, and feature alignment. Besides, feature reduction can be further divided into several types such as feature mapping, feature clustering, feature selection, and feature encoding. A complete feature transformation process designed in an algorithm may consist of several operations.  \n\n<html><body><table><thead><tr><td><b>Measurement</b></td><td><b>Related Algorithms</b></td></tr></thead><tbody><tr><td>Maximum Mean Discrepancy [35]</td><td>[36] [37] [38] [39]. . .</td></tr><tr><td>Kullback-Leibler Divergence [40]</td><td>[41] [42] [43] [44].. .</td></tr><tr><td>Jensen-Shannon Divergence [45]</td><td>[46] [47] [48] [49].. .</td></tr><tr><td>Bregman Divergence [50]</td><td>[51] [52] [53] [54]...</td></tr><tr><td>Hilbert-Schmidt Independence Criterion [55]</td><td>[36] [56] [57] [58]. . .</td></tr></tbody></table></body></html>  \n\n# 4.2.1 Distribution Difference Metric  \n\nOne primary objective of feature transformation is to reduce the distribution difference of the source and the target domain instances. Therefore, how to measure the distribution difference or the similarity between domains effectively is an important issue.  \n\nThe measurement termed Maximum Mean Discrepancy (MMD) is widely used in the field of transfer learning, which is formulated as follows [35]:  \n\n# 4.2.2 Feature Augmentation  \n\nFeature augmentation operations are widely used in feature transformation, especially in symmetric feature-based approaches. To be more specific, there are several ways to realize feature augmentation such as feature replication and feature stacking. For better understanding, we start with a simple transfer learning approach which is established based on feature replication.  \n\nThe work by Daume´ proposes a simple domain adaptation method, i.e., Feature Augmentation Method (FAM) [64]. This method transforms the original features by simple feature replication. Specifically, in single-source transfer learning scenario, the feature space is augmented to three times its original size. The new feature representation consists of general features, source-specific features, and targetspecific features. Note that, for the transformed sourcedomain instances, their target-specific features are set to zero. Similarly, for the transformed target-domain instances, their source-specific features are set to zero. The new feature representation of FAM is presented as follows:  \n\n$$\n\\Phi_{S}(\\mathbf{x}_{i}^{S})=\\langle\\mathbf{x}_{i}^{S},\\mathbf{x}_{i}^{S},\\mathbf{0}\\rangle,\\;\\Phi_{T}(\\mathbf{x}_{j}^{T})=\\langle\\mathbf{x}_{j}^{T},\\mathbf{0},\\mathbf{x}_{j}^{T}\\rangle,\n$$  \n\nwhere $\\Phi_{S}$ and $\\Phi_{T}$ denote the mappings to the new feature space from the source and the target domain, respectively. The final classifier is trained on the transformed labeled instances. It is worth mentioning that this augmentation method is actually redundant. In other words, augmenting the feature space in other ways (with fewer dimensions) may be able to produce competent performance. The superiority of FAM is that its feature expansion has an elegant form, which results in some good properties such as the generalization to multi-source scenarios. An extension of FAM is proposed in [65] by Daume´ et al., which utilizes the unlabeled instances to further facilitate the knowledge transfer process.  \n\nHowever, FAM may not work well in handling heterogeneous transfer learning tasks. The reason is that directly replicating features and padding zero vectors are less effective when the source and the target domains have different feature representations. To solve this problem, Li et al. proposed an approach termed Heterogeneous Feature Augmentation (HFA) [66], [67]. The feature representation of HFA is presented below:  \n\nMMD can be easily computed by using kernel trick. Briefly, MMD quantifies the distribution difference by calculating the distance of the mean values of the instances in a RKHS. Note that the above-mentioned KMM actually produces the weights of instances by minimizing the MMD distance between domains.  \n\n$$\n\\mathrm{MMD}(X^{S},X^{T})=\\left\\|\\frac{1}{n^{S}}\\sum_{i=1}^{n^{S}}\\Phi(\\mathbf{x}_{i}^{S})-\\frac{1}{n^{T}}\\sum_{j=1}^{n^{T}}\\Phi(\\mathbf{x}_{j}^{T})\\right\\|_{\\mathbb{H}}^{2}.\n$$  \n\nTable. 2 lists some commonly used metrics and the related algorithms. In addition to Table. 2, there are some other measurement criteria adopted in transfer learning, including Wasserstein distance [59], [60], central moment discrepancy [61], etc. Some studies focus on optimizing and improving the existing measurements. Take MMD as an example. Gretton et al. proposed a multi-kernel version of MMD, i.e., MK-MMD [62], which takes advantage of multiple kernels. Besides, Yan et al. proposed a weighted version of MMD [63], which attempts to address the issue of class weight bias.  \n\n$$\n\\Phi_{S}(\\mathbf{x}_{i}^{S})=\\langle W^{S}\\mathbf{x}_{i}^{S},\\mathbf{x}_{i}^{S},\\mathbf{0}^{T}\\rangle,\\;\\Phi_{T}(\\mathbf{x}_{j}^{T})=\\langle W^{T}\\mathbf{x}_{j}^{T},\\mathbf{0}^{S},\\mathbf{x}_{j}^{T}\\rangle,\n$$  \n\nwhere $W^{S}\\mathbf{x}_{i}^{S}$ and $W^{T}\\mathbf{x}_{j}^{T}$ have the same dimension; ${\\bf0}^{S}$ and $\\mathbf{0}^{T}$ denote the zero vectors with the dimensions of $\\mathbf{x}^{S}$ and $\\mathbf{x}^{T}$ , respectively. HFA maps the original features into a common feature space, and then performs a feature stacking operation. The mapped features, original features, and zero elements are stacked in a particular order to produce a new feature representation.  \n\n# 4.2.3 Feature Mapping  \n\nIn the field of traditional machine learning, there are many feasible mapping-based methods of extracting features such as Principal Component Analysis (PCA) [68] and Kernelized-PCA (KPCA) [69]. However, these methods mainly focus on the data variance rather than the distribution difference. In order to solve the distribution difference, some feature extraction methods are proposed for transfer learning. Let us first consider a simple scenario where there is little difference in the conditional distributions of the domains. In this case, the following simple objective function can be used to find a mapping for feature extraction:  \n\n$$\n\\operatorname*{min}_{\\Phi}\\big(\\mathrm{DIST}(X^{S},X^{T};\\Phi)+\\lambda\\Omega(\\Phi)\\big)/\\big(\\mathrm{VAR}(X^{S}\\cup X^{T};\\Phi)\\big),\n$$  \n\nwhere $\\Phi$ is a low-dimensional mapping function, $\\mathrm{DIST}(\\cdot)$ represents a distribution difference metric, $\\Omega(\\Phi)$ is a regularizer controlling the complexity of $\\Phi$ , and $\\operatorname{VAR}(\\cdot)$ represents the variance of instances. This objective function aims to find a mapping function $\\Phi$ that minimizes the marginal distribution difference between domains and meanwhile makes the variance of the instances as large as possible. The objective corresponding to the denominator can be optimized in several ways. One possible way is to optimize the objective of the numerator with a variance constraint. For example, the scatter matrix of the mapped instances can be enforced as an identity matrix. Another way is to optimize the objective of the numerator in a high-dimensional feature space at first. Then, a dimension reduction algorithm such as PCA or KPCA can be performed to realize the objective of the denominator.  \n\nFurther, finding the explicit formulation of $\\Phi(\\cdot)$ is nontrivial. To solve this problem, some approaches adopt linear mapping technique or turn to the kernel trick. In general, there are three main ideas to deal with the above optimization problems.  \n\n• (Mapping Learning $^+$ Feature Extraction) A possible way is to find a high-dimensional space at first where the objectives are met by solving a kernel matrix learning problem or a transformation matrix finding problem. Then, the high-dimensional features are compacted to form a lowdimensional feature representation. For example, once the kernel matrix is learned, the principal components of the implicit high-dimensional features can be extracted to construct a new feature representation based on PCA.  \n\n• (Mapping Construction $^+$ Mapping Learning) Another way is to map the original features to a constructed highdimensional feature space, and then a low-dimensional mapping is learned to satisfy the objective function. For example, a kernel matrix can be constructed based on a selected kernel function at first. Then, the transformation matrix can be learned, which projects the highdimensional features into a common latent subspace.  \n\n• (Direct Low-dimensional Mapping Learning) It is usually difficult to find a desired low-dimensional mapping directly. However, if the mapping is assumed to satisfy certain conditions, it may be solvable. For example, if the low-dimensional mapping is restricted to be a linear one, the optimization problem can be easily solved.  \n\nSome approaches also attempt to match the conditional distributions and preserve the structures of the data. To achieve this, the above simple objective function needs to incorporate new terms or/and constraints. For example, the following general objective function is a possible choice:  \n\n$$\n\\begin{array}{r l}&{\\underset{\\Phi}{\\operatorname*{min}}\\,\\mu\\mathrm{DIST}(X^{S},X^{T};\\Phi)+\\lambda_{1}\\Omega^{\\mathrm{GEO}}(\\Phi)+\\lambda_{2}\\Omega(\\Phi)}\\\\ &{\\quad\\quad+\\,(1-\\mu)\\mathrm{DIST}(Y^{S}|X^{S},Y^{T}|X^{T};\\Phi),}\\\\ &{s.t.\\;\\Phi(X)^{\\mathrm{T}}H\\Phi(X)=I,\\;\\mathrm{with}\\;H=I-(\\mathbf{1}/n)\\in\\mathbb{R}^{n\\times n},}\\end{array}\n$$  \n\nwhere $\\mu$ is a parameter balancing the marginal and the conditional distribution difference [70], $\\Omega^{\\mathrm{GE\\breve{O}}}(\\Phi)$ is a regularizer controlling the geometric structure, $\\Phi(X)$ is the matrix whose rows are the instances from both the source and the target domains with the extracted new feature representation, $H$ is the centering matrix for constructing the scatter matrix, and the constraint is used to maximize the variance. The last term in the objective function denotes the measurement of the conditional distribution difference.  \n\nBefore the further discussion about the above objective function, it is worth mentioning that the label information of the target-domain instances is often limited or even unknown. The lack of the label information makes it difficult to estimate the distribution difference. In order to solve this problem, some approaches resort to the pseudo-label strategy, i.e., assigning pseudo labels to the unlabeled targetdomain instances. A simple method of realizing this is to train a base classifier to assign pseudo labels. By the way, there are some other methods of providing pseudo labels such as co-training [71], [72] and tri-training [73], [74]. Once the pseudo-label information is complemented, the conditional distribution difference can be measured. For example, MMD can be modified and extended to measure the conditional distribution difference. Specifically, for each label, the source-domain and the target-domain instances that belong to the same class are collected, and the estimation expression of the conditional distribution difference is given by [38]:  \n\n$$\n\\sum_{k=1}^{|\\mathcal{Y}|}\\left|\\left|\\frac{1}{n_{k}^{S}}\\sum_{i=1}^{n_{k}^{S}}\\Phi(\\mathbf{x}_{i}^{S})-\\frac{1}{n_{k}^{T}}\\sum_{j=1}^{n_{k}^{T}}\\Phi(\\mathbf{x}_{j}^{T})\\right|\\right|_{\\mathcal{H}}^{2},\n$$  \n\nwhere $n_{k}^{S}$ and $n_{k}^{T}$ denote the numbers of the instances in the source and the target domains with the same label $\\smash{\\mathcal{V}_{\\boldsymbol{k}}}$ , respectively. This estimation actually measures the classconditional distribution (i.e., $P(\\mathbf{x}|y))$ difference to approximate the conditional distribution (i.e., $P(y|\\mathbf x))$ difference. Some studies improve the above estimation. For example, the work by Wang et al. uses a weighted method to additionally solve the class imbalance problem [70]. For better understanding, the transfer learning approaches that are the special cases of the general objective function presented in the previous paragraph are detailed as follows.  \n\n• $\\lvert\\mu=1$ and $\\lambda_{1}\\neq0_{.}$ ) The objective function of Maximum Mean Discrepancy Embedding (MMDE) is given by [75]:  \n\n$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{K}\\mathrm{MMD}(X^{S},X^{T};\\Phi)-\\frac{\\lambda_{1}}{n^{S}+n^{T}}\\sum_{i\\neq j}||\\Phi(\\mathbf{x}_{i})-\\Phi(\\mathbf{x}_{j})||^{2}}\\\\ &{s.t.\\;\\forall(\\mathbf{x}_{i}\\in k\\!-\\!\\mathrm{NN}(\\mathbf{x}_{j}))\\land(\\mathbf{x}_{j}\\in k\\!-\\!\\mathrm{NN}(\\mathbf{x}_{i})),}\\\\ &{\\quad\\quad||\\Phi(\\mathbf{x}_{i})-\\Phi(\\mathbf{x}_{j})||^{2}=||\\mathbf{x}_{i}-\\mathbf{x}_{j}||^{2},(\\mathbf{x}_{i},\\mathbf{x}_{j}\\in X^{S}\\cup X^{T}),}\\end{array}\n$$  \n\nwhere $k{\\mathrm{-}}\\mathrm{NN}(\\mathbf{x})$ represents the $k$ nearest neighbors of the instance $\\mathbf{x}$ . The authors design the above objective function motivated by Maximum Variance Unfolding (MVU) [76]. Instead of employing a scatter matrix constraint, the constraints and the second term of this objective function aim to maximize the distance between instances as well as preserve local geometry. The desired kernel matrix $K$ can be learned by solving a Semi-Definite Programming (SDP) [77] problem. After obtaining the kernel matrix,  \n\nPCA is applied to it, and then the leading eigenvectors are selected to help construct a low-dimensional feature representation.  \n\n• $(\\mu\\,\\,=\\,\\,1\\$ and $\\lambda_{1}~=~0_{.}$ ) The work by Pan et al. proposes an approach termed Transfer Component Analysis (TCA) [36], [78]. TCA adopts MMD to measure the marginal distribution difference and enforces the scatter matrix as the constraint. Different from MMDE that learns the kernel matrix and then further adopts PCA, TCA is a unified method that just needs to learn a linear mapping from an empirical kernel feature space to a low-dimensional feature space. In this way, it avoids solving the SDP problem, which results in relatively low computational burden. The final optimization problem can be easily solved via eigendecomposition. TCA can also be extended to utilize the label information. In the extended version, the scatter matrix constraint is replaced by a new one that balances the label dependence (measured by HSIC) and the data variance. Besides, a graph Laplacian regularizer [30] is also added to preserve the geometry of the manifold. Similarly, the final optimization problem can also be solved by eigen-decomposition.  \n\n• $(\\mu\\ =\\ 0.5$ and $\\lambda_{1}~=~0)$ ) Long et al. proposed an approach termed Joint Distribution Adaptation (JDA) [38]. JDA attempts to find a transformation matrix that maps the instances to a low-dimensional space where both the marginal and the conditional distribution difference are minimized. To realize it, the MMD metric and the pseudolabel strategy are adopted. The desired transformation matrix can be obtained by solving a trace optimization problem via eigen-decomposition. Further, it is obvious that the accuracy of the estimated pseudo labels affects the performance of JDA. In order to improve the labeling quality, the authors adopt the iterative refinement operations. Specifically, in each iteration, JDA is performed, and then a classifier is trained on the instances with the extracted features. Next, the pseudo labels are updated based on the trained classifier. After that, JDA is performed repeatedly with the updated pseudo labels. The iteration ends when convergence occurs. Note that JDA can be extended by utilizing the label and structure information [79], clustering information [80], various statistical and geometrical information [81], etc.  \n\n• $(\\mu\\in\\overline{{(0,1)}}$ and $\\lambda_{1}=0$ ) The paper by Wang et al. proposes an approach termed Balanced Distribution Adaptation (BDA) [70], which is an extension of JDA. Different from JDA which assumes that the marginal and the conditional distributions have the same importance in adaptation, BDA attempts to balance the marginal and the conditional distribution adaptation. The operations of BDA are similar to JDA. In addition, the authors also proposed the Weighted BDA (WBDA). In WBDA, the conditional distribution difference is measured by a weighted version of MMD to solve the class imbalance problem.  \n\nIt is worth mentioning that some approaches transform the features into a new feature space (usually of a high dimension) and train an adaptive classifier simultaneously. To realize this, the mapping function of the features and the decision function of the classifier need to be associated. One possible way is to define the following decision function:  \n\n$f(\\mathbf{x})=\\pmb{\\theta}\\cdot\\Phi(\\mathbf{x})\\!+\\!b,$ where $\\pmb{\\theta}$ denotes the classifier parameter; $b$ denotes the bias. In light of the representer theorem [82], the parameter $\\pmb{\\theta}$ can be defined as $\\begin{array}{r}{\\pmb{\\dot{\\theta}}=\\sum_{i=1}^{n}\\alpha_{i}\\Phi(\\mathbf{x}_{i})}\\end{array}$ , and thus we have  \n\n$$\nf(\\mathbf{x})=\\sum_{i=1}^{n}\\alpha_{i}\\Phi(\\mathbf{x}_{i})\\cdot\\Phi(\\mathbf{x})+b=\\sum_{i=1}^{n}\\alpha_{i}\\kappa(\\mathbf{x}_{i},\\mathbf{x})+b,\n$$  \n\nwhere $\\kappa$ denotes the kernel function. By using the kernel matrix as the bridge, the regularizers designed for the mapping function can be incorporated into the classifier’s objective function. In this way, the final optimization problem is usually about the parameter (e.g., $\\alpha_{i}$ ) or the kernel function. For example, the paper by Long et al. proposes a general framework termed Adaptation Regularization Based Transfer Learning (ARTL) [39]. The goals of ARTL are to learn the adaptive classifier, to minimize the structural risk, to jointly reduce the marginal and the conditional distribution difference, and to maximize the manifold consistency between the data structure and the predictive structure. The authors also proposed two specific algorithms under this framework based on different loss functions. In these two algorithms, the coefficient matrix for computing MMD and the graph Laplacian matrix for manifold regularization are constructed at first. Then, a kernel function is selected to construct the kernel matrix. After that, the classifier learning problem is converted into a parameter (i.e., $\\alpha_{i}$ ) solving problem, and the solution formula is also given in [39].  \n\nIn ARTL, the choice of the kernel function affects the performance of the final classifier. In order to construct a robust classifier, some studies turn to kernel learning. For example, the paper by Duan et al. proposes a unified framework termed Domain Transfer Multiple Kernel Learning (DTMKL) [83]. In DTMKL, the kernel function is assumed to be a linear combination of a group of base kernels, i.e., $\\begin{array}{r}{\\kappa(\\mathbf{x}_{i},\\mathbf{x}_{j})\\,=\\,\\sum_{k=1}^{N}\\beta_{k}\\kappa_{k}(\\mathbf{x}_{i},\\mathbf{x}_{j})}\\end{array}$ . DTMKL aims to minimize the distrib ution difference, the classification error, etc., simultaneously. The general objective function of DTMKL can be written as follows:  \n\n$$\n\\operatorname*{min}_{\\beta_{k},f}\\sigma\\bigl(\\mathrm{MMD}(X^{S},X^{T};\\kappa)\\bigr)+\\lambda\\Omega^{L}(\\beta_{k},f),\n$$  \n\nwhere $\\sigma$ is any monotonically increasing function, $f$ is the decision function with the same definition as the one in ARTL, and ΩL(βk, f) is a general term representing a group of regularizers defined on the labeled instances such as the ones for minimizing the classification error and controlling the complexity of the resultant model. The authors developed an algorithm to learn the kernel and the decision function simultaneously by using the reduced gradient descent method [84]. In each iteration, the weight coefficients of base kernels are fixed to update the decision function at first. Then, the decision function is fixed to update the weight coefficients. Note that DTMKL can incorporate many existing kernel methods. The authors proposed two specific algorithms under this framework. The first one implements the framework by using hinge loss and Support Vector Machine (SVM). The second one is an extension of the first one with an additional regularizer utilizing pseudolabel information, and the pseudo labels of the unlabeled instances are generated by using base classifiers.  \n\nFeature clustering aims to find a more abstract feature representation of the original features. Although it can be regarded as a way of feature extraction, it is different from the above-mentioned mapping-based extraction.  \n\nFor example, some transfer learning approaches implicitly reduce the features by using the co-clustering technique, i.e., simultaneously clustering both the columns and rows of (or say, co-cluster) a contingency table based on the information theory [85]. The paper by Dai et al. [41] proposes an algorithm termed Co-Clustering Based Classification (CoCC), which is used for document classification. In a document classification problem, the transfer learning task is to classify the target-domain documents (represented by a document-to-word matrix) with the help of the labeled source document-to-word data. CoCC regards the co-clustering technique as a bridge to transfer the knowledge. In CoCC algorithm, both the source and the target document-to-word matrices are co-clustered. The source document-to-word matrix is co-clustered to generate the word clusters based on the known label information, and these word clusters are used as constraints during the co-clustering process of the target-domain data. The coclustering criterion is to minimize the loss in mutual information, and the clustering results are obtained by iteration. Each iteration contains the following two steps.  \n\n1. Document Clustering: Each row of the target documentto-word matrix is re-ordered based on the objective function for updating the document clusters. 2. Word Clustering: The word clusters are adjusted to minimize the joint mutual-information loss of the source and the target document-word matrices.  \n\nAfter several times of iterations, the algorithm converges, and the classification results are obtained. Note that, in CoCC, the word clustering process implicitly extracts the word features to form unified word clusters.  \n\nDai et al. also proposed an unsupervised clustering approach, which is termed as Self-Taught Clustering (STC) [42]. Similar to CoCC, this algorithm is also a co-clusteringbased one. However, STC does not need the label information. STC aims to simultaneously co-cluster the sourcedomain and the target-domain instances with the assumption that these two domains share the same feature clusters in their common feature space. Therefore, two co-clustering tasks are separately performed at the same time to find the shared feature clusters. Each iteration of STC has the following steps.  \n\n1. Instance Clustering: The clustering results of the sourcedomain and the target domain instances are updated to minimize their respective loss in mutual information. 2. Feature Clustering: The feature clusters are updated to minimize the joint loss in mutual information.  \n\nWhen the algorithm converges, the clustering results of the target-domain instances are obtained.  \n\nDifferent from the above-mentioned co-clustering-based ones, some approaches extract the original features into concepts (or topics). In the document classification problem, the concepts represent the high-level abstractness of the words (e.g., word clusters). In order to introduce the concept-based transfer learning approaches easily, let us briefly review the  \n\nLatent Semantic Analysis (LSA) [86], the Probabilistic LSA (PLSA) [87], and the Dual-PLSA [88].  \n\n• LSA: LSA is an approach to mapping the document-toword matrix to a low-dimensional space (i.e., a latent semantic space) based on the SVD technique. In short, LSA attempts to find the true meanings of the words. To realize this, SVD technique is used to reduce the dimensionality, which can remove the irrelevant information and filter out the noise information from the raw data.  \n\n• PLSA: PLSA is developed based on a statistical view of LSA. PLSA assumes that there is a latent class variable $z,$ , which reflects the concept, associating the document $d$ and the word $w$ . Besides, $d$ and $w$ are independently conditioned on the concept $z$ . The diagram of this graphical model is presented as follows:  \n\n$$\nd\\;{\\xrightarrow{P(d_{i}\\mid z_{k})}}\\;{\\begin{array}{l}{P(z_{k})}\\\\ {\\downarrow}\\end{array}}\\;{\\xrightarrow{P(w_{j}\\mid z_{k})}}\\;w,\n$$  \n\nwhere the subscripts $i,j$ and $k$ represent the indexes of the document, the word, and the concept, respectively. PLSA constructs a Bayesian network, and the parameters are estimated by using the Expectation-Maximization (EM) algorithm [89].  \n\n• Dual-PLSA: The Dual-PLSA is an extension of PLSA. This approach assumes there are two latent variables $z^{d}$ and $z^{w}$ associating the documents and the words. Specifically, the variables $z^{d}$ and $z^{w}$ reflect the concepts behind the documents and the words, respectively. The diagram of the Dual-PLSA is provided below:  \n\n$$\nd\\;\\overbrace{\\vphantom{\\int}\\,}^{P(d_{i}\\,|\\,z_{k_{1}}^{d})}\\;z^{d}\\;\\overbrace{\\vphantom{\\int}\\,}^{P(z_{k_{1}}^{d},z_{k_{2}}^{w})}\\;z^{w}\\;\\xrightarrow{P(w_{j}\\,|\\,z_{k_{2}}^{w})\\,}w.\n$$  \n\nThe parameters of the Dual-PLSA can also be obtained based on the EM algorithm.  \n\nSome concept-based transfer learning approaches are established based on PLSA. For example, the paper by Xue et al. proposes a cross-domain text classification approach termed Topic-Bridged Probabilistic Latent Semantic Analysis (TPLSA) [90]. TPLSA, which is an extension of PLSA, assumes that the source-domain and the target-domain instances share the same mixing concepts of the words. Instead of performing two PLSAs for the source domain and the target domain separately, the authors merge those two PLSAs as an integrated one by using the mixing concept $z$ as a bridge, i.e., each concept has some probabilities to produce the source-domain and the target-domain documents. The diagram of TPLSA is provided below:  \n\n$$\n\\overset{d^{S}}{\\underset{d^{T}}{\\sim}}\\frac{P(d_{i}^{S}|z_{k})}{P(d_{i}^{T}|z_{k})}\\underset{z}{\\z}\\leftarrow\\frac{P(z_{k}|w_{j})}{}~w.\n$$  \n\nNote that PLSA does not require the label information. In order to exploit the label information, the authors add the concept constraints, which include must-link and cannotlink constraints, as the penalty terms in the objective function of TPLSA. Finally, the objective function is iteratively optimized to obtain the classification results (i.e., arg $\\bar{\\operatorname*{max}_{z}}\\dot{P}(z|d_{i}^{T}))$ by using the EM algorithm.  \n\nThe work by Zhuang et al. proposes an approach termed Collaborative Dual-PLSA (CD-PLSA) for multi-domain text classification $\\cdot m^{S}$ source domains and $m^{T}$ target domains)  \n\n[91], [92]. CD-PLSA is an extension of Dual-PLSA. Its diagram is shown below:  \n\n$$\n\\begin{array}{r l r}{\\lefteqn{P(\\mathcal{D}_{k_{0}})}}&{}&{P(d_{i}|z_{k_{1}}^{d},\\mathcal{D}_{k_{0}})}\\\\ {\\mathcal{D}}&{\\to}&{d\\quad\\quad\\leftarrow z^{d}\\underset{\\leftarrow}{\\overset{P(z_{k_{1}}^{d},z_{k_{2}}^{w})}{\\longleftrightarrow}}z^{w}\\rightarrow\\quad\\quad\\overset{\\Downarrow}{w}^{w},}\\end{array}\n$$  \n\nwhere $1\\ \\leq\\ k_{0}\\ \\leq\\ m^{S}\\ +m^{T}$ denotes the domain index. The domain $\\mathcal{D}$ connects both the variables $d$ and $w_{\\prime}$ , but is independent of the variables $z^{d}$ and $z^{w}$ . The label information of the source-domain instances is utilized by initializing the value $P(d_{i}|z_{k_{1}}^{d},\\mathcal{D}_{k_{0}})$ $(k_{0}=1,\\cdots,m^{S})$ . Due to the lack of the target-domain label information, the value $P(d_{i}|z_{k_{1}}^{d},\\mathcal{D}_{k_{0}})$ $(\\breve{k_{0}}~=~m^{S}+1,\\cdot\\cdot\\cdot,m^{S}\\,+\\,m^{T})$ can be initialized based on any supervised classifier. Similarly, the authors adopt the EM algorithm to find the parameters. Through the iterations, all the parameters in the Bayesian network are obtained. Thus, the class label of the $i$ -th document in a target domain (denoted by $\\mathcal{D}_{k}$ ) can be predicted by computing the posterior probabilities, i.e., a $\\cdot\\mathrm{g}\\operatorname*{max}_{z^{d}}P(z^{\\check{d}}|d_{i},\\mathcal{D}_{k}^{\\acute{\\mathbf{\\alpha}}})$ .  \n\nZhuang et al. further proposed a general framework that is termed as Homogeneous-Identical-Distinct-Concept Model (HIDC) [93]. This framework is also an extension of Dual-PLSA. HIDC is composed of three generative models, i.e., identical-concept, homogeneous-concept, and distinctconcept models. These three graphical models are presented below:  \n\nIdentical-Concept Model:  \n\nHomogeneous-Concept Model:  \n\nDistinct-Concept Model:  \n\nThe original word concept $z^{w}$ is divided into three types, i.e., $z_{\\mathrm{IC}}^{w},\\ z_{\\mathrm{HC}}^{w},$ and $z_{\\mathrm{DC}}^{w}$ . In the identical-concept model, the word distributions only rely on the word concepts, and the word concepts are independent of the domains. However, in the homogeneous-concept model, the word distributions also depend on the domains. The difference between the identical and the homogeneous concepts is that $z_{\\mathrm{IC}}^{w}$ is directly transferable, while $z_{\\mathrm{HC}}^{w}$ is the domain-specific transferable one that may have different effects on the word distributions for different domains. In the distinct-concept model, $z_{\\mathrm{DC}}^{w}$ is actually the nontransferable domain-specific one, which may only appear in a specific domain. The abovementioned three models are combined as an integrated one, i.e., HIDC. Similar to other PLSA-related algorithms, HIDC also uses EM algorithm to get the parameters.  \n\n# 4.2.5 Feature Selection  \n\nFeature selection is another kind of operation for feature reduction, which is used to extract the pivot features. The pivot features are the ones that behave in the same way in different domains. Due to the stability of these features, they can be used as the bridge to transfer the knowledge. For example, Blitzer et al. proposed an approach termed  \n\nStructural Correspondence Learning (SCL) [94]. Briefly, SCL consists of the following steps to construct a new feature representation.  \n\n1. Feature Selection: SCL first performs feature selection operations to obtain the pivot features.   \n2. Mapping Learning: The pivot features are utilized to find a low-dimensional common latent feature space by using the structural learning technique [95].   \n3. Feature Stacking: A new feature representation is constructed by feature augmentation, i.e., stacking the original features with the obtained low-dimensional features.  \n\nTake the part-of-speech tagging problem as an example. The selected pivot features should occur frequently in source and target domains. Therefore, determiners can be included in pivot features. Once all the pivot features are defined and selected, a number of binary linear classifiers whose function is to predict the occurrence of each pivot feature are constructed. Without losing generality, the decision function of the $i$ -th classifier, which is used to predict the $i^{\\cdot}$ -th pivot feature, can be formulated as $f_{i}(\\mathbf{x})=\\mathrm{sign}(\\pmb{\\theta}_{i}\\cdot\\mathbf{x}).$ , where $\\mathbf{x}$ is assumed to be a binary feature input. And the $i$ -th classifier is trained on all the instances excluding the features derived from the $i$ -th pivot feature. The following formula can be used to estimate the $i$ -th classifier’s parameters, i.e.,  \n\n$$\n\\theta_{i}=\\underset{\\theta}{\\arg\\operatorname*{min}}\\,\\frac{1}{n}\\sum_{j=1}^{n}\\mathcal{L}\\left(\\theta\\cdot\\mathbf{x}_{j},\\mathrm{Row}_{i}(\\mathbf{x}_{j})\\right)+\\lambda||\\theta||^{2},\n$$  \n\nwhere $\\operatorname{Row}_{i}(\\mathbf{x}_{j})$ denotes the true value of the unlabeled instance $\\mathbf{x}_{j}$ in terms of the $i$ -th pivot feature. By stacking the obtained parameter vectors as column elements, a matrix $\\Tilde{W}$ is obtained. Next, based on singular value decomposition (SVD), the top- $\\cdot k$ left singular vectors, which are the principal components of the matrix $\\tilde{W}_{\\cdot}$ , are taken to construct the transformation matrix $W$ . At last, the final classifier is trained on the labeled instances in an augmented feature space, i.e., $([\\mathbf{x}_{i}^{L};W^{\\mathrm{T}}\\mathbf{x}_{i}^{L}]^{\\mathrm{T}},y_{i}^{L})$ .  \n\n# 4.2.6 Feature Encoding  \n\nIn addition to feature extraction and selection, feature encoding is also an effective tool. For example, autoencoders, which are often adopted in deep learning area, can be used for feature encoding. An autoencoder consists of an encoder and a decoder. The encoder tries to produce a more abstract representation of the input, while the decoder aims to map back that representation and to minimize the reconstruction error. Autoencoders can be stacked to build a deep learning architecture. Once an autoencoder completes the training process, another autoencoder can be stacked at the top of it. The newly added autoencoder is then trained by using the encoded output of the upper-level autoencoder as its input. In this way, deep learning architectures can thus be constructed.  \n\nSome transfer learning approaches are developed based on autoencoders. For example, the paper by Glorot et al. proposes an approach termed Stacked Denoising Autoencoder (SDA) [96]. The denoising autoencoder, which can enhance the robustness, is an extension of the basic one [97]. This kind of autoencoder contains a randomly corrupting mechanism that adds noise to the input before mapping. For example, an input can be corrupted or partially destroyed by adding a masking noise or Gaussian noise. The denoising autoencoder is then trained to minimize the denoising reconstruction error between the original clean input and the output. The SDA algorithm proposed in the paper mainly encompasses the following steps.  \n\n1. Autoencoder Training: The source-domain and targetdomain instances are used to train a stack of denoising autoencoders in a greedy layer-by-layer way.   \n2. Feature Encoding & Stacking: A new feature representation is constructed by stacking the encoding output of intermediate layers, and the features of the instances are transformed into the obtained new representation.   \n3. Learner Training: The target classifier is trained on the transformed labeled instances.  \n\nAlthough the SDA algorithm has excellent performance for feature extraction, it still has some drawbacks such as high computational and parameter-estimation cost. In order to shorten the training time and to speed up traditional SDA algorithms, Chen et al. proposed a modified version of SDA, i.e., Marginalized Stacked Linear Denoising Autoencoder (mSLDA) [98], [99]. This algorithm adopts linear autoencoders and marginalizes the randomly corrupting step in a closed form. It may seem that linear autoencoders are too simple to learn complex features. However, the authors observe that linear autoencoders are often sufficient to achieve competent performance when encountering high dimensional data. The basic architecture of mSLDA is a single-layer linear autoencoder. The corresponding singlelayer mapping matrix $W$ (augmented with a bias column for convenience) should minimize the expected squared reconstruction loss function, i.e.,  \n\n$$\nW=\\underset{W}{\\arg\\operatorname*{min}}\\,\\frac{1}{2n}\\sum_{i=1}^{n}\\mathbb{E}_{P(\\tilde{\\mathbf{x}}_{i}|\\mathbf{x})}\\,\\big[||\\mathbf{x}_{i}-W\\tilde{\\mathbf{x}}_{i}||^{2}\\big],\n$$  \n\nwhere $\\tilde{\\mathbf{x}}_{i}$ denotes the corrupted version of the input $\\mathbf{x}_{i}$ . The solution of $W$ is given by [98], [99]:  \n\n$$\nW=\\left(\\sum_{i=1}^{n}\\mathbf{x}_{i}\\mathbb{E}[\\tilde{\\mathbf{x}}_{i}]^{\\mathrm{T}}\\right)\\left(\\sum_{i=1}^{n}\\mathbb{E}\\left[\\tilde{\\mathbf{x}}_{i}\\tilde{\\mathbf{x}}_{i}^{\\mathrm{T}}\\right]\\right)^{-1}.\n$$  \n\nWhen the corruption strategy is determined, the above formulas can be further expanded and simplified into a specific form. Note that, in order to insert nonlinearity, a nonlinear function is used to squash the output of each autoencoder after we obtain the matrix $W$ in a closed form. Then, the next linear autoencoder is stacked to the current one in a similar way to SDA. In order to deal with high dimensional data, the authors also put forward an extension approach to further reduce the computational complexity.  \n\n# 4.2.7 Feature Alignment  \n\nNote that feature augmentation and feature reduction mainly focus on the explicit features in a feature space. In contrast, in addition to the explicit features, feature alignment also focuses on some implicit features such as the statistic features and the spectral features. Therefore, feature alignment can play various roles in the feature transformation process. For example, the explicit features can be aligned to generate a new feature representation, or the implicit features can be aligned to construct a satisfied feature transformation.  \n\nThere are several kinds of features that can be aligned, which includes subspace features, spectral features, and statistic features. Take the subspace feature alignment as an example. A typical approach mainly has the following steps.  \n\n1. Subspace Generation: In this step, the instances are used to generate the respective subspaces for the source and the target domains. The orthonormal bases of the source and the target domain subspaces are then obtained, which are denoted by $M_{S}$ and $M_{T}.$ , respectively. These bases are used to learn the shift between the subspaces.   \n2. Subspace Alignment: In the second step, a mapping, which aligns the bases $M_{S}$ and $M_{T}$ of the subspaces, is learned. And the features of the instances are projected to the aligned subspaces to generate new feature representation.   \n3. Learner Training: Finally, the target learner is trained on the transformed instances.  \n\nFor example, the work by Fernando et al. proposes an approach termed Subspace Alignment (SA) [100]. In SA, the subspaces are generated by performing PCA; the bases $M_{S}$ and $M_{T}$ are obtained by selecting the leading eigenvectors. Then, a transformation matrix $W$ is learned to align the subspaces, which is given by [100]:  \n\n$$\nW=\\underset{W}{\\arg\\operatorname*{min}}\\,||M_{S}W-M_{T}||_{F}^{2}=M_{S}^{\\mathrm{T}}M_{T},\n$$  \n\nwhere $||\\cdot||_{F}$ denotes the Frobenius norm. Note that the matrix $W$ aligns $M_{S}$ with $M_{T},$ or say, transforms the source subspace coordinate system into the target subspace coordinate system. The transformed low-dimensional sourcedomain and target-domain instances are given by $X^{S}M_{S}W$ and $X^{T}M_{T.}$ , respectively. Finally, a learner can be trained on the resultant transformed instances.  \n\nIn light of $S\\mathrm{A},$ a number of transfer learning approaches are established. For example, the paper by Sun and Saenko proposes an approach that aligns both the subspace bases and the distributions [101], which is termed as Subspace Distribution Alignment between Two Subspaces (SDA-TS). In SDA-TS, the transformation matrix $W$ is formulated as $W\\ =\\ M_{S}^{\\mathrm{T}}M_{T}Q,$ , where $Q$ is a matrix used to align the distribution difference. The transformation matrix $W$ in SA is a special case of the one in SDA-TS by setting $Q$ to an identity matrix. Note that SA is a symmetrical feature-based approach, while SDA-TS is an asymmetrical one. In SDATS, the labeled source-domain instances are projected to the source subspace, then mapped to the target subspace, and finally mapped back to the target domain. The transformed source-domain instances are formulated as $X^{S}M_{S}W M_{T}^{\\mathrm{T}}$ .  \n\nAnother representative subspace feature alignment approach is Geodesic Flow Kernel (GFK) [102], which is proposed by Gong et al. GFK is closely related to a previous approach termed Geodesic Flow Subspaces (GFS) [103]. Before introducing GFK, let us review the steps of GFS at first. GFS is inspired by incremental learning. Intuitively, utilizing the information conveyed by the potential path between two domains may be beneficial to the domain adaptation. GFS generally takes the following steps to align features.  \n\n1. Subspace Generation: GFS first generates two subspaces of the source and the target domains by performing PCA, respectively.   \n2. Subspace Interpolation: The two obtained subspaces can be viewed as two points on the Grassmann manifold [104]. A finite number of the interpolated subspaces are generated between these two subspaces based on the geometric properties of the manifold.   \n3. Feature Projection & Stacking: The original features are transformed by stacking the corresponding projections from all the obtained subspaces.  \n\nDespite the usefulness and superiority of GFS, there is a problem about how to determine the number of the interpolated subspaces. GFK resolves this problem by integrating infinite number of the subspaces located on the geodesic curve from the source subspace to the target one. The key of GFK is to construct an infinite-dimensional feature space that incorporating the information of all the subspaces lying on the geodesic flow. In order to compute the inner product in the resultant infinite-dimensional space, the geodesicflow kernel is defined and derived. In addition, a subspacedisagreement measure is proposed to select the optimal dimensionality of the subspaces; a rank-of-domain metric is also proposed to select the optimal source domain when multi-source domains are available.  \n\nStatistic feature alignment is another kind of feature alignment. For example, Sun et al. proposed an approach termed Co-Relation Alignment (CORAL) [105]. CORAL constructs the transformation matrix of the source features by aligning the second-order statistic features, i.e., the covariance matrices. The transformation matrix $W$ is given by [105]:  \n\n$$\nW=\\underset{W}{\\arg\\operatorname*{min}}\\,||W^{\\mathrm{T}}C_{S}W-C_{T}||_{F}^{2},\n$$  \n\nwhere $C$ denotes the covariance matrix. Note that, compared to the above subspace-based approaches, CORAL avoids subspace generation as well as projection and is very easy to implement.  \n\nSome transfer learning approaches are established based on spectral feature alignment. In traditional machine learning area, spectral clustering is a clustering technique based on graph theory. The key of this technique is to utilize the spectrum, i.e., eigenvalues, of the similarity matrix to reduce the dimension of the features before clustering. The similarity matrix is constructed to quantitatively assess the relative similarity of each pair of data/vertices. On the basis of spectral clustering and feature alignment, Spectral Feature Alignment (SFA) [106] is proposed by Pan et al. SFA is an algorithm for sentiment classification. This algorithm tries to identify the domain-specific words and domainindependent words in different domains, and then aligns these domain-specific word features to construct a lowdimensional feature representation. SFA generally contains the following five steps.  \n\n1. Feature Selection: In this step, feature selection operations are performed to select the domainindependent/pivot features. The paper presents three strategies to select domain-independent features. These strategies are based on the occurrence frequency of words, the mutual information between features and labels [107], and the mutual information between features and domains, respectively.  \n\n2. Similarity Matrix Construction: Once the domain-specific and the domain-independent features are identified, a bipartite graph is constructed. Each edge of this bipartite graph is assigned with a weight that measures the co-occurrence relationship between a domain-specific word and a domain-independent word. Based on the bipartite graph, a similarity matrix is then constructed.   \n3. Spectral Feature Alignment: In this step, a spectral clustering algorithm is adapted and performed to align domain-specific features [108], [109]. Specifically, based on the eigenvectors of the graph Laplacian, a feature alignment mapping is constructed, and the domainspecific features are mapped into a low-dimensional feature space.   \n4. Feature Stacking: The original features and the lowdimensional features are stacked to produce the final feature representation.   \n5. Learner Training: The target learner is trained on the labeled instances with the final feature representation.  \n\nThere are some other spectral transfer learning approaches. For example, the work by Ling et al. proposes an approach termed Cross-Domain Spectral Classifier (CDSC) [110]. The general ideas and steps of this approach are presented as follows.  \n\n1. Similarity Matrix Construction: In the first step, two similarity matrices are constructed corresponding to the whole instances and the target-domain instances, respectively.   \n2. Spectral Feature Alignment: An objective function is designed with respect to a graph-partition indicator vector; a constraint matrix is constructed, which contains pair-wise must-link information. Instead of seeking the discrete solution of the indicator vector, the solution is relaxed to be continuous, and the eigen-system problem corresponding to the objective function is solved to construct the aligned spectral features [111].   \n3. Learner Training: A traditional classifier is trained on the transformed instances.  \n\nTo be more specific, the objective function has a form of the generalized Rayleigh quotient, which aims to find the optimal graph partition that respects the label information with small cut-size [112], to maximize the separation of the target-domain instances, and to fit the constraints of the pair-wise property. After eigen-decomposition, the last eigenvectors are selected and combined as a matrix, and then the matrix is normalized. Each row of the normalized matrix represents a transformed instance.  \n\n# 5 MODEL-BASED INTERPRETATION  \n\nTransfer learning approaches can also be interpreted from the model perspective. Fig. 4 shows the corresponding strategies and the objectives. The main objective of a transfer learning model is to make accurate prediction results on the target domain, e.g., classification or clustering results. Note that a transfer learning model may consist of a few submodules such as classifiers, extractors, or encoders. These sub-modules may play different roles, e.g., feature adaptation or pseudo label generation. In this section, some related transfer learning approaches are introduced in proper order according to the strategies shown in Fig. 4.  \n\n![](images/7b80e943a6a212bfd57062fd5bb71effabadd60040132406c76fa57a106f5f56.jpg)  \nFig. 4. Strategies and objectives of the transfer learning approaches from the model perspective.  \n\n# 5.1 Model Control Strategy  \n\nFrom the perspective of model, a natural thought is to directly add the model-level regularizers to the learner’s objective function. In this way, the knowledge contained in the pre-obtained source models can be transferred into the target model during the training process. For example, the paper by Duan et al. proposes a general framework termed Domain Adaptation Machine (DAM) [113], [114], which is designed for multi-source transfer learning. The goal of DAM is to construct a robust classifier for the target domain with the help of some pre-obtained base classifiers that are respectively trained on multiple source domains. The objective function is given by:  \n\n$$\n\\operatorname*{min}_{f^{T}}\\mathcal{L}^{T,L}(f^{T})+\\lambda_{1}\\Omega^{\\mathrm{D}}(f^{T})+\\lambda_{2}\\Omega(f^{T}),\n$$  \n\nwhere the first term represents the loss function used to minimize the classification error of the labeled target-domain instances, the second term denotes different regularizers, and the third term is used to control the complexity of the final decision function $f^{T}$ . Different types of the loss functions can be adopted in $\\bar{\\mathcal{L}}^{T,L}(f^{T})$ such as the square error or the cross-entropy loss. Some transfer learning approaches can be regarded as the special cases of this framework to some extent.  \n\n• (Consensus Regularizer) The work by Luo et al. proposes a framework termed Consensus Regularization Framework (CRF) [115], [116]. CRF is designed for multi-source transfer learning with no labeled target-domain instances. The framework constructs $m^{S}$ classifiers corresponding to each source domain, and these classifiers are required to reach mutual consensuses on the target domain. The objective function of each source classifier, denoted by $f_{k}^{S}$ (with $k=1,\\cdots,m^{S})$ , is similar to that of DAM, which is presented below:  \n\n$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{f_{k}^{S}}\\displaystyle-\\sum_{i=1}^{n^{S_{k}}}\\log P(\\boldsymbol{y}_{i}^{S_{k}}|\\mathbf{x}_{i}^{S_{k}};f_{k}^{S})+\\lambda_{2}\\Omega(f_{k}^{S})}\\\\ &{\\displaystyle\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ n^{T,U}\\sum_{i=1}^{n^{T,U}}S\\Big(\\displaystyle\\frac{1}{m^{S}}\\sum_{k_{0}=1}^{m^{S}}P(\\boldsymbol{y}_{j}|\\mathbf{x}_{i}^{T,U};f_{k_{0}}^{S})\\Big),}\\end{array}\n$$  \n\nwhere $f_{k}^{S}$ denotes the decision function corresponding to the $k$ -th source domain, and $S(x)\\;=\\;-x\\log{\\bar{x}}$ . The first term is used to quantify the classification error of the $k$ -th classifier on the $k$ -th source domain, and the last term is the consensus regularizer in the form of crossentropy. The consensus regularizer can not only enhance the agreement of all the classifiers, but also reduce the uncertainty of the predictions on the target domain. The authors implement this framework based on the logistic regression. A difference between DAM and CRF is that DAM explicitly constructs the target classifier, while CRF makes the target predictions based on the reached consensus from the source classifiers.  \n\n• (Domain-dependent Regularizer) Fast-DAM is a specific algorithm of DAM [113]. In light of the manifold assumption [30] and the graph-based regularizer [117], [118], Fast-DAM designs a domain-dependent regularizer. The objective function is given by:  \n\n$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{f^{T}}\\sum_{j=1}^{n^{T,L}}\\Big(f^{T}(\\mathbf{x}_{j}^{T,L})-y_{j}^{T,L}\\Big)^{2}+\\lambda_{2}\\Omega(f^{T})}\\\\ &{\\quad\\quad+\\lambda_{1}\\displaystyle\\sum_{k=1}^{m^{S}}\\beta_{k}\\sum_{i=1}^{n^{T,U}}\\Big(f^{T}(\\mathbf{x}_{i}^{T,U})-f_{k}^{S}(\\mathbf{x}_{i}^{T,U})\\Big)^{2},}\\end{array}\n$$  \n\nwhere $f_{k}^{S}\\ (k\\ =\\ 1,2,\\cdots,m^{S})$ denotes the pre-obtained source decision function for the $k$ -th source domain and $\\beta_{k}$ represents the weighting parameter that is determined by the relevance between the target domain and the $k$ - th source domain and can be measured based on the MMD metric. The third term is the domain-dependent regularizer, which transfers the knowledge contained in the source classifier motivated by domain dependence. In [113], the authors also introduce and add a new term to the above objective function based on $\\varepsilon$ -insensitive loss function [119], which makes the resultant model have high computational efficiency.  \n\n• (Domain-dependent Regularizer $^+$ Universum Regularizer) Univer-DAM is an extension of the Fast-DAM [114]. Its objective function contains an additional regularizer, i.e., Universum regularizer. This regularizer usually utilizes an additional dataset termed Universum where the instances do not belong to either the positive or the negative class [120]. The authors treat the source-domain instances as the Universum for the target domain, and the objective function of Univer-DAM is presented as follows:  \n\n$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{f^{T}}\\sum_{j=1}^{n^{T,L}}\\left(f^{T}(\\mathbf{x}_{j}^{T,L})-y_{j}^{T,L}\\right)^{2}+\\lambda_{2}\\!\\displaystyle\\sum_{j=1}^{n^{S}}\\left(f^{T}(\\mathbf{x}_{j}^{S})\\right)^{2}}\\\\ &{\\quad+\\lambda_{1}\\displaystyle\\sum_{k=1}^{m^{S}}\\beta_{k}\\displaystyle\\sum_{i=1}^{n^{T,U}}\\left(f^{T}(\\mathbf{x}_{i}^{T,U})-f_{k}^{S}(\\mathbf{x}_{i}^{T,U})\\right)^{2}+\\lambda_{3}\\Omega(f^{T}).}\\end{array}\n$$  \n\nSimilar to Fast-DAM, the $\\varepsilon$ -insensitive loss function can also be utilized [114].  \n\n# 5.2 Parameter Control Strategy  \n\nThe parameter control strategy focuses on the parameters of models. For example, in the application of object categorization, the knowledge from known source categories can be transferred into target categories via object attributes such as shape and color [121]. The attribute priors, i.e., probabilistic distribution parameters of the image features corresponding to each attribute, can be learned from the source domain and then used to facilitate learning the target classifier. The parameters of a model actually reflect the knowledge learned by the model. Therefore, it is possible to transfer the knowledge at the parametric level.  \n\n# 5.2.1 Parameter Sharing  \n\nAn intuitive way of controlling the parameters is to directly share the parameters of the source learner to the target learner. Parameter sharing is widely employed especially in the network-based approaches. For example, if we have a neural network for the source task, we can freeze (or say, share) most of its layers and only finetune the last few layers to produce a target network. The network-based approaches are introduced in Section 5.4.  \n\nIn addition to network-based parameter sharing, matrixfactorization-based parameter sharing is also workable. For example, Zhuang et al. proposed an approach for text classification, which is referred to as Matrix Tri-Factorization Based Classification Framework (MTrick) [122]. The authors observe that, in different domains, different words or phrases sometimes express the same or similar connotative meaning. Thus, it is more effective to use the concepts behind the words rather than the words themselves as a bridge to transfer the knowledge in source domains. Different from PLSA-based transfer learning approaches that utilize the concepts by constructing Bayesian networks, MTrick attempts to find the connections between the document classes and the concepts conveyed by the word clusters through matrix tri-factorization. These connections are considered to be the stable knowledge that is supposed to be transferred. The main idea is to decompose a document-toword matrix into three matrices, i.e., document-to-cluster, connection, and cluster-to-word matrices. Specifically, by performing the matrix tri-factorization operations on the source and the target document-to-word matrices respectively, a joint optimization problem is constructed, which is given by  \n\n$$\n\\begin{array}{r l}{\\underset{Q,R,W}{\\operatorname*{min}}\\,||X^{S}-Q^{S}R W^{S}||^{2}+\\lambda_{1}||X^{T}-Q^{T}R W^{T}||^{2}}&{{}}\\\\ {+\\lambda_{2}||Q^{S}-\\check{Q}^{S}||^{2}}&{{}}\\end{array}\n$$  \n\ns.t. Normalization Constraints, where $X$ denotes the document-to-word matrix, $Q$ denotes the document-to-cluster matrix, $R$ represents the transformation matrix from document clusters to word clusters, $W$ denotes the cluster-to-word matrix, $n^{d}$ denotes the number of the documents, and ${\\breve{Q}}^{S}$ represents the label matrix. The matrix ${\\breve{Q}}^{S}$ is constructed based on the class information of the source-domain documents. If the $i$ -th document belongs to the $k$ -th class, $\\breve{Q}_{[i,k]}^{S}=1$ . In the above objective function, the matrix is actually the shared parameter. The first term aims to tri-factorize the source document-to-word matrix, and the second term decomposes the target document-toword matrix. The last term incorporates the source-domain label information. The optimization problem is solved based on the alternating iteration method. Once the solution of $Q^{T}$ is obtained, the class index of the $k$ -th target-domain instance is the one with the maximum value in the $k^{-}$ -th row of $Q^{T}$ .  \n\nFurther, Zhuang et al. extended MTrick and proposed an approach termed Triplex Transfer Learning (TriTL) [123]. MTrick assumes that the domains share the similar concepts behind their word clusters. In contrast, TriTL assumes that the concepts of these domains can be further divided into three types, i.e., domain-independent, transferable domain-specific, and nontransferable domain-specific concepts, which is similar to HIDC. This idea is motivated by Dual Transfer Learning (DTL), where the concepts are assumed to be composed of the domain-independent ones and the transferable domain-specific ones [124]. The objective function of TriTL is provided as follows:  \n\n$$\n\\operatorname*{min}_{Q,R,W}\\sum_{k=1}^{m^{S}+m^{T}}||X_{k}-Q_{k}\\left[R^{\\mathrm{DI}}\\quad R^{\\mathrm{TD}}\\quad R_{k}^{\\mathrm{ND}}\\right]\\left[\\displaylimits_{W_{k}^{\\mathrm{ND}}}^{W^{\\mathrm{DI}}}\\right]||^{2}\n$$  \n\ns.t. Normalization Constraints, where the definitions of the symbols are similar to those of MTrick and the subscript $k$ denotes the index of the domains with the assumption that the first $m^{S}$ domains are the source domains and the last $m^{T}$ domains are the target domains. The authors proposed an iterative algorithm to solve the optimization problem. And in the initialization phase, $W^{\\mathrm{DI}}$ and $W_{k}^{\\mathrm{TD}}$ are initialized based on the clustering results of the PLSA algorithm, while $W_{k}^{\\mathrm{UT}}$ is randomly initialized; the PLSA algorithm is performed on the combination of the instances from all the domains.  \n\nThere are some other approaches developed based on matrix factorization. Wang et al. proposed a transfer learning framework for image classification [125]. Wang et al.  \n\nproposed a softly associative approach that integrates two matrix tri-factorizations into a joint framework [126]. Do et al. utilized matrix tri-factorization to discover both the implicit and the explicit similarities for cross-domain recommendation [127].  \n\n# 5.2.2 Parameter Restriction  \n\nAnother parameter-control-type strategy is to restrict the parameters. Different from the parameter sharing strategy that enforces the models share some parameters, parameter restriction strategy only requires the parameters of the source and the target models to be similar.  \n\nTake the approaches to category learning as examples. The category-learning problem is to learn a new decision function for predicting a new category (denoted by the $(k+1)$ -th category) with only limited target-domain instances and $k$ pre-obtained binary decision functions. The function of these pre-obtained decision functions is to predict which of the $k$ categories an instance belongs to. In order to solve the category-learning problem, Tommasi et al. proposed an approach termed Single-Model Knowledge Transfer (SMKL) [128]. SMKL is based on Least-Squares SVM (LS-SVM). The advantage of LS-SVM is that LS-SVM transforms inequality constraints to equality constraints and has high computational efficiency; its optimization is equivalent to solving a linear equation system problem instead of a quadratic programming problem. SMKL selects one of the pre-obtained binary decision functions, and transfers the knowledge contained in its parameters. The objective function is given by  \n\n$$\n\\operatorname*{min}_{f}\\frac{1}{2}\\left|\\left|\\pmb{\\theta}-\\beta\\pmb{\\tilde{\\theta}}\\right|\\right|^{2}+\\frac{\\lambda}{2}\\sum_{j=1}^{n^{T,L}}\\eta_{j}\\left(f(\\mathbf{x}_{j}^{T,L})-y_{j}^{T,L}\\right)^{2},\n$$  \n\nwhere $f(\\mathbf{x})\\,=\\,{\\pmb\\theta}\\cdot\\,\\Phi(\\mathbf{x})\\,+\\,b,\\;\\beta$ is the weighting parameter controlling the transfer degree, θ˜ is the parameter of a selected pre-obtained model, and $\\eta_{j}$ is the coefficient for resolving the label imbalance problem. The kernel parameter and the tradeoff parameter are chosen based on crossvalidation. In order to find the optimal weighting parameter, the authors refer to an earlier work [129]. In [129], Cawley proposed a model selection mechanism for LS-SVM, which is based on the leave-one-out cross-validation method. The superiority of this method is that the leave-one-out error for each instance can be obtained in a closed form without performing the real cross-validation experiment. Motivated by Cawley’s work, the generalization error can be easily estimated to guide the parameter setting in SMKL.  \n\nTommasi et al. further extended SMKL by utilizing all the pre-obtained decision functions. In [130], an approach that is referred to as Multi-Model Knowledge Transfer (MMKL) is proposed. Its objective function is presented as follows:  \n\n$$\n\\operatorname*{min}_{f}\\frac{1}{2}\\left\\|\\pmb{\\theta}-\\sum_{i=1}^{k}\\beta_{i}\\pmb{\\theta}_{i}\\right\\|^{2}+\\frac{\\lambda}{2}\\sum_{j=1}^{n^{T,L}}\\eta_{j}\\left(f(\\mathbf{x}_{j}^{T,L})-y_{j}^{T,L}\\right)^{2},\n$$  \n\nwhere $\\theta_{i}$ and $\\beta_{i}$ are the model parameter and the weighting parameter of the $i$ -th pre-obtained decision function, respectively. The leave-one-out error can also be obtained in a closed form, and the optimal value of $\\beta_{i}$ $(i\\,=\\,1,2,\\cdots\\,,k)$ is the one that maximizes the generalization performance.  \n\n# 5.3 Model Ensemble Strategy  \n\nIn sentiment analysis applications related to product reviews, data or models from multiple product domains are available and can be used as the source domains [131]. Combining data or models directly into a single domain may not be successful because the distributions of these domains are different from each other. Model ensemble is another commonly used strategy. This strategy aims to combine a number of weak classifiers to make the final predictions. Some previously mentioned transfer learning approaches already adopted this strategy. For example, TrAdaBoost and MsTrAdaBoost ensemble the weak classifiers via voting and weighting, respectively. In this subsection, several typical ensemble-based transfer learning approaches are introduced to help readers better understand the function and the appliance of this strategy.  \n\nAs mentioned in Section 4.1, TaskTrAdaBoost, which is an extension of TrAdaBoost for handling multi-source scenarios, is proposed in the paper [33]. TaskTrAdaBoost mainly has the following two stages.  \n\n1. Candidate Classifier Construction: In the first stage, a group of candidate classifiers are constructed by performing AdaBoost on each source domain. Note that, for each source domain, each iteration of AdaBoost results in a new weak classifier. In order to avoid the overfitting problem, the authors introduced a threshold to pick the suitable classifiers into the candidate group. 2. Classifier Selection and Ensemble: In the second stage, a revised version of AdaBoost is performed on the targetdomain instances to construct the final classifier. In each iteration, an optimal candidate classifier which has the lowest classification error on the labeled target-domain instances is picked out and assigned with a weight based on the classification error. Then, the weight of each target-domain instance is updated based on the performance of the selected classifier on the target domain. After the iteration process, the selected classifiers are ensembled to produce the final predictions.  \n\nThe difference between the original AdaBoost and the second stage of TaskTrAdaBoost is that, in each iteration, the former constructs a new candidate classifier on the weighted target-domain instances, while the latter selects one preobtained candidate classifier which has the minimal classification error on the weighted target-domain instances.  \n\nThe paper by Gao et al. proposes another ensemblebased framework that is referred to as Locally Weighted Ensemble (LWE) [132]. LWE focuses on the ensemble process of various learners; these learners could be constructed on different source domains, or be built by performing different learning algorithms on a single source domain. Different from TaskTrAdaBoost that learns the global weight of each learner, the authors adopted the local-weight strategy, i.e., assigning adaptive weights to the learners based on the local manifold structure of the target-domain test set. In LWE, a learner is usually assigned with different weights when classifying different target-domain instances. Specifically, the authors adopt a graph-based approach to estimate the weights. The steps for weighting are outlined below.  \n\n1. Graph Construction: For the $i$ -th source learner, a graph $G_{S_{i}}^{T}$ is constructed by using the learner to classify the target-domain instances in the test set; if two instances are classified into the same class, they are connected in the graph. Another graph $G^{T}$ is constructed for the target-domain instances as well by performing a clustering algorithm.  \n\n2. Learner Weighting: The weight of the $i$ -th learner for the $j$ i-tmhi ltaarritgye tb-edtowmeaeinn t ihnes itnasntcaen $\\mathbf{x}_{j}^{T}$ ilso cparl ostprourctitounreasl  itno $G_{S_{i}}^{T}$ and . And the similarity can be measured by the percentage of the common neighbors of $\\mathbf{x}_{j}^{T}$ in these two graphs.  \n\nNote that this weighting scheme is based on the clusteringmanifold assumption, i.e., if two instances are close to each other in a high-density region, they often have similar labels. In order to check the validity of this assumption for the task, the target task is tested on the source-domain training set(s). Specifically, the clustering quality of the training set(s) is quantified and checked by using a metric such as purity or entropy. If the clustering quality is not satisfactory, uniform weights are assigned to the learners instead. Besides, it is intuitive that if the measured structure similarity is particularly low for every learner, weighting and combining these learners seems unwise. Therefore, the authors introduce a threshold and compare it to the average similarity. If the similarity is lower than the threshold, the label of $\\mathbf{\\widetilde{x}}_{j}^{T}$ is determined by the voting scheme among its reliable neighbors, where the reliable neighbors are the ones whose label predictions are made by the combined classifier.  \n\nThe above-mentioned TaskTrAdaBoost and LWE approaches mainly focus on the ensemble process. In contrast, some studies focus more on the construction of weak learners. For example, Ensemble Framework of Anchor Adapters (ENCHOR) [133] is a weighting ensemble framework proposed by Zhuang et al. An anchor is a specific instance. Different from TrAdaBoost which adjusts weights of instances to train and produce a new learner iteratively, ENCHOR constructs a group of weak learners via using different representations of the instances produced by anchors. The thought is that the higher similarity between a certain instance and an anchor, the more likely the feature of that instance remains unchanged relative to the anchor, where the similarity can be measured by using the cosine or Gaussian distance function. ENCHOR contains the following steps.  \n\n1. Anchor Selection: In this step, a group of anchors are selected. These anchors can be selected based on some rules or even randomly. In order to improve the final performance of ENCHOR, the authors proposed a method of selecting high-quality anchors [133].   \n2. Anchor-based Representation Generation: For each anchor and each instance, the feature vector of an instance is directly multiplied by a coefficient that measures the distance from the instance to the anchor. In this way, each anchor produces a new pair of anchor-adapted source and target instance sets.   \n3. Learner Training and Ensemble: The obtained pairs of instance sets can be respectively used to train learners. Then, the resultant learners are weighted and combined to make the final predictions.  \n\nThe framework ENCHOR is easy to be realized in a parallel manner in that the operations performed on each anchor are  \n\nindependent.  \n\n# 5.4 Deep Learning Technique  \n\nDeep learning methods are particularly popular in the field of machine learning. Many researchers utilize the deep learning techniques to construct transfer learning models. For example, the SDA and the mSLDA approaches mentioned in Section 4.2.6 utilize the deep learning techniques. In this subsection, we specifically discuss the deep-learningrelated transfer learning models. The deep learning approaches introduced are divided into two types, i.e., nonadversarial (or say, traditional) ones and adversarial ones.  \n\n# 5.4.1 Traditional Deep Learning  \n\nAs said earlier, autoencoders are often used in deep learning area. In addition to SDA and mSLDA, there are some other reconstruction-based transfer learning approaches. For example, the paper by Zhuang et al. proposes an approach termed Transfer Learning with Deep Autoencoders (TLDA) [44], [134]. TLDA adopts two autoencoders for the source and the target domains, respectively. These two autoencoders share the same parameters. The encoder and the decoder both have two layers with activation functions. The diagram of the two autoencoders is presented as follows:  \n\n$$\n\\begin{array}{r l}&{X^{S}\\xrightarrow[]{(W_{1},b_{1})}\\!\\!\\!\\!\\!\\!Q^{S}\\xrightarrow[]{(W_{2},b_{2})}\\!\\!\\!\\!R^{S}\\xrightarrow[]{(\\hat{W}_{2},\\hat{b}_{2})}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$  \n\nThere are several objectives of TLDA, which are listed as follows.  \n\n1. Reconstruction Error Minimization: The output of the decoder should be extremely close to the input of encoder. In other words, the distance between $\\dot{X}^{S}$ and $\\tilde{X}^{S}$ as well as the distance between $X^{T}$ and $\\tilde{X}^{T}$ should be minimized.   \n2. Distribution Adaptation: The distribution difference between $Q^{S}$ and $\\dot{Q}^{T}$ should be minimized.   \n3. Regression Error Minimization: The output of the encoder on the labeled source-domain instances, i.e., $R^{S}$ , should be consistent with the corresponding label information $Y^{S}$ .  \n\nTherefore, the objective function of TLDA is given by  \n\n$$\n\\begin{array}{r l}&{\\underset{\\Theta}{\\operatorname*{min}}\\ \\mathcal{L}_{\\mathrm{REC}}(X,\\tilde{X})+\\lambda_{1}\\mathrm{KL}(Q^{S}||Q^{T})+\\lambda_{2}\\Omega(W,b,\\hat{W},\\hat{b})}\\\\ &{\\quad\\quad+\\lambda_{3}\\mathcal{L}_{\\mathrm{REG}}(R^{S},Y^{S}),}\\end{array}\n$$  \n\nwhere the first term represents the reconstruction error, $\\operatorname{KL}(\\cdot)$ represents the KL divergence, the third term controls the complexity, and the last term represents the regression error. TLDA is trained by using a gradient descent method. The final predictions can be made in two different ways. The first way is to directly use the output of the encoder to make predictions. And the second way is to treat the autoencoder as a feature extractor, and then train the target classifier on the labeled instances with the feature representation produced by the encoder’s first-layer output.  \n\nIn addition to the reconstruction-based domain adaptation, discrepancy-based domain adaptation is also a popular direction. In earlier research, the shallow neural networks are tried to learn the domain-independent feature representation [135]. It is found that the shallow architectures often make it difficult for the resultant models to achieve excellent performance. Therefore, many studies turn to utilize deep neural networks. Tzeng et al. [136] added a single adaptation layer and a discrepancy loss to the deep neural network, which improves the performance. Further, Long et al. performed multi-layer adaptation and utilized multikernel technique, and they proposed an architecture termed Deep Adaptation Networks (DAN) [137].  \n\nFor better understanding, let us review DAN in detail. DAN is based on AlexNet [138] and its architecture is presented below [137].  \n\nIn the above network, the features are first extracted by five convolutional layers in a general-to-specific manner. Next, the extracted features are fed into one of the two fully connected networks switched by their original domains. These two networks both consist of three fully connected layers that are specialized for the source and the target domains. DAN has the following objectives.  \n\n1. Classification Error Minimization: The classification error of the labeled instances should be minimized. The cross-entropy loss function is adopted to measure the prediction error of the labeled instances.   \n2. Distribution Adaptation: Multiple layers, which include the representation layers and the output layer, can be jointly adapted in a layer-wise manner. Instead of using the single-kernel MMD to measure the distribution difference, the authors turn to MK-MMD. The authors adopt the linear-time unbiased estimation of MK-MMD to avoid numerous inner product operations [62].   \n3. Kernel Parameter Optimization: The weighting parameters of the multiple kernels in MK-MMD should be optimized to maximize the test power [62].  \n\nThe objective function of the DAN network is given by:  \n\n$$\n\\operatorname*{min}_{\\Theta}\\operatorname*{max}_{\\kappa}\\sum_{i=1}^{n^{L}}\\mathcal{L}\\left(f(\\mathbf{x}_{i}^{L}),y_{i}^{L}\\right)+\\lambda\\sum_{l=6}^{8}\\mathrm{MK}\\mathrm{-}\\mathrm{MMD}(R_{l}^{S},R_{l}^{T};\\kappa),\n$$  \n\nwhere $l$ denotes the index of the layer. The above optimization is actually a minimax optimization problem. The maximization of the objective function with respect to the kernel function $\\kappa$ aims to maximize the test power. After this step, the subtle difference between the source and the target domains are magnified. This train of thought is similar to the Generative Adversarial Network (GAN) [139]. In the training process, the DAN network is initialized by a pretrained AlexNet [138]. There are two categories of parameters that should be learned, i.e., the network parameters and the weighting parameters of the multiple kernels. Given that the first three convolutional layers output the general features and are transferable, the authors freeze them and fine-turn the last two convolutional layers and the two fully connected layers [140]. The last fully connected layer (or say, the classifier layer) is trained from scratch.  \n\nLong et al. further extended the above DAN approach and proposed the DAN framework [141]. The new characteristics are summarized as follows.  \n\n1. Regularizer Adding: The framework introduces an additional regularizer to minimize the uncertainty of the predicted labels of the unlabeled target-domain instances, which is motivated by entropy minimization criterion [142].   \n2. Architecture Generalizing: The DAN framework can be applied to many other architectures such as GoogLeNet [143] and ResNet [144].   \n3. Measurement Generalizing: The distribution difference can be estimated by other metrics. For example, in addition to MK-MMD, the authors also present the Mean Embedding test for distribution adaptation [145].  \n\nThe objective function of the DAN framework is given by:  \n\n$$\n\\begin{array}{r l}&{\\underset{\\Theta}{\\operatorname*{min}}\\underset{\\kappa}{\\operatorname*{max}}\\underset{i=1}{\\overset{n^{L}}{\\sum}}\\mathcal{L}\\left(f(\\mathbf{x}_{i}^{L}),y_{i}^{L}\\right)+\\lambda_{1}\\underset{l=l_{\\mathrm{strt}}}{\\overset{l_{\\mathrm{end}}}{\\sum}}\\mathrm{DIST}(R_{l}^{S},R_{l}^{T})}\\\\ &{\\quad\\quad\\quad\\quad+\\lambda_{2}\\underset{i=1}{\\overset{n^{T,U}}{\\sum}}\\underset{y_{j}\\in\\mathcal{N}}{\\sum}S\\left(P(y_{j}|f(\\mathbf{x}_{i}^{T,U}))\\right),}\\end{array}\n$$  \n\nwhere $l_{\\mathrm{strt}}$ and $l_{\\mathrm{end}}$ denote the boundary indexes of the fully connected layers for adapting the distributions.  \n\nThere are some other impressive works. For example, Long et al. constructed residual transfer networks for domain adaptation, which is motivated by deep residual learning [146]. Besides, another work by Long et al. proposes the Joint Adaptation Network (JAN) [147], which adapts the joint distribution difference of multiple layers. Sun and Saenko extended CORAL for deep domain adaptation and proposed an approach termed Deep CORAL (DCORAL), in which the CORAL loss is added to minimize the feature covariance [148]. Chen et al. realized that the instances with the same label should be close to each other in the feature space, and they not only add the CORAL loss but also add an instance-based class-level discrepancy loss [149]. Pan et al. constructed three prototypical networks (corresponding to $\\mathcal{D}_{S},\\mathcal{D}_{T}$ and $\\mathcal{D}_{S}\\cup\\mathcal{D}_{T}\\mathrm{~}$ ) and incorporated the thought of multi-model consensus. They also adopt pseudo-label strategy and adapt both the instance-level and class-level discrepancy [150]. Kang et al. proposed the Contrastive Adaptation Network (CAN), which is based on the discrepancy metric termed contrastive domain discrepancy [151]. Zhu et al. aimed to adapt the extracted multiple feature representations and proposed the Multi-Representation Adaptation Network (MRAN) [152].  \n\nDeep learning technique can also be used for multisource transfer learning. For example, the work by Zhu et al. proposes a framework that is referred to as Multiple Feature Spaces Adaptation Network (MFSAN) [153]. The architecture of MFSAN consists of a common-feature extractor, $m^{S}$ domain-specific feature extractors, and $m^{S}$ domain-specific classifiers. The corresponding schematic diagram is shown below.  \n\n$$\n\\begin{array}{r l}&{X_{1}^{S}\\cdot\\cdot\\cdot X_{k}^{S}\\cdot\\cdot\\cdot X_{m^{S}}^{S}\\xrightarrow[\\mathrm{Extractor}]{\\mathrm{Common}}\\textit{Q}_{1}^{S}\\cdot\\cdot\\cdot Q_{k}^{S}\\cdot\\cdot\\cdot Q_{m^{S}}^{S}\\xrightarrow[\\mathrm{Extractors}]{\\mathrm{Domain-Specific}}}\\\\ &{X^{T}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\xrightarrow[\\mathrm{Extractor}]{\\mathrm{Extractor}}\\textit{Q}^{T}\\quad\\quad\\quad\\quad\\quad\\quad\\mathrm{Extractors}}\\\\ &{R_{1}^{S}\\cdot\\cdot\\cdot R_{k}^{S}\\cdot\\cdot\\cdot R_{m^{S}}^{S}\\xrightarrow[\\mathrm{Ensin-Specific}]{\\mathrm{Domain-Specific}}\\hat{Y}_{1}^{S}\\cdot\\cdot\\cdot\\hat{Y}_{k}^{S}\\cdot\\cdot\\cdot\\hat{Y}_{m^{S}}^{S}}\\\\ &{R_{1}^{T}\\cdot\\cdot\\cdot R_{k}^{T}\\cdot\\cdot\\cdot R_{m^{S}}^{T}\\xrightarrow[\\mathrm{Classifiers}]{\\mathrm{Classifiers}}\\hat{Y}_{1}^{T}\\cdot\\cdot\\cdot\\hat{Y}_{k}^{T}\\cdot\\cdot\\cdot\\hat{Y}_{m^{S}}^{T}}\\end{array}\n$$  \n\nIn each iteration, MFSAN has the following steps.  \n\n1. Common Feature Extraction: For each source domain (denoted by $\\mathcal{D}_{S_{k}}$ with $k\\ =\\ 1,\\cdots\\ ,m^{S}),$ , the sourcedomain instances (denoted by $X_{k}^{S}$ ) are separately input to the common-feature extractor to produce instances in a common latent feature space (denoted by $Q_{k}^{S}$ ). Similar operations are also performed on the target-domain instances (denoted by $X^{T}$ ), which produces ${\\bf\\bar{\\Psi}}Q^{T}$ .   \n2. Specific Feature Extraction: For each source domain, the extracted common features $Q_{k}^{S}$ is fed to the $k^{-}$ - th domain-specific feature extractor. Meanwhile, $Q^{T}$ is fed to all the domain-specific feature extractors, which results in $R_{k}^{T}$ with $k={\\dot{1}},\\dots,m^{S}$ .   \n3. Data Classification: The output of the $k$ -th domainspecific feature extractor is input to the $k$ -th classifier. In this way, $m^{S}$ pairs of the classification results are predicted in the form of probability.   \n4. Parameter Updating: The parameters of the network are updated to optimize the objective function.  \n\nThere are three objectives in MFSAN, i.e., classification error minimization, distribution adaptation, and consensus regularization. The objective function is given by:  \n\n$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{\\Theta}\\,\\sum_{i=1}^{m^{S}}\\mathcal{L}(\\hat{Y}_{i}^{S},Y_{i}^{S})+\\lambda_{1}\\sum_{i=1}^{m^{S}}\\mathrm{MMD}(R_{i}^{S},R_{i}^{T})}\\\\ &{\\displaystyle\\qquad\\quad+\\,\\lambda_{2}\\sum_{i\\neq j}^{m^{S}}\\left|\\hat{Y}_{i}^{T}-\\hat{Y}_{j}^{T}\\right|,}\\end{array}\n$$  \n\nwhere the first term represents the classification error of the labeled source-domain instances, the second term measures the distribution difference, and the third term measures the discrepancy of the predictions on the target-domain instances.  \n\n# 5.4.2 Adversarial Deep Learning  \n\nThe thought of adversarial learning can be integrated into deep-learning-based transfer learning approaches. As mentioned above, in the DAN framework, the network $\\Theta$ and the kernel $\\kappa$ play a minimax game, which reflects the thought of adversarial learning. However, the DAN framework is a little different from the traditional GAN-based methods in terms of the adversarial matching. In the DAN framework, there is only a few parameters to be optimized in the max game, which makes the optimization easier to achieve equilibrium. Before introducing the adversarial transfer learning approaches, let us briefly review the original GAN framework and the related work.  \n\nThe original GAN [139], which is inspired by the twoplayer game, is composed of two models, a generator $\\mathcal{G}$ and a discriminator $\\mathcal{D}$ . The generator produces the counterfeits of the true data for the purpose of confusing the discriminator and making the discriminator produce wrong detection.  \n\nThe discriminator is fed with the mixture of the true data and the counterfeits, and it aims to detect whether a data is the true one or the fake one. These two models actually play a two-player minimax game, and the objective function is as follows:  \n\n$$\n\\operatorname*{min}_{\\mathcal{G}}\\operatorname*{max}_{\\mathcal{D}}\\mathbb{E}_{\\mathbf{x}\\sim P_{\\mathrm{true}}}\\left[\\log\\mathcal{D}(\\mathbf{x})\\right]+\\mathbb{E}_{\\widetilde{\\mathbf{z}}\\sim P_{\\widetilde{\\mathbf{z}}}}\\left[\\log\\left(1-\\mathcal{D}(\\mathcal{G}(\\widetilde{\\mathbf{z}}))\\right)\\right]\\!,\n$$  \n\nwhere $\\tilde{\\mathbf{z}}$ represents the noise instances (sampled from a certain noise distribution) used as the input of the generator for producing the counterfeits. The entire GAN can be trained by using the back-propagation algorithm. When the two-player game achieves equilibrium, the generator can produce almost true-looking instances.  \n\nMotivated by GAN, many transfer learning approaches are established based on the assumption that a good feature representation contains almost no discriminative information about the instances’ original domains. For example, the work by Ganin et al. proposes a deep architecture termed Domain-Adversarial Neural Network (DANN) for domain adaptation [154], [155]. DANN assumes that there is no labeled target-domain instance to work with. Its architecture consists of a feature extractor, a label predictor, and a domain classifier. The corresponding diagram is as follows.  \n\n$$\n\\begin{array}{r l}&{\\frac{\\mathrm{Label}\\quad}{\\mathrm{Predictor}}\\!\\!\\!\\!\\!\\!\\!\\!\\hat{Y}^{S,L}}\\\\ &{X^{S,L}\\xrightarrow[]{\\quad\\mathrm{Feature}\\quad}Q^{\\hat{S},L}}\\\\ &{X^{T,U}\\xrightarrow[\\mathrm{Extractor}]{\\quad\\mathrm{Feature}\\quad}Q^{T,U}\\left\\}\\frac{\\mathrm{Domain}\\quad\\hat{S}}{\\mathrm{Classifier}}\\!\\!\\!\\!\\!\\!\\hat{T}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\int\\!\\!\\!\\!\\!\\!\\!\\!\\!\\hat{\\cal T}^{T,U}}\\end{array}\n$$  \n\nThe feature extractor acts like the generator, which aims to produce the domain-independent feature representation for confusing the domain classifier. The domain classifier plays the role like the discriminator, which attempts to detect whether the extracted features come from the source domain or the target domain. Besides, the label predictor produces the label prediction of the instances, which is trained on the extracted features of the labeled source-domain instances, i.e., $Q^{S,L}$ . DANN can be trained by inserting a special gradient reversal layer (GRL). After the training of the whole system, the feature extractor learns the deep feature of the instances, and the output $\\hat{Y}^{T,U}$ is the predicted labels of the unlabeled target-domain instances.  \n\nThere are some other related impressive works. The work by Tzeng et al. proposes a unified adversarial domain adaptation framework [156]. The work by Shen et al. adopts Wasserstein distance for domain adaptation [59]. Hoffman et al. adopted cycle-consistency loss to ensure the structural and semantic consistency [157]. Long et al. proposed the Conditional Domain Adversarial Network (CDAN), which utilizes a conditional domain discriminator to assist adversarial adaptation [158]. Zhang et al. adopted a symmetric design for the source and the target classifiers [159]. Zhao et al. utilized domain adversarial networks to solve the multisource transfer learning problem [160]. Yu et al. proposed a dynamic adversarial adaptation network [161].  \n\nSome approaches are designed for some special scenarios. Take the partial transfer learning as an example. The partial transfer learning approaches are designed for the scenario that the target-domain classes are less than the sourcedomain classes, i.e., $y^{S}\\ \\subseteq\\ 3^{T}$ . In this case, the sourcedomain instances with different labels may have different importance for domain adaptation. To be more specific, the source-domain and the target-domain instances with the same label are more likely to be potentially associated. However, since the target-domain instances are unlabeled, how to identify and partially transfer the important information from the labeled source-domain instances is a critical issue.  \n\nThe paper by Zhang et al. proposes an approach for partial domain adaptation, which is called Importance Weighted Adversarial Nets-Based Domain Adaptation (IWANDA) [162]. The architecture of IWANDA is different from that of DANN. DANN adopts one common feature extractor based on the assumption that there exists a common feature space where $Q^{S,L}$ and $Q^{T,U}$ have the similar distribution. However, IWANDA uses two domain-specific feature extractors for the source and the target domains, respectively. Specifically, IWANDA consists of two feature extractors, two domain classifiers, and one label predictor. The diagram of IWANDA is presented below.  \n\n$$\n\\begin{array}{r l}&{\\frac{\\mathrm{Label}\\quad\\hat{Y}^{S,L}}{\\mathrm{Predictor}}\\,\\hat{Y}^{T,U}}\\\\ &{X^{S,L}\\,\\xrightarrow[\\mathrm{Ertactor}]{\\mathrm{Source\\,Feature}}\\,\\hat{Q}^{S,L}}\\\\ &{X^{T,U}\\,\\xrightarrow[\\mathrm{Extractor}]{\\mathrm{Target\\,Fotatwe}}\\,\\hat{Q}^{T,U}\\,\\Big\\}\\,\\frac{+\\beta^{S}}{+\\hat{Y}^{T,U}}\\frac{2\\mathrm{nd}\\,\\mathrm{Domain}}{\\mathrm{Classiter}}\\,\\hat{\\,}\\hat{Z}}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\frac{\\mathrm{lst\\,Domain}}{\\mathrm{Classiter}}\\,\\hat{\\,\\,}\\hat{S_{1}}\\,\\xrightarrow[\\mathrm{Ructor}]{\\mathrm{wetat}}\\,\\hat{\\,}\\beta^{S}}\\end{array}\n$$  \n\ntransfer learning. For example, the work by Wang et al. proposes a minimax-based approach to select high-quality source-domain data [164]. Chen et al. investigated the transferability and the discriminability in the adversarial domain adaptation, and proposed a spectral penalization approach to boost the existing adversarial transfer learning methods [165].  \n\n# 6 APPLICATION  \n\nBefore training, the source feature extractor and the label predictor are pre-trained on the labeled source-domain instances. These two components are frozen in the training process, which means that only the target feature extractor and the domain classifiers should be optimized. In each iteration, the above network is optimized by taking the following steps.  \n\nIn previous sections, a number of representative transfer learning approaches are introduced, which have been applied to solving a variety of text-related/image-related problems in their original papers. For example, MTrick [122] and TriTL [123] utilize the matrix factorization technique to solve cross-domain text classification problems; the deeplearning-based approaches such as DAN [137], DCORAL [148], and DANN [154], [155] are applied to solving image classification problems. Instead of focusing on the general text-related or image-related applications, in this section, we mainly focus on the transfer learning applications in specific areas such as medicine, bioinformatics, transportation, and recommender systems.  \n\n1. Instance Weighting: In order to solve the partial transfer issue, the source-domain instances are assigned with weights based on the output of the first domain classifier. The first domain classifier is fed with $Q^{S,L}$ and $Q^{T,U}$ , and then outputs the probabilistic predictions of their domains. If a source domain instance is predicted with a high probability of belonging to the target domain, this instance is highly likely to associate with the target domain. Thus, this instance is assigned with a larger weight and vice versa.  \n\n2. Prediction Making: The label predictor outputs the label predictions of the instances. The second classifier predicts which domain an instance belongs to.  \n\n3. Parameter Updating: The first classifier is optimized to minimize the domain classification error. The second classifier plays a minmax game with the target feature extractor. This classifier aims to detect whether a instance is the instance from the target domain or the weighted instance from the source domain, and to reduce the uncertainty of the label prediction $\\hat{Y}^{T,U}$ . The target feature extractor aims to confuse the second classifier. These components can be optimized in a similar way to GAN or by inserting a GRL.  \n\nIn addition to IWANDA, the work by Cao et al. constructs the selective adversarial network for partial transfer learning [163]. There are some other studies related to  \n\n# 6.1 Medical Application  \n\nMedical imaging plays an important role in the medical area, which is a powerful tool for diagnosis. With the development of computer technology such as machine learning, computer-aided diagnosis has become a popular and promising direction. Note that medical images are generated by special medical equipment, and their labeling often relies on experienced doctors. Therefore, in many cases, it is expensive and hard to collect sufficient training data. Transfer learning technology can be utilized for medical imaging analysis. A commonly used transfer learning approach is to pre-train a neural network on the source domain (e.g., ImageNet, which is an image database containing more than fourteen million annotated images with more than twenty thousand categories [166]) and then finetune it based on the instances from the target domain.  \n\nFor example, Maqsood et al. finetuned the AlexNet [138] for the detection of Alzheimer’s disease [167]. Their approach has the following four steps. First, the MRI images from the target domain are pre-processed by performing contrast stretching operations. Second, the AlexNet architecture [138] is pre-trained over ImageNet [166] (i.e., the source domain) as a starting point to learn the new task. Third, the convolutional layers of AlexNet are fixed, and the last three fully connected layers are replaced by the new ones including one softmax layer, one fully connected layer, and one output layer. Finally, the modified AlexNet is finetuned by training on the Alzheimer’s dataset [168] (i.e., the target domain). The experimental results show that the proposed approach achieves the highest accuracy for the multi-class classification problem (i.e, Alzheimer’s stage detection).  \n\nSimilarly, Shin et al. finetuned the pre-trained deep neural network for solving the computer-aided detection problems [169]. Byra et al. utilized the transfer learning technology to help assess knee osteoarthritis [170]. In addition to imaging analysis, transfer learning has some other applications in the medical area. For example, the work by Tang et al. combines the active learning and the domain adaptation technologies for the classification of various medical data [171]. Zeng et al. utilized transfer learning for automatically encoding ICD-9 codes that are used to describe a patient’s diagnosis [172].  \n\n# 6.2 Bioinformatics Application  \n\nBiological sequence analysis is an important task in the bioinformatics area. Since the understanding of some organisms can be transferred to other organisms, transfer learning can be applied to facilitate the biological sequence analysis. The distribution difference problem exists significantly in this application. For example, the function of some biological substances may remain unchanged but with the composition changed between two organisms, which may result in the marginal distribution difference. Besides, if two organisms have a common ancestor but with long evolutionary distance, the conditional distribution difference would be significant. The work by Schweikert et al. uses the mRNA splice site prediction problem as the example to analyze the effectiveness of transfer learning approaches [173]. In their experiments, the source domain contains the sequence instances from a well-studied model organism, i.e., C. elegans, and the target organisms include two additional nematodes (i.e., C. remanei and P. pacificus), D. melanogaster, and the plant A. thaliana. A number of transfer learning approaches, e.g., FAM [64] and the variant of KMM [5], are compared with each other. The experimental results show that transfer learning can help improve the classification performance.  \n\nAnother widely encountered task in the bioinformatics area is gene expression analysis, e.g., predicting associations between genes and phenotypes. In this application, one of the main challenges is the data sparsity problem, since there is usually very little data of the known associations. Transfer learning can be used to leverage this problem by providing additional information and knowledge. For example, Petegrosso et al. [174] proposed a transfer learning approach to analyze and predict the gene-phenotype associations based on the Label Propagation Algorithm (LPA) [175]. LPA utilizes the Protein-Protein Interaction (PPI) network and the initial labeling to predict the target associations based on the assumption that the genes that are connected in the PPI network should have the similar labels. The authors extended LPA by incorporating multi-task and transfer-learning technologies. First, Human Phenotype Ontology (HPO), which provides a standardized vocabulary of phenotypic features of human diseases, is utilized to form the auxiliary task. In this way, the associations can be predicted by utilizing phenotype paths and both the linkage knowledge in HPO and in the PPI network; the interacted genes in PPI are more likely to be associated with the same phenotype and the connected phenotypes in HPO are more likely to be associated with the same gene. Second, Gene Ontology (GO), which contains the association information between gene functions and genes, is used as the source domain. Additional regularizers are designed, and the PPI network and the common genes are used as the bridge for knowledge transfer. The gene-GO term and gene-HPO phenotype associations are constructed simultaneously for all the genes in the PPI network. By transferring additional knowledge, the predicted gene-phenotype associations can be more reliable.  \n\nTransfer learning can also be applied to solving the PPI prediction problems. Xu et al. [176] proposed an approach to transfer the linkage knowledge from the source PPI network to the target one. The proposed approach is based on the collective matrix factorization technique [177], where a factor matrix is shared across domains.  \n\n# 6.3 Transportation Application  \n\nOne application of transfer learning in the transportation area is to understand the traffic scene images. In this application, a challenge problem is that the images taken from a certain location often suffer from variations because of different weather and light conditions. In order to solve this problem, Di et al. proposed an approach that attempts to transfer the information of the images that were taken from the same location in different conditions [178]. In the first step, a pretrained network is finetuned to extract the feature representations of images. In the second step, the feature transformation strategy is adopted to construct a new feature representation. Specifically, the dimension reduction algorithm (i.e., partial least squares regression [179]) is performed on the extracted features to generate low-dimension features. Then, a transformation matrix is learned to minimize the domain discrepancy of the dimension-reduced data. Next, the subspace alignment operations are adopted to further reduce the domain discrepancy. Note that, although images under different conditions often have different appearances, they often have the similar layout structure. Therefore, in the final step, the cross-domain dense correspondences are established between the test image and the retrieved best matching image at first, and then the annotations of the best matching image are transferred to the test image via the Markov random field model [180], [181].  \n\nTransfer learning can also be applied to the task of driver behavior modeling. In this task, sufficient personalized data of each individual driver are usually unavailable. In such situations, transferring the knowledge contained in the historical data for the newly-involved driver is a promising alternative. For example, Lu et al. proposed an approach to driver model adaptation in lane-changing scenarios [182]. The source domain contains the sufficient data describing the behavior of the source drivers, while the target domain has a few numbers of data about the target driver. In the first step, the data from both domains are pre-processed by performing PCA to generate low-dimension features. The authors assume that the source and the target data are from two manifolds. Therefore, in the second step, a manifold alignment approach is adopted for domain adaptation. Specifically, the dynamic time warping algorithm [183] is applied to measuring similarity and finding the corresponding source-domain data point of each targetdomain data point. Then, local Procrustes analysis [184] is adopted to align the two manifolds based on the obtained correspondences between data points. In this way, the data from the source domain can be transferred to the target domain. And in the final step, a stochastic modeling method (e.g., Gaussian mixture regression [185]) is used to model the behavior of the target driver. The experimental results demonstrate that the transfer learning approach can help the target driver even when few target-domain data are available. Besides, the results also show that when the number of target instances are very small or very large, the superiority of their approach is not obvious. This may because the relationship across domains cannot be found exactly with few target-domain instances, and in the case of sufficient target-domain instances, the necessity of transfer learning is reduced.  \n\nBesides, there are some other applications of transfer learning in the transportation area. For example, Liu et al. applied transfer learning to driver poses recognition [186]. Wang et al. adopted the regularization technique in transfer learning for vehicle type recognition [187]. Transfer learning can also be utilized for anomalous activity detection [188], [189], traffic sign recognition [190], etc.  \n\n# 6.4 Recommender-System Application  \n\nDue to the rapid increase of the amount of information, how to effectively recommend the personalized content for individual users is an important issue. In the field of recommender systems, some traditional recommendation methods, e.g., factorization-based collaborative filtering, often rely on the factorization of the user-item interaction matrix to obtain the predictive function. These methods often require a large amount of training data to make accurate recommendations. However, the necessary training data, e.g., the historical interaction data, are often sparse in real-world scenarios. Besides, for new registered users or new items, traditional methods are often hard to make effective recommendations, which is also known as the coldstart problem.  \n\nRecognizing the above-mentioned problems in recommender systems, kinds of transfer learning approaches, e.g., instance-based and feature-based approaches, have been proposed. These approaches attempt to make use of the data from other recommender systems (i.e., the source domains) to help construct the recommender system in the target domain. Instance-based approaches mainly focus on transferring different types of instances, e.g., ratings, feedbacks, and examinations, from the source domain to the target domain. The work by Pan et al. [191] leverages the uncertain ratings (represented as rating distributions) of the source domain for knowledge transfer. Specifically, the source-domain uncertain ratings are used as constraints to help complete the rating matrix factorization task on the target domain. Hu et al. [192] proposed an approach termed transfer meeting hybrid, which extracts the knowledge from unstructured text by using an attentive memory network and selectively transfer the useful information.  \n\nFeature-based approaches often leverage and transfer the information from a latent feature space. For example, Pan et al. proposed an approach termed Coordinate System Transfer (CST) [193] to leverage both the user-side and the item-side latent features. The source-domain instances come from another recommender system, sharing common users and items with the target domain. CST is developed based on the assumption that the principle coordinates, which reflect the tastes of users or the factors of items, characterize the domain-independent structure and are transferable across domains. CST first constructs two principle coordinate systems, which are actually the latent features of users and items, by applying sparse matrix tri-factorization on the source-domain data, and then transfer the coordinate systems to the target domain by setting them as constraints. The experimental results show that CST significantly outperforms the non-transfer baselines (i.e., average filling model and latent factorization model) in all data sparsity levels [193].  \n\nThere are some other studies about cross-domain recommendation [194], [195], [196], [197]. For example, He et al. proposed a transfer learning framework based on the Bayesian neural network [198]. Zhu et al. [199] proposed a deep framework, which first generates the user and item feature representations based on the matrix factorization technique, and then employs a deep neural network to learn the mapping of features across domains. Yuan et al. [200] proposed a deep domain adaptation approach based on autoencoders and a modified DANN [154], [155] to extract and transfer the instances from rating matrices.  \n\n# 6.5 Other Applications  \n\nCommunication Application: In addition to WiFi localization tasks [2], [36], transfer learning has also been employed in wireless-network applications. For example, Bastug et al. proposed a caching mechanism [201]; the knowledge contained in contextual information, which is extracted from the interactions between devices, is transferred to the target domain. Besides, some studies focus on the energy saving problems. The work by Li et al. proposes an energy saving scheme for cellular radio access networks, which utilizes the transfer-learning expertise [202]. The work by Zhao and Grace applies transfer learning to topology management for reducing energy consumption [203].  \n\nUrban-Computing Application: With a large amount of data related to our cities, urban-computing is a promising researching track in directions of traffic monitoring, health care, social security, etc. Transfer learning has been applied to alleviate the data scarcity problem in many urban computing applications. For example, Guo et al. [204] proposed an approach for chain store site recommendation, which leverages the knowledge from semantically-relevant domains (e.g., other cities with the same store and other chain stores in the target city) to the target city. Wei et al. [205] proposed a flexible multi-modal transfer learning approach that transfers knowledge from a city that have sufficient multi-model data and labels to the target city to alleviate the data sparsity problem.  \n\nTransfer learning has been applied to some recognition tasks such as hand gesture recognition [206], face recognition [207], activity recognition [208], and speech emotion recognition [209]. Besides, transfer-learning expertise has also been incorporated into some other areas such as sentiment analysis [28], [96], [210], fraud detection [211], social network [212], and hyperspectral image analysis [54], [213].  \n\n# 7 EXPERIMENT  \n\nTransfer learning techniques have been successfully applied in many real-world applications. In this section, we perform experiments to evaluate the performance of some representative transfer learning models1 [214] of different categories on two mainstream research areas, i.e., object recognition and text classification. The datasets are introduced at first. Then, the experimental results and further analyses are provided.  \n\n# 7.1 Dataset and Preprocessing  \n\nThree datasets are studied in the experiments, i.e., Office31, Reuters-21578, and Amazon Reviews. For simplicity, we focus on the classification tasks. The statistical information of the preprocessed datasets is listed in Table 3.  \n\n• Amazon Reviews2 [107] is a multi-domain sentiment dataset which contains product reviews taken from Amazon.com of four domains (Books, Kitchen, Electronics and DVDs). Each review in the four domains has a text and a rating from zero to five. In the experiments, the ratios that are less than three are defined as the negative ones, while others are defined as the positive ones. The frequency of each word in all reviews is calculated. Then, the five thousand words with the highest frequency are selected as the attributes of each review. In this way, we finally have 1000 positive instances, 1000 negative instances, and about 5000 unlabeled instances in each domain. In the experiments, every two of the four domains are selected to generate twelve tasks.  \n\n• Reuters- $\\bf{21578^{3}}$ is a dataset for text categorization, which has a hierarchical structure. The dataset contains 5 top categories (Exchanges, Orgs, People, Places, Topics). In out experiment, we use the top three big category Orgs, People and Places to generate three classification tasks (Orgs vs People, Orgs vs Places and People vs Places). In each task, the subcategories in the corresponding two categories are separately divided into two parts. Then, the resultant four parts are used as the components to form two domains. Each domain has about 1000 instances, and each instance has about 4500 features. Specifically, taking the task Orgs vs People as an example, one part from Orgs and one part from People and combined to form the source domain; similarly, the rest two parts form the target domain. Note that the instances in the three categories are all labeled. In order to generate the unlabeled instances, the labeled instances are selected from the dataset, and their labels are ignored.  \n\n• Office-31 [215] is an object recognition dataset which contains thirty-one categories and three domains, i.e., Amazon, Webcam, and DSLR. These three domains have 2817, 498, and 795 instances, respectively. The images in Amazon are the online e-commerce pictures taken from Amazon.com. The images in Webcam are the lowresolution pictures taken by web cameras. And the images in DSLR are the high-resolution pictures taken by DSLR cameras. In the experiments, every two of the three domains (with the order considered) are selected as the source and the target domains, which results in six tasks.  \n\n![](images/d6162238d01e7c93f07f2ae0cdc3c09ed1ba80a9111540202b921de3725eedc2.jpg)  \nFig. 5. Comparison results on Amazon Reviews.  \n\n# 7.2 Experiment Setting  \n\nExperiments are conducted to compare some representative transfer learning models. Specifically, eight algorithms are performed on the dataset Office-31 for solving the object recognition problem. Besides, fourteen algorithms are performed and evaluated on the dataset Reuters-21578 for solving the text classification problem. In the sentiment classification problem, eleven algorithms are performed on Amazon Reviews. The classification results are evaluated by accuracy, which is defined as follows:  \n\n$$\n\\mathrm{accuracy}=\\frac{|\\{\\mathbf{x}|\\mathbf{x}_{i}\\in\\mathcal{D}_{\\mathrm{test}}\\land f(\\mathbf{x}_{i})=y_{i}\\}|}{|\\mathcal{D}_{\\mathrm{test}}|}\n$$  \n\nwhere $\\mathcal{D}_{\\mathrm{test}}$ denotes the test data and $y$ denotes the truth classification label; $f(\\mathbf{x})$ represents the predicted classification result. Note that some algorithms need the base classifier. In these cases, an SVM with a linear kernel is adopted as the base classifier in the experiments. Besides, the sourcedomain instances are all labeled. And for the performed algorithms (except TrAdaBoost), the target-domain instances are unlabeled. Each algorithm was executed three times, and the average results are adopted as our experimental results.  \n\nThe evaluated transfer learning models include: HIDC [93], TriTL [123], CD-PLSA [91], [92], MTrick [122], SFA [106], mSLDA [98], [99], SDA [96], GFK [102], SCL [94], TCA [36], [78], CoCC [41], JDA [38], TrAdaBoost [31], DAN [137], DCORAL [148], MRAN [152], CDAN [158], DANN [154], [155], JAN [147], and CAN [151].  \n\n# 7.3 Experiment Result  \n\nIn this subsection, we compare over twenty algorithms on three datasets in total. The parameters of all algorithms are set to the default values or the recommended values mentioned in the original papers. The experimental results are presented in Tables 4, 5, and 6 corresponding to Amazon Reviews, Reuters-21578, and Office-31, respectively. In order to allow readers to understand the experimental results more intuitively, three radar maps, i.e., Figs. 5, 6, and 7, are provided, which visualize the experimental results. In the radar maps, each direction represents a task. The general performance of an algorithm is demonstrated by a polygon whose vertices representing the accuracy of the algorithm for dealing with different tasks.  \n\nTABLE 3 Statistical information of the preprocessed datasets.   \n\n\n<html><body><table><thead><tr><td><b>Area</b></td><td><b>Dataset</b></td><td><b> Domain</b></td><td><b>Attribute</b></td><td><b> Total Instances</b></td><td><b>Tasks</b></td></tr></thead><tbody><tr><td> Sentiment Classification</td><td>Amazon Reviews</td><td>4</td><td>5000</td><td>27677</td><td>12</td></tr><tr><td> Text Classification</td><td>Reuters-21578</td><td>3</td><td>4772</td><td>6570</td><td>3</td></tr><tr><td> Object Recognition</td><td>Office-31</td><td>3</td><td>800</td><td>4110</td><td>6</td></tr></tbody></table></body></html>  \n\nFig. 6. Comparison results on Reuters-21578.  \n\nTABLE 4 Accuracy performance on the Amazon Reviews of four domains: Kitchen (K), Electronics (E), DVDs (D) and Books (B).   \n\n\n<html><body><table><thead><tr><td><b>Model</b></td><td><b>K→D</b></td><td><b>K→B</b></td><td><b>K→E</b></td><td><b>D-→K</b></td><td><b>D→B</b></td><td><b>D→E</b></td><td><b>B→K</b></td><td><b>B→D</b></td><td><b>B→E</b></td><td><b>E→K</b></td><td><b>E→D</b></td><td><b>E→B</b></td><td><b>Average</b></td></tr></thead><tbody><tr><td>HIDC</td><td>0.8800</td><td>0.8750</td><td>0.8800</td><td>0.7925</td><td>0.8100</td><td>0.8025</td><td>0.7925</td><td>0.8175</td><td>0.8075</td><td>0.8075</td><td>0.8700</td><td>0.8700</td><td>0.8338</td></tr><tr><td>TriTL</td><td>0.7150</td><td>0.7250</td><td>0.6775</td><td>0.5725</td><td>0.5250</td><td>0.5775</td><td>0.6150</td><td>0.6125</td><td>0.6000</td><td>0.6250</td><td>0.6100</td><td>0.6150</td><td>0.6225</td></tr><tr><td>CD-PLSA</td><td>0.7475</td><td>0.7225</td><td>0.7200</td><td>0.6075</td><td>0.6175</td><td>0.6075</td><td>0.5750</td><td>0.6100</td><td>0.6425</td><td>0.7225</td><td>0.7450</td><td>0.7000</td><td>0.6681</td></tr><tr><td>MTrick</td><td>0.8200</td><td>0.8350</td><td>0.8125</td><td>0.7725</td><td>0.7475</td><td>0.7275</td><td>0.7550</td><td>0.7450</td><td>0.7800</td><td>0.7900</td><td>0.7975</td><td>0.8100</td><td>0.7827</td></tr><tr><td>SFA</td><td>0.8525</td><td>0.8575</td><td>0.8675</td><td>0.7825</td><td>0.8050</td><td>0.7750</td><td>0.7925</td><td>0.7850</td><td>0.7775</td><td>0.8400</td><td>0.8525</td><td>0.8400</td><td>0.8190</td></tr><tr><td>mSLDA</td><td>0.7975</td><td>0.7825</td><td>0.7925</td><td>0.6350</td><td>0.6450</td><td>0.6325</td><td>0.6525</td><td>0.6675</td><td>0.6625</td><td>0.7225</td><td>0.7150</td><td>0.7125</td><td>0.7015</td></tr><tr><td>SDA</td><td>0.8425</td><td>0.7925</td><td>0.8025</td><td>0.7450</td><td>0.7600</td><td>0.7650</td><td>0.7625</td><td>0.7475</td><td>0.7425</td><td>0.8175</td><td>0.8050</td><td>0.8100</td><td>0.7827</td></tr><tr><td>GFK</td><td>0.6200</td><td>0.6275</td><td>0.6325</td><td>0.6200</td><td>0.6100</td><td>0.6225</td><td>0.5800</td><td>0.5650</td><td>0.5725</td><td>0.6575</td><td>0.6500</td><td>0.6325</td><td>0.6158</td></tr><tr><td>SCL</td><td>0.8575</td><td>0.8625</td><td>0.8725</td><td>0.7800</td><td>0.7850</td><td>0.7825</td><td>0.7925</td><td>0.7925</td><td>0.7825</td><td>0.8425</td><td>0.8525</td><td>0.8450</td><td>0.8206</td></tr><tr><td>TCA</td><td>0.7550</td><td>0.7550</td><td>0.7550</td><td>0.6475</td><td>0.6475</td><td>0.6500</td><td>0.5800</td><td>0.5825</td><td>0.5850</td><td>0.7175</td><td>0.7150</td><td>0.7125</td><td>0.6752</td></tr><tr><td>Baseline</td><td>0.7270</td><td>0.7090</td><td>0.8270</td><td>0.7400</td><td>0.7280</td><td>0.7300</td><td>0.7450</td><td>0.7720</td><td>0.7080</td><td>0.8400</td><td>0.7060</td><td>0.7070</td><td>0.7449</td></tr></tbody></table></body></html>  \n\nTable 4 shows the experimental results on Amazon Reviews. The baseline is a linear classifier trained only on the source domain (here we directly use the results from the paper [107]). Fig. 5 visualizes the results. As shown in Fig. 5, most algorithms are relatively well-performed when the source domain is electronics or kitchen, which indicates that these two domains may contains more transferable information than the other two domains. In addition, it can be observed that HIDC, SCL, SFA, MTrick and SDA perform well and relatively stable in all the twelve tasks. Meanwhile, other algorithms, especially mSLDA, CD-PLSA, and TriTL, are relatively unstable; the performance of them fluctuates in a range about twenty percent. TriTL has a relatively high accuracy on the tasks where the source domain is kitchen, but has a relatively low accuracy on other tasks.  \n\nThe algorithms TCA, mSLDA, and CD-PLSA have similar performance on all the tasks with an accuracy about seventy percent on average. Among the well-performed algorithms, HIDC and MTrick are based on feature reduction (feature clustering), while the others are based on feature encoding (SDA), feature alignment (SFA), and feature selection (SCL). Those strategies are currently the mainstreams of featurebased transfer learning.  \n\nTable 5 presents the comparison results on Reuter-21578 (here we directly use the results of the baseline and CoCC from papers [78] and [41]). The baseline is a regularized least square regression model trained only on the labeled target domain instances [78]. Fig. 6, which has the same structure of Fig. 5, visualizes the performance. For clarity, thirteen algorithms are divided into two parts that correspond to the two subfigures in Fig. 6. It can be observed that most algorithms are relatively well-performed for Orgs vs Places and Orgs vs People, but poor for People vs Places. This phenomenon indicates that the discrepancy between People and Places may be relatively large. TrAdaBoost has a relatively good performance in this experiment because it uses the labels of the instances in the target domain to reduce the impact of the distribution difference. Besides, the algorithms HIDC, SFA, and MTrick have relatively consistent performance in the three tasks. These algorithms are also wellperformed in the previous experiment on Amazon Reviews. In addition, the top two well-performed algorithms in terms of People vs Places are CoCC and TrAdaBoost.  \n\n![](images/217939640e7112361d7d12a41f53df518a1238688cc35969046dd06a7c0b6b74.jpg)  \nFig. 7. Comparison results on Office-31.  \n\nTABLE 5 Accuracy performance on the Reuters-21578 of three domains: Orgs, People, and Places.   \n\n\n<html><body><table><thead><tr><td><b>Model</b></td><td><b>Orgs vs Places People vs Places Orgs vs People Average</b></td></tr></thead><tbody><tr><td>HIDC</td><td>0.7698</td><td>0.6945</td><td>0.8375</td><td>0.7673</td></tr><tr><td>TriTL</td><td>0.7338</td><td>0.5517</td><td>0.7505</td><td>0.6787</td></tr><tr><td>CD-PLSA</td><td>0.5624</td><td>0.5749</td><td>0.7826</td><td>0.6400</td></tr><tr><td>MTrick</td><td>0.7494</td><td>0.6457</td><td>0.7930</td><td>0.7294</td></tr><tr><td>CoCC</td><td>0.6704</td><td>0.8264</td><td>0.7644</td><td>0.7537</td></tr><tr><td>SFA</td><td>0.7468</td><td>0.6768</td><td>0.7906</td><td>0.7381</td></tr><tr><td>mSLDA</td><td>0.5645</td><td>0.6064</td><td>0.5289</td><td>0.5666</td></tr><tr><td>SDA</td><td>0.6603</td><td>0.5556</td><td>0.5992</td><td>0.6050</td></tr><tr><td>GFK</td><td>0.6220</td><td>0.5417</td><td>0.6446</td><td>0.6028</td></tr><tr><td>SCL</td><td>0.6794</td><td>0.5046</td><td>0.6694</td><td>0.6178</td></tr><tr><td>TCA</td><td>0.7368</td><td>0.6065</td><td>0.7562</td><td>0.6998</td></tr><tr><td>JDA</td><td>0.5694</td><td>0.6296</td><td>0.7424</td><td>0.6471</td></tr><tr><td>TrAdaBoost</td><td>0.7336</td><td>0.7052</td><td>0.7879</td><td>0.7422</td></tr><tr><td>Baseline</td><td>0.6683</td></tr></tbody></table></body></html>  \n\nTABLE 6 Accuracy performance on Office-31 of three domains: Amazon (A), Webcam (W), and DSLR (D).   \n\n\n<html><body><table><thead><tr><td><b>Model</b></td><td><b>A→W</b></td><td><b>D→W W→DA→DD→A W→A</b></td><td><b>Average</b></td></tr></thead><tbody><tr><td>DAN</td><td>0.826</td><td>0.977</td><td>1.00</td><td>0.831</td><td>0.668</td><td>0.666</td><td>0.828</td></tr><tr><td>DCORAL</td><td>0.790</td><td>0.980</td><td>1.00</td><td>0.827</td><td>0.653</td><td>0.645</td><td>0.816</td></tr><tr><td>MRAN</td><td>0.914</td><td>0.969</td><td>0.998</td><td>0.864</td><td>0.683</td><td>0.709</td><td>0.856</td></tr><tr><td>CDAN</td><td>0.931</td><td>0.982</td><td>1.00</td><td>0.898</td><td>0.701</td><td>0.680</td><td>0.865</td></tr><tr><td>DANN</td><td>0.826</td><td>0.978</td><td>1.00</td><td>0.833</td><td>0.668</td><td>0.661</td><td>0.828</td></tr><tr><td> JAN</td><td>0.854</td><td>0.974</td><td>0.998</td><td>0.847</td><td>0.686</td><td>0.700</td><td>0.843</td></tr><tr><td>CAN</td><td>0.945</td><td>0.991</td><td>0.998</td><td>0.950</td><td>0.780</td><td>0.770</td><td>0.906</td></tr><tr><td>Baseline</td><td>0.616</td><td>0.954</td><td>0.990</td></tr></tbody></table></body></html>  \n\nIn the third experiment, seven deep-learning-based transfer learning models (i.e., DAN, DCORAL, MRAN, CDAN, DANN, JAN, and CAN) and the baseline (i.e., the Alexnet [138], [140] pre-trained on ImageNet [166] and then directly trained on the target domain) are performed on the dataset Office-31 (here we directly use the results of CDAN, JAN, CAN, and the baseline from the original papers [137], [147], [151], [158]). The ResNet-50 [144] is used as the backbone network for all these three models. The experimental results are provided in Table 6 and the average performance is visualized in Fig. 7. As shown in Fig. 7, all of these seven algorithms have excellent performance, especially on the tasks $\\mathrm{~D~}\\rightarrow\\mathrm{~W~}$ and ${\\cal W}\\,\\rightarrow\\,{\\cal D},$ , whose accuracy is very close to one hundred percent. This phenomenon reflects the superiority of the deep-learning based approaches, and is consistent with the fact that the difference between Webcam and DSLR is smaller than that between Webcam/DSLR and Amazon. Clearly, CAN outperforms the other six algorithms. In all the six tasks, the performance of DANN is similar to that of DAN, and is better than that of DCORAL, which indicates the effectiveness and the practicability of incorporating adversarial learning.  \n\nIt is worth mentioning that, in the above experiments, the performance of some algorithms is not ideal. One reason is that we use the default parameter settings provided in the algorithms’ original papers, which may not be suitable for the dataset we selected. For example, GFK was originally designed for object recognition, and we directly adopt it into text classification in the first experiment, which turns out to produce an unsatisfactory result (having about sixtytwo percent accuracy on average). The above experimental results are just for reference. These results demonstrate that some algorithms may not be suitable for the datasets of certain domains. Therefore, it is important to choose the appropriate algorithms as the baselines in the process of research. Besides, in practical applications, it is also necessary to find a suitable algorithm.  \n\n# 8 CONCLUSION AND FUTURE DIRECTION  \n\nIn this survey paper, we have summarized the mechanisms and the strategies of transfer learning from the perspectives of data and model. The survey gives the clear definitions about transfer learning and manages to use a unified symbol system to describe a large number of representative transfer learning approaches and related works. We have basically introduced the objectives and strategies in transfer learning based on data-based interpretation and modelbased interpretation. Data-based interpretation introduces the objectives, the strategies, and some transfer learning approaches from the data perspective. Similarly, modelbased interpretation introduces the mechanisms and the strategies of transfer learning but from the model level. The applications of transfer learning have also been introduced. At last, experiments have been conducted to evaluate the performance of representative transfer learning models on two mainstream area, i.e., object recognition and text categorization. The comparisons of the models have also been given, which reflects that the selection of the transfer learning model is an important research topic as well as a complex issue in practical applications.  \n\nSeveral directions are available for future research in the transfer learning area. First, transfer learning techniques can be further explored and applied to a wider range of applications. And new approaches are needed to solve the knowledge transfer problems in more complex scenarios. For example, in real-world scenarios, sometimes the userrelevant source-domain data comes from another company. In this case, how to transfer the knowledge contained in the source domain while protecting user privacy is an important issue. Second, how to measure the transferability across domains and avoid negative transfer is also an important issue. Although there have been some studies on negative transfer, negative transfer still needs further systematic analyses [3]. Third, the interpretability of transfer learning also needs to be investigated further [216]. Finally, theoretical studies can be further conducted to provide theoretical support for the effectiveness and applicability of transfer learning. As a popular and promising area in machine learning, transfer learning shows some advantages over traditional machine learning such as less data dependency and less label dependency. We hope our work can help readers have a better understanding of the research status and the research ideas.", "appendix": "# ACKNOWLEDGMENTS  \n\nThe research work is supported by the National Key Research and Development Program of China under Grant No. 2018YFB1004300, the National Natural Science Foundation of China under Grant No. U1836206, U1811461, 61773361, 61836013, and the Project of Youth Innovation Promotion Association CAS under Grant No. 2017146."}], "origin_content": "", "origin_outline": "#\n \nR\ne\nc\ne\nn\nt\n \nA\nd\nv\na\nn\nc\ne\ns\n \ni\nn\n \nD\ni\nr\ne\nc\nt\n \nS\np\ne\ne\nc\nh\n-\nt\no\n-\nt\ne\nx\nt\n \nT\nr\na\nn\ns\nl\na\nt\ni\no\nn\n\n\n\n\n#\n#\n \n1\n.\n \nI\nn\nt\nr\no\nd\nu\nc\nt\ni\no\nn\n\n\nE\nx\nt\nr\na\nc\nt\n \nt\nh\ne\n \nd\ne\nf\ni\nn\ni\nt\ni\no\nn\n \no\nf\n \nd\ni\nr\ne\nc\nt\n \ns\np\ne\ne\nc\nh\n-\nt\no\n-\nt\ne\nx\nt\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n,\n \ni\nt\ns\n \na\np\np\nl\ni\nc\na\nt\ni\no\nn\n \ns\nc\ne\nn\na\nr\ni\no\ns\n,\n \na\nn\nd\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \no\nf\n \nt\nr\na\nd\ni\nt\ni\no\nn\na\nl\n \nc\na\ns\nc\na\nd\ne\nd\n \na\np\np\nr\no\na\nc\nh\ne\ns\n \n(\nA\nS\nR\n+\nM\nT\n)\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nt\nh\ne\n \nm\no\nt\ni\nv\na\nt\ni\no\nn\ns\n \nf\no\nr\n \ne\nx\np\nl\no\nr\ni\nn\ng\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \na\np\np\nr\no\na\nc\nh\ne\ns\n,\n \nf\no\nc\nu\ns\ni\nn\ng\n \no\nn\n \nb\ne\nn\ne\nf\ni\nt\ns\n \ns\nu\nc\nh\n \na\ns\n \ns\ni\nm\np\nl\ne\nr\n \ns\ny\ns\nt\ne\nm\ns\n,\n \no\nv\ne\nr\nc\no\nm\ni\nn\ng\n \ne\nr\nr\no\nr\n \np\nr\no\np\na\ng\na\nt\ni\no\nn\n,\n \ne\nn\na\nb\nl\ni\nn\ng\n \nj\no\ni\nn\nt\n \no\np\nt\ni\nm\ni\nz\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nb\nr\ni\nd\ng\ni\nn\ng\n \nt\nh\ne\n \nm\no\nd\na\nl\ni\nt\ny\n \ng\na\np\n.\n \nG\na\nt\nh\ne\nr\n \nb\na\nc\nk\ng\nr\no\nu\nn\nd\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \no\nn\n \nt\nh\ne\n \ni\nn\nc\nr\ne\na\ns\ni\nn\ng\n \ns\ni\ng\nn\ni\nf\ni\nc\na\nn\nc\ne\n \no\nf\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nm\no\nd\ne\nl\ns\n \ni\nn\n \ns\np\ne\ne\nc\nh\n \np\nr\no\nc\ne\ns\ns\ni\nn\ng\n \na\nn\nd\n \nm\na\nc\nh\ni\nn\ne\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \nm\no\nt\ni\nv\na\nt\ni\no\nn\n \nf\no\nr\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n,\n \ns\nu\nc\nh\n \na\ns\n \nh\na\nn\nd\nl\ni\nn\ng\n \na\nu\nd\ni\no\nb\no\no\nk\n \nc\no\nn\nt\ne\nn\nt\n \nd\ni\nr\ne\nc\nt\nl\ny\n \n[\n'\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\na\nu\nt\no\nm\na\nt\ni\nc\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\no\nf\n_\na\nu\nd\ni\no\nb\no\no\nk\ns\n'\n]\n.\n\n\nS\ny\nn\nt\nh\ne\ns\ni\nz\ne\n \na\nn\n \ni\nn\nt\nr\no\nd\nu\nc\nt\ni\no\nn\n \nt\nh\na\nt\n \nc\nl\ne\na\nr\nl\ny\n \nd\ne\nf\ni\nn\ne\ns\n \nd\ni\nr\ne\nc\nt\n \ns\np\ne\ne\nc\nh\n-\nt\no\n-\nt\ne\nx\nt\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \na\nn\nd\n \ni\nt\ns\n \ng\nr\no\nw\ni\nn\ng\n \ni\nm\np\no\nr\nt\na\nn\nc\ne\n.\n \nA\nr\nt\ni\nc\nu\nl\na\nt\ne\n \nt\nh\ne\n \na\nd\nv\na\nn\nt\na\ng\ne\ns\n \no\nf\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \na\np\np\nr\no\na\nc\nh\ne\ns\n \no\nv\ne\nr\n \nt\nr\na\nd\ni\nt\ni\no\nn\na\nl\n \nc\na\ns\nc\na\nd\ne\nd\n \ns\ny\ns\nt\ne\nm\ns\n,\n \ne\nm\np\nh\na\ns\ni\nz\ni\nn\ng\n \na\ns\np\ne\nc\nt\ns\n \nl\ni\nk\ne\n \nr\ne\nd\nu\nc\ne\nd\n \ne\nr\nr\no\nr\n \np\nr\no\np\na\ng\na\nt\ni\no\nn\n \na\nn\nd\n \nj\no\ni\nn\nt\n \nt\nr\na\ni\nn\ni\nn\ng\n.\n \nE\nx\np\nl\na\ni\nn\n \nt\nh\ne\n \ns\nh\ni\nf\nt\n \nf\nr\no\nm\n \nt\nr\na\nd\ni\nt\ni\no\nn\na\nl\n \nc\na\ns\nc\na\nd\ne\nd\n \na\np\np\nr\no\na\nc\nh\ne\ns\n \nt\no\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nm\no\nd\ne\nl\ns\n,\n \ne\nm\np\nh\na\ns\ni\nz\ni\nn\ng\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nb\ne\nn\ne\nf\ni\nt\ns\n \na\nn\nd\n \nt\nh\ne\n \nk\ne\ny\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \nt\nh\na\nt\n \nm\no\nt\ni\nv\na\nt\ne\n \nt\nh\ne\n \nr\ne\ns\ne\na\nr\nc\nh\n \ni\nn\n \nt\nh\ni\ns\n \na\nr\ne\na\n \n[\n'\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\na\nn\nd\n_\nt\nh\ne\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\np\nr\no\nm\ni\ns\ne\n_\nt\na\nk\ni\nn\ng\n_\ns\nt\no\nc\nk\n_\no\nf\n_\nw\nh\ne\nr\ne\n_\nw\ne\n_\na\nr\ne\n'\n]\n.\n \nH\ni\ng\nh\nl\ni\ng\nh\nt\n \nt\nh\ne\n \nk\ne\ny\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \nt\nh\na\nt\n \nm\no\nt\ni\nv\na\nt\ne\n \nr\ne\ns\ne\na\nr\nc\nh\n \ni\nn\n \nt\nh\ni\ns\n \na\nr\ne\na\n,\n \ns\nu\nc\nh\n \na\ns\n \nd\na\nt\na\n \ns\nc\na\nr\nc\ni\nt\ny\n \na\nn\nd\n \nt\nh\ne\n \nm\no\nd\na\nl\ni\nt\ny\n \ng\na\np\n \nb\ne\nt\nw\ne\ne\nn\n \ns\np\ne\ne\nc\nh\n \na\nn\nd\n \nt\ne\nx\nt\n.\n \nS\ne\nt\n \nt\nh\ne\n \ns\nc\no\np\ne\n \no\nf\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\n \nb\ny\n \no\nu\nt\nl\ni\nn\ni\nn\ng\n \ni\nt\ns\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\n \na\nn\nd\n \nt\nh\ne\n \ns\ne\nc\nt\ni\no\nn\ns\n \nt\nh\na\nt\n \nw\ni\nl\nl\n \nd\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\ns\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \na\nd\nv\na\nn\nc\ne\nm\ne\nn\nt\ns\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \nm\no\nt\ni\nv\na\nt\ni\no\nn\ns\n \na\nn\nd\n \na\nd\nv\na\nn\nt\na\ng\ne\ns\n \no\nf\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \ns\np\ne\ne\nc\nh\n-\nt\no\n-\nt\ne\nx\nt\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \nb\ne\nn\ne\nf\ni\nt\ns\n \no\nf\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \na\np\np\nr\no\na\nc\nh\ne\ns\n \no\nv\ne\nr\n \nc\na\ns\nc\na\nd\ne\nd\n \ns\ny\ns\nt\ne\nm\ns\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \ns\ne\nt\nu\np\ns\n,\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n,\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \nt\nh\ne\n \ni\nn\nt\nr\no\nd\nu\nc\nt\ni\no\nn\n \ns\ne\nc\nt\ni\no\nn\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \nl\ni\nn\ng\nu\ni\ns\nt\ni\nc\ns\n \na\nn\nd\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nt\nh\ne\no\nr\ny\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n \n2\n.\n \nH\ni\ns\nt\no\nr\ni\nc\na\nl\n \nO\nv\ne\nr\nv\ni\ne\nw\n \no\nf\n \nS\np\ne\ne\nc\nh\n \nT\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n\n\nB\nu\ni\nl\nd\ni\nn\ng\n \nu\np\no\nn\n \nt\nh\ne\n \ni\nn\nt\nr\no\nd\nu\nc\nt\ni\no\nn\n \no\nf\n \nd\ni\nr\ne\nc\nt\n \ns\np\ne\ne\nc\nh\n-\nt\no\n-\nt\ne\nx\nt\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \ni\nn\n \nt\nh\ne\n \np\nr\ne\nv\ni\no\nu\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n \nd\ne\nl\nv\ne\ns\n \ni\nn\nt\no\n \nt\nh\ne\n \nh\ni\ns\nt\no\nr\ni\nc\na\nl\n \ne\nv\no\nl\nu\nt\ni\no\nn\n \no\nf\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n.\n \nT\nr\na\nc\ne\n \nt\nh\ne\n \nh\ni\ns\nt\no\nr\ni\nc\na\nl\n \ne\nv\no\nl\nu\nt\ni\no\nn\n \no\nf\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n.\n \nE\nx\nt\nr\na\nc\nt\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \na\nb\no\nu\nt\n \nt\nh\ne\n \np\nr\no\ng\nr\ne\ns\ns\ni\no\nn\n \nf\nr\no\nm\n \nl\no\no\ns\ne\nl\ny\n \nc\no\nu\np\nl\ne\nd\n \nc\na\ns\nc\na\nd\ne\ns\n \nt\no\n \nt\ni\ng\nh\nt\nl\ny\n \nc\no\nu\np\nl\ne\nd\n \ns\ny\ns\nt\ne\nm\ns\n \na\nn\nd\n \nf\ni\nn\na\nl\nl\ny\n \nt\no\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nm\no\nd\ne\nl\ns\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nc\no\nm\np\nl\ne\nx\ni\nt\ni\ne\ns\n \no\nf\n \nc\na\ns\nc\na\nd\ne\nd\n \ns\ny\ns\nt\ne\nm\ns\n,\n \ni\nn\nc\nl\nu\nd\ni\nn\ng\n \ne\nr\nr\no\nr\n \np\nr\no\np\na\ng\na\nt\ni\no\nn\n \nb\ne\nt\nw\ne\ne\nn\n \nA\nS\nR\n \na\nn\nd\n \nM\nT\n \nc\no\nm\np\no\nn\ne\nn\nt\ns\n \na\nn\nd\n \ns\ne\np\na\nr\na\nt\ne\n \no\np\nt\ni\nm\ni\nz\na\nt\ni\no\nn\n \np\nr\no\nc\ne\ns\ns\ne\ns\n.\n \nE\nx\nt\nr\na\nc\nt\n \nt\nh\ne\n \nt\nh\ne\no\nr\ne\nt\ni\nc\na\nl\n \nu\nn\nd\ne\nr\np\ni\nn\nn\ni\nn\ng\ns\n \no\nf\n \ne\na\nr\nl\ny\n \ni\nn\nt\ne\ng\nr\na\nt\ne\nd\n \na\nn\nd\n \np\nh\nr\na\ns\ne\n-\nb\na\ns\ne\nd\n \na\np\np\nr\no\na\nc\nh\ne\ns\n,\n \na\ns\n \nw\ne\nl\nl\n \na\ns\n \nt\nh\ne\n \nm\no\nd\ne\nl\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n \na\nn\nd\n \nt\nr\na\ni\nn\ni\nn\ng\n \np\nr\no\nc\ne\ns\ns\n \no\nf\n \np\nh\nr\na\ns\ne\n-\nb\na\ns\ne\nd\n \nS\nT\n \n[\n'\ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n_\np\nh\nr\na\ns\ne\n_\nb\na\ns\ne\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n]\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nB\nu\ni\nl\nd\ni\nn\ng\n \nu\np\no\nn\n \nt\nh\ne\n \ni\nn\nt\nr\no\nd\nu\nc\nt\ni\no\nn\n \no\nf\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n \na\nn\nd\n \ni\nt\ns\n \nm\no\nt\ni\nv\na\nt\ni\no\nn\ns\n,\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n \nd\ne\nl\nv\ne\ns\n \ni\nn\nt\no\n \nt\nh\ne\n \nh\ni\ns\nt\no\nr\ni\nc\na\nl\n \no\nv\ne\nr\nv\ni\ne\nw\n \no\nf\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n,\n \nc\no\nn\nt\nr\na\ns\nt\ni\nn\ng\n \nt\nh\ne\n \ne\nv\no\nl\nu\nt\ni\no\nn\n \nf\nr\no\nm\n \nt\nr\na\nd\ni\nt\ni\no\nn\na\nl\n \nc\na\ns\nc\na\nd\ne\nd\n \nm\no\nd\ne\nl\ns\n \na\nn\nd\n \ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n \np\nh\nr\na\ns\ne\n-\nb\na\ns\ne\nd\n \na\np\np\nr\no\na\nc\nh\ne\ns\n \nt\no\n \nm\no\nd\ne\nr\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \nd\nr\ni\nv\ni\nn\ng\n \nf\no\nr\nc\ne\ns\n \nb\ne\nh\ni\nn\nd\n \nt\nh\ni\ns\n \ns\nh\ni\nf\nt\n,\n \ns\nu\nc\nh\n \na\ns\n \nt\nh\ne\n \nd\ne\ns\ni\nr\ne\n \nt\no\n \nm\ni\nt\ni\ng\na\nt\ne\n \ne\nr\nr\no\nr\n \np\nr\no\np\na\ng\na\nt\ni\no\nn\n,\n \na\nc\nh\ni\ne\nv\ne\n \nj\no\ni\nn\nt\n \nt\nr\na\ni\nn\ni\nn\ng\n,\n \na\nn\nd\n \ns\ni\nm\np\nl\ni\nf\ny\n \ns\ny\ns\nt\ne\nm\n \nd\ne\ns\ni\ng\nn\n \n[\n'\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\na\nn\nd\n_\nt\nh\ne\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\np\nr\no\nm\ni\ns\ne\n_\nt\na\nk\ni\nn\ng\n_\ns\nt\no\nc\nk\n_\no\nf\n_\nw\nh\ne\nr\ne\n_\nw\ne\n_\na\nr\ne\n'\n,\n \n'\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nc\no\nu\np\nl\ni\nn\ng\n_\no\nf\n_\nr\ne\nc\no\ng\nn\ni\nt\ni\no\nn\n_\na\nn\nd\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n]\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \nt\nh\ne\no\nr\ne\nt\ni\nc\na\nl\n \nu\nn\nd\ne\nr\np\ni\nn\nn\ni\nn\ng\ns\n \no\nf\n \ne\na\nr\nl\ny\n \ni\nn\nt\ne\ng\nr\na\nt\ne\nd\n \na\nn\nd\n \np\nh\nr\na\ns\ne\n-\nb\na\ns\ne\nd\n \na\np\np\nr\no\na\nc\nh\ne\ns\n,\n \nc\no\nn\nt\nr\na\ns\nt\ni\nn\ng\n \nt\nh\ne\nm\n \nw\ni\nt\nh\n \nc\na\ns\nc\na\nd\ne\nd\n \na\nn\nd\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nm\no\nd\ne\nl\ns\n.\n \nC\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nc\na\ns\nc\na\nd\ne\nd\n,\n \ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n \np\nh\nr\na\ns\ne\n-\nb\na\ns\ne\nd\n \na\np\np\nr\no\na\nc\nh\ne\ns\n,\n \na\nn\nd\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nm\ne\nt\nh\no\nd\ns\n,\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \nt\nh\ne\n \na\nd\nv\na\nn\nt\na\ng\ne\ns\n \na\nn\nd\n \nd\ni\ns\na\nd\nv\na\nn\nt\na\ng\ne\ns\n \no\nf\n \ne\na\nc\nh\n \ni\nn\n \nt\ne\nr\nm\ns\n \no\nf\n \nm\no\nd\ne\nl\n \nc\no\nm\np\nl\ne\nx\ni\nt\ny\n,\n \nd\na\nt\na\n \nd\ne\np\ne\nn\nd\ne\nn\nc\ny\n,\n \na\nn\nd\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \n[\n'\ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n_\np\nh\nr\na\ns\ne\n_\nb\na\ns\ne\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n,\n \n'\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\na\nn\nd\n_\nt\nh\ne\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\np\nr\no\nm\ni\ns\ne\n_\nt\na\nk\ni\nn\ng\n_\ns\nt\no\nc\nk\n_\no\nf\n_\nw\nh\ne\nr\ne\n_\nw\ne\n_\na\nr\ne\n'\n,\n \n'\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nc\no\nu\np\nl\ni\nn\ng\n_\no\nf\n_\nr\ne\nc\no\ng\nn\ni\nt\ni\no\nn\n_\na\nn\nd\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n]\n.\n \nD\ni\ns\nc\nu\ns\ns\n \nh\no\nw\n \nt\nh\ne\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\n \ni\nn\nt\ne\nr\na\nc\nt\ni\no\nn\n \nb\ne\nt\nw\ne\ne\nn\n \nr\ne\nc\no\ng\nn\ni\nt\ni\no\nn\n \na\nn\nd\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nh\na\ns\n \ne\nv\no\nl\nv\ne\nd\n,\n \nl\ne\na\nd\ni\nn\ng\n \nt\no\n \nt\nh\ne\n \nc\nu\nr\nr\ne\nn\nt\n \nd\no\nm\ni\nn\na\nn\nc\ne\n \no\nf\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nm\no\nd\ne\nl\ns\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ne\nv\no\nl\nu\nt\ni\no\nn\n \no\nf\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n \nf\nr\no\nm\n \nc\na\ns\nc\na\nd\ne\nd\n \nt\no\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \na\np\np\nr\no\na\nc\nh\ne\ns\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \no\nf\n \nc\na\ns\nc\na\nd\ne\nd\n,\n \np\nh\nr\na\ns\ne\n-\nb\na\ns\ne\nd\n,\n \na\nn\nd\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \na\np\np\nr\no\na\nc\nh\ne\ns\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \ns\ne\nt\nu\np\ns\n,\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n,\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n'\n,\n \n'\nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n'\n,\n \n'\nD\na\nt\na\ns\ne\nt\ns\n'\n,\n \n'\nK\ne\ny\n \nF\ni\nn\nd\ni\nn\ng\ns\n \n(\nP\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \nM\ne\nt\nr\ni\nc\ns\n)\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ne\nv\no\nl\nu\nt\ni\no\nn\n \no\nf\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \ns\ne\nt\nu\np\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \no\nr\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \nt\nh\ne\n \nb\na\nc\nk\ng\nr\no\nu\nn\nd\n \ns\ne\nc\nt\ni\no\nn\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \nh\ni\ns\nt\no\nr\ni\nc\na\nl\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nt\nh\ne\no\nr\ny\n \na\nn\nd\n \nc\no\nm\np\nu\nt\na\nt\ni\no\nn\na\nl\n \nl\ni\nn\ng\nu\ni\ns\nt\ni\nc\ns\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n#\n \n2\n.\n1\n \nS\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n \nP\nh\nr\na\ns\ne\n-\nB\na\ns\ne\nd\n \nS\np\ne\ne\nc\nh\n \nT\nr\na\nn\ns\nl\na\nt\ni\no\nn\n\n\nD\ne\nt\na\ni\nl\n \nm\no\nd\ne\nl\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n \na\nn\nd\n \nt\nr\na\ni\nn\ni\nn\ng\n \np\nr\no\nc\ne\ns\ns\n \no\nf\n \np\nh\nr\na\ns\ne\n-\nb\na\ns\ne\nd\n \nS\nT\n.\n \nE\nx\nt\nr\na\nc\nt\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \na\nb\no\nu\nt\n \nt\nh\ne\n \nm\no\nd\ne\nl\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nt\nr\na\ni\nn\ni\nn\ng\n \np\nr\no\nc\ne\ns\ns\n,\n \na\nn\nd\n \nr\ne\np\no\nr\nt\ne\nd\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \na\ns\n \na\n \nr\ne\np\nr\ne\ns\ne\nn\nt\na\nt\ni\nv\ne\n \no\nf\n \ne\na\nr\nl\ni\ne\nr\n \na\np\np\nr\no\na\nc\nh\ne\ns\n \nt\no\n \ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n \np\nh\nr\na\ns\ne\n-\nb\na\ns\ne\nd\n \nS\nT\n \n[\n'\ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n_\np\nh\nr\na\ns\ne\n_\nb\na\ns\ne\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n]\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \nm\no\nd\ne\nl\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n \na\nn\nd\n \nt\nr\na\ni\nn\ni\nn\ng\n \np\nr\no\nc\ne\ns\ns\n \no\nf\n \ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n \np\nh\nr\na\ns\ne\n-\nb\na\ns\ne\nd\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nE\nv\na\nl\nu\na\nt\ne\n \ni\nt\ns\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n,\n \na\nd\nv\na\nn\nt\na\ng\ne\ns\n,\n \na\nn\nd\n \nd\ni\ns\na\nd\nv\na\nn\nt\na\ng\ne\ns\n \ni\nn\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n \nt\no\n \nc\na\ns\nc\na\nd\ne\nd\n \na\nn\nd\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \na\np\np\nr\no\na\nc\nh\ne\ns\n.\n \nD\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \nc\no\nm\np\nl\ne\nx\ni\nt\ni\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\nh\ne\nr\ne\nn\nt\n \ni\nn\n \ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n \np\nh\nr\na\ns\ne\n-\nb\na\ns\ne\nd\n \nS\nT\n,\n \ns\ne\nt\nt\ni\nn\ng\n \nt\nh\ne\n \ns\nt\na\ng\ne\n \nf\no\nr\n \nt\nh\ne\n \ne\nv\no\nl\nu\nt\ni\no\nn\n \nt\no\nw\na\nr\nd\ns\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nm\no\nd\ne\nl\ns\n \n[\n'\ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n_\np\nh\nr\na\ns\ne\n_\nb\na\ns\ne\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n]\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n \np\nh\nr\na\ns\ne\n-\nb\na\ns\ne\nd\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \no\nf\n \ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n \np\nh\nr\na\ns\ne\n-\nb\na\ns\ne\nd\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \ns\ne\nt\nu\np\ns\n,\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n,\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n'\n,\n \n'\nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n'\n,\n \n'\nD\na\nt\na\ns\ne\nt\ns\n'\n,\n \n'\nK\ne\ny\n \nF\ni\nn\nd\ni\nn\ng\ns\n \n(\nP\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \nM\ne\nt\nr\ni\nc\ns\n)\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n \np\nh\nr\na\ns\ne\n-\nb\na\ns\ne\nd\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n \np\nh\nr\na\ns\ne\n-\nb\na\ns\ne\nd\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n \np\nh\nr\na\ns\ne\n-\nb\na\ns\ne\nd\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \nm\na\nc\nh\ni\nn\ne\n \nl\ne\na\nr\nn\ni\nn\ng\n \na\nn\nd\n \ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n \nm\no\nd\ne\nl\ni\nn\ng\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n \n3\n.\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \nA\nd\nv\na\nn\nc\ne\ns\n \ni\nn\n \nE\nn\nd\n-\nt\no\n-\nE\nn\nd\n \nS\np\ne\ne\nc\nh\n-\nt\no\n-\nt\ne\nx\nt\n \nT\nr\na\nn\ns\nl\na\nt\ni\no\nn\n\n\nH\na\nv\ni\nn\ng\n \ne\ns\nt\na\nb\nl\ni\ns\nh\ne\nd\n \nt\nh\ne\n \nh\ni\ns\nt\no\nr\ni\nc\na\nl\n \nc\no\nn\nt\ne\nx\nt\n,\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n \ne\nx\np\nl\no\nr\ne\ns\n \nr\ne\nc\ne\nn\nt\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \na\nd\nv\na\nn\nc\ne\ns\n \ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \ns\np\ne\ne\nc\nh\n-\nt\no\n-\nt\ne\nx\nt\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nF\no\nr\n \ne\na\nc\nh\n \np\na\np\ne\nr\n \nf\no\nc\nu\ns\ni\nn\ng\n \no\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n,\n \ni\nd\ne\nn\nt\ni\nf\ny\n \nt\nh\ne\n \np\nr\no\np\no\ns\ne\nd\n \nm\no\nd\ne\nl\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n,\n \nk\ne\ny\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n,\n \na\nn\nd\n \ns\np\ne\nc\ni\nf\ni\nc\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \nc\no\nm\np\no\nn\ne\nn\nt\ns\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\ne\nt\na\ni\nl\ns\n \na\nb\no\nu\nt\n \nh\no\nw\n \nt\nh\ne\ns\ne\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n \na\nn\nd\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n \na\ni\nm\n \nt\no\n \ni\nm\np\nr\no\nv\ne\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n,\n \ni\nn\nc\nl\nu\nd\ni\nn\ng\n \ns\nt\nr\na\nt\ne\ng\ni\ne\ns\n \nf\no\nr\n \nm\no\nd\na\nl\ni\nt\ny\n \ng\na\np\n \nb\nr\ni\nd\ng\ni\nn\ng\n,\n \na\nd\na\np\nt\na\nt\ni\no\nn\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n,\n \na\nn\nd\n \nt\nr\na\ni\nn\ni\nn\ng\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n.\n \nN\no\nt\ne\n \na\nn\ny\n \ns\np\ne\nc\ni\nf\ni\nc\n \nd\ne\ns\ni\ng\nn\n \nc\nh\no\ni\nc\ne\ns\n,\n \nc\nl\na\ni\nm\ne\nd\n \ni\nm\np\nr\no\nv\ne\nm\ne\nn\nt\ns\n \nf\no\nr\n \ne\na\nc\nh\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \na\nn\nd\n \nt\nh\ne\ni\nr\n \nf\no\nc\nu\ns\n \no\nn\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \na\nn\nd\n \nd\na\nt\na\n \ns\nc\na\nl\na\nb\ni\nl\ni\nt\ny\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nW\ni\nt\nh\n \nt\nh\ne\n \nb\na\nc\nk\ng\nr\no\nu\nn\nd\n \ne\ns\nt\na\nb\nl\ni\ns\nh\ne\nd\n,\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n \ne\nx\np\nl\no\nr\ne\ns\n \nr\ne\nc\ne\nn\nt\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \na\nd\nv\na\nn\nc\ne\ns\n \ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \ns\np\ne\ne\nc\nh\n-\nt\no\n-\nt\ne\nx\nt\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nC\na\nt\ne\ng\no\nr\ni\nz\ne\n \na\nn\nd\n \nc\no\nm\np\na\nr\ne\n \nt\nh\ne\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n \na\nn\nd\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n \np\nr\ne\ns\ne\nn\nt\ne\nd\n \ni\nn\n \nt\nh\ne\n \np\na\np\ne\nr\ns\n,\n \nf\no\nc\nu\ns\ni\nn\ng\n \no\nn\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \na\nn\nd\n \nd\na\nt\na\n \ns\nc\na\nl\na\nb\ni\nl\ni\nt\ny\n.\n \nA\nn\na\nl\ny\nz\ne\n \nh\no\nw\n \ne\na\nc\nh\n \na\np\np\nr\no\na\nc\nh\n \na\nd\nd\nr\ne\ns\ns\ne\ns\n \nt\nh\ne\n \ni\nn\nh\ne\nr\ne\nn\nt\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \no\nf\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n,\n \ns\nu\nc\nh\n \na\ns\n \nt\nh\ne\n \nm\no\nd\na\nl\ni\nt\ny\n \ng\na\np\n \na\nn\nd\n \nt\nh\ne\n \nn\ne\ne\nd\n \nf\no\nr\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\n \nt\nr\na\ni\nn\ni\nn\ng\n \ns\nt\nr\na\nt\ne\ng\ni\ne\ns\n \ni\nn\n \nt\nh\ne\n \nc\no\nn\nt\ne\nx\nt\n \no\nf\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \na\nn\nd\n \nl\na\nr\ng\ne\n \nd\na\nt\na\ns\ne\nt\ns\n.\n \nG\nr\no\nu\np\n \ns\ni\nm\ni\nl\na\nr\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n \na\nn\nd\n \ni\nd\ne\nn\nt\ni\nf\ny\n \nt\nr\ne\nn\nd\ns\n \ni\nn\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \nd\ne\ns\ni\ng\nn\n \nt\nh\na\nt\n \ne\nm\np\nh\na\ns\ni\nz\ne\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \na\nn\nd\n \nd\na\nt\na\n \ns\nc\na\nl\na\nb\ni\nl\ni\nt\ny\n.\n \nC\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \ni\nn\nn\no\nv\na\nt\ni\no\nn\ns\n \np\nr\no\np\no\ns\ne\nd\n,\n \nd\ni\ns\nc\nu\ns\ns\ni\nn\ng\n \nt\nh\ne\ni\nr\n \ns\nt\nr\ne\nn\ng\nt\nh\ns\n \na\nn\nd\n \nw\ne\na\nk\nn\ne\ns\ns\ne\ns\n \nb\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\ni\nr\n \nd\ne\ns\nc\nr\ni\np\nt\ni\no\nn\ns\n \na\nn\nd\n \nr\ne\np\no\nr\nt\ne\nd\n \ni\nm\np\nr\no\nv\ne\nm\ne\nn\nt\ns\n,\n \nf\no\nc\nu\ns\ni\nn\ng\n \no\nn\n \nh\no\nw\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \nc\nh\no\ni\nc\ne\ns\n \na\ni\nm\n \nt\no\n \na\nd\nd\nr\ne\ns\ns\n \ns\np\ne\nc\ni\nf\ni\nc\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n \nw\ni\nt\nh\ni\nn\n \nt\nh\ne\n \nc\no\nn\nt\ne\nx\nt\n \no\nf\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \na\nn\nd\n \nd\na\nt\na\n \ns\nc\na\nl\na\nb\ni\nl\ni\nt\ny\n \n[\n'\np\nr\ne\n_\nt\nr\na\ni\nn\ni\nn\ng\n_\no\nn\n_\nh\ni\ng\nh\n_\nr\ne\ns\no\nu\nr\nc\ne\n_\ns\np\ne\ne\nc\nh\n_\nr\ne\nc\no\ng\nn\ni\nt\ni\no\nn\n_\ni\nm\np\nr\no\nv\ne\ns\n_\nl\no\nw\n_\nr\ne\ns\no\nu\nr\nc\ne\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n,\n \n'\ni\nn\nv\ne\ns\nt\ni\ng\na\nt\ni\nn\ng\n_\ns\ne\nl\nf\n_\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n_\np\nr\ne\n_\nt\nr\na\ni\nn\ni\nn\ng\n_\nf\no\nr\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n,\n \n'\ng\ni\ng\na\ns\nt\n_\na\n_\n1\n0\n0\n0\n0\n_\nh\no\nu\nr\n_\np\ns\ne\nu\nd\no\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nc\no\nr\np\nu\ns\n'\n]\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \na\nd\nv\na\nn\nc\ne\ns\n \ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \ns\np\ne\ne\nc\nh\n-\nt\no\n-\nt\ne\nx\nt\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \no\nf\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \ni\nn\nn\no\nv\na\nt\ni\no\nn\ns\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \ns\ne\nt\nu\np\ns\n,\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n,\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nW\nh\na\nt\n \na\nr\ne\n \nt\nh\ne\n \nt\nr\na\nd\ne\n-\no\nf\nf\ns\n \n(\ne\n.\ng\n.\n,\n \nc\no\nm\np\nl\ne\nx\ni\nt\ny\n \nv\ns\n.\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n,\n \nd\na\nt\na\n \ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n \nv\ns\n.\n \nc\no\nm\np\nu\nt\na\nt\ni\no\nn\na\nl\n \nc\no\ns\nt\n)\n \na\ns\ns\no\nc\ni\na\nt\ne\nd\n \nw\ni\nt\nh\n \ne\na\nc\nh\n \np\nr\no\np\no\ns\ne\nd\n \nm\ne\nt\nh\no\nd\n \no\nr\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n?\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n'\n,\n \n'\nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n'\n,\n \n'\nD\na\nt\na\ns\ne\nt\ns\n'\n,\n \n'\nK\ne\ny\n \nF\ni\nn\nd\ni\nn\ng\ns\n \n(\nP\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \nM\ne\nt\nr\ni\nc\ns\n)\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \na\nd\nv\na\nn\nc\ne\ns\n \ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \ns\np\ne\ne\nc\nh\n-\nt\no\n-\nt\ne\nx\nt\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \no\nb\ns\ne\nr\nv\ne\nd\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\ns\ne\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \na\nn\nd\n \nd\na\nt\na\n \ns\nc\na\nl\na\nb\ni\nl\ni\nt\ny\n \na\np\np\nr\no\na\nc\nh\ne\ns\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \na\nd\nv\na\nn\nc\ne\ns\n \nf\no\nr\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \ns\np\ne\ne\nc\nh\n-\nt\no\n-\nt\ne\nx\nt\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ne\ne\np\n \nl\ne\na\nr\nn\ni\nn\ng\n \nt\nh\ne\no\nr\ny\n \na\nn\nd\n \nc\no\nm\np\nu\nt\ne\nr\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n#\n \n3\n.\n1\n \nP\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \nT\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \nf\no\nr\n \nM\no\nd\na\nl\ni\nt\ny\n \nG\na\np\n \nB\nr\ni\nd\ng\ni\nn\ng\n\n\nF\no\nc\nu\ns\n \no\nn\n \np\na\np\ne\nr\ns\n \ni\nn\nv\ne\ns\nt\ni\ng\na\nt\ni\nn\ng\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \nf\no\nr\n \nS\nT\n \nt\no\n \nb\nr\ni\nd\ng\ne\n \nt\nh\ne\n \nm\no\nd\na\nl\ni\nt\ny\n \ng\na\np\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\ne\nt\na\ni\nl\ns\n \na\nb\no\nu\nt\n \nt\nh\ne\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \nl\ne\na\nr\nn\ni\nn\ng\n \nm\ne\nt\nh\no\nd\ns\n \nu\ns\ne\nd\n \n(\ne\n.\ng\n.\n,\n \nC\nP\nC\n,\n \nw\na\nv\n2\nv\ne\nc\n,\n \nM\na\ns\nk\ne\nd\n \nA\nc\no\nu\ns\nt\ni\nc\n \nM\no\nd\ne\nl\ni\nn\ng\n)\n,\n \nt\nh\ne\n \nt\ny\np\ne\ns\n \no\nf\n \nu\nn\nl\na\nb\ne\nl\ne\nd\n \ns\np\ne\ne\nc\nh\n \nd\na\nt\na\n \nl\ne\nv\ne\nr\na\ng\ne\nd\n,\n \na\nn\nd\n \nh\no\nw\n \nt\nh\ne\ns\ne\n \np\nr\ne\n-\nt\nr\na\ni\nn\ne\nd\n \nr\ne\np\nr\ne\ns\ne\nn\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n \ni\nn\nt\ne\ng\nr\na\nt\ne\nd\n \ni\nn\nt\no\n \nt\nh\ne\n \nS\nT\n \nm\no\nd\ne\nl\n \nt\no\n \nb\nr\ni\nd\ng\ne\n \nt\nh\ne\n \nm\no\nd\na\nl\ni\nt\ny\n \ng\na\np\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \ng\na\ni\nn\ns\n \na\nc\nh\ni\ne\nv\ne\nd\n \nt\nh\nr\no\nu\ng\nh\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n,\n \ne\ns\np\ne\nc\ni\na\nl\nl\ny\n \ni\nn\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \ns\ne\nt\nt\ni\nn\ng\ns\n,\n \na\nn\nd\n \nt\nh\ne\n \ns\np\ne\nc\ni\nf\ni\nc\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \nt\na\ns\nk\ns\n \ne\nm\np\nl\no\ny\ne\nd\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n \nF\no\nc\nu\ns\n \no\nn\n \np\na\np\ne\nr\ns\n \nt\nh\na\nt\n \ni\nn\nt\nr\no\nd\nu\nc\ne\n \nS\nu\nb\nn\ne\nt\n-\nC\no\nn\ns\ni\ns\nt\ne\nn\nc\ny\n \na\nn\nd\n \nR\no\nl\ne\n \nC\no\nn\ns\ni\ns\nt\ne\nn\nc\ny\n \ni\nn\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n \nl\ni\nk\ne\n \nT\nC\nE\nN\n \nf\no\nr\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\ne\nt\na\ni\nl\ns\n \na\nb\no\nu\nt\n \nt\nh\ne\n \nc\no\nn\nc\ne\np\nt\ns\n \no\nf\n \nS\nu\nb\nn\ne\nt\n-\nC\no\nn\ns\ni\ns\nt\ne\nn\nc\ny\n \na\nn\nd\n \nR\no\nl\ne\n \nC\no\nn\ns\ni\ns\nt\ne\nn\nc\ny\n,\n \na\nn\nd\n \nh\no\nw\n \nt\nh\ne\ny\n \na\nr\ne\n \ni\nm\np\nl\ne\nm\ne\nn\nt\ne\nd\n \ni\nn\n \nT\nC\nE\nN\n \nt\no\n \nb\nr\ni\nd\ng\ne\n \nt\nh\ne\n \ng\na\np\n \nb\ne\nt\nw\ne\ne\nn\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \na\nn\nd\n \nf\ni\nn\ne\n-\nt\nu\nn\ni\nn\ng\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \ns\np\ne\nc\ni\nf\ni\nc\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \nc\no\nm\np\no\nn\ne\nn\nt\ns\n \na\nn\nd\n \nt\nr\na\ni\nn\ni\nn\ng\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n \nu\ns\ne\nd\n \nt\no\n \ne\nn\nf\no\nr\nc\ne\n \nt\nh\ne\ns\ne\n \nc\no\nn\ns\ni\ns\nt\ne\nn\nc\ni\ne\ns\n \na\nn\nd\n \nt\nh\ne\ni\nr\n \ni\nm\np\na\nc\nt\n \no\nn\n \nS\nT\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \n[\n'\nb\nr\ni\nd\ng\ni\nn\ng\n_\nt\nh\ne\n_\ng\na\np\n_\nb\ne\nt\nw\ne\ne\nn\n_\np\nr\ne\n_\nt\nr\na\ni\nn\ni\nn\ng\n_\na\nn\nd\n_\nf\ni\nn\ne\n_\nt\nu\nn\ni\nn\ng\n_\nf\no\nr\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n]\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n \nF\no\nc\nu\ns\n \no\nn\n \np\na\np\ne\nr\ns\n \ni\nn\nt\nr\no\nd\nu\nc\ni\nn\ng\n \nS\nt\na\nc\nk\ne\nd\n \nA\nc\no\nu\ns\nt\ni\nc\n-\na\nn\nd\n-\nT\ne\nx\nt\nu\na\nl\n \nE\nn\nc\no\nd\ni\nn\ng\n \n(\nS\nA\nT\nE\n)\n \nf\no\nr\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\ne\nt\na\ni\nl\ns\n \na\nb\no\nu\nt\n \nt\nh\ne\n \nS\nA\nT\nE\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \np\na\nr\nt\ni\nc\nu\nl\na\nr\nl\ny\n \nh\no\nw\n \ni\nt\n \ni\nn\nt\ne\ng\nr\na\nt\ne\ns\n \np\nr\ne\n-\nt\nr\na\ni\nn\ne\nd\n \nm\no\nd\ne\nl\ns\n \ni\nn\nt\no\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \ne\nn\nc\no\nd\ne\nr\ns\n \nb\ny\n \ns\nt\na\nc\nk\ni\nn\ng\n \na\nc\no\nu\ns\nt\ni\nc\n \na\nn\nd\n \nt\ne\nx\nt\nu\na\nl\n \ne\nn\nc\no\nd\ne\nr\ns\n \n[\n'\ns\nt\na\nc\nk\ne\nd\n_\na\nc\no\nu\ns\nt\ni\nc\n_\na\nn\nd\n_\nt\ne\nx\nt\nu\na\nl\n_\ne\nn\nc\no\nd\ni\nn\ng\n_\ni\nn\nt\ne\ng\nr\na\nt\ni\nn\ng\n_\nt\nh\ne\n_\np\nr\ne\n_\nt\nr\na\ni\nn\ne\nd\n_\nm\no\nd\ne\nl\ns\n_\ni\nn\nt\no\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\ne\nn\nc\no\nd\ne\nr\ns\n'\n]\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \ns\np\ne\nc\ni\nf\ni\nc\n \np\nr\ne\n-\nt\nr\na\ni\nn\ne\nd\n \nm\no\nd\ne\nl\ns\n \nu\ns\ne\nd\n,\n \nt\nh\ne\n \nf\nu\ns\ni\no\nn\n \nm\ne\nc\nh\na\nn\ni\ns\nm\ns\n \ne\nm\np\nl\no\ny\ne\nd\n,\n \na\nn\nd\n \nt\nh\ne\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \ng\na\ni\nn\ns\n \na\nc\nh\ni\ne\nv\ne\nd\n \nb\ny\n \nS\nA\nT\nE\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n \nF\no\nc\nu\ns\n \no\nn\n \np\na\np\ne\nr\ns\n \nt\nh\na\nt\n \ni\nn\nt\nr\no\nd\nu\nc\ne\n \nN\ne\nu\nr\na\nl\n \nA\nc\no\nu\ns\nt\ni\nc\n \nF\ne\na\nt\nu\nr\ne\n \nM\no\nd\ne\nl\ni\nn\ng\n \n(\nN\nA\nF\nM\n)\n \nf\no\nr\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\ne\nt\na\ni\nl\ns\n \na\nb\no\nu\nt\n \nt\nh\ne\n \nN\nA\nF\nM\n \na\np\np\nr\no\na\nc\nh\n,\n \ni\nt\ns\n \ng\no\na\nl\n \no\nf\n \nd\ni\nr\ne\nc\nt\nl\ny\n \nm\no\nd\ne\nl\ni\nn\ng\n \na\nc\no\nu\ns\nt\ni\nc\n \nf\ne\na\nt\nu\nr\ne\ns\n \nw\ni\nt\nh\ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n,\n \na\nn\nd\n \ni\nt\ns\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \ni\nm\np\nl\ne\nm\ne\nn\nt\na\nt\ni\no\nn\n \n[\n'\nr\ne\nv\ni\ns\ni\nt\ni\nn\ng\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nf\nr\no\nm\n_\ns\nc\nr\na\nt\nc\nh\n'\n]\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \ns\np\ne\nc\ni\nf\ni\nc\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \nu\ns\ne\nd\n \ni\nn\n \nN\nA\nF\nM\n \na\nn\nd\n \nt\nh\ne\n \nr\ne\np\no\nr\nt\ne\nd\n \nb\ne\nn\ne\nf\ni\nt\ns\n \ni\nn\n \nt\ne\nr\nm\ns\n \no\nf\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \na\nn\nd\n \nm\no\nd\ne\nl\n \ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ni\nm\np\na\nc\nt\n \no\nf\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \no\nn\n \nE\n2\nE\n \nS\nT\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \ni\nn\n \nb\nr\ni\nd\ng\ni\nn\ng\n \nt\nh\ne\n \nm\no\nd\na\nl\ni\nt\ny\n \ng\na\np\n.\n \nC\no\nm\np\na\nr\ne\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \nm\ne\nt\nh\no\nd\ns\n \n(\ne\n.\ng\n.\n,\n \nw\na\nv\n2\nv\ne\nc\n \nv\ns\n.\n \nC\nP\nC\n \nv\ns\n.\n \nM\nA\nM\n)\n \ni\nn\n \nt\nh\ne\n \nc\no\nn\nt\ne\nx\nt\n \no\nf\n \nS\nT\n,\n \nf\no\nc\nu\ns\ni\nn\ng\n \no\nn\n \nt\nh\ne\ni\nr\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \ni\nn\n \nl\ne\na\nr\nn\ni\nn\ng\n \nc\nr\no\ns\ns\n-\nm\no\nd\na\nl\n \nr\ne\np\nr\ne\ns\ne\nn\nt\na\nt\ni\no\nn\ns\n \nw\ni\nt\nh\no\nu\nt\n \nl\na\nb\ne\nl\ne\nd\n \nS\nT\n \nd\na\nt\na\n \n[\n'\ni\nn\nv\ne\ns\nt\ni\ng\na\nt\ni\nn\ng\n_\ns\ne\nl\nf\n_\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n_\np\nr\ne\n_\nt\nr\na\ni\nn\ni\nn\ng\n_\nf\no\nr\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n,\n \n'\nw\na\nv\n2\nv\ne\nc\n_\nu\nn\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n_\np\nr\ne\n_\nt\nr\na\ni\nn\ni\nn\ng\n_\nf\no\nr\n_\ns\np\ne\ne\nc\nh\n_\nr\ne\nc\no\ng\nn\ni\nt\ni\no\nn\n'\n]\n.\n \nE\nv\na\nl\nu\na\nt\ne\n \nt\nh\ne\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \no\nf\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \ni\nn\n \nb\no\nt\nh\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \na\nn\nd\n \nh\ni\ng\nh\n-\nr\ne\ns\no\nu\nr\nc\ne\n \ns\nc\ne\nn\na\nr\ni\no\ns\n \nf\no\nr\n \nb\nr\ni\nd\ng\ni\nn\ng\n \nt\nh\ne\n \nm\no\nd\na\nl\ni\nt\ny\n \ng\na\np\n.\n \nD\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \nb\ne\nn\ne\nf\ni\nt\ns\n \no\nf\n \nu\ns\ni\nn\ng\n \nu\nn\nl\na\nb\ne\nl\ne\nd\n \ns\np\ne\ne\nc\nh\n \nd\na\nt\na\n \na\nn\nd\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \no\nf\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \nl\ne\na\nr\nn\ni\nn\ng\n \nt\no\n \nm\ni\nt\ni\ng\na\nt\ne\n \nm\no\nd\na\nl\ni\nt\ny\n \ng\na\np\n \na\nn\nd\n \nd\na\nt\na\n \ns\nc\na\nr\nc\ni\nt\ny\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \ni\nn\n \nS\nT\n.\n \nC\no\nm\np\na\nr\ne\n \nt\nh\ne\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \ng\na\ni\nn\ns\n \na\nc\nh\ni\ne\nv\ne\nd\n \nb\ny\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \na\nn\nd\n \nd\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \nr\ne\na\ns\no\nn\ns\n \nb\ne\nh\ni\nn\nd\n \nt\nh\ne\ni\nr\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \ni\nn\n \nb\nr\ni\nd\ng\ni\nn\ng\n \nt\nh\ne\n \nm\no\nd\na\nl\ni\nt\ny\n \ng\na\np\n \n[\n'\ni\nn\nv\ne\ns\nt\ni\ng\na\nt\ni\nn\ng\n_\ns\ne\nl\nf\n_\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n_\np\nr\ne\n_\nt\nr\na\ni\nn\ni\nn\ng\n_\nf\no\nr\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n,\n \n'\ns\ne\nl\nf\n_\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n_\nr\ne\np\nr\ne\ns\ne\nn\nt\na\nt\ni\no\nn\ns\n_\ni\nm\np\nr\no\nv\ne\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n]\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \no\nf\n \nS\nu\nb\nn\ne\nt\n-\nC\no\nn\ns\ni\ns\nt\ne\nn\nc\ny\n \na\nn\nd\n \nR\no\nl\ne\n \nC\no\nn\ns\ni\ns\nt\ne\nn\nc\ny\n,\n \na\ns\n \ni\nm\np\nl\ne\nm\ne\nn\nt\ne\nd\n \ni\nn\n \nT\nC\nE\nN\n,\n \ni\nn\n \nb\nr\ni\nd\ng\ni\nn\ng\n \nt\nh\ne\n \ng\na\np\n \nb\ne\nt\nw\ne\ne\nn\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \na\nn\nd\n \nf\ni\nn\ne\n-\nt\nu\nn\ni\nn\ng\n \nf\no\nr\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \n[\n'\nb\nr\ni\nd\ng\ni\nn\ng\n_\nt\nh\ne\n_\ng\na\np\n_\nb\ne\nt\nw\ne\ne\nn\n_\np\nr\ne\n_\nt\nr\na\ni\nn\ni\nn\ng\n_\na\nn\nd\n_\nf\ni\nn\ne\n_\nt\nu\nn\ni\nn\ng\n_\nf\no\nr\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n]\n.\n \nE\nv\na\nl\nu\na\nt\ne\n \nh\no\nw\n \nt\nh\ne\ns\ne\n \nc\no\nn\ns\ni\ns\nt\ne\nn\nc\ny\n \nm\ne\nc\nh\na\nn\ni\ns\nm\ns\n \nc\no\nn\nt\nr\ni\nb\nu\nt\ne\n \nt\no\n \ni\nm\np\nr\no\nv\ne\nd\n \nm\no\nd\ne\nl\n \ng\ne\nn\ne\nr\na\nl\ni\nz\na\nt\ni\no\nn\n \na\nn\nd\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n,\n \np\na\nr\nt\ni\nc\nu\nl\na\nr\nl\ny\n \ni\nn\n \nt\nh\ne\n \nc\no\nn\nt\ne\nx\nt\n \no\nf\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \nt\nr\na\nn\ns\nf\ne\nr\n.\n \nD\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \na\nn\nd\n \nt\nr\na\ni\nn\ni\nn\ng\n \ni\nn\nn\no\nv\na\nt\ni\no\nn\ns\n \ni\nn\n \nT\nC\nE\nN\n \nt\nh\na\nt\n \nl\ne\nv\ne\nr\na\ng\ne\n \nS\nu\nb\nn\ne\nt\n-\nC\no\nn\ns\ni\ns\nt\ne\nn\nc\ny\n \na\nn\nd\n \nR\no\nl\ne\n \nC\no\nn\ns\ni\ns\nt\ne\nn\nc\ny\n \nt\no\n \ne\nn\nh\na\nn\nc\ne\n \nS\nT\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \n[\n'\nb\nr\ni\nd\ng\ni\nn\ng\n_\nt\nh\ne\n_\ng\na\np\n_\nb\ne\nt\nw\ne\ne\nn\n_\np\nr\ne\n_\nt\nr\na\ni\nn\ni\nn\ng\n_\na\nn\nd\n_\nf\ni\nn\ne\n_\nt\nu\nn\ni\nn\ng\n_\nf\no\nr\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n]\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \nS\nt\na\nc\nk\ne\nd\n \nA\nc\no\nu\ns\nt\ni\nc\n-\na\nn\nd\n-\nT\ne\nx\nt\nu\na\nl\n \nE\nn\nc\no\nd\ni\nn\ng\n \n(\nS\nA\nT\nE\n)\n \na\np\np\nr\no\na\nc\nh\n \na\nn\nd\n \ni\nt\ns\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \ni\nn\n \ni\nm\np\nr\no\nv\ni\nn\ng\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \n[\n'\ns\nt\na\nc\nk\ne\nd\n_\na\nc\no\nu\ns\nt\ni\nc\n_\na\nn\nd\n_\nt\ne\nx\nt\nu\na\nl\n_\ne\nn\nc\no\nd\ni\nn\ng\n_\ni\nn\nt\ne\ng\nr\na\nt\ni\nn\ng\n_\nt\nh\ne\n_\np\nr\ne\n_\nt\nr\na\ni\nn\ne\nd\n_\nm\no\nd\ne\nl\ns\n_\ni\nn\nt\no\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\ne\nn\nc\no\nd\ne\nr\ns\n'\n]\n.\n \nE\nv\na\nl\nu\na\nt\ne\n \nh\no\nw\n \nt\nh\ne\n \ni\nn\nt\ne\ng\nr\na\nt\ni\no\nn\n \no\nf\n \np\nr\ne\n-\nt\nr\na\ni\nn\ne\nd\n \na\nc\no\nu\ns\nt\ni\nc\n \na\nn\nd\n \nt\ne\nx\nt\nu\na\nl\n \nm\no\nd\ne\nl\ns\n \nw\ni\nt\nh\ni\nn\n \nS\nA\nT\nE\n \ne\nn\nh\na\nn\nc\ne\ns\n \ne\nn\nc\no\nd\ni\nn\ng\n \na\nn\nd\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nq\nu\na\nl\ni\nt\ny\n.\n \nD\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \na\nd\nv\na\nn\nt\na\ng\ne\ns\n \na\nn\nd\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \no\nf\n \nS\nA\nT\nE\n,\n \nf\no\nc\nu\ns\ni\nn\ng\n \no\nn\n \ni\nt\ns\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \nd\ne\ns\ni\ng\nn\n \na\nn\nd\n \nf\nu\ns\ni\no\nn\n \nm\ne\nc\nh\na\nn\ni\ns\nm\ns\n \n[\n'\ns\nt\na\nc\nk\ne\nd\n_\na\nc\no\nu\ns\nt\ni\nc\n_\na\nn\nd\n_\nt\ne\nx\nt\nu\na\nl\n_\ne\nn\nc\no\nd\ni\nn\ng\n_\ni\nn\nt\ne\ng\nr\na\nt\ni\nn\ng\n_\nt\nh\ne\n_\np\nr\ne\n_\nt\nr\na\ni\nn\ne\nd\n_\nm\no\nd\ne\nl\ns\n_\ni\nn\nt\no\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\ne\nn\nc\no\nd\ne\nr\ns\n'\n]\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \nN\ne\nu\nr\na\nl\n \nA\nc\no\nu\ns\nt\ni\nc\n \nF\ne\na\nt\nu\nr\ne\n \nM\no\nd\ne\nl\ni\nn\ng\n \n(\nN\nA\nF\nM\n)\n \na\np\np\nr\no\na\nc\nh\n \na\nn\nd\n \ni\nt\ns\n \ni\nm\np\na\nc\nt\n \no\nn\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \n[\n'\nr\ne\nv\ni\ns\ni\nt\ni\nn\ng\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nf\nr\no\nm\n_\ns\nc\nr\na\nt\nc\nh\n'\n]\n.\n \nE\nv\na\nl\nu\na\nt\ne\n \nh\no\nw\n \nd\ni\nr\ne\nc\nt\nl\ny\n \nm\no\nd\ne\nl\ni\nn\ng\n \na\nc\no\nu\ns\nt\ni\nc\n \nf\ne\na\nt\nu\nr\ne\ns\n \nw\ni\nt\nh\ni\nn\n \nt\nh\ne\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nf\nr\na\nm\ne\nw\no\nr\nk\n \na\nf\nf\ne\nc\nt\ns\n \nm\no\nd\ne\nl\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \na\nn\nd\n \ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n.\n \nD\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \na\nd\nv\na\nn\nt\na\ng\ne\ns\n \na\nn\nd\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nd\nr\na\nw\nb\na\nc\nk\ns\n \no\nf\n \nN\nA\nF\nM\n,\n \nf\no\nc\nu\ns\ni\nn\ng\n \no\nn\n \ni\nt\ns\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \ni\nn\nn\no\nv\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nm\no\nd\ne\nl\ni\nn\ng\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \n[\n'\nr\ne\nv\ni\ns\ni\nt\ni\nn\ng\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nf\nr\no\nm\n_\ns\nc\nr\na\nt\nc\nh\n'\n]\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \nf\no\nr\n \nm\no\nd\na\nl\ni\nt\ny\n \ng\na\np\n \nb\nr\ni\nd\ng\ni\nn\ng\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \no\nf\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \ni\nn\n \nb\nr\ni\nd\ng\ni\nn\ng\n \nt\nh\ne\n \nm\no\nd\na\nl\ni\nt\ny\n \ng\na\np\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \ns\ne\nt\nu\np\ns\n,\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n,\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nW\nh\na\nt\n \na\nr\ne\n \nt\nh\ne\n \nt\nr\na\nd\ne\n-\no\nf\nf\ns\n \n(\ne\n.\ng\n.\n,\n \nc\no\nm\np\nl\ne\nx\ni\nt\ny\n \nv\ns\n.\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n,\n \nd\na\nt\na\n \ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n \nv\ns\n.\n \nc\no\nm\np\nu\nt\na\nt\ni\no\nn\na\nl\n \nc\no\ns\nt\n)\n \na\ns\ns\no\nc\ni\na\nt\ne\nd\n \nw\ni\nt\nh\n \ne\na\nc\nh\n \np\nr\no\np\no\ns\ne\nd\n \nm\ne\nt\nh\no\nd\n \no\nr\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n?\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n'\n,\n \n'\nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n'\n,\n \n'\nD\na\nt\na\ns\ne\nt\ns\n'\n,\n \n'\nK\ne\ny\n \nF\ni\nn\nd\ni\nn\ng\ns\n \n(\nP\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \nM\ne\nt\nr\ni\nc\ns\n)\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \nf\no\nr\n \nm\no\nd\na\nl\ni\nt\ny\n \ng\na\np\n \nb\nr\ni\nd\ng\ni\nn\ng\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \no\nb\ns\ne\nr\nv\ne\nd\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\ns\ne\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \na\nn\nd\n \nd\na\nt\na\n \ns\nc\na\nl\na\nb\ni\nl\ni\nt\ny\n \na\np\np\nr\no\na\nc\nh\ne\ns\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \nf\no\nr\n \nm\no\nd\na\nl\ni\nt\ny\n \ng\na\np\n \nb\nr\ni\nd\ng\ni\nn\ng\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \nr\ne\np\nr\ne\ns\ne\nn\nt\na\nt\ni\no\nn\n \nl\ne\na\nr\nn\ni\nn\ng\n \na\nn\nd\n \nt\nr\na\nn\ns\nf\ne\nr\n \nl\ne\na\nr\nn\ni\nn\ng\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n#\n \n3\n.\n2\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n \nf\no\nr\n \nD\na\nt\na\n \nS\nc\na\nl\na\nb\ni\nl\ni\nt\ny\n \na\nn\nd\n \nE\nf\nf\ni\nc\ni\ne\nn\nt\n \nT\nr\na\ni\nn\ni\nn\ng\n\n\nF\no\nc\nu\ns\n \no\nn\n \np\na\np\ne\nr\ns\n \nd\ni\ns\nc\nu\ns\ns\ni\nn\ng\n \ns\nc\na\nl\na\nb\nl\ne\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n \nf\no\nr\n \nl\na\nr\ng\ne\n \nd\na\nt\na\ns\ne\nt\ns\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\ne\nt\na\ni\nl\ns\n \na\nb\no\nu\nt\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \nd\ne\ns\ni\ng\nn\ns\n \nt\nh\na\nt\n \nf\na\nc\ni\nl\ni\nt\na\nt\ne\n \nt\nr\na\ni\nn\ni\nn\ng\n \no\nn\n \nl\na\nr\ng\ne\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \ns\nu\nc\nh\n \na\ns\n \nm\no\nd\ne\nl\n \np\na\nr\na\nl\nl\ne\nl\ni\ns\nm\n,\n \nd\na\nt\na\n \np\na\nr\na\nl\nl\ne\nl\ni\ns\nm\n,\n \na\nn\nd\n \ne\nf\nf\ni\nc\ni\ne\nn\nt\n \na\nt\nt\ne\nn\nt\ni\no\nn\n \nm\ne\nc\nh\na\nn\ni\ns\nm\ns\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \nt\ny\np\ne\ns\n \no\nf\n \nl\na\nr\ng\ne\n \nd\na\nt\na\ns\ne\nt\ns\n \nu\ns\ne\nd\n \na\nn\nd\n \nt\nh\ne\n \nr\ne\np\no\nr\nt\ne\nd\n \ns\nc\na\nl\na\nb\ni\nl\ni\nt\ny\n \no\nf\n \nt\nh\ne\ns\ne\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n \n[\n'\ng\ni\ng\na\ns\nt\n_\na\n_\n1\n0\n0\n0\n0\n_\nh\no\nu\nr\n_\np\ns\ne\nu\nd\no\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nc\no\nr\np\nu\ns\n'\n]\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n \nF\no\nc\nu\ns\n \no\nn\n \np\na\np\ne\nr\ns\n \nt\nh\na\nt\n \nu\nt\ni\nl\ni\nz\ne\n \nP\na\nr\na\nm\ne\nt\ne\nr\ni\nz\ne\nd\n \nD\ni\ns\nt\na\nn\nc\ne\n \nP\ne\nn\na\nl\nt\ny\n \n(\nP\nD\nP\n)\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\ne\nt\na\ni\nl\ns\n \na\nb\no\nu\nt\n \nh\no\nw\n \nP\nD\nP\n \ni\ns\n \ni\nm\np\nl\ne\nm\ne\nn\nt\ne\nd\n,\n \ni\nt\ns\n \nr\no\nl\ne\n \ni\nn\n \nl\no\nc\na\nl\ni\nt\ny\n \nm\no\nd\ne\nl\ni\nn\ng\n,\n \na\nn\nd\n \ni\nt\ns\n \ni\nm\np\na\nc\nt\n \no\nn\n \nt\nr\na\ni\nn\ni\nn\ng\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n \nm\no\nd\ne\nl\ns\n \nf\nr\no\nm\n \ns\nc\nr\na\nt\nc\nh\n \n[\n'\nr\ne\nv\ni\ns\ni\nt\ni\nn\ng\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nf\nr\no\nm\n_\ns\nc\nr\na\nt\nc\nh\n'\n]\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \ns\np\ne\nc\ni\nf\ni\nc\n \nf\no\nr\nm\nu\nl\na\nt\ni\no\nn\ns\n \no\nf\n \nP\nD\nP\n \na\nn\nd\n \nt\nh\ne\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \ni\nm\np\nr\no\nv\ne\nm\ne\nn\nt\ns\n \no\nb\ns\ne\nr\nv\ne\nd\n \nw\nh\ne\nn\n \nu\ns\ni\nn\ng\n \nP\nD\nP\n,\n \ne\ns\np\ne\nc\ni\na\nl\nl\ny\n \ni\nn\n \nm\no\nd\ne\nl\ns\n \nt\nr\na\ni\nn\ne\nd\n \nf\nr\no\nm\n \ns\nc\nr\na\nt\nc\nh\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \ni\nn\nn\no\nv\na\nt\ni\no\nn\ns\n \nt\nh\na\nt\n \ne\nn\na\nb\nl\ne\n \nt\nr\na\ni\nn\ni\nn\ng\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \nm\no\nd\ne\nl\ns\n \no\nn\n \nl\na\nr\ng\ne\n \nd\na\nt\na\ns\ne\nt\ns\n.\n \nC\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \ns\nc\na\nl\na\nb\nl\ne\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n,\n \ne\nv\na\nl\nu\na\nt\ni\nn\ng\n \nt\nh\ne\ni\nr\n \ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n \na\nn\nd\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \ng\na\ni\nn\ns\n \nw\nh\ne\nn\n \nt\nr\na\ni\nn\ne\nd\n \no\nn\n \nl\na\nr\ng\ne\n \nd\na\nt\na\ns\ne\nt\ns\n.\n \nD\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \ns\no\nl\nu\nt\ni\no\nn\ns\n \ni\nn\n \ns\nc\na\nl\ni\nn\ng\n \nu\np\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \nm\no\nd\ne\nl\ns\n \nf\no\nr\n \nb\ni\ng\n \nd\na\nt\na\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \nl\ni\nk\ne\n \nc\no\nm\np\nu\nt\na\nt\ni\no\nn\na\nl\n \nr\ne\ns\no\nu\nr\nc\ne\ns\n \na\nn\nd\n \nt\nr\na\ni\nn\ni\nn\ng\n \nt\ni\nm\ne\n \n[\n'\ng\ni\ng\na\ns\nt\n_\na\n_\n1\n0\n0\n0\n0\n_\nh\no\nu\nr\n_\np\ns\ne\nu\nd\no\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nc\no\nr\np\nu\ns\n'\n]\n.\n \nE\nv\na\nl\nu\na\nt\ne\n \nt\nh\ne\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \no\nf\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n \nd\ne\ns\ni\ng\nn\ne\nd\n \nf\no\nr\n \nl\na\nr\ng\ne\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \nd\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\ni\nr\n \ni\nm\np\nl\ni\nc\na\nt\ni\no\nn\ns\n \nf\no\nr\n \na\nd\nv\na\nn\nc\ni\nn\ng\n \nt\nh\ne\n \nf\ni\ne\nl\nd\n \no\nf\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \n[\n'\ng\ni\ng\na\ns\nt\n_\na\n_\n1\n0\n0\n0\n0\n_\nh\no\nu\nr\n_\np\ns\ne\nu\nd\no\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nc\no\nr\np\nu\ns\n'\n]\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \no\nf\n \nP\na\nr\na\nm\ne\nt\ne\nr\ni\nz\ne\nd\n \nD\ni\ns\nt\na\nn\nc\ne\n \nP\ne\nn\na\nl\nt\ny\n \n(\nP\nD\nP\n)\n \ni\nn\n \nl\no\nc\na\nl\ni\nt\ny\n \nm\no\nd\ne\nl\ni\nn\ng\n \nf\no\nr\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n,\n \np\na\nr\nt\ni\nc\nu\nl\na\nr\nl\ny\n \ni\nn\n \ns\nc\ne\nn\na\nr\ni\no\ns\n \nw\nh\ne\nr\ne\n \nm\no\nd\ne\nl\ns\n \na\nr\ne\n \nt\nr\na\ni\nn\ne\nd\n \nf\nr\no\nm\n \ns\nc\nr\na\nt\nc\nh\n \n[\n'\nr\ne\nv\ni\ns\ni\nt\ni\nn\ng\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nf\nr\no\nm\n_\ns\nc\nr\na\nt\nc\nh\n'\n]\n.\n \nE\nv\na\nl\nu\na\nt\ne\n \nh\no\nw\n \nP\nD\nP\n \ne\nn\nh\na\nn\nc\ne\ns\n \nm\no\nd\ne\nl\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \na\nn\nd\n \nd\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nm\ne\nc\nh\na\nn\ni\ns\nm\ns\n \nt\nh\nr\no\nu\ng\nh\n \nw\nh\ni\nc\nh\n \nP\nD\nP\n \ni\nm\np\nr\no\nv\ne\ns\n \nl\no\nc\na\nl\ni\nt\ny\n \nm\no\nd\ne\nl\ni\nn\ng\n.\n \nC\no\nm\np\na\nr\ne\n \ns\ny\ns\nt\ne\nm\ns\n \nu\ns\ni\nn\ng\n \nP\nD\nP\n \nw\ni\nt\nh\n \nt\nh\no\ns\ne\n \nw\ni\nt\nh\no\nu\nt\n,\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \nt\nh\ne\n \nb\ne\nn\ne\nf\ni\nt\ns\n \no\nf\n \nP\nD\nP\n \ni\nn\n \nt\nr\na\ni\nn\ni\nn\ng\n \nE\n2\nE\n \nS\nT\n \nm\no\nd\ne\nl\ns\n \nf\nr\no\nm\n \ns\nc\nr\na\nt\nc\nh\n \n[\n'\nr\ne\nv\ni\ns\ni\nt\ni\nn\ng\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nf\nr\no\nm\n_\ns\nc\nr\na\nt\nc\nh\n'\n]\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n \nf\no\nr\n \nd\na\nt\na\n \ns\nc\na\nl\na\nb\ni\nl\ni\nt\ny\n \na\nn\nd\n \ne\nf\nf\ni\nc\ni\ne\nn\nt\n \nt\nr\na\ni\nn\ni\nn\ng\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ns\nc\na\nl\na\nb\ni\nl\ni\nt\ny\n \na\nn\nd\n \ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n \no\nf\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \ns\ne\nt\nu\np\ns\n,\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n,\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nW\nh\na\nt\n \na\nr\ne\n \nt\nh\ne\n \nt\nr\na\nd\ne\n-\no\nf\nf\ns\n \n(\ne\n.\ng\n.\n,\n \nc\no\nm\np\nl\ne\nx\ni\nt\ny\n \nv\ns\n.\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n,\n \nd\na\nt\na\n \ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n \nv\ns\n.\n \nc\no\nm\np\nu\nt\na\nt\ni\no\nn\na\nl\n \nc\no\ns\nt\n)\n \na\ns\ns\no\nc\ni\na\nt\ne\nd\n \nw\ni\nt\nh\n \ne\na\nc\nh\n \np\nr\no\np\no\ns\ne\nd\n \nm\ne\nt\nh\no\nd\n \no\nr\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n?\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n'\n,\n \n'\nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n'\n,\n \n'\nD\na\nt\na\ns\ne\nt\ns\n'\n,\n \n'\nK\ne\ny\n \nF\ni\nn\nd\ni\nn\ng\ns\n \n(\nP\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \nM\ne\nt\nr\ni\nc\ns\n)\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n \nf\no\nr\n \nd\na\nt\na\n \ns\nc\na\nl\na\nb\ni\nl\ni\nt\ny\n \na\nn\nd\n \ne\nf\nf\ni\nc\ni\ne\nn\nt\n \nt\nr\na\ni\nn\ni\nn\ng\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \no\nb\ns\ne\nr\nv\ne\nd\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\ns\ne\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \na\nn\nd\n \nd\na\nt\na\n \ns\nc\na\nl\na\nb\ni\nl\ni\nt\ny\n \na\np\np\nr\no\na\nc\nh\ne\ns\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n \nf\no\nr\n \nd\na\nt\na\n \ns\nc\na\nl\na\nb\ni\nl\ni\nt\ny\n \na\nn\nd\n \ne\nf\nf\ni\nc\ni\ne\nn\nt\n \nt\nr\na\ni\nn\ni\nn\ng\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\ns\nt\nr\ni\nb\nu\nt\ne\nd\n \nc\no\nm\np\nu\nt\ni\nn\ng\n \na\nn\nd\n \no\np\nt\ni\nm\ni\nz\na\nt\ni\no\nn\n \nt\nh\ne\no\nr\ny\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n#\n \n3\n.\n3\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \nA\nd\na\np\nt\na\nt\ni\no\nn\ns\n \nf\no\nr\n \nT\nr\na\nn\ns\nf\ne\nr\n \nL\ne\na\nr\nn\ni\nn\ng\n\n\nF\no\nc\nu\ns\n \no\nn\n \np\na\np\ne\nr\ns\n \nt\nh\na\nt\n \np\nr\no\np\no\ns\ne\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \na\nd\na\np\nt\na\nt\ni\no\nn\ns\n \nt\no\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nl\ny\n \nt\nr\na\nn\ns\nf\ne\nr\n \nk\nn\no\nw\nl\ne\nd\ng\ne\n \nf\nr\no\nm\n \np\nr\ne\n-\nt\nr\na\ni\nn\ne\nd\n \nm\no\nd\ne\nl\ns\n \nt\no\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \nt\na\ns\nk\ns\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\ne\nt\na\ni\nl\ns\n \na\nb\no\nu\nt\n \nt\nh\ne\n \ns\np\ne\nc\ni\nf\ni\nc\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \nm\no\nd\ni\nf\ni\nc\na\nt\ni\no\nn\ns\n,\n \ns\nu\nc\nh\n \na\ns\n \na\nd\na\np\nt\no\nr\n \nm\no\nd\nu\nl\ne\ns\n,\n \ns\np\ne\nc\ni\na\nl\ni\nz\ne\nd\n \nl\na\ny\ne\nr\ns\n,\n \no\nr\n \nm\no\nd\ni\nf\ni\ne\nd\n \na\nt\nt\ne\nn\nt\ni\no\nn\n \nm\ne\nc\nh\na\nn\ni\ns\nm\ns\n,\n \nd\ne\ns\ni\ng\nn\ne\nd\n \nt\no\n \nf\na\nc\ni\nl\ni\nt\na\nt\ne\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \nt\nr\na\nn\ns\nf\ne\nr\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \nt\ny\np\ne\ns\n \no\nf\n \np\nr\ne\n-\nt\nr\na\ni\nn\ne\nd\n \nm\no\nd\ne\nl\ns\n \nu\ns\ne\nd\n \n(\ne\n.\ng\n.\n,\n \nA\nS\nR\n,\n \nM\nT\n,\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n)\n \na\nn\nd\n \nh\no\nw\n \nt\nh\ne\ns\ne\n \na\nd\na\np\nt\na\nt\ni\no\nn\ns\n \ni\nm\np\nr\no\nv\ne\n \nt\nr\na\nn\ns\nf\ne\nr\n \nl\ne\na\nr\nn\ni\nn\ng\n \ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n \na\nn\nd\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \ni\nn\n \nS\nT\n \n[\n'\np\nr\ne\n_\nt\nr\na\ni\nn\ni\nn\ng\n_\no\nn\n_\nh\ni\ng\nh\n_\nr\ne\ns\no\nu\nr\nc\ne\n_\ns\np\ne\ne\nc\nh\n_\nr\ne\nc\no\ng\nn\ni\nt\ni\no\nn\n_\ni\nm\np\nr\no\nv\ne\ns\n_\nl\no\nw\n_\nr\ne\ns\no\nu\nr\nc\ne\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n,\n \n'\ns\nt\na\nc\nk\ne\nd\n_\na\nc\no\nu\ns\nt\ni\nc\n_\na\nn\nd\n_\nt\ne\nx\nt\nu\na\nl\n_\ne\nn\nc\no\nd\ni\nn\ng\n_\ni\nn\nt\ne\ng\nr\na\nt\ni\nn\ng\n_\nt\nh\ne\n_\np\nr\ne\n_\nt\nr\na\ni\nn\ne\nd\n_\nm\no\nd\ne\nl\ns\n_\ni\nn\nt\no\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\ne\nn\nc\no\nd\ne\nr\ns\n'\n]\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \na\nd\na\np\nt\na\nt\ni\no\nn\n \ns\nt\nr\na\nt\ne\ng\ni\ne\ns\n \np\nr\no\np\no\ns\ne\nd\n \nf\no\nr\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \nt\nr\na\nn\ns\nf\ne\nr\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n.\n \nC\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\ns\ne\n \na\nd\na\np\nt\na\nt\ni\no\nn\ns\n,\n \nf\no\nc\nu\ns\ni\nn\ng\n \no\nn\n \nt\nh\ne\ni\nr\n \nm\ne\nc\nh\na\nn\ni\ns\nm\ns\n \nf\no\nr\n \nk\nn\no\nw\nl\ne\nd\ng\ne\n \ni\nn\nt\ne\ng\nr\na\nt\ni\no\nn\n \na\nn\nd\n \nt\nh\ne\ni\nr\n \ni\nm\np\na\nc\nt\n \no\nn\n \nS\nT\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n.\n \nE\nv\na\nl\nu\na\nt\ne\n \nt\nh\ne\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \no\nf\n \nt\nh\ne\ns\ne\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \na\nd\na\np\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \nl\ne\nv\ne\nr\na\ng\ni\nn\ng\n \np\nr\ne\n-\nt\nr\na\ni\nn\ne\nd\n \nk\nn\no\nw\nl\ne\nd\ng\ne\n \na\nn\nd\n \nd\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \nc\no\nn\nd\ni\nt\ni\no\nn\ns\n \nu\nn\nd\ne\nr\n \nw\nh\ni\nc\nh\n \nc\ne\nr\nt\na\ni\nn\n \na\nd\na\np\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n \nm\no\nr\ne\n \nb\ne\nn\ne\nf\ni\nc\ni\na\nl\n.\n \nI\nn\nv\ne\ns\nt\ni\ng\na\nt\ne\n \nh\no\nw\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \na\nd\na\np\nt\na\nt\ni\no\nn\ns\n \nf\na\nc\ni\nl\ni\nt\na\nt\ne\n \nt\nh\ne\n \nt\nr\na\nn\ns\nf\ne\nr\n \no\nf\n \np\nr\ne\n-\nt\nr\na\ni\nn\ne\nd\n \nk\nn\no\nw\nl\ne\nd\ng\ne\n \nf\nr\no\nm\n \nm\no\nd\ne\nl\ns\n \nl\ni\nk\ne\n \nA\nS\nR\n \na\nn\nd\n \nM\nT\n \nt\no\n \nS\nT\n,\n \na\nn\nd\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \ni\nm\np\nr\no\nv\ne\nm\ne\nn\nt\ns\n \na\nc\nh\ni\ne\nv\ne\nd\n \n[\n'\np\nr\ne\n_\nt\nr\na\ni\nn\ni\nn\ng\n_\no\nn\n_\nh\ni\ng\nh\n_\nr\ne\ns\no\nu\nr\nc\ne\n_\ns\np\ne\ne\nc\nh\n_\nr\ne\nc\no\ng\nn\ni\nt\ni\no\nn\n_\ni\nm\np\nr\no\nv\ne\ns\n_\nl\no\nw\n_\nr\ne\ns\no\nu\nr\nc\ne\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n,\n \n'\ns\nt\na\nc\nk\ne\nd\n_\na\nc\no\nu\ns\nt\ni\nc\n_\na\nn\nd\n_\nt\ne\nx\nt\nu\na\nl\n_\ne\nn\nc\no\nd\ni\nn\ng\n_\ni\nn\nt\ne\ng\nr\na\nt\ni\nn\ng\n_\nt\nh\ne\n_\np\nr\ne\n_\nt\nr\na\ni\nn\ne\nd\n_\nm\no\nd\ne\nl\ns\n_\ni\nn\nt\no\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\ne\nn\nc\no\nd\ne\nr\ns\n'\n]\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \na\nd\na\np\nt\na\nt\ni\no\nn\ns\n \nf\no\nr\n \nt\nr\na\nn\ns\nf\ne\nr\n \nl\ne\na\nr\nn\ni\nn\ng\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \no\nf\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \na\nd\na\np\nt\na\nt\ni\no\nn\n \ns\nt\nr\na\nt\ne\ng\ni\ne\ns\n \nf\no\nr\n \nt\nr\na\nn\ns\nf\ne\nr\n \nl\ne\na\nr\nn\ni\nn\ng\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \ns\ne\nt\nu\np\ns\n,\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n,\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nW\nh\na\nt\n \na\nr\ne\n \nt\nh\ne\n \nt\nr\na\nd\ne\n-\no\nf\nf\ns\n \n(\ne\n.\ng\n.\n,\n \nc\no\nm\np\nl\ne\nx\ni\nt\ny\n \nv\ns\n.\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n,\n \nd\na\nt\na\n \ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n \nv\ns\n.\n \nc\no\nm\np\nu\nt\na\nt\ni\no\nn\na\nl\n \nc\no\ns\nt\n)\n \na\ns\ns\no\nc\ni\na\nt\ne\nd\n \nw\ni\nt\nh\n \ne\na\nc\nh\n \np\nr\no\np\no\ns\ne\nd\n \nm\ne\nt\nh\no\nd\n \no\nr\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n?\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n'\n,\n \n'\nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n'\n,\n \n'\nD\na\nt\na\ns\ne\nt\ns\n'\n,\n \n'\nK\ne\ny\n \nF\ni\nn\nd\ni\nn\ng\ns\n \n(\nP\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \nM\ne\nt\nr\ni\nc\ns\n)\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \na\nd\na\np\nt\na\nt\ni\no\nn\ns\n \nf\no\nr\n \nt\nr\na\nn\ns\nf\ne\nr\n \nl\ne\na\nr\nn\ni\nn\ng\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \no\nb\ns\ne\nr\nv\ne\nd\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\ns\ne\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \na\nn\nd\n \nd\na\nt\na\n \ns\nc\na\nl\na\nb\ni\nl\ni\nt\ny\n \na\np\np\nr\no\na\nc\nh\ne\ns\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \na\nd\na\np\nt\na\nt\ni\no\nn\ns\n \nf\no\nr\n \nt\nr\na\nn\ns\nf\ne\nr\n \nl\ne\na\nr\nn\ni\nn\ng\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \nt\nr\na\nn\ns\nf\ne\nr\n \nl\ne\na\nr\nn\ni\nn\ng\n \nt\nh\ne\no\nr\ny\n \na\nn\nd\n \nd\no\nm\na\ni\nn\n \na\nd\na\np\nt\na\nt\ni\no\nn\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n \n4\n.\n \nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \ni\nn\n \nE\nn\nd\n-\nt\no\n-\nE\nn\nd\n \nS\np\ne\ne\nc\nh\n \nT\nr\na\nn\ns\nl\na\nt\ni\no\nn\n\n\nM\no\nv\ni\nn\ng\n \nf\nr\no\nm\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\na\nl\n \na\nd\nv\na\nn\nc\ne\ns\n \nt\no\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n,\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\ns\n \np\na\np\ne\nr\ns\n \nt\nh\na\nt\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nd\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \ni\nn\nh\ne\nr\ne\nn\nt\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \no\nf\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n \na\np\np\nr\no\na\nc\nh\ne\ns\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\ne\nt\na\ni\nl\ns\n \na\nb\no\nu\nt\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \nl\ni\nk\ne\n \ne\nr\nr\no\nn\ne\no\nu\ns\n \ne\na\nr\nl\ny\n \nd\ne\nc\ni\ns\ni\no\nn\ns\n,\n \nm\ni\ns\nm\na\nt\nc\nh\ne\nd\n \nm\no\nd\na\nl\ni\nt\ny\n,\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nl\no\ns\ns\n,\n \na\nn\nd\n \nd\na\nt\na\n \ni\nn\ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n \n[\n'\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\na\nn\nd\n_\nt\nh\ne\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\np\nr\no\nm\ni\ns\ne\n_\nt\na\nk\ni\nn\ng\n_\ns\nt\no\nc\nk\n_\no\nf\n_\nw\nh\ne\nr\ne\n_\nw\ne\n_\na\nr\ne\n'\n]\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \ns\np\ne\nc\ni\nf\ni\nc\n \np\nr\no\nb\nl\ne\nm\ns\n \na\ns\ns\no\nc\ni\na\nt\ne\nd\n \nw\ni\nt\nh\n \ne\na\nc\nh\n \nc\nh\na\nl\nl\ne\nn\ng\ne\n \na\nn\nd\n \nt\nh\ne\ni\nr\n \ni\nm\np\na\nc\nt\n \no\nn\n \nS\nT\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nD\ne\ns\np\ni\nt\ne\n \nt\nh\ne\n \na\nd\nv\na\nn\nc\ne\nm\ne\nn\nt\ns\n,\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n \ns\nt\ni\nl\nl\n \nf\na\nc\ne\ns\n \ns\ne\nv\ne\nr\na\nl\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n,\n \nw\nh\ni\nc\nh\n \na\nr\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n.\n \nP\nr\no\nv\ni\nd\ne\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \na\nn\na\nl\ny\ns\ni\ns\n \no\nf\n \nt\nh\ne\n \ni\nn\nh\ne\nr\ne\nn\nt\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \no\nf\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n \na\np\np\nr\no\na\nc\nh\ne\ns\n.\n \nD\ni\ns\nc\nu\ns\ns\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \nl\ni\nk\ne\n \ne\nr\nr\no\nn\ne\no\nu\ns\n \ne\na\nr\nl\ny\n \nd\ne\nc\ni\ns\ni\no\nn\ns\n,\n \nm\ni\ns\nm\na\nt\nc\nh\ne\nd\n \nm\no\nd\na\nl\ni\nt\ny\n,\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nl\no\ns\ns\n,\n \na\nn\nd\n \nd\na\nt\na\n \ni\nn\ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n,\n \nd\nr\na\nw\ni\nn\ng\n \nu\np\no\nn\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nt\nh\ne\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \n[\n'\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\na\nn\nd\n_\nt\nh\ne\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\np\nr\no\nm\ni\ns\ne\n_\nt\na\nk\ni\nn\ng\n_\ns\nt\no\nc\nk\n_\no\nf\n_\nw\nh\ne\nr\ne\n_\nw\ne\n_\na\nr\ne\n'\n]\n.\n \nC\na\nt\ne\ng\no\nr\ni\nz\ne\n \na\nn\nd\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ni\nm\np\na\nc\nt\n \no\nf\n \nt\nh\ne\ns\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \no\nn\n \nt\nh\ne\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \na\nn\nd\n \nr\no\nb\nu\ns\nt\nn\ne\ns\ns\n \no\nf\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n \ns\ny\ns\nt\ne\nm\ns\n.\n \nC\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\ns\n \no\nn\n \nt\nh\ne\ns\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nd\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \nt\nr\na\nd\ne\n-\no\nf\nf\ns\n \ni\nn\nv\no\nl\nv\ne\nd\n \ni\nn\n \na\nd\nd\nr\ne\ns\ns\ni\nn\ng\n \nt\nh\ne\nm\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nt\nh\ne\n \n'\nE\nr\nr\no\nn\ne\no\nu\ns\n \nE\na\nr\nl\ny\n \nD\ne\nc\ni\ns\ni\no\nn\ns\n'\n,\n \n'\nM\ni\ns\nm\na\nt\nc\nh\ne\nd\n \nM\no\nd\na\nl\ni\nt\ny\n'\n,\n \n'\nI\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nL\no\ns\ns\n'\n,\n \na\nn\nd\n \n'\nD\na\nt\na\n \nI\nn\ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n'\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n.\n \nH\no\nw\n \nm\ni\ng\nh\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \nl\ni\nk\ne\n \nc\no\ng\nn\ni\nt\ni\nv\ne\n \np\ns\ny\nc\nh\no\nl\no\ng\ny\n \n(\nf\no\nr\n \ne\nr\nr\no\nr\n \np\nr\no\np\na\ng\na\nt\ni\no\nn\n \ni\nn\n \nh\nu\nm\na\nn\n \nc\no\ng\nn\ni\nt\ni\no\nn\n)\n,\n \ns\ni\ng\nn\na\nl\n \np\nr\no\nc\ne\ns\ns\ni\nn\ng\n \n(\nf\no\nr\n \nm\no\nd\na\nl\ni\nt\ny\n \nm\ni\ns\nm\na\nt\nc\nh\n)\n,\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nt\nh\ne\no\nr\ny\n \n(\nf\no\nr\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nl\no\ns\ns\n \nq\nu\na\nn\nt\ni\nf\ni\nc\na\nt\ni\no\nn\n)\n,\n \no\nr\n \ne\nc\no\nn\no\nm\ni\nc\ns\n \n(\nf\no\nr\n \nd\na\nt\na\n \ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n)\n \no\nf\nf\ne\nr\n \nn\ne\nw\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\ns\n \no\nr\n \ns\no\nl\nu\nt\ni\no\nn\ns\n?\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \nn\na\nt\nu\nr\ne\n \na\nn\nd\n \ni\nm\np\na\nc\nt\n \no\nf\n \nt\nh\ne\ns\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \ns\ne\nt\nu\np\ns\n,\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n,\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nC\nh\na\nl\nl\ne\nn\ng\ne\n'\n,\n \n'\nD\ne\ns\nc\nr\ni\np\nt\ni\no\nn\n'\n,\n \n'\nI\nm\np\na\nc\nt\n \no\nn\n \nS\nT\n'\n,\n \n'\nP\nr\no\np\no\ns\ne\nd\n \nS\no\nl\nu\nt\ni\no\nn\ns\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \nc\no\ng\nn\ni\nt\ni\nv\ne\n \ns\nc\ni\ne\nn\nc\ne\n,\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nt\nh\ne\no\nr\ny\n,\n \na\nn\nd\n \nl\ni\nn\ng\nu\ni\ns\nt\ni\nc\ns\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n#\n \n4\n.\n1\n \nE\nr\nr\no\nn\ne\no\nu\ns\n \nE\na\nr\nl\ny\n \nD\ne\nc\ni\ns\ni\no\nn\ns\n\n\nF\no\nc\nu\ns\n \no\nn\n \np\na\np\ne\nr\ns\n \nt\nh\na\nt\n \nd\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\n \no\nf\n \ne\nr\nr\no\nn\ne\no\nu\ns\n \ne\na\nr\nl\ny\n \nd\ne\nc\ni\ns\ni\no\nn\ns\n \ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\ne\nt\na\ni\nl\ns\n \na\nb\no\nu\nt\n \nh\no\nw\n \ne\na\nr\nl\ny\n \ne\nr\nr\no\nr\ns\n \ni\nn\n \ns\np\ne\ne\nc\nh\n \nr\ne\nc\no\ng\nn\ni\nt\ni\no\nn\n \no\nr\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nc\na\nn\n \np\nr\no\np\na\ng\na\nt\ne\n \na\nn\nd\n \nn\ne\ng\na\nt\ni\nv\ne\nl\ny\n \ni\nm\np\na\nc\nt\n \nt\nh\ne\n \nf\ni\nn\na\nl\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nq\nu\na\nl\ni\nt\ny\n.\n \nN\no\nt\ne\n \na\nn\ny\n \np\nr\no\np\no\ns\ne\nd\n \ns\no\nl\nu\nt\ni\no\nn\ns\n \no\nr\n \nm\ni\nt\ni\ng\na\nt\ni\no\nn\n \ns\nt\nr\na\nt\ne\ng\ni\ne\ns\n \nf\no\nr\n \nt\nh\ni\ns\n \nc\nh\na\nl\nl\ne\nn\ng\ne\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\nr\no\nb\nl\ne\nm\n \no\nf\n \ne\nr\nr\no\nn\ne\no\nu\ns\n \ne\na\nr\nl\ny\n \nd\ne\nc\ni\ns\ni\no\nn\ns\n \ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n \na\nn\nd\n \ni\nt\ns\n \nc\no\nn\ns\ne\nq\nu\ne\nn\nc\ne\ns\n \nf\no\nr\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nq\nu\na\nl\ni\nt\ny\n.\n \nE\nv\na\nl\nu\na\nt\ne\n \nt\nh\ne\n \ns\ne\nv\ne\nr\ni\nt\ny\n \no\nf\n \ne\nr\nr\no\nr\n \np\nr\no\np\na\ng\na\nt\ni\no\nn\n \na\nn\nd\n \nd\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \nm\ne\nc\nh\na\nn\ni\ns\nm\ns\n \nt\nh\nr\no\nu\ng\nh\n \nw\nh\ni\nc\nh\n \ne\na\nr\nl\ny\n \ne\nr\nr\no\nr\ns\n \na\nf\nf\ne\nc\nt\n \nd\no\nw\nn\ns\nt\nr\ne\na\nm\n \np\nr\no\nc\ne\ns\ns\ni\nn\ng\n.\n \nC\no\nm\np\na\nr\ne\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \na\np\np\nr\no\na\nc\nh\ne\ns\n \nt\no\n \nm\ni\nt\ni\ng\na\nt\ne\n \ne\nr\nr\no\nn\ne\no\nu\ns\n \ne\na\nr\nl\ny\n \nd\ne\nc\ni\ns\ni\no\nn\ns\n \na\nn\nd\n \na\ns\ns\ne\ns\ns\n \nt\nh\ne\ni\nr\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \ni\nn\n \ni\nm\np\nr\no\nv\ni\nn\ng\n \nt\nh\ne\n \nr\no\nb\nu\ns\nt\nn\ne\ns\ns\n \no\nf\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n \ns\ny\ns\nt\ne\nm\ns\n.\n \nW\nh\na\nt\n \na\nr\ne\n \nt\nh\ne\n \nr\no\no\nt\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ni\ns\n \nc\nh\na\nl\nl\ne\nn\ng\ne\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n?\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \ne\nr\nr\no\nn\ne\no\nu\ns\n \ne\na\nr\nl\ny\n \nd\ne\nc\ni\ns\ni\no\nn\ns\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ni\nm\np\na\nc\nt\n \na\nn\nd\n \nm\ni\nt\ni\ng\na\nt\ni\no\nn\n \no\nf\n \ne\nr\nr\no\nn\ne\no\nu\ns\n \ne\na\nr\nl\ny\n \nd\ne\nc\ni\ns\ni\no\nn\ns\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \ns\ne\nt\nu\np\ns\n,\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n,\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nC\nh\na\nl\nl\ne\nn\ng\ne\n'\n,\n \n'\nD\ne\ns\nc\nr\ni\np\nt\ni\no\nn\n'\n,\n \n'\nI\nm\np\na\nc\nt\n \no\nn\n \nS\nT\n'\n,\n \n'\nP\nr\no\np\no\ns\ne\nd\n \nS\no\nl\nu\nt\ni\no\nn\ns\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \ne\nr\nr\no\nn\ne\no\nu\ns\n \ne\na\nr\nl\ny\n \nd\ne\nc\ni\ns\ni\no\nn\ns\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \ne\nr\nr\no\nn\ne\no\nu\ns\n \ne\na\nr\nl\ny\n \nd\ne\nc\ni\ns\ni\no\nn\ns\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \ne\nr\nr\no\nn\ne\no\nu\ns\n \ne\na\nr\nl\ny\n \nd\ne\nc\ni\ns\ni\no\nn\ns\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \nc\no\ng\nn\ni\nt\ni\nv\ne\n \np\ns\ny\nc\nh\no\nl\no\ng\ny\n \na\nn\nd\n \ne\nr\nr\no\nr\n \na\nn\na\nl\ny\ns\ni\ns\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n#\n \n4\n.\n2\n \nM\ni\ns\nm\na\nt\nc\nh\ne\nd\n \nM\no\nd\na\nl\ni\nt\ny\n \na\nn\nd\n \nD\ni\ns\nt\nr\ni\nb\nu\nt\ni\no\nn\n\n\nF\no\nc\nu\ns\n \no\nn\n \np\na\np\ne\nr\ns\n \na\nd\nd\nr\ne\ns\ns\ni\nn\ng\n \nt\nh\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \no\nf\n \nm\ni\ns\nm\na\nt\nc\nh\ne\nd\n \nm\no\nd\na\nl\ni\nt\ny\n \na\nn\nd\n \nd\ni\ns\nt\nr\ni\nb\nu\nt\ni\no\nn\n \ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\ne\nt\na\ni\nl\ns\n \na\nb\no\nu\nt\n \nt\nh\ne\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \nb\ne\nt\nw\ne\ne\nn\n \ns\np\ne\ne\nc\nh\n \na\nn\nd\n \nt\ne\nx\nt\n \nm\no\nd\na\nl\ni\nt\ni\ne\ns\n,\n \na\nn\nd\n \nh\no\nw\n \nt\nh\ne\ns\ne\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \np\no\ns\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \nf\no\nr\n \nd\ni\nr\ne\nc\nt\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \ni\ns\ns\nu\ne\ns\n \na\nr\ni\ns\ni\nn\ng\n \nf\nr\no\nm\n \nd\ni\ns\nt\nr\ni\nb\nu\nt\ni\no\nn\na\nl\n \nm\ni\ns\nm\na\nt\nc\nh\ne\ns\n \nb\ne\nt\nw\ne\ne\nn\n \ns\np\ne\ne\nc\nh\n \na\nn\nd\n \nt\ne\nx\nt\n \nd\na\nt\na\n,\n \na\nn\nd\n \na\nn\ny\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \np\nr\no\np\no\ns\ne\nd\n \nt\no\n \na\nd\nd\nr\ne\ns\ns\n \nt\nh\ne\ns\ne\n \nm\ni\ns\nm\na\nt\nc\nh\ne\ns\n,\n \ns\nu\nc\nh\n \na\ns\n \na\nd\nv\ne\nr\ns\na\nr\ni\na\nl\n \nr\ne\ng\nu\nl\na\nr\ni\nz\ne\nr\ns\n \nt\no\n \nb\nr\ni\nd\ng\ne\n \nm\no\nd\na\nl\ni\nt\ny\n \ng\na\np\ns\n \nb\ne\nt\nw\ne\ne\nn\n \nA\nS\nR\n \na\nn\nd\n \nN\nM\nT\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \na\nn\nd\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \nw\ni\nt\nh\n \na\nu\nx\ni\nl\ni\na\nr\ny\n \nd\na\nt\na\n \ns\no\nu\nr\nc\ne\ns\n \n[\n'\nb\nr\ni\nd\ng\ni\nn\ng\n_\nt\nh\ne\n_\nm\no\nd\na\nl\ni\nt\ny\n_\ng\na\np\n_\nf\no\nr\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n]\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\na\nt\na\ns\ne\nt\ns\n \nu\ns\ne\nd\n \nf\no\nr\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \na\nn\nd\n \nd\na\nt\na\n \na\nu\ng\nm\ne\nn\nt\na\nt\ni\no\nn\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \nu\ns\ne\nd\n \ni\nn\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \nS\nT\n,\n \na\nn\nd\n \nt\no\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\ni\nr\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \n[\n'\na\nn\na\nl\ny\nz\ni\nn\ng\n_\na\ns\nr\n_\np\nr\ne\nt\nr\na\ni\nn\ni\nn\ng\n_\nf\no\nr\n_\nl\no\nw\n_\nr\ne\ns\no\nu\nr\nc\ne\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n]\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \np\no\ns\ne\nd\n \nb\ny\n \nm\ni\ns\nm\na\nt\nc\nh\ne\nd\n \nm\no\nd\na\nl\ni\nt\ny\n \na\nn\nd\n \nd\ni\ns\nt\nr\ni\nb\nu\nt\ni\no\nn\n \ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n.\n \nE\nv\na\nl\nu\na\nt\ne\n \nt\nh\ne\n \ni\nm\np\na\nc\nt\n \no\nf\n \nm\no\nd\na\nl\ni\nt\ny\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \no\nn\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nq\nu\na\nl\ni\nt\ny\n \na\nn\nd\n \nd\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n.\n \nC\no\nm\np\na\nr\ne\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \ns\nt\nr\na\nt\ne\ng\ni\ne\ns\n \nf\no\nr\n \nb\nr\ni\nd\ng\ni\nn\ng\n \nt\nh\ne\n \nm\no\nd\na\nl\ni\nt\ny\n \ng\na\np\n \na\nn\nd\n \nm\ni\nt\ni\ng\na\nt\ni\nn\ng\n \nd\ni\ns\nt\nr\ni\nb\nu\nt\ni\no\nn\na\nl\n \nm\ni\ns\nm\na\nt\nc\nh\ne\ns\n,\n \ns\nu\nc\nh\n \na\ns\n \nd\ne\nc\no\nu\np\nl\ni\nn\ng\n \ne\nn\nc\no\nd\ne\nr\ns\n \na\nn\nd\n \nc\nr\no\ns\ns\n-\nm\no\nd\na\nl\n \na\nd\na\np\nt\na\nt\ni\no\nn\n.\n \nA\ns\ns\ne\ns\ns\n \nt\nh\ne\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \no\nf\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \nd\ne\ns\ni\ng\nn\ne\nd\n \nt\no\n \nh\na\nn\nd\nl\ne\n \nm\no\nd\na\nl\ni\nt\ny\n \na\nn\nd\n \nd\ni\ns\nt\nr\ni\nb\nu\nt\ni\no\nn\n \nm\ni\ns\nm\na\nt\nc\nh\ne\ns\n \na\nn\nd\n \nd\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \nr\ne\nm\na\ni\nn\ni\nn\ng\n \no\np\ne\nn\n \nq\nu\ne\ns\nt\ni\no\nn\ns\n \ni\nn\n \nt\nh\ni\ns\n \na\nr\ne\na\n \n[\n'\nb\nr\ni\nd\ng\ni\nn\ng\n_\nt\nh\ne\n_\ng\na\np\n_\nb\ne\nt\nw\ne\ne\nn\n_\np\nr\ne\n_\nt\nr\na\ni\nn\ni\nn\ng\n_\na\nn\nd\n_\nf\ni\nn\ne\n_\nt\nu\nn\ni\nn\ng\n_\nf\no\nr\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n]\n.\n \nW\nh\na\nt\n \na\nr\ne\n \nt\nh\ne\n \nr\no\no\nt\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ni\ns\n \nc\nh\na\nl\nl\ne\nn\ng\ne\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n?\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nm\ni\ns\nm\na\nt\nc\nh\ne\nd\n \nm\no\nd\na\nl\ni\nt\ny\n \na\nn\nd\n \nd\ni\ns\nt\nr\ni\nb\nu\nt\ni\no\nn\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ni\nm\np\na\nc\nt\n \na\nn\nd\n \nm\ni\nt\ni\ng\na\nt\ni\no\nn\n \no\nf\n \nm\ni\ns\nm\na\nt\nc\nh\ne\nd\n \nm\no\nd\na\nl\ni\nt\ny\n \na\nn\nd\n \nd\ni\ns\nt\nr\ni\nb\nu\nt\ni\no\nn\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \ns\ne\nt\nu\np\ns\n,\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n,\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nC\nh\na\nl\nl\ne\nn\ng\ne\n'\n,\n \n'\nD\ne\ns\nc\nr\ni\np\nt\ni\no\nn\n'\n,\n \n'\nI\nm\np\na\nc\nt\n \no\nn\n \nS\nT\n'\n,\n \n'\nP\nr\no\np\no\ns\ne\nd\n \nS\no\nl\nu\nt\ni\no\nn\ns\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nm\ni\ns\nm\na\nt\nc\nh\ne\nd\n \nm\no\nd\na\nl\ni\nt\ny\n \na\nn\nd\n \nd\ni\ns\nt\nr\ni\nb\nu\nt\ni\no\nn\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \nm\ni\ns\nm\na\nt\nc\nh\ne\nd\n \nm\no\nd\na\nl\ni\nt\ny\n \na\nn\nd\n \nd\ni\ns\nt\nr\ni\nb\nu\nt\ni\no\nn\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \nm\ni\ns\nm\na\nt\nc\nh\ne\nd\n \nm\no\nd\na\nl\ni\nt\ny\n \na\nn\nd\n \nd\ni\ns\nt\nr\ni\nb\nu\nt\ni\no\nn\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \ns\ni\ng\nn\na\nl\n \np\nr\no\nc\ne\ns\ns\ni\nn\ng\n \na\nn\nd\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nt\nh\ne\no\nr\ny\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n#\n \n4\n.\n3\n \nI\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nL\no\ns\ns\n\n\nF\no\nc\nu\ns\n \no\nn\n \np\na\np\ne\nr\ns\n \nd\ni\ns\nc\nu\ns\ns\ni\nn\ng\n \nt\nh\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\n \no\nf\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nl\no\ns\ns\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\ne\nt\na\ni\nl\ns\n \na\nb\no\nu\nt\n \nh\no\nw\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nc\na\nn\n \nb\ne\n \nl\no\ns\nt\n \nw\nh\ne\nn\n \nd\ni\nr\ne\nc\nt\nl\ny\n \nm\na\np\np\ni\nn\ng\n \nf\nr\no\nm\n \ns\np\ne\ne\nc\nh\n \nt\no\n \nt\ne\nx\nt\n,\n \nc\no\nm\np\na\nr\ne\nd\n \nt\no\n \nc\na\ns\nc\na\nd\ne\nd\n \ns\ny\ns\nt\ne\nm\ns\n \nt\nh\na\nt\n \nh\na\nv\ne\n \ni\nn\nt\ne\nr\nm\ne\nd\ni\na\nt\ne\n \nt\ne\nx\nt\n \nr\ne\np\nr\ne\ns\ne\nn\nt\na\nt\ni\no\nn\ns\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \nt\ny\np\ne\ns\n \no\nf\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nt\nh\na\nt\n \na\nr\ne\n \nm\no\ns\nt\n \ns\nu\ns\nc\ne\np\nt\ni\nb\nl\ne\n \nt\no\n \nl\no\ns\ns\n \na\nn\nd\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nc\no\nn\ns\ne\nq\nu\ne\nn\nc\ne\ns\n \nf\no\nr\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \na\nc\nc\nu\nr\na\nc\ny\n \na\nn\nd\n \nf\nl\nu\ne\nn\nc\ny\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\ne\nt\na\ni\nl\ns\n \na\nb\no\nu\nt\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \nl\ni\nk\ne\n \nc\nu\nr\nr\ni\nc\nu\nl\nu\nm\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \na\nn\nd\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nl\ny\n \np\nr\ne\nt\nr\na\ni\nn\ni\nn\ng\n \nd\ne\nc\no\nd\ne\nr\n \nt\no\n \nm\ni\nt\ni\ng\na\nt\ne\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nl\no\ns\ns\n \n[\n'\ne\nf\nf\ne\nc\nt\ni\nv\ne\nl\ny\n_\np\nr\ne\nt\nr\na\ni\nn\ni\nn\ng\n_\na\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nd\ne\nc\no\nd\ne\nr\n_\nw\ni\nt\nh\n_\nm\na\nc\nh\ni\nn\ne\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nd\na\nt\na\n'\n,\n \n'\nc\nu\nr\nr\ni\nc\nu\nl\nu\nm\n_\np\nr\ne\n_\nt\nr\na\ni\nn\ni\nn\ng\n_\nf\no\nr\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n]\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\n \no\nf\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nl\no\ns\ns\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \na\nn\nd\n \ni\nt\ns\n \ni\nm\np\na\nc\nt\n \no\nn\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nq\nu\na\nl\ni\nt\ny\n.\n \nE\nv\na\nl\nu\na\nt\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nd\ni\nr\ne\nc\nt\n \nm\no\nd\ne\nl\ns\n \ns\nu\nf\nf\ne\nr\n \nf\nr\no\nm\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nl\no\ns\ns\n \nc\no\nm\np\na\nr\ne\nd\n \nt\no\n \nc\na\ns\nc\na\nd\ne\nd\n \nm\no\nd\ne\nl\ns\n.\n \nD\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \nt\ny\np\ne\ns\n \no\nf\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nl\no\ns\ns\n \nt\nh\na\nt\n \na\nr\ne\n \nm\no\ns\nt\n \nc\nr\ni\nt\ni\nc\na\nl\n \na\nn\nd\n \ne\nx\np\nl\no\nr\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \ns\nt\nr\na\nt\ne\ng\ni\ne\ns\n \nt\no\n \nm\ni\nn\ni\nm\ni\nz\ne\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nl\no\ns\ns\n \ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n.\n \nW\nh\na\nt\n \na\nr\ne\n \nt\nh\ne\n \nr\no\no\nt\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ni\ns\n \nc\nh\na\nl\nl\ne\nn\ng\ne\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n?\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nl\no\ns\ns\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ni\nm\np\na\nc\nt\n \na\nn\nd\n \nm\ni\nt\ni\ng\na\nt\ni\no\nn\n \no\nf\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nl\no\ns\ns\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \ns\ne\nt\nu\np\ns\n,\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n,\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nC\nh\na\nl\nl\ne\nn\ng\ne\n'\n,\n \n'\nD\ne\ns\nc\nr\ni\np\nt\ni\no\nn\n'\n,\n \n'\nI\nm\np\na\nc\nt\n \no\nn\n \nS\nT\n'\n,\n \n'\nP\nr\no\np\no\ns\ne\nd\n \nS\no\nl\nu\nt\ni\no\nn\ns\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nl\no\ns\ns\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nl\no\ns\ns\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nl\no\ns\ns\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nt\nh\ne\no\nr\ny\n \na\nn\nd\n \nl\ni\nn\ng\nu\ni\ns\nt\ni\nc\ns\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n#\n \n4\n.\n4\n \nD\na\nt\na\n \nI\nn\ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n\n\nF\no\nc\nu\ns\n \no\nn\n \np\na\np\ne\nr\ns\n \na\nd\nd\nr\ne\ns\ns\ni\nn\ng\n \nt\nh\ne\n \nd\na\nt\na\n \ni\nn\ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n \no\nf\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n \nm\no\nd\ne\nl\ns\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\ne\nt\na\ni\nl\ns\n \na\nb\no\nu\nt\n \nt\nh\ne\n \nl\na\nr\ng\ne\n \na\nm\no\nu\nn\nt\ns\n \no\nf\n \np\na\nr\na\nl\nl\ne\nl\n \ns\np\ne\ne\nc\nh\n-\nt\ne\nx\nt\n \nd\na\nt\na\n \nr\ne\nq\nu\ni\nr\ne\nd\n \nt\no\n \nt\nr\na\ni\nn\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n \ns\ny\ns\nt\ne\nm\ns\n,\n \nc\no\nm\np\na\nr\ne\nd\n \nt\no\n \nc\na\ns\nc\na\nd\ne\nd\n \ns\ny\ns\nt\ne\nm\ns\n \nt\nh\na\nt\n \nc\na\nn\n \nl\ne\nv\ne\nr\na\ng\ne\n \ns\ne\np\na\nr\na\nt\ne\n \nA\nS\nR\n \na\nn\nd\n \nM\nT\n \nd\na\nt\na\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \np\no\ns\ne\nd\n \nb\ny\n \nd\na\nt\na\n \ns\nc\na\nr\nc\ni\nt\ny\n,\n \ne\ns\np\ne\nc\ni\na\nl\nl\ny\n \nf\no\nr\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \nl\na\nn\ng\nu\na\ng\ne\ns\n,\n \na\nn\nd\n \np\no\nt\ne\nn\nt\ni\na\nl\n \ns\no\nl\nu\nt\ni\no\nn\ns\n \nl\ni\nk\ne\n \nd\na\nt\na\n \na\nu\ng\nm\ne\nn\nt\na\nt\ni\no\nn\n \na\nn\nd\n \nt\nr\na\nn\ns\nf\ne\nr\n \nl\ne\na\nr\nn\ni\nn\ng\n \n[\n'\nb\nr\ni\nd\ng\ni\nn\ng\n_\nt\nh\ne\n_\nm\no\nd\na\nl\ni\nt\ny\n_\ng\na\np\n_\nf\no\nr\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n]\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \nd\na\nt\na\n \ni\nn\ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n \nc\nh\na\nl\nl\ne\nn\ng\ne\n \ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n \na\nn\nd\n \ni\nt\ns\n \ni\nm\np\nl\ni\nc\na\nt\ni\no\nn\ns\n \nf\no\nr\n \nm\no\nd\ne\nl\n \nt\nr\na\ni\nn\ni\nn\ng\n \na\nn\nd\n \nd\ne\np\nl\no\ny\nm\ne\nn\nt\n,\n \ne\ns\np\ne\nc\ni\na\nl\nl\ny\n \nf\no\nc\nu\ns\ni\nn\ng\n \no\nn\n \nd\na\nt\na\n \na\nu\ng\nm\ne\nn\nt\na\nt\ni\no\nn\n \ni\nn\n \nd\na\nt\na\n-\ns\nc\na\nr\nc\ne\n \ns\nc\ne\nn\na\nr\ni\no\ns\n \n[\n'\nb\nr\ni\nd\ng\ni\nn\ng\n_\nt\nh\ne\n_\nm\no\nd\na\nl\ni\nt\ny\n_\ng\na\np\n_\nf\no\nr\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n]\n.\n \nE\nv\na\nl\nu\na\nt\ne\n \nt\nh\ne\n \nd\na\nt\na\n \nr\ne\nq\nu\ni\nr\ne\nm\ne\nn\nt\ns\n \no\nf\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n \nm\no\nd\ne\nl\ns\n \nc\no\nm\np\na\nr\ne\nd\n \nt\no\n \nc\na\ns\nc\na\nd\ne\nd\n \nm\no\nd\ne\nl\ns\n.\n \nD\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nd\na\nt\na\n \ni\nn\ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n \na\nn\nd\n \ne\nx\np\nl\no\nr\ne\n \ns\nt\nr\na\nt\ne\ng\ni\ne\ns\n \nt\no\n \ni\nm\np\nr\no\nv\ne\n \nd\na\nt\na\n \ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n,\n \ns\nu\nc\nh\n \na\ns\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n,\n \nt\nr\na\nn\ns\nf\ne\nr\n \nl\ne\na\nr\nn\ni\nn\ng\n,\n \na\nn\nd\n \nd\na\nt\na\n \na\nu\ng\nm\ne\nn\nt\na\nt\ni\no\nn\n.\n \nW\nh\na\nt\n \na\nr\ne\n \nt\nh\ne\n \nr\no\no\nt\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ni\ns\n \nc\nh\na\nl\nl\ne\nn\ng\ne\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n?\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nd\na\nt\na\n \ni\nn\ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ni\nm\np\na\nc\nt\n \na\nn\nd\n \nm\ni\nt\ni\ng\na\nt\ni\no\nn\n \no\nf\n \nd\na\nt\na\n \ni\nn\ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \ns\ne\nt\nu\np\ns\n,\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n,\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nC\nh\na\nl\nl\ne\nn\ng\ne\n'\n,\n \n'\nD\ne\ns\nc\nr\ni\np\nt\ni\no\nn\n'\n,\n \n'\nI\nm\np\na\nc\nt\n \no\nn\n \nS\nT\n'\n,\n \n'\nP\nr\no\np\no\ns\ne\nd\n \nS\no\nl\nu\nt\ni\no\nn\ns\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nd\na\nt\na\n \ni\nn\ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \nd\na\nt\na\n \ni\nn\ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \nd\na\nt\na\n \ni\nn\ne\nf\nf\ni\nc\ni\ne\nn\nc\ny\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\na\nt\na\n \na\nu\ng\nm\ne\nn\nt\na\nt\ni\no\nn\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \na\nn\nd\n \nt\nr\na\nn\ns\nf\ne\nr\n \nl\ne\na\nr\nn\ni\nn\ng\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n \n5\n.\n \nD\na\nt\na\n-\nC\ne\nn\nt\nr\ni\nc\n \nA\nd\nv\na\nn\nc\ne\ns\n:\n \nD\na\nt\na\ns\ne\nt\ns\n \na\nn\nd\n \nP\ns\ne\nu\nd\no\n-\nD\na\nt\na\n \nS\nt\nr\na\nt\ne\ng\ni\ne\ns\n\n\nA\nd\nd\nr\ne\ns\ns\ni\nn\ng\n \nt\nh\ne\n \nd\na\nt\na\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \ni\nn\n \nt\nh\ne\n \np\nr\ne\nv\ni\no\nu\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n \nr\ne\nv\ni\ne\nw\ns\n \nd\na\nt\na\n-\nc\ne\nn\nt\nr\ni\nc\n \na\nd\nv\na\nn\nc\ne\ns\n,\n \nd\ne\ns\nc\nr\ni\nb\ni\nn\ng\n \nt\nh\ne\n \nl\na\nn\nd\ns\nc\na\np\ne\n \no\nf\n \nd\na\nt\na\ns\ne\nt\ns\n \na\nn\nd\n \nr\ne\ns\no\nu\nr\nc\ne\ns\n \nu\ns\ne\nd\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \nr\ne\ns\ne\na\nr\nc\nh\n,\n \ne\nm\np\nh\na\ns\ni\nz\ni\nn\ng\n \nd\na\nt\na\n-\nc\ne\nn\nt\nr\ni\nc\n \na\nd\nv\na\nn\nc\ne\ns\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \np\na\np\ne\nr\ns\n \nt\nh\na\nt\n \ni\nn\nt\nr\no\nd\nu\nc\ne\n \no\nr\n \nu\nt\ni\nl\ni\nz\ne\n \ns\np\ne\nc\ni\nf\ni\nc\n \nd\na\nt\na\ns\ne\nt\ns\n \nr\ne\nl\ne\nv\na\nn\nt\n \nt\no\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \no\nr\n \nr\ne\nl\na\nt\ne\nd\n \nt\na\ns\nk\ns\n,\n \nf\no\nc\nu\ns\ni\nn\ng\n \no\nn\n \nd\na\nt\na\n-\nc\ne\nn\nt\nr\ni\nc\n \na\nd\nv\na\nn\nc\ne\ns\n.\n \nE\nx\nt\nr\na\nc\nt\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \na\nb\no\nu\nt\n \nd\na\nt\na\ns\ne\nt\n \nc\nh\na\nr\na\nc\nt\ne\nr\ni\ns\nt\ni\nc\ns\n,\n \ni\nn\nc\nl\nu\nd\ni\nn\ng\n \ns\ni\nz\ne\n,\n \nl\na\nn\ng\nu\na\ng\ne\n \np\na\ni\nr\ns\n,\n \nd\no\nm\na\ni\nn\n,\n \nd\na\nt\na\n \ns\no\nu\nr\nc\ne\n,\n \na\nn\nd\n \nc\nr\ne\na\nt\ni\no\nn\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ny\n \n(\ni\nn\nc\nl\nu\nd\ni\nn\ng\n \np\ns\ne\nu\nd\no\n-\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \na\np\np\nr\no\na\nc\nh\ne\ns\n)\n.\n \nN\no\nt\ne\n \na\nn\ny\n \np\nu\nb\nl\ni\nc\nl\ny\n \na\nv\na\ni\nl\na\nb\nl\ne\n \nr\ne\ns\no\nu\nr\nc\ne\ns\n,\n \np\nr\ne\n-\nt\nr\na\ni\nn\ne\nd\n \nm\no\nd\ne\nl\ns\n,\n \no\nr\n \ns\nc\nr\ni\np\nt\ns\n \nr\ne\nl\ne\na\ns\ne\nd\n \na\nl\no\nn\ng\n \nw\ni\nt\nh\n \nt\nh\ne\ns\ne\n \nd\na\nt\na\ns\ne\nt\ns\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nA\nd\nd\nr\ne\ns\ns\ni\nn\ng\n \nt\nh\ne\n \nd\na\nt\na\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n,\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n \nr\ne\nv\ni\ne\nw\ns\n \nd\na\nt\na\n-\nc\ne\nn\nt\nr\ni\nc\n \na\nd\nv\na\nn\nc\ne\ns\n,\n \nd\ne\ns\nc\nr\ni\nb\ni\nn\ng\n \nt\nh\ne\n \nl\na\nn\nd\ns\nc\na\np\ne\n \no\nf\n \nd\na\nt\na\ns\ne\nt\ns\n \na\nn\nd\n \nr\ne\ns\no\nu\nr\nc\ne\ns\n \nu\ns\ne\nd\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \nr\ne\ns\ne\na\nr\nc\nh\n,\n \ne\nm\np\nh\na\ns\ni\nz\ni\nn\ng\n \nd\na\nt\na\n-\nc\ne\nn\nt\nr\ni\nc\n \na\nd\nv\na\nn\nc\ne\ns\n.\n \nC\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nc\nh\na\nr\na\nc\nt\ne\nr\ni\ns\nt\ni\nc\ns\n \no\nf\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \ni\nn\nc\nl\nu\nd\ni\nn\ng\n \nc\no\nr\np\no\nr\na\n \nl\ni\nk\ne\n \nL\ni\nb\nr\ni\nS\np\ne\ne\nc\nh\n,\n \nM\nu\nS\nT\n-\nC\n,\n \na\nn\nd\n \nG\ni\ng\na\nS\nT\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ni\nm\np\na\nc\nt\n \no\nf\n \nd\na\nt\na\ns\ne\nt\n \ns\ni\nz\ne\n,\n \nq\nu\na\nl\ni\nt\ny\n,\n \na\nn\nd\n \nd\no\nm\na\ni\nn\n \ns\np\ne\nc\ni\nf\ni\nc\ni\nt\ny\n \no\nn\n \nm\no\nd\ne\nl\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n.\n \nD\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \nr\no\nl\ne\n \no\nf\n \np\ns\ne\nu\nd\no\n-\nt\nr\na\nn\ns\nl\na\nt\ne\nd\n \nd\na\nt\na\n \na\nn\nd\n \nu\nn\nl\na\nb\ne\nl\ne\nd\n \ns\np\ne\ne\nc\nh\n \nd\na\nt\na\n \ni\nn\n \ns\nc\na\nl\ni\nn\ng\n \nu\np\n \nS\nT\n \nr\ne\ns\no\nu\nr\nc\ne\ns\n \na\nn\nd\n \na\nd\nd\nr\ne\ns\ns\ni\nn\ng\n \nd\na\nt\na\n \ns\nc\na\nr\nc\ni\nt\ny\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nd\na\nt\na\n-\nc\ne\nn\nt\nr\ni\nc\n \na\nd\nv\na\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\ns\n \na\nn\nd\n \np\ns\ne\nu\nd\no\n-\nd\na\nt\na\n \ns\nt\nr\na\nt\ne\ng\ni\ne\ns\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ni\nm\np\na\nc\nt\n \no\nf\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \nd\na\nt\na\ns\ne\nt\ns\n \na\nn\nd\n \np\ns\ne\nu\nd\no\n-\nd\na\nt\na\n \ns\nt\nr\na\nt\ne\ng\ni\ne\ns\n \no\nn\n \nS\nT\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\ns\n \nc\nh\na\nr\na\nc\nt\ne\nr\ni\ns\nt\ni\nc\ns\n,\n \nc\nr\ne\na\nt\ni\no\nn\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n,\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nW\nh\na\nt\n \na\nr\ne\n \nt\nh\ne\n \nt\nr\na\nd\ne\n-\no\nf\nf\ns\n \n(\ne\n.\ng\n.\n,\n \nd\na\nt\na\n \nq\nu\na\nl\ni\nt\ny\n \nv\ns\n.\n \nq\nu\na\nn\nt\ni\nt\ny\n,\n \na\nn\nn\no\nt\na\nt\ni\no\nn\n \nc\no\ns\nt\n \nv\ns\n.\n \nd\na\nt\na\ns\ne\nt\n \ns\ni\nz\ne\n)\n \na\ns\ns\no\nc\ni\na\nt\ne\nd\n \nw\ni\nt\nh\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \nd\na\nt\na\ns\ne\nt\n \nc\nr\ne\na\nt\ni\no\nn\n \ns\nt\nr\na\nt\ne\ng\ni\ne\ns\n?\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nD\na\nt\na\ns\ne\nt\n \nC\nh\na\nr\na\nc\nt\ne\nr\ni\ns\nt\ni\nc\ns\n'\n,\n \n'\nC\nr\ne\na\nt\ni\no\nn\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n'\n,\n \n'\nS\ni\nz\ne\n'\n,\n \n'\nL\na\nn\ng\nu\na\ng\ne\n \nP\na\ni\nr\ns\n'\n,\n \n'\nI\nm\np\na\nc\nt\n \no\nn\n \nS\nT\n \nP\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nd\na\nt\na\n-\nc\ne\nn\nt\nr\ni\nc\n \na\nd\nv\na\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\ns\n \na\nn\nd\n \np\ns\ne\nu\nd\no\n-\nd\na\nt\na\n \ns\nt\nr\na\nt\ne\ng\ni\ne\ns\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \nd\na\nt\na\n-\nc\ne\nn\nt\nr\ni\nc\n \na\nd\nv\na\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\ns\n \na\nn\nd\n \np\ns\ne\nu\nd\no\n-\nd\na\nt\na\n \ns\nt\nr\na\nt\ne\ng\ni\ne\ns\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \nd\na\nt\na\n-\nc\ne\nn\nt\nr\ni\nc\n \na\nd\nv\na\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\ns\n \na\nn\nd\n \np\ns\ne\nu\nd\no\n-\nd\na\nt\na\n \ns\nt\nr\na\nt\ne\ng\ni\ne\ns\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \nc\no\nr\np\nu\ns\n \nl\ni\nn\ng\nu\ni\ns\nt\ni\nc\ns\n \na\nn\nd\n \nd\na\nt\na\n \ne\nn\ng\ni\nn\ne\ne\nr\ni\nn\ng\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n#\n \n5\n.\n1\n \nI\nm\np\na\nc\nt\n \na\nn\nd\n \nA\nn\na\nl\ny\ns\ni\ns\n \no\nf\n \nK\ne\ny\n \nD\na\nt\na\ns\ne\nt\ns\n\n\nF\no\nc\nu\ns\n \no\nn\n \np\na\np\ne\nr\ns\n \nt\nh\na\nt\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ni\nm\np\na\nc\nt\n \no\nf\n \nk\ne\ny\n \nd\na\nt\na\ns\ne\nt\ns\n \no\nn\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \nr\ne\ns\ne\na\nr\nc\nh\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\ne\nt\na\ni\nl\ne\nd\n \nc\nh\na\nr\na\nc\nt\ne\nr\ni\ns\nt\ni\nc\ns\n \na\nn\nd\n \na\nn\na\nl\ny\ns\ne\ns\n \no\nf\n \nd\na\nt\na\ns\ne\nt\ns\n \nl\ni\nk\ne\n \nM\nu\nS\nT\n-\nC\n \na\nn\nd\n \na\nu\nd\ni\no\nb\no\no\nk\n \nc\no\nr\np\no\nr\na\n \n[\n'\nm\nu\ns\nt\n_\nc\n_\na\n_\nm\nu\nl\nt\ni\nl\ni\nn\ng\nu\na\nl\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nc\no\nr\np\nu\ns\n'\n,\n \n'\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\na\nu\nt\no\nm\na\nt\ni\nc\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\no\nf\n_\na\nu\nd\ni\no\nb\no\no\nk\ns\n'\n]\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \ns\np\ne\nc\ni\nf\ni\nc\n \nf\ni\nn\nd\ni\nn\ng\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nd\na\nt\na\ns\ne\nt\n \np\nr\no\np\ne\nr\nt\ni\ne\ns\n \na\nn\nd\n \nt\nh\ne\ni\nr\n \ni\nn\nf\nl\nu\ne\nn\nc\ne\n \no\nn\n \nm\no\nd\ne\nl\n \nt\nr\na\ni\nn\ni\nn\ng\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ni\nm\np\na\nc\nt\n \na\nn\nd\n \nc\nh\na\nr\na\nc\nt\ne\nr\ni\ns\nt\ni\nc\ns\n \no\nf\n \nk\ne\ny\n \nd\na\nt\na\ns\ne\nt\ns\n \nu\ns\ne\nd\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n,\n \ns\nu\nc\nh\n \na\ns\n \nM\nu\nS\nT\n-\nC\n \na\nn\nd\n \na\nu\nd\ni\no\nb\no\no\nk\n \nc\no\nr\np\no\nr\na\n \n[\n'\nm\nu\ns\nt\n_\nc\n_\na\n_\nm\nu\nl\nt\ni\nl\ni\nn\ng\nu\na\nl\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nc\no\nr\np\nu\ns\n'\n,\n \n'\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\na\nu\nt\no\nm\na\nt\ni\nc\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\no\nf\n_\na\nu\nd\ni\no\nb\no\no\nk\ns\n'\n]\n.\n \nC\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\ns\ne\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \ne\nv\na\nl\nu\na\nt\ni\nn\ng\n \nt\nh\ne\ni\nr\n \ns\nt\nr\ne\nn\ng\nt\nh\ns\n \na\nn\nd\n \nw\ne\na\nk\nn\ne\ns\ns\ne\ns\n \nf\no\nr\n \nt\nr\na\ni\nn\ni\nn\ng\n \na\nn\nd\n \nb\ne\nn\nc\nh\nm\na\nr\nk\ni\nn\ng\n \nS\nT\n \nm\no\nd\ne\nl\ns\n.\n \nD\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \nf\nr\no\nm\n \ns\nt\nu\nd\ni\ne\ns\n \nt\nh\na\nt\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ni\nn\nf\nl\nu\ne\nn\nc\ne\n \no\nf\n \nd\na\nt\na\ns\ne\nt\n \np\nr\no\np\ne\nr\nt\ni\ne\ns\n \no\nn\n \nm\no\nd\ne\nl\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \n[\n'\nm\nu\ns\nt\n_\nc\n_\na\n_\nm\nu\nl\nt\ni\nl\ni\nn\ng\nu\na\nl\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nc\no\nr\np\nu\ns\n'\n,\n \n'\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\na\nu\nt\no\nm\na\nt\ni\nc\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\no\nf\n_\na\nu\nd\ni\no\nb\no\no\nk\ns\n'\n]\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ni\nm\np\na\nc\nt\n \na\nn\nd\n \na\nn\na\nl\ny\ns\ni\ns\n \no\nf\n \nk\ne\ny\n \nd\na\nt\na\ns\ne\nt\ns\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ni\nn\nf\nl\nu\ne\nn\nc\ne\n \no\nf\n \nd\na\nt\na\ns\ne\nt\n \nc\nh\na\nr\na\nc\nt\ne\nr\ni\ns\nt\ni\nc\ns\n \no\nn\n \nS\nT\n \nm\no\nd\ne\nl\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\n \np\nr\no\np\ne\nr\nt\ni\ne\ns\n \na\nn\na\nl\ny\nz\ne\nd\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nD\na\nt\na\ns\ne\nt\n'\n,\n \n'\nC\nh\na\nr\na\nc\nt\ne\nr\ni\ns\nt\ni\nc\ns\n \nA\nn\na\nl\ny\nz\ne\nd\n'\n,\n \n'\nK\ne\ny\n \nF\ni\nn\nd\ni\nn\ng\ns\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ni\nm\np\na\nc\nt\n \na\nn\nd\n \na\nn\na\nl\ny\ns\ni\ns\n \no\nf\n \nk\ne\ny\n \nd\na\nt\na\ns\ne\nt\ns\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \ni\nm\np\na\nc\nt\n \na\nn\nd\n \na\nn\na\nl\ny\ns\ni\ns\n \no\nf\n \nk\ne\ny\n \nd\na\nt\na\ns\ne\nt\ns\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \ni\nm\np\na\nc\nt\n \na\nn\nd\n \na\nn\na\nl\ny\ns\ni\ns\n \no\nf\n \nk\ne\ny\n \nd\na\nt\na\ns\ne\nt\ns\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \nc\no\nr\np\nu\ns\n \nl\ni\nn\ng\nu\ni\ns\nt\ni\nc\ns\n \na\nn\nd\n \nd\na\nt\na\n \nq\nu\na\nl\ni\nt\ny\n \na\ns\ns\ne\ns\ns\nm\ne\nn\nt\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n#\n \n5\n.\n2\n \nL\na\nr\ng\ne\n-\nS\nc\na\nl\ne\n \nP\ns\ne\nu\nd\no\n \nS\np\ne\ne\nc\nh\n \nT\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nC\no\nr\np\no\nr\na\n \na\nn\nd\n \nQ\nu\na\nl\ni\nt\ny\n \nC\no\nn\nt\nr\no\nl\n\n\nF\no\nc\nu\ns\n \no\nn\n \np\na\np\ne\nr\ns\n \ni\nn\nt\nr\no\nd\nu\nc\ni\nn\ng\n \na\nn\nd\n \na\nn\na\nl\ny\nz\ni\nn\ng\n \nl\na\nr\ng\ne\n-\ns\nc\na\nl\ne\n \np\ns\ne\nu\nd\no\n \nS\nT\n \nc\no\nr\np\no\nr\na\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\ne\nt\na\ni\nl\ne\nd\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \no\nn\n \nh\no\nw\n \nt\nh\ne\ns\ne\n \nc\no\nr\np\no\nr\na\n \na\nr\ne\n \nc\nr\ne\na\nt\ne\nd\n \n(\ne\n.\ng\n.\n,\n \nm\na\nc\nh\ni\nn\ne\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \no\nf\n \nA\nS\nR\n \nt\nr\na\nn\ns\nc\nr\ni\np\nt\ns\n)\n,\n \nt\nh\ne\n \ns\nc\na\nl\ne\n \no\nf\n \nt\nh\ne\n \nc\no\nr\np\no\nr\na\n \n(\ne\n.\ng\n.\n,\n \nG\ni\ng\na\nS\nT\n)\n \n[\n'\ng\ni\ng\na\ns\nt\n_\na\n_\n1\n0\n0\n0\n0\n_\nh\no\nu\nr\n_\np\ns\ne\nu\nd\no\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nc\no\nr\np\nu\ns\n'\n]\n,\n \na\nn\nd\n \nt\nh\ne\n \nl\na\nn\ng\nu\na\ng\ne\ns\n \nc\no\nv\ne\nr\ne\nd\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \nq\nu\na\nl\ni\nt\ny\n \na\ns\ns\ne\ns\ns\nm\ne\nn\nt\n \nm\ne\nt\nh\no\nd\ns\n \nu\ns\ne\nd\n \nf\no\nr\n \np\ns\ne\nu\nd\no\n-\nt\nr\na\nn\ns\nl\na\nt\ne\nd\n \nd\na\nt\na\n \na\nn\nd\n \nt\nh\ne\n \nr\ne\np\no\nr\nt\ne\nd\n \ni\nm\np\nr\no\nv\ne\nm\ne\nn\nt\ns\n \ni\nn\n \nS\nT\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \nw\nh\ne\nn\n \nu\ns\ni\nn\ng\n \nt\nh\ne\ns\ne\n \nc\no\nr\np\no\nr\na\n \n[\n'\ng\ni\ng\na\ns\nt\n_\na\n_\n1\n0\n0\n0\n0\n_\nh\no\nu\nr\n_\np\ns\ne\nu\nd\no\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nc\no\nr\np\nu\ns\n'\n]\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nE\nv\na\nl\nu\na\nt\ne\n \nt\nh\ne\n \nc\no\nn\nt\nr\ni\nb\nu\nt\ni\no\nn\n \no\nf\n \nl\na\nr\ng\ne\n-\ns\nc\na\nl\ne\n \np\ns\ne\nu\nd\no\n \nS\nT\n \nc\no\nr\np\no\nr\na\n \nt\no\n \nt\nh\ne\n \nf\ni\ne\nl\nd\n,\n \np\na\nr\nt\ni\nc\nu\nl\na\nr\nl\ny\n \ni\nn\n \na\nd\nd\nr\ne\ns\ns\ni\nn\ng\n \nd\na\nt\na\n \ns\nc\na\nr\nc\ni\nt\ny\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ny\n \nu\ns\ne\nd\n \nt\no\n \nc\nr\ne\na\nt\ne\n \nc\no\nr\np\no\nr\na\n \nl\ni\nk\ne\n \nG\ni\ng\na\nS\nT\n \na\nn\nd\n \na\ns\ns\ne\ns\ns\n \nt\nh\ne\n \nq\nu\na\nl\ni\nt\ny\n \na\nn\nd\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nb\ni\na\ns\ne\ns\n \ni\nn\nt\nr\no\nd\nu\nc\ne\nd\n \nb\ny\n \nm\na\nc\nh\ni\nn\ne\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \ni\nn\n \ng\ne\nn\ne\nr\na\nt\ni\nn\ng\n \np\ns\ne\nu\nd\no\n-\nt\nr\na\nn\ns\nl\na\nt\ne\nd\n \nd\na\nt\na\n \n[\n'\ng\ni\ng\na\ns\nt\n_\na\n_\n1\n0\n0\n0\n0\n_\nh\no\nu\nr\n_\np\ns\ne\nu\nd\no\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nc\no\nr\np\nu\ns\n'\n]\n.\n \nD\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \ni\nm\np\na\nc\nt\n \no\nf\n \ns\nu\nc\nh\n \nc\no\nr\np\no\nr\na\n \no\nn\n \ni\nm\np\nr\no\nv\ni\nn\ng\n \nS\nT\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n,\n \ne\ns\np\ne\nc\ni\na\nl\nl\ny\n \ni\nn\n \nd\na\nt\na\n-\ns\nc\na\nr\nc\ne\n \ns\nc\ne\nn\na\nr\ni\no\ns\n \n[\n'\ng\ni\ng\na\ns\nt\n_\na\n_\n1\n0\n0\n0\n0\n_\nh\no\nu\nr\n_\np\ns\ne\nu\nd\no\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nc\no\nr\np\nu\ns\n'\n]\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nl\na\nr\ng\ne\n-\ns\nc\na\nl\ne\n \np\ns\ne\nu\nd\no\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nc\no\nr\np\no\nr\na\n \na\nn\nd\n \nq\nu\na\nl\ni\nt\ny\n \nc\no\nn\nt\nr\no\nl\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \na\nn\nd\n \nq\nu\na\nl\ni\nt\ny\n \no\nf\n \nl\na\nr\ng\ne\n-\ns\nc\na\nl\ne\n \np\ns\ne\nu\nd\no\n \nS\nT\n \nc\no\nr\np\no\nr\na\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \np\ns\ne\nu\nd\no\n-\nd\na\nt\na\n \nc\nr\ne\na\nt\ni\no\nn\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n \na\nn\nd\n \nq\nu\na\nl\ni\nt\ny\n \na\ns\ns\ne\ns\ns\nm\ne\nn\nt\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nC\no\nr\np\nu\ns\n'\n,\n \n'\nC\nr\ne\na\nt\ni\no\nn\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n'\n,\n \n'\nS\nc\na\nl\ne\n'\n,\n \n'\nQ\nu\na\nl\ni\nt\ny\n \nA\ns\ns\ne\ns\ns\nm\ne\nn\nt\n'\n,\n \n'\nI\nm\np\na\nc\nt\n \no\nn\n \nS\nT\n \nP\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nl\na\nr\ng\ne\n-\ns\nc\na\nl\ne\n \np\ns\ne\nu\nd\no\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nc\no\nr\np\no\nr\na\n \na\nn\nd\n \nq\nu\na\nl\ni\nt\ny\n \nc\no\nn\nt\nr\no\nl\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \nl\na\nr\ng\ne\n-\ns\nc\na\nl\ne\n \np\ns\ne\nu\nd\no\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nc\no\nr\np\no\nr\na\n \na\nn\nd\n \nq\nu\na\nl\ni\nt\ny\n \nc\no\nn\nt\nr\no\nl\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \nl\na\nr\ng\ne\n-\ns\nc\na\nl\ne\n \np\ns\ne\nu\nd\no\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nc\no\nr\np\no\nr\na\n \na\nn\nd\n \nq\nu\na\nl\ni\nt\ny\n \nc\no\nn\nt\nr\no\nl\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \nm\na\nc\nh\ni\nn\ne\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nq\nu\na\nl\ni\nt\ny\n \ne\ns\nt\ni\nm\na\nt\ni\no\nn\n \na\nn\nd\n \nd\na\nt\na\n \nv\na\nl\ni\nd\na\nt\ni\no\nn\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n#\n \n5\n.\n3\n \nL\ne\nv\ne\nr\na\ng\ni\nn\ng\n \nU\nn\nl\na\nb\ne\nl\ne\nd\n \nS\np\ne\ne\nc\nh\n \nD\na\nt\na\n \nv\ni\na\n \nS\ne\nl\nf\n-\nS\nu\np\ne\nr\nv\ni\ns\ne\nd\n \nL\ne\na\nr\nn\ni\nn\ng\n \nD\na\nt\na\ns\ne\nt\ns\n\n\nF\no\nc\nu\ns\n \no\nn\n \np\na\np\ne\nr\ns\n \nt\nh\na\nt\n \nl\ne\nv\ne\nr\na\ng\ne\n \nu\nn\nl\na\nb\ne\nl\ne\nd\n \ns\np\ne\ne\nc\nh\n \nd\na\nt\na\n \nf\no\nr\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \nv\ni\na\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \nl\ne\na\nr\nn\ni\nn\ng\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \ns\nu\nc\nh\n \na\ns\n \nL\ni\nb\nr\ni\nS\np\ne\ne\nc\nh\n \n[\n'\nl\ni\nb\nr\ni\ns\np\ne\ne\nc\nh\n_\na\nn\n_\na\ns\nr\n_\nc\no\nr\np\nu\ns\n_\nb\na\ns\ne\nd\n_\no\nn\n_\np\nu\nb\nl\ni\nc\n_\nd\no\nm\na\ni\nn\n_\na\nu\nd\ni\no\n_\nb\no\no\nk\ns\n'\n]\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\ne\nt\na\ni\nl\ns\n \na\nb\no\nu\nt\n \nh\no\nw\n \nu\nn\nl\na\nb\ne\nl\ne\nd\n \ns\np\ne\ne\nc\nh\n \nd\na\nt\na\ns\ne\nt\ns\n \na\nr\ne\n \nu\nt\ni\nl\ni\nz\ne\nd\n \ni\nn\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \nf\no\nr\n \nS\nT\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \nt\ny\np\ne\ns\n \no\nf\n \nu\nn\nl\na\nb\ne\nl\ne\nd\n \nd\na\nt\na\n,\n \nt\nh\ne\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \nl\ne\na\nr\nn\ni\nn\ng\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \na\np\np\nl\ni\ne\nd\n,\n \na\nn\nd\n \nt\nh\ne\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \ng\na\ni\nn\ns\n \na\nc\nh\ni\ne\nv\ne\nd\n \nb\ny\n \nl\ne\nv\ne\nr\na\ng\ni\nn\ng\n \nt\nh\ne\ns\ne\n \nd\na\nt\na\ns\ne\nt\ns\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \no\nf\n \nl\ne\nv\ne\nr\na\ng\ni\nn\ng\n \nu\nn\nl\na\nb\ne\nl\ne\nd\n \ns\np\ne\ne\nc\nh\n \nd\na\nt\na\n \nv\ni\na\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \nl\ne\na\nr\nn\ni\nn\ng\n \nd\na\nt\na\ns\ne\nt\ns\n \nl\ni\nk\ne\n \nL\ni\nb\nr\ni\nS\np\ne\ne\nc\nh\n \nf\no\nr\n \ni\nm\np\nr\no\nv\ni\nn\ng\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \n[\n'\nl\ni\nb\nr\ni\ns\np\ne\ne\nc\nh\n_\na\nn\n_\na\ns\nr\n_\nc\no\nr\np\nu\ns\n_\nb\na\ns\ne\nd\n_\no\nn\n_\np\nu\nb\nl\ni\nc\n_\nd\no\nm\na\ni\nn\n_\na\nu\nd\ni\no\n_\nb\no\no\nk\ns\n'\n]\n.\n \nE\nv\na\nl\nu\na\nt\ne\n \nt\nh\ne\n \ni\nm\np\na\nc\nt\n \no\nf\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \no\nn\n \nS\nT\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n,\n \np\na\nr\nt\ni\nc\nu\nl\na\nr\nl\ny\n \ni\nn\n \ns\nc\ne\nn\na\nr\ni\no\ns\n \nw\ni\nt\nh\n \nl\ni\nm\ni\nt\ne\nd\n \nl\na\nb\ne\nl\ne\nd\n \nS\nT\n \nd\na\nt\na\n.\n \nD\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \nb\ne\nn\ne\nf\ni\nt\ns\n \na\nn\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \no\nf\n \nu\ns\ni\nn\ng\n \nu\nn\nl\na\nb\ne\nl\ne\nd\n \ns\np\ne\ne\nc\nh\n \nd\na\nt\na\n \na\nn\nd\n \na\ns\ns\ne\ns\ns\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \no\nf\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \nl\ne\na\nr\nn\ni\nn\ng\n \nd\na\nt\na\ns\ne\nt\ns\n \nt\no\n \nm\ni\nt\ni\ng\na\nt\ne\n \nd\na\nt\na\n \ns\nc\na\nr\nc\ni\nt\ny\n \ni\nn\n \nS\nT\n \n[\n'\nl\ni\nb\nr\ni\ns\np\ne\ne\nc\nh\n_\na\nn\n_\na\ns\nr\n_\nc\no\nr\np\nu\ns\n_\nb\na\ns\ne\nd\n_\no\nn\n_\np\nu\nb\nl\ni\nc\n_\nd\no\nm\na\ni\nn\n_\na\nu\nd\ni\no\n_\nb\no\no\nk\ns\n'\n]\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nl\ne\nv\ne\nr\na\ng\ni\nn\ng\n \nu\nn\nl\na\nb\ne\nl\ne\nd\n \ns\np\ne\ne\nc\nh\n \nd\na\nt\na\n \nv\ni\na\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \nl\ne\na\nr\nn\ni\nn\ng\n \nd\na\nt\na\ns\ne\nt\ns\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \no\nf\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \nl\ne\na\nr\nn\ni\nn\ng\n \no\nn\n \nu\nn\nl\na\nb\ne\nl\ne\nd\n \ns\np\ne\ne\nc\nh\n \nd\na\nt\na\n \nf\no\nr\n \nS\nT\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \na\nn\nd\n \nd\na\nt\na\ns\ne\nt\ns\n \nu\ns\ne\nd\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nD\na\nt\na\ns\ne\nt\n'\n,\n \n'\nS\ne\nl\nf\n-\nS\nu\np\ne\nr\nv\ni\ns\ne\nd\n \nT\ne\nc\nh\nn\ni\nq\nu\ne\n'\n,\n \n'\nD\na\nt\na\n \nT\ny\np\ne\n'\n,\n \n'\nP\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \nG\na\ni\nn\ns\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nl\ne\nv\ne\nr\na\ng\ni\nn\ng\n \nu\nn\nl\na\nb\ne\nl\ne\nd\n \ns\np\ne\ne\nc\nh\n \nd\na\nt\na\n \nv\ni\na\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \nl\ne\na\nr\nn\ni\nn\ng\n \nd\na\nt\na\ns\ne\nt\ns\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \nl\ne\nv\ne\nr\na\ng\ni\nn\ng\n \nu\nn\nl\na\nb\ne\nl\ne\nd\n \ns\np\ne\ne\nc\nh\n \nd\na\nt\na\n \nv\ni\na\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \nl\ne\na\nr\nn\ni\nn\ng\n \nd\na\nt\na\ns\ne\nt\ns\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \nl\ne\nv\ne\nr\na\ng\ni\nn\ng\n \nu\nn\nl\na\nb\ne\nl\ne\nd\n \ns\np\ne\ne\nc\nh\n \nd\na\nt\na\n \nv\ni\na\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \nl\ne\na\nr\nn\ni\nn\ng\n \nd\na\nt\na\ns\ne\nt\ns\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \nu\nn\ns\nu\np\ne\nr\nv\ni\ns\ne\nd\n \nl\ne\na\nr\nn\ni\nn\ng\n \na\nn\nd\n \nr\ne\np\nr\ne\ns\ne\nn\nt\na\nt\ni\no\nn\n \nl\ne\na\nr\nn\ni\nn\ng\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n#\n \n5\n.\n4\n \nH\nu\nm\na\nn\n-\nA\nn\nn\no\nt\na\nt\ne\nd\n \nS\nT\n \nC\no\nr\np\no\nr\na\n \na\nn\nd\n \nB\ne\nn\nc\nh\nm\na\nr\nk\ni\nn\ng\n \nD\na\nt\na\ns\ne\nt\ns\n\n\nF\no\nc\nu\ns\n \no\nn\n \np\na\np\ne\nr\ns\n \nt\nh\na\nt\n \nu\nt\ni\nl\ni\nz\ne\n \nh\nu\nm\na\nn\n-\na\nn\nn\no\nt\na\nt\ne\nd\n \nS\nT\n \nc\no\nr\np\no\nr\na\n \na\nn\nd\n \nb\ne\nn\nc\nh\nm\na\nr\nk\ni\nn\ng\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \ns\nu\nc\nh\n \na\ns\n \nM\nu\nS\nT\n-\nC\n \n[\n'\nm\nu\ns\nt\n_\nc\n_\na\n_\nm\nu\nl\nt\ni\nl\ni\nn\ng\nu\na\nl\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nc\no\nr\np\nu\ns\n'\n]\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\ne\nt\na\ni\nl\ns\n \na\nb\no\nu\nt\n \nt\nh\ne\n \nc\nh\na\nr\na\nc\nt\ne\nr\ni\ns\nt\ni\nc\ns\n \no\nf\n \nh\nu\nm\na\nn\n-\na\nn\nn\no\nt\na\nt\ne\nd\n \nS\nT\n \nc\no\nr\np\no\nr\na\n,\n \nt\nh\ne\ni\nr\n \nr\no\nl\ne\n \ni\nn\n \nb\ne\nn\nc\nh\nm\na\nr\nk\ni\nn\ng\n \np\nr\no\ng\nr\ne\ns\ns\n,\n \na\nn\nd\n \nt\nh\ne\ni\nr\n \ni\nm\np\na\nc\nt\n \no\nn\n \nt\nh\ne\n \nf\ni\ne\nl\nd\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \ns\np\ne\nc\ni\nf\ni\nc\n \nd\na\nt\na\ns\ne\nt\ns\n \nu\ns\ne\nd\n \nf\no\nr\n \nb\ne\nn\nc\nh\nm\na\nr\nk\ni\nn\ng\n \na\nn\nd\n \nt\nh\ne\ni\nr\n \ni\nn\nf\nl\nu\ne\nn\nc\ne\n \no\nn\n \ne\nv\na\nl\nu\na\nt\ni\nn\ng\n \na\nn\nd\n \nc\no\nm\np\na\nr\ni\nn\ng\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \nS\nT\n \nm\no\nd\ne\nl\ns\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ns\ni\ng\nn\ni\nf\ni\nc\na\nn\nc\ne\n \no\nf\n \nh\nu\nm\na\nn\n-\na\nn\nn\no\nt\na\nt\ne\nd\n \nS\nT\n \nc\no\nr\np\no\nr\na\n \na\nn\nd\n \nb\ne\nn\nc\nh\nm\na\nr\nk\ni\nn\ng\n \nd\na\nt\na\ns\ne\nt\ns\n \nl\ni\nk\ne\n \nM\nu\nS\nT\n-\nC\n \ni\nn\n \na\nd\nv\na\nn\nc\ni\nn\ng\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \nr\ne\ns\ne\na\nr\nc\nh\n \n[\n'\nm\nu\ns\nt\n_\nc\n_\na\n_\nm\nu\nl\nt\ni\nl\ni\nn\ng\nu\na\nl\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nc\no\nr\np\nu\ns\n'\n]\n.\n \nE\nv\na\nl\nu\na\nt\ne\n \nh\no\nw\n \nt\nh\ne\ns\ne\n \nd\na\nt\na\ns\ne\nt\ns\n \nf\na\nc\ni\nl\ni\nt\na\nt\ne\n \nr\ne\np\nr\no\nd\nu\nc\ni\nb\nl\ne\n \nr\ne\ns\ne\na\nr\nc\nh\n,\n \ns\nt\na\nn\nd\na\nr\nd\ni\nz\ne\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nd\nr\ni\nv\ne\n \np\nr\no\ng\nr\ne\ns\ns\n \ni\nn\n \nt\nh\ne\n \nf\ni\ne\nl\nd\n.\n \nD\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \nc\nh\na\nr\na\nc\nt\ne\nr\ni\ns\nt\ni\nc\ns\n \na\nn\nd\n \nb\ne\nn\ne\nf\ni\nt\ns\n \no\nf\n \nu\ns\ni\nn\ng\n \nh\nu\nm\na\nn\n-\na\nn\nn\no\nt\na\nt\ne\nd\n \nc\no\nr\np\no\nr\na\n \nf\no\nr\n \nb\ne\nn\nc\nh\nm\na\nr\nk\ni\nn\ng\n \na\nn\nd\n \na\ns\ns\ne\ns\ns\ni\nn\ng\n \nt\nh\ne\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \no\nf\n \nS\nT\n \nm\no\nd\ne\nl\ns\n \n[\n'\nm\nu\ns\nt\n_\nc\n_\na\n_\nm\nu\nl\nt\ni\nl\ni\nn\ng\nu\na\nl\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nc\no\nr\np\nu\ns\n'\n]\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nh\nu\nm\na\nn\n-\na\nn\nn\no\nt\na\nt\ne\nd\n \nS\nT\n \nc\no\nr\np\no\nr\na\n \na\nn\nd\n \nb\ne\nn\nc\nh\nm\na\nr\nk\ni\nn\ng\n \nd\na\nt\na\ns\ne\nt\ns\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \nr\no\nl\ne\n \na\nn\nd\n \ni\nm\np\na\nc\nt\n \no\nf\n \nh\nu\nm\na\nn\n-\na\nn\nn\no\nt\na\nt\ne\nd\n \nS\nT\n \nc\no\nr\np\no\nr\na\n \ni\nn\n \nb\ne\nn\nc\nh\nm\na\nr\nk\ni\nn\ng\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nd\na\nt\na\ns\ne\nt\n \nc\nh\na\nr\na\nc\nt\ne\nr\ni\ns\nt\ni\nc\ns\n \na\nn\nd\n \nb\ne\nn\nc\nh\nm\na\nr\nk\ni\nn\ng\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nD\na\nt\na\ns\ne\nt\n'\n,\n \n'\nA\nn\nn\no\nt\na\nt\ni\no\nn\n \nT\ny\np\ne\n'\n,\n \n'\nB\ne\nn\nc\nh\nm\na\nr\nk\ni\nn\ng\n \nR\no\nl\ne\n'\n,\n \n'\nI\nm\np\na\nc\nt\n \no\nn\n \nR\ne\ns\ne\na\nr\nc\nh\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nh\nu\nm\na\nn\n-\na\nn\nn\no\nt\na\nt\ne\nd\n \nS\nT\n \nc\no\nr\np\no\nr\na\n \na\nn\nd\n \nb\ne\nn\nc\nh\nm\na\nr\nk\ni\nn\ng\n \nd\na\nt\na\ns\ne\nt\ns\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \nh\nu\nm\na\nn\n-\na\nn\nn\no\nt\na\nt\ne\nd\n \nS\nT\n \nc\no\nr\np\no\nr\na\n \na\nn\nd\n \nb\ne\nn\nc\nh\nm\na\nr\nk\ni\nn\ng\n \nd\na\nt\na\ns\ne\nt\ns\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \nh\nu\nm\na\nn\n-\na\nn\nn\no\nt\na\nt\ne\nd\n \nS\nT\n \nc\no\nr\np\no\nr\na\n \na\nn\nd\n \nb\ne\nn\nc\nh\nm\na\nr\nk\ni\nn\ng\n \nd\na\nt\na\ns\ne\nt\ns\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \np\ns\ny\nc\nh\no\nl\ni\nn\ng\nu\ni\ns\nt\ni\nc\ns\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ny\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n \n6\n.\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n \nC\na\nm\np\na\ni\ng\nn\ns\n \na\nn\nd\n \nM\ne\nt\nr\ni\nc\ns\n\n\nT\no\n \nm\ne\na\ns\nu\nr\ne\n \nt\nh\ne\n \np\nr\no\ng\nr\ne\ns\ns\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \ns\no\n \nf\na\nr\n,\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n \ns\nu\nm\nm\na\nr\ni\nz\ne\ns\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nc\na\nm\np\na\ni\ng\nn\ns\n \na\nn\nd\n \nm\ne\nt\nr\ni\nc\ns\n \ni\nn\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ni\nn\ng\n \nt\nh\ne\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \nf\ni\nn\nd\ni\nn\ng\ns\n \nf\nr\no\nm\n \nr\ne\nc\ne\nn\nt\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nc\na\nm\np\na\ni\ng\nn\ns\n,\n \np\na\nr\nt\ni\nc\nu\nl\na\nr\nl\ny\n \nI\nW\nS\nL\nT\n,\n \na\nn\nd\n \na\nn\na\nl\ny\nz\ne\n \nh\no\nw\n \nt\nh\ne\ns\ne\n \nc\na\nm\np\na\ni\ng\nn\ns\n \nc\no\nn\nt\nr\ni\nb\nu\nt\ne\n \nt\no\n \nb\ne\nn\nc\nh\nm\na\nr\nk\ni\nn\ng\n \np\nr\no\ng\nr\ne\ns\ns\n \na\nn\nd\n \ni\nd\ne\nn\nt\ni\nf\ny\ni\nn\ng\n \no\np\ne\nn\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \ni\nn\n \nt\nh\ne\n \nf\ni\ne\nl\nd\n.\n \nE\nx\na\nm\ni\nn\ne\n \np\na\np\ne\nr\ns\n \nd\ne\ns\nc\nr\ni\nb\ni\nn\ng\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nc\na\nm\np\na\ni\ng\nn\ns\n \na\nn\nd\n \nb\ne\nn\nc\nh\nm\na\nr\nk\n \nd\na\nt\na\ns\ne\nt\ns\n \ni\nn\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \nI\nW\nS\nL\nT\n \n[\n'\nf\ni\nn\nd\ni\nn\ng\ns\n_\no\nf\n_\nt\nh\ne\n_\ni\nw\ns\nl\nt\n_\n2\n0\n2\n2\n_\ne\nv\na\nl\nu\na\nt\ni\no\nn\n_\nc\na\nm\np\na\ni\ng\nn\n'\n]\n.\n \nE\nx\nt\nr\na\nc\nt\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \na\nb\no\nu\nt\n \ns\nh\na\nr\ne\nd\n \nt\na\ns\nk\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \ni\nn\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nc\na\nm\np\na\ni\ng\nn\ns\n \na\nn\nd\n \nt\nh\ne\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n \nu\ns\ne\nd\n \n[\n'\nf\ni\nn\nd\ni\nn\ng\ns\n_\no\nf\n_\nt\nh\ne\n_\ni\nw\ns\nl\nt\n_\n2\n0\n2\n2\n_\ne\nv\na\nl\nu\na\nt\ni\no\nn\n_\nc\na\nm\np\na\ni\ng\nn\n'\n]\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \nf\ni\nn\nd\ni\nn\ng\ns\n \nf\nr\no\nm\n \nr\ne\nc\ne\nn\nt\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nc\na\nm\np\na\ni\ng\nn\ns\n \na\nn\nd\n \na\nn\ny\n \nr\ne\np\no\nr\nt\ne\nd\n \nb\ne\nn\nc\nh\nm\na\nr\nk\n \nr\ne\ns\nu\nl\nt\ns\n \nu\ns\ni\nn\ng\n \nt\no\no\nl\nk\ni\nt\ns\n \nl\ni\nk\ne\n \nE\nS\nP\nn\ne\nt\n-\nS\nT\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nT\no\n \nm\ne\na\ns\nu\nr\ne\n \nt\nh\ne\n \np\nr\no\ng\nr\ne\ns\ns\n,\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n \ns\nu\nm\nm\na\nr\ni\nz\ne\ns\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nc\na\nm\np\na\ni\ng\nn\ns\n \na\nn\nd\n \nm\ne\nt\nr\ni\nc\ns\n \ni\nn\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ni\nn\ng\n \nt\nh\ne\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \nf\ni\nn\nd\ni\nn\ng\ns\n \nf\nr\no\nm\n \nr\ne\nc\ne\nn\nt\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nc\na\nm\np\na\ni\ng\nn\ns\n,\n \np\na\nr\nt\ni\nc\nu\nl\na\nr\nl\ny\n \nI\nW\nS\nL\nT\n,\n \na\nn\nd\n \na\nn\na\nl\ny\nz\ne\n \nh\no\nw\n \nt\nh\ne\ns\ne\n \nc\na\nm\np\na\ni\ng\nn\ns\n \nc\no\nn\nt\nr\ni\nb\nu\nt\ne\n \nt\no\n \nb\ne\nn\nc\nh\nm\na\nr\nk\ni\nn\ng\n \np\nr\no\ng\nr\ne\ns\ns\n \na\nn\nd\n \ni\nd\ne\nn\nt\ni\nf\ny\ni\nn\ng\n \no\np\ne\nn\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \ni\nn\n \nt\nh\ne\n \nf\ni\ne\nl\nd\n.\n \nD\ni\ns\nc\nu\ns\ns\n \nh\no\nw\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nc\na\nm\np\na\ni\ng\nn\ns\n \na\nn\nd\n \nb\ne\nn\nc\nh\nm\na\nr\nk\n \nd\na\nt\na\ns\ne\nt\ns\n \nf\na\nc\ni\nl\ni\nt\na\nt\ne\n \nr\ne\np\nr\no\nd\nu\nc\ni\nb\nl\ne\n \nr\ne\ns\ne\na\nr\nc\nh\n \na\nn\nd\n \nd\nr\ni\nv\ne\n \np\nr\no\ng\nr\ne\ns\ns\n \ni\nn\n \nt\nh\ne\n \nf\ni\ne\nl\nd\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nc\na\nm\np\na\ni\ng\nn\ns\n \na\nn\nd\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ni\nm\np\na\nc\nt\n \na\nn\nd\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \no\nf\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nc\na\nm\np\na\ni\ng\nn\ns\n \na\nn\nd\n \nm\ne\nt\nr\ni\nc\ns\n \ni\nn\n \nS\nT\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n \na\nn\nd\n \nm\ne\nt\nr\ni\nc\ns\n \nu\ns\ne\nd\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nC\na\nm\np\na\ni\ng\nn\n/\nM\ne\nt\nr\ni\nc\n'\n,\n \n'\nF\no\nc\nu\ns\n'\n,\n \n'\nK\ne\ny\n \nF\ni\nn\nd\ni\nn\ng\ns\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nc\na\nm\np\na\ni\ng\nn\ns\n \na\nn\nd\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nc\na\nm\np\na\ni\ng\nn\ns\n \na\nn\nd\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nc\na\nm\np\na\ni\ng\nn\ns\n \na\nn\nd\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ny\n \na\nn\nd\n \nb\ne\nn\nc\nh\nm\na\nr\nk\ni\nn\ng\n \np\nr\na\nc\nt\ni\nc\ne\ns\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n#\n \n6\n.\n1\n \nS\nh\na\nr\ne\nd\n \nT\na\ns\nk\ns\n \ni\nn\n \nI\nW\nS\nL\nT\n\n\nP\nr\no\nv\ni\nd\ne\n \na\nn\n \no\nv\ne\nr\nv\ni\ne\nw\n \no\nf\n \nt\nh\ne\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \ns\nh\na\nr\ne\nd\n \nt\na\ns\nk\ns\n \ni\nn\n \nI\nW\nS\nL\nT\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \n[\n'\nf\ni\nn\nd\ni\nn\ng\ns\n_\no\nf\n_\nt\nh\ne\n_\ni\nw\ns\nl\nt\n_\n2\n0\n2\n2\n_\ne\nv\na\nl\nu\na\nt\ni\no\nn\n_\nc\na\nm\np\na\ni\ng\nn\n'\n]\n.\n \nE\nx\nt\nr\na\nc\nt\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \na\nb\no\nu\nt\n \nt\nh\ne\n \ns\np\ne\nc\ni\nf\ni\nc\n \nt\na\ns\nk\ns\n,\n \ns\nu\nc\nh\n \na\ns\n \nS\ni\nm\nu\nl\nt\na\nn\ne\no\nu\ns\n \nS\nT\n,\n \nO\nf\nf\nl\ni\nn\ne\n \nS\nT\n,\n \nL\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \nS\nT\n,\n \na\nn\nd\n \nM\nu\nl\nt\ni\nl\ni\nn\ng\nu\na\nl\n \nS\nT\n,\n \na\nn\nd\n \nt\nh\ne\ni\nr\n \no\nb\nj\ne\nc\nt\ni\nv\ne\ns\n \n[\n'\nf\ni\nn\nd\ni\nn\ng\ns\n_\no\nf\n_\nt\nh\ne\n_\ni\nw\ns\nl\nt\n_\n2\n0\n2\n2\n_\ne\nv\na\nl\nu\na\nt\ni\no\nn\n_\nc\na\nm\np\na\ni\ng\nn\n'\n]\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \ne\nv\no\nl\nu\nt\ni\no\nn\n \no\nf\n \ns\nh\na\nr\ne\nd\n \nt\na\ns\nk\ns\n \no\nv\ne\nr\n \nt\nh\ne\n \ny\ne\na\nr\ns\n \na\nn\nd\n \nt\nh\ne\ni\nr\n \nf\no\nc\nu\ns\n \no\nn\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nS\nT\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \nv\na\nr\ni\ne\nt\ny\n \no\nf\n \ns\nh\na\nr\ne\nd\n \nt\na\ns\nk\ns\n \ni\nn\n \nI\nW\nS\nL\nT\n \na\nn\nd\n \nt\nh\ne\ni\nr\n \nf\no\nc\nu\ns\n \no\nn\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nS\nT\n,\n \ns\nu\nc\nh\n \na\ns\n \ns\ni\nm\nu\nl\nt\na\nn\ne\ni\nt\ny\n,\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \ns\nc\ne\nn\na\nr\ni\no\ns\n,\n \na\nn\nd\n \nm\nu\nl\nt\ni\nl\ni\nn\ng\nu\na\nl\ni\nt\ny\n \n[\n'\nf\ni\nn\nd\ni\nn\ng\ns\n_\no\nf\n_\nt\nh\ne\n_\ni\nw\ns\nl\nt\n_\n2\n0\n2\n2\n_\ne\nv\na\nl\nu\na\nt\ni\no\nn\n_\nc\na\nm\np\na\ni\ng\nn\n'\n]\n.\n \nE\nv\na\nl\nu\na\nt\ne\n \nh\no\nw\n \nt\nh\ne\ns\ne\n \ns\nh\na\nr\ne\nd\n \nt\na\ns\nk\ns\n \nc\no\nn\nt\nr\ni\nb\nu\nt\ne\n \nt\no\n \na\nd\nd\nr\ne\ns\ns\ni\nn\ng\n \ns\np\ne\nc\ni\nf\ni\nc\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \na\nd\nv\na\nn\nc\ni\nn\ng\n \nr\ne\ns\ne\na\nr\nc\nh\n \ni\nn\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \nd\ni\nm\ne\nn\ns\ni\no\nn\ns\n \no\nf\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \n[\n'\nf\ni\nn\nd\ni\nn\ng\ns\n_\no\nf\n_\nt\nh\ne\n_\ni\nw\ns\nl\nt\n_\n2\n0\n2\n2\n_\ne\nv\na\nl\nu\na\nt\ni\no\nn\n_\nc\na\nm\np\na\ni\ng\nn\n'\n]\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \ns\nh\na\nr\ne\nd\n \nt\na\ns\nk\ns\n \ni\nn\n \nI\nW\nS\nL\nT\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ni\nm\np\na\nc\nt\n \na\nn\nd\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \no\nf\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \nI\nW\nS\nL\nT\n \ns\nh\na\nr\ne\nd\n \nt\na\ns\nk\ns\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nt\na\ns\nk\n \nd\ne\ns\ni\ng\nn\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nS\nh\na\nr\ne\nd\n \nT\na\ns\nk\n'\n,\n \n'\nF\no\nc\nu\ns\n \nA\nr\ne\na\n'\n,\n \n'\nO\nb\nj\ne\nc\nt\ni\nv\ne\ns\n'\n,\n \n'\nI\nm\np\na\nc\nt\n \no\nn\n \nS\nT\n \nR\ne\ns\ne\na\nr\nc\nh\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \ns\nh\na\nr\ne\nd\n \nt\na\ns\nk\ns\n \ni\nn\n \nI\nW\nS\nL\nT\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \ns\nh\na\nr\ne\nd\n \nt\na\ns\nk\ns\n \ni\nn\n \nI\nW\nS\nL\nT\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \ns\nh\na\nr\ne\nd\n \nt\na\ns\nk\ns\n \ni\nn\n \nI\nW\nS\nL\nT\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nd\ne\ns\ni\ng\nn\n \na\nn\nd\n \nc\no\nm\nm\nu\nn\ni\nt\ny\n \nb\ne\nn\nc\nh\nm\na\nr\nk\ni\nn\ng\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n#\n \n6\n.\n2\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n \nM\ne\nt\nr\ni\nc\ns\n \ni\nn\n \nS\np\ne\ne\nc\nh\n \nT\nr\na\nn\ns\nl\na\nt\ni\no\nn\n\n\nD\ne\nt\na\ni\nl\n \nc\no\nm\nm\no\nn\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n \nu\ns\ne\nd\n \ni\nn\n \nS\nT\n \nr\ne\ns\ne\na\nr\nc\nh\n \na\nn\nd\n \nI\nW\nS\nL\nT\n \n[\n'\nf\ni\nn\nd\ni\nn\ng\ns\n_\no\nf\n_\nt\nh\ne\n_\ni\nw\ns\nl\nt\n_\n2\n0\n2\n2\n_\ne\nv\na\nl\nu\na\nt\ni\no\nn\n_\nc\na\nm\np\na\ni\ng\nn\n'\n]\n.\n \nE\nx\nt\nr\na\nc\nt\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \na\nb\no\nu\nt\n \nm\ne\nt\nr\ni\nc\ns\n \nl\ni\nk\ne\n \nB\nL\nE\nU\n,\n \nT\nE\nR\n,\n \na\nn\nd\n \nM\nE\nT\nE\nO\nR\n,\n \na\nn\nd\n \nt\nh\ne\ni\nr\n \nr\ne\nl\ne\nv\na\nn\nc\ne\n \ni\nn\n \na\ns\ns\ne\ns\ns\ni\nn\ng\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nq\nu\na\nl\ni\nt\ny\n \na\nn\nd\n \ns\ny\ns\nt\ne\nm\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \n[\n'\nf\ni\nn\nd\ni\nn\ng\ns\n_\no\nf\n_\nt\nh\ne\n_\ni\nw\ns\nl\nt\n_\n2\n0\n2\n2\n_\ne\nv\na\nl\nu\na\nt\ni\no\nn\n_\nc\na\nm\np\na\ni\ng\nn\n'\n]\n.\n \nN\no\nt\ne\n \na\nn\ny\n \nd\ni\ns\nc\nu\ns\ns\ni\no\nn\ns\n \no\nn\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \no\nf\n \nc\nu\nr\nr\ne\nn\nt\n \nm\ne\nt\nr\ni\nc\ns\n \na\nn\nd\n \np\nr\no\np\no\ns\na\nl\ns\n \nf\no\nr\n \ni\nm\np\nr\no\nv\ne\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nh\no\nd\ns\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nD\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n \nc\no\nm\nm\no\nn\nl\ny\n \nu\ns\ne\nd\n \ni\nn\n \nS\nT\n,\n \ns\nu\nc\nh\n \na\ns\n \nB\nL\nE\nU\n,\n \nT\nE\nR\n,\n \na\nn\nd\n \nM\nE\nT\nE\nO\nR\n,\n \na\nn\nd\n \nt\nh\ne\ni\nr\n \nr\ne\nl\ne\nv\na\nn\nc\ne\n \ni\nn\n \na\ns\ns\ne\ns\ns\ni\nn\ng\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nq\nu\na\nl\ni\nt\ny\n \na\nn\nd\n \ns\ny\ns\nt\ne\nm\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \n[\n'\nf\ni\nn\nd\ni\nn\ng\ns\n_\no\nf\n_\nt\nh\ne\n_\ni\nw\ns\nl\nt\n_\n2\n0\n2\n2\n_\ne\nv\na\nl\nu\na\nt\ni\no\nn\n_\nc\na\nm\np\na\ni\ng\nn\n'\n]\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ns\nt\nr\ne\nn\ng\nt\nh\ns\n \na\nn\nd\n \nw\ne\na\nk\nn\ne\ns\ns\ne\ns\n \no\nf\n \nt\nh\ne\ns\ne\n \nm\ne\nt\nr\ni\nc\ns\n \ni\nn\n \nt\nh\ne\n \nc\no\nn\nt\ne\nx\nt\n \no\nf\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \n[\n'\nf\ni\nn\nd\ni\nn\ng\ns\n_\no\nf\n_\nt\nh\ne\n_\ni\nw\ns\nl\nt\n_\n2\n0\n2\n2\n_\ne\nv\na\nl\nu\na\nt\ni\no\nn\n_\nc\na\nm\np\na\ni\ng\nn\n'\n]\n.\n \nE\nv\na\nl\nu\na\nt\ne\n \nt\nh\ne\n \na\np\np\nr\no\np\nr\ni\na\nt\ne\nn\ne\ns\ns\n \no\nf\n \nc\nu\nr\nr\ne\nn\nt\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n \na\nn\nd\n \nd\ni\ns\nc\nu\ns\ns\n \np\no\nt\ne\nn\nt\ni\na\nl\n \ni\nm\np\nr\no\nv\ne\nm\ne\nn\nt\ns\n \no\nr\n \na\nl\nt\ne\nr\nn\na\nt\ni\nv\ne\n \nm\ne\nt\nr\ni\nc\ns\n \nf\no\nr\n \na\n \nm\no\nr\ne\n \nc\no\nm\np\nr\ne\nh\ne\nn\ns\ni\nv\ne\n \na\ns\ns\ne\ns\ns\nm\ne\nn\nt\n \no\nf\n \nS\nT\n \ns\ny\ns\nt\ne\nm\ns\n \n[\n'\nf\ni\nn\nd\ni\nn\ng\ns\n_\no\nf\n_\nt\nh\ne\n_\ni\nw\ns\nl\nt\n_\n2\n0\n2\n2\n_\ne\nv\na\nl\nu\na\nt\ni\no\nn\n_\nc\na\nm\np\na\ni\ng\nn\n'\n]\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n \ni\nn\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ns\nt\nr\ne\nn\ng\nt\nh\ns\n \na\nn\nd\n \nw\ne\na\nk\nn\ne\ns\ns\ne\ns\n \no\nf\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nm\ne\nt\nr\ni\nc\n \np\nr\no\np\ne\nr\nt\ni\ne\ns\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nf\no\nc\nu\ns\ne\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nM\ne\nt\nr\ni\nc\n'\n,\n \n'\nA\ns\np\ne\nc\nt\ns\n \nA\ns\ns\ne\ns\ns\ne\nd\n'\n,\n \n'\nS\nt\nr\ne\nn\ng\nt\nh\ns\n'\n,\n \n'\nW\ne\na\nk\nn\ne\ns\ns\ne\ns\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n \ni\nn\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n \ni\nn\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n \ni\nn\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \nn\na\nt\nu\nr\na\nl\n \nl\na\nn\ng\nu\na\ng\ne\n \np\nr\no\nc\ne\ns\ns\ni\nn\ng\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \na\nn\nd\n \nh\nu\nm\na\nn\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nh\no\nd\ns\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n \n7\n.\n \nD\ni\nr\ne\nc\nt\n \nS\nT\n \ni\nn\n \nL\no\nw\n-\nR\ne\ns\no\nu\nr\nc\ne\n \nS\ne\nt\nt\ni\nn\ng\ns\n\n\nA\n \nc\nr\ni\nt\ni\nc\na\nl\n \na\nr\ne\na\n \nw\ni\nt\nh\ni\nn\n \nS\nT\n \ni\ns\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \ns\ne\nt\nt\ni\nn\ng\ns\n,\n \nw\nh\ni\nc\nh\n \nw\ne\n \na\nd\nd\nr\ne\ns\ns\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \na\nn\na\nl\ny\nz\ni\nn\ng\n \nt\nh\ne\n \np\na\nr\nt\ni\nc\nu\nl\na\nr\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \no\nf\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \ni\nn\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \ns\ne\nt\nt\ni\nn\ng\ns\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \np\na\np\ne\nr\ns\n \nt\nh\na\nt\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \na\nd\nd\nr\ne\ns\ns\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \ni\nn\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \ns\ne\nt\nt\ni\nn\ng\ns\n.\n \nE\nx\nt\nr\na\nc\nt\n \nt\nh\ne\n \ns\np\ne\nc\ni\nf\ni\nc\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ne\nd\n \nf\no\nr\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \nl\na\nn\ng\nu\na\ng\ne\ns\n,\n \ns\nu\nc\nh\n \na\ns\n \nd\na\nt\na\n \ns\nc\na\nr\nc\ni\nt\ny\n \na\nn\nd\n \nl\na\nn\ng\nu\na\ng\ne\n-\ns\np\ne\nc\ni\nf\ni\nc\n \na\nc\no\nu\ns\nt\ni\nc\n \no\nr\n \nl\ni\nn\ng\nu\ni\ns\nt\ni\nc\n \np\nr\no\np\ne\nr\nt\ni\ne\ns\n.\n \nN\no\nt\ne\n \nt\nh\ne\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \np\nr\no\np\no\ns\ne\nd\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ne\nd\n \nt\no\n \ni\nm\np\nr\no\nv\ne\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \ni\nn\n \nt\nh\ne\ns\ne\n \ns\ne\nt\nt\ni\nn\ng\ns\n,\n \ni\nn\nc\nl\nu\nd\ni\nn\ng\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n,\n \nt\nr\na\nn\ns\nf\ne\nr\n \nl\ne\na\nr\nn\ni\nn\ng\n,\n \nd\na\nt\na\n \na\nu\ng\nm\ne\nn\nt\na\nt\ni\no\nn\n,\n \na\nn\nd\n \ns\np\ne\nc\ni\na\nl\ni\nz\ne\nd\n \nm\ne\nt\nh\no\nd\ns\n \nf\no\nr\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \ns\nc\ne\nn\na\nr\ni\no\ns\n \n[\n'\np\nr\ne\n_\nt\nr\na\ni\nn\ni\nn\ng\n_\no\nn\n_\nh\ni\ng\nh\n_\nr\ne\ns\no\nu\nr\nc\ne\n_\ns\np\ne\ne\nc\nh\n_\nr\ne\nc\no\ng\nn\ni\nt\ni\no\nn\n_\ni\nm\np\nr\no\nv\ne\ns\n_\nl\no\nw\n_\nr\ne\ns\no\nu\nr\nc\ne\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n,\n \n'\na\nn\na\nl\ny\nz\ni\nn\ng\n_\na\ns\nr\n_\np\nr\ne\nt\nr\na\ni\nn\ni\nn\ng\n_\nf\no\nr\n_\nl\no\nw\n_\nr\ne\ns\no\nu\nr\nc\ne\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n,\n \n'\nr\ne\nv\ni\ns\ni\nt\ni\nn\ng\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nf\nr\no\nm\n_\ns\nc\nr\na\nt\nc\nh\n'\n]\n.\n \nE\nx\nt\nr\na\nc\nt\n \nd\na\nt\na\ns\ne\nt\ns\n \nu\ns\ne\nd\n \nf\no\nr\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \na\nn\nd\n \nd\na\nt\na\n \na\nu\ng\nm\ne\nn\nt\na\nt\ni\no\nn\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \nu\ns\ne\nd\n \ni\nn\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \nS\nT\n,\n \na\nn\nd\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\ni\nr\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \n[\n'\na\nn\na\nl\ny\nz\ni\nn\ng\n_\na\ns\nr\n_\np\nr\ne\nt\nr\na\ni\nn\ni\nn\ng\n_\nf\no\nr\n_\nl\no\nw\n_\nr\ne\ns\no\nu\nr\nc\ne\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n]\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nA\n \nc\nr\ni\nt\ni\nc\na\nl\n \na\nr\ne\na\n \nw\ni\nt\nh\ni\nn\n \nS\nT\n \ni\ns\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \ns\ne\nt\nt\ni\nn\ng\ns\n,\n \nw\nh\ni\nc\nh\n \nw\ne\n \na\nd\nd\nr\ne\ns\ns\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \na\nn\na\nl\ny\nz\ni\nn\ng\n \nt\nh\ne\n \np\na\nr\nt\ni\nc\nu\nl\na\nr\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \no\nf\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \ni\nn\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \ns\ne\nt\nt\ni\nn\ng\ns\n \na\nn\nd\n \nc\no\nm\np\na\nr\ne\n \nt\nh\ne\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \no\nf\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \n(\np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n,\n \nt\nr\na\nn\ns\nf\ne\nr\n \nl\ne\na\nr\nn\ni\nn\ng\n,\n \ns\ne\nl\nf\n-\ns\nu\np\ne\nr\nv\ni\ns\ni\no\nn\n,\n \nd\na\nt\na\n \na\nu\ng\nm\ne\nn\nt\na\nt\ni\no\nn\n)\n \ni\nn\n \na\nd\nd\nr\ne\ns\ns\ni\nn\ng\n \nd\na\nt\na\n \ns\nc\na\nr\nc\ni\nt\ny\n \na\nn\nd\n \ni\nm\np\nr\no\nv\ni\nn\ng\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \nf\no\nr\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \nl\na\nn\ng\nu\na\ng\ne\ns\n \n[\n'\np\nr\ne\n_\nt\nr\na\ni\nn\ni\nn\ng\n_\no\nn\n_\nh\ni\ng\nh\n_\nr\ne\ns\no\nu\nr\nc\ne\n_\ns\np\ne\ne\nc\nh\n_\nr\ne\nc\no\ng\nn\ni\nt\ni\no\nn\n_\ni\nm\np\nr\no\nv\ne\ns\n_\nl\no\nw\n_\nr\ne\ns\no\nu\nr\nc\ne\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n,\n \n'\na\nn\na\nl\ny\nz\ni\nn\ng\n_\na\ns\nr\n_\np\nr\ne\nt\nr\na\ni\nn\ni\nn\ng\n_\nf\no\nr\n_\nl\no\nw\n_\nr\ne\ns\no\nu\nr\nc\ne\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n,\n \n'\nr\ne\nv\ni\ns\ni\nt\ni\nn\ng\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nf\nr\no\nm\n_\ns\nc\nr\na\nt\nc\nh\n'\n]\n.\n \nC\no\nm\np\na\nr\ne\n \nt\nh\ne\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \ni\nm\np\nr\no\nv\ne\nm\ne\nn\nt\ns\n \na\nc\nh\ni\ne\nv\ne\nd\n \nb\ny\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \ni\nn\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \nS\nT\n \ns\nc\ne\nn\na\nr\ni\no\ns\n.\n \nD\ni\ns\nc\nu\ns\ns\n \nt\nh\ne\n \nr\ne\nm\na\ni\nn\ni\nn\ng\n \ng\na\np\ns\n \ni\nn\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \na\nn\nd\n \nt\nh\ne\n \nf\nu\nt\nu\nr\ne\n \nr\ne\ns\ne\na\nr\nc\nh\n \nd\ni\nr\ne\nc\nt\ni\no\nn\ns\n \nn\ne\ne\nd\ne\nd\n \nt\no\n \nf\nu\nr\nt\nh\ne\nr\n \na\nd\nv\na\nn\nc\ne\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nt\nh\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \ni\nn\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n.\n \nH\no\nw\n \nm\ni\ng\nh\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \nl\ni\nk\ne\n \nl\ni\nn\ng\nu\ni\ns\nt\ni\nc\ns\n \n(\nf\no\nr\n \nl\na\nn\ng\nu\na\ng\ne\n-\ns\np\ne\nc\ni\nf\ni\nc\n \np\nr\no\np\ne\nr\nt\ni\ne\ns\n)\n,\n \ns\nt\na\nt\ni\ns\nt\ni\nc\ns\n \n(\nf\no\nr\n \nd\na\nt\na\n \na\nu\ng\nm\ne\nn\nt\na\nt\ni\no\nn\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n)\n,\n \no\nr\n \nt\nr\na\nn\ns\nf\ne\nr\n \nl\ne\na\nr\nn\ni\nn\ng\n \n(\nf\no\nr\n \nc\nr\no\ns\ns\n-\nl\ni\nn\ng\nu\na\nl\n \nk\nn\no\nw\nl\ne\nd\ng\ne\n \nt\nr\na\nn\ns\nf\ne\nr\n)\n \no\nf\nf\ne\nr\n \nn\ne\nw\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\ns\n \no\nr\n \ns\no\nl\nu\nt\ni\no\nn\ns\n?\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \ni\nn\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \ns\ne\nt\nt\ni\nn\ng\ns\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \no\nf\n \nd\ni\nf\nf\ne\nr\ne\nn\nt\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \nf\no\nr\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \nS\nT\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \ns\nc\ne\nn\na\nr\ni\no\ns\n,\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \na\np\np\nl\ni\ne\nd\n,\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nr\ni\nc\ns\n.\n \nW\nh\na\nt\n \na\nr\ne\n \nt\nh\ne\n \nt\nr\na\nd\ne\n-\no\nf\nf\ns\n \n(\ne\n.\ng\n.\n,\n \nc\no\nm\np\nu\nt\na\nt\ni\no\nn\na\nl\n \nc\no\ns\nt\n \nv\ns\n.\n \np\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \ng\na\ni\nn\n,\n \nd\na\nt\na\n \na\nu\ng\nm\ne\nn\nt\na\nt\ni\no\nn\n \ne\nf\nf\ne\nc\nt\ni\nv\ne\nn\ne\ns\ns\n \nv\ns\n.\n \nl\na\nn\ng\nu\na\ng\ne\n \ns\np\ne\nc\ni\nf\ni\nc\ni\nt\ny\n)\n \na\ns\ns\no\nc\ni\na\nt\ne\nd\n \nw\ni\nt\nh\n \ne\na\nc\nh\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\n \ni\nn\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \ns\ne\nt\nt\ni\nn\ng\ns\n?\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n,\n \ns\nu\nc\nh\n \na\ns\n \n'\nL\no\nw\n-\nR\ne\ns\no\nu\nr\nc\ne\n \nC\nh\na\nl\nl\ne\nn\ng\ne\n'\n,\n \n'\nP\nr\no\np\no\ns\ne\nd\n \nS\no\nl\nu\nt\ni\no\nn\n'\n,\n \n'\nT\ne\nc\nh\nn\ni\nq\nu\ne\n'\n,\n \n'\nP\ne\nr\nf\no\nr\nm\na\nn\nc\ne\n \nI\nm\np\nr\no\nv\ne\nm\ne\nn\nt\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n,\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \ni\nn\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \ns\ne\nt\nt\ni\nn\ng\ns\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \ni\nn\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \ns\ne\nt\nt\ni\nn\ng\ns\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n \ni\nn\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \ns\ne\nt\nt\ni\nn\ng\ns\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \nl\ni\nn\ng\nu\ni\ns\nt\ni\nc\ns\n,\n \nc\nr\no\ns\ns\n-\nl\ni\nn\ng\nu\na\nl\n \nt\nr\na\nn\ns\nf\ne\nr\n,\n \na\nn\nd\n \nf\ne\nw\n-\ns\nh\no\nt\n \nl\ne\na\nr\nn\ni\nn\ng\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n.\n\n\n\n\n#\n#\n \n8\n.\n \nF\nu\nt\nu\nr\ne\n \nD\ni\nr\ne\nc\nt\ni\no\nn\ns\n \na\nn\nd\n \nO\np\ne\nn\n \nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n\n\nL\no\no\nk\ni\nn\ng\n \na\nh\ne\na\nd\n \nf\nr\no\nm\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \ns\ne\nt\nt\ni\nn\ng\ns\n,\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\ns\n \nf\nu\nt\nu\nr\ne\n \nd\ni\nr\ne\nc\nt\ni\no\nn\ns\n \na\nn\nd\n \no\np\ne\nn\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n,\n \ns\ny\nn\nt\nh\ne\ns\ni\nz\ni\nn\ng\n \nt\nh\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \no\np\ne\nn\n \nq\nu\ne\ns\nt\ni\no\nn\ns\n \nf\na\nc\ni\nn\ng\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n \nr\ne\ns\ne\na\nr\nc\nh\n \ni\nn\n \ng\ne\nn\ne\nr\na\nl\n,\n \nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \nt\nh\ne\ns\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \ni\nn\nt\no\n \na\nr\ne\na\ns\n \nl\ni\nk\ne\n \nd\na\nt\na\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nm\no\nd\ne\nl\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n \ni\nm\np\nr\no\nv\ne\nm\ne\nn\nt\ns\n,\n \nt\nr\na\ni\nn\ni\nn\ng\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\n \nr\ne\nf\ni\nn\ne\nm\ne\nn\nt\ns\n,\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ny\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \np\na\np\ne\nr\ns\n \nt\nh\na\nt\n \nd\ni\ns\nc\nu\ns\ns\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \no\np\ne\nn\n \nr\ne\ns\ne\na\nr\nc\nh\n \nq\nu\ne\ns\nt\ni\no\nn\ns\n,\n \na\nn\nd\n \nf\nu\nt\nu\nr\ne\n \nd\ni\nr\ne\nc\nt\ni\no\nn\ns\n \ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n \nr\ne\ns\ne\na\nr\nc\nh\n \n[\n'\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\na\nn\nd\n_\nt\nh\ne\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\np\nr\no\nm\ni\ns\ne\n_\nt\na\nk\ni\nn\ng\n_\ns\nt\no\nc\nk\n_\no\nf\n_\nw\nh\ne\nr\ne\n_\nw\ne\n_\na\nr\ne\n'\n,\n \n'\nr\ne\nv\ni\ns\ni\nt\ni\nn\ng\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nf\nr\no\nm\n_\ns\nc\nr\na\nt\nc\nh\n'\n,\n \n'\ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n_\np\nh\nr\na\ns\ne\n_\nb\na\ns\ne\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n]\n.\n \nE\nx\nt\nr\na\nc\nt\n \nm\ne\nn\nt\ni\no\nn\ns\n \no\nf\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \na\nn\nd\n \np\no\nt\ne\nn\nt\ni\na\nl\n \na\nv\ne\nn\nu\ne\ns\n \nf\no\nr\n \nf\nu\nt\nu\nr\ne\n \nw\no\nr\nk\n.\n \nN\no\nt\ne\n \na\nn\ny\n \ns\np\ne\nc\ni\nf\ni\nc\n \ns\nu\ng\ng\ne\ns\nt\ni\no\nn\ns\n \nf\no\nr\n \na\nd\nd\nr\ne\ns\ns\ni\nn\ng\n \nc\nu\nr\nr\ne\nn\nt\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \no\nr\n \ne\nx\np\nl\no\nr\ni\nn\ng\n \nn\ne\nw\n \nr\ne\ns\ne\na\nr\nc\nh\n \nd\ni\nr\ne\nc\nt\ni\no\nn\ns\n.\n \nE\nx\nt\nr\na\nc\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nr\ne\ns\nu\nl\nt\ns\n \n(\ne\n.\ng\n.\n,\n \nW\nE\nR\n,\n \nB\nL\nE\nU\n \ns\nc\no\nr\ne\ns\n,\n \nc\no\nm\np\na\nr\na\nt\ni\nv\ne\n \nt\na\nb\nl\ne\ns\n)\n,\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \na\n \n\"\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n\"\n \n(\nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \ns\nh\no\nr\nt\nc\no\nm\ni\nn\ng\ns\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nD\na\nt\na\n,\n \nM\ne\nt\nh\no\nd\no\nl\no\ng\ny\n,\n \nA\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n,\n \nE\nv\na\nl\nu\na\nt\ni\no\nn\n,\n \na\nn\nd\n \nG\ne\nn\ne\nr\na\nl\ni\nz\na\nb\ni\nl\ni\nt\ny\n)\n \nf\nr\no\nm\n \ne\na\nc\nh\n \np\na\np\ne\nr\n.\n\n\nL\no\no\nk\ni\nn\ng\n \na\nh\ne\na\nd\n,\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\ns\n \nf\nu\nt\nu\nr\ne\n \nd\ni\nr\ne\nc\nt\ni\no\nn\ns\n \na\nn\nd\n \no\np\ne\nn\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n,\n \ns\ny\nn\nt\nh\ne\ns\ni\nz\ni\nn\ng\n \nt\nh\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \no\np\ne\nn\n \nq\nu\ne\ns\nt\ni\no\nn\ns\n \nf\na\nc\ni\nn\ng\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \nS\nT\n \nr\ne\ns\ne\na\nr\nc\nh\n \ni\nn\n \ng\ne\nn\ne\nr\na\nl\n,\n \nc\na\nt\ne\ng\no\nr\ni\nz\ni\nn\ng\n \nt\nh\ne\ns\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \ni\nn\nt\no\n \na\nr\ne\na\ns\n \nl\ni\nk\ne\n \nd\na\nt\na\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nm\no\nd\ne\nl\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n \ni\nm\np\nr\no\nv\ne\nm\ne\nn\nt\ns\n,\n \nt\nr\na\ni\nn\ni\nn\ng\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\n \nr\ne\nf\ni\nn\ne\nm\ne\nn\nt\ns\n,\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ny\n.\n \nD\ni\ns\nc\nu\ns\ns\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nf\nu\nt\nu\nr\ne\n \nr\ne\ns\ne\na\nr\nc\nh\n \nd\ni\nr\ne\nc\nt\ni\no\nn\ns\n \nt\nh\na\nt\n \nc\no\nu\nl\nd\n \na\nd\nd\nr\ne\ns\ns\n \nt\nh\ne\ns\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n,\n \ns\nu\nc\nh\n \na\ns\n \ne\nx\np\nl\no\nr\ni\nn\ng\n \nn\no\nv\ne\nl\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \ns\nt\nr\na\nt\ne\ng\ni\ne\ns\n,\n \nd\ne\nv\ne\nl\no\np\ni\nn\ng\n \nm\no\nr\ne\n \nr\no\nb\nu\ns\nt\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n,\n \nl\ne\nv\ne\nr\na\ng\ni\nn\ng\n \nm\nu\nl\nt\ni\nm\no\nd\na\nl\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n,\n \na\nn\nd\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \ni\nm\np\nr\no\nv\ni\nn\ng\n \nl\no\nw\n-\nr\ne\ns\no\nu\nr\nc\ne\n \nS\nT\n.\n \nE\nn\nc\no\nu\nr\na\ng\ne\n \np\nr\no\np\no\ns\ni\nn\ng\n \na\nc\nt\ni\no\nn\na\nb\nl\ne\n \na\nn\nd\n \ni\nn\nn\no\nv\na\nt\ni\nv\ne\n \ns\no\nl\nu\nt\ni\no\nn\ns\n \nt\no\n \nt\nh\ne\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \ni\nn\n \np\nr\ne\nv\ni\no\nu\ns\n \ns\ne\nc\nt\ni\no\nn\ns\n,\n \np\no\nt\ne\nn\nt\ni\na\nl\nl\ny\n \na\nd\no\np\nt\ni\nn\ng\n \nm\ne\nt\nh\no\nd\ns\n \nf\nr\no\nm\n \no\nt\nh\ne\nr\n \nf\ni\ne\nl\nd\ns\n \no\nr\n \nl\ne\nv\ne\nr\na\ng\ni\nn\ng\n \ns\nu\nc\nc\ne\ns\ns\nf\nu\nl\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n \nf\nr\no\nm\n \nt\nh\ne\n \nh\ni\ns\nt\no\nr\ny\n \no\nf\n \ns\np\ne\ne\nc\nh\n \na\nn\nd\n \nl\na\nn\ng\nu\na\ng\ne\n \np\nr\no\nc\ne\ns\ns\ni\nn\ng\n \n[\n'\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\na\nn\nd\n_\nt\nh\ne\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\np\nr\no\nm\ni\ns\ne\n_\nt\na\nk\ni\nn\ng\n_\ns\nt\no\nc\nk\n_\no\nf\n_\nw\nh\ne\nr\ne\n_\nw\ne\n_\na\nr\ne\n'\n,\n \n'\nr\ne\nv\ni\ns\ni\nt\ni\nn\ng\n_\ne\nn\nd\n_\nt\no\n_\ne\nn\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\no\n_\nt\ne\nx\nt\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n_\nf\nr\no\nm\n_\ns\nc\nr\na\nt\nc\nh\n'\n,\n \n'\ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n_\np\nh\nr\na\ns\ne\n_\nb\na\ns\ne\nd\n_\ns\np\ne\ne\nc\nh\n_\nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n'\n]\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n,\n \nw\nh\na\nt\n \ns\np\ne\nc\ni\nf\ni\nc\n \na\nn\nd\n \na\nc\nt\ni\no\nn\na\nb\nl\ne\n \nr\ne\ns\ne\na\nr\nc\nh\n \nd\ni\nr\ne\nc\nt\ni\no\nn\ns\n \nc\na\nn\n \nb\ne\n \np\nr\no\np\no\ns\ne\nd\n?\n \nE\nn\ns\nu\nr\ne\n \nt\nh\na\nt\n \ne\na\nc\nh\n \np\nr\no\np\no\ns\ne\nd\n \nr\ne\ns\ne\na\nr\nc\nh\n \nd\ni\nr\ne\nc\nt\ni\no\nn\n \ni\ns\n \ns\np\ne\nc\ni\nf\ni\nc\n,\n \na\nc\nt\ni\no\nn\na\nb\nl\ne\n,\n \na\nn\nd\n \ni\nn\nc\nl\nu\nd\ne\ns\n \na\n \nc\nl\ne\na\nr\n \nr\na\nt\ni\no\nn\na\nl\ne\n \nf\no\nr\n \ni\nt\ns\n \np\no\nt\ne\nn\nt\ni\na\nl\n \ni\nm\np\na\nc\nt\n \no\nn\n \na\nd\nv\na\nn\nc\ni\nn\ng\n \nt\nh\ne\n \nf\ni\ne\nl\nd\n \no\nf\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n.\n \nA\nv\no\ni\nd\n \ng\ne\nn\ne\nr\na\nl\n \ns\nt\na\nt\ne\nm\ne\nn\nt\ns\n \na\nn\nd\n \ni\nn\ns\nt\ne\na\nd\n \nf\no\nc\nu\ns\n \no\nn\n \nc\no\nn\nc\nr\ne\nt\ne\n \nr\ne\ns\ne\na\nr\nc\nh\n \na\ng\ne\nn\nd\na\ns\n \na\nn\nd\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n.\n \nP\nr\no\np\no\ns\ne\n \ni\nn\nn\no\nv\na\nt\ni\nv\ne\n \ns\no\nl\nu\nt\ni\no\nn\ns\n \nb\ny\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nt\nh\ne\n \na\nd\no\np\nt\ni\no\nn\n \no\nf\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n \nf\nr\no\nm\n \no\nt\nh\ne\nr\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \nc\no\nm\np\nu\nt\ne\nr\n \nv\ni\ns\ni\no\nn\n,\n \nN\nL\nP\n,\n \nc\no\ng\nn\ni\nt\ni\nv\ne\n \ns\nc\ni\ne\nn\nc\ne\n,\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nt\nh\ne\no\nr\ny\n,\n \na\nn\nd\n \nc\no\nn\nt\nr\no\nl\n \nt\nh\ne\no\nr\ny\n,\n \na\nm\no\nn\ng\n \no\nt\nh\ne\nr\ns\n.\n \nF\no\nr\n \ne\na\nc\nh\n \np\nr\no\np\no\ns\ne\nd\n \ns\no\nl\nu\nt\ni\no\nn\n,\n \nb\nr\ni\ne\nf\nl\ny\n \ne\nx\np\nl\na\ni\nn\n \nh\no\nw\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n \nf\nr\no\nm\n \nt\nh\ne\ns\ne\n \nf\ni\ne\nl\nd\ns\n \nc\na\nn\n \nb\ne\n \na\nd\na\np\nt\ne\nd\n \no\nr\n \na\np\np\nl\ni\ne\nd\n \nt\no\n \na\nd\nd\nr\ne\ns\ns\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \na\nn\nd\n \nd\ni\ns\nc\nu\ns\ns\n \na\nt\n \nl\ne\na\ns\nt\n \no\nn\ne\n \ns\nu\nc\nc\ne\ns\ns\nf\nu\nl\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ny\n \no\nr\n \np\na\nr\na\nd\ni\ng\nm\n \ns\nh\ni\nf\nt\n \nf\nr\no\nm\n \nt\nh\ne\n \nh\ni\ns\nt\no\nr\ny\n \no\nf\n \ns\np\ne\ne\nc\nh\n \na\nn\nd\n \nl\na\nn\ng\nu\na\ng\ne\n \np\nr\no\nc\ne\ns\ns\ni\nn\ng\n \n(\ne\n.\ng\n.\n,\n \nH\ni\nd\nd\ne\nn\n \nM\na\nr\nk\no\nv\n \nM\no\nd\ne\nl\ns\n,\n \nn\n-\ng\nr\na\nm\n \nl\na\nn\ng\nu\na\ng\ne\n \nm\no\nd\ne\nl\ns\n,\n \nr\nu\nl\ne\n-\nb\na\ns\ne\nd\n \ns\ny\ns\nt\ne\nm\ns\n)\n \nt\nh\na\nt\n,\n \ni\nn\n \na\n \nm\no\nd\ne\nr\nn\n \na\nd\na\np\nt\na\nt\ni\no\nn\n \no\nr\n \nh\ny\nb\nr\ni\nd\n \na\np\np\nr\no\na\nc\nh\n,\n \nc\no\nu\nl\nd\n \no\nf\nf\ne\nr\n \nn\no\nv\ne\nl\n \ns\no\nl\nu\nt\ni\no\nn\ns\n \nt\no\n \nc\nu\nr\nr\ne\nn\nt\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \nS\nT\n.\n \nE\nx\np\nl\na\ni\nn\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nb\ne\nn\ne\nf\ni\nt\ns\n \na\nn\nd\n \na\nd\na\np\nt\na\nt\ni\no\nn\ns\n \nr\ne\nq\nu\ni\nr\ne\nd\n \nf\no\nr\n \ns\nu\nc\nh\n \na\n \nr\ne\nv\ni\nv\na\nl\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \np\na\np\ne\nr\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nf\nu\nt\nu\nr\ne\n \nd\ni\nr\ne\nc\nt\ni\no\nn\ns\n \na\nn\nd\n \no\np\ne\nn\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \ns\np\ne\ne\nc\nh\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nf\nu\nt\nu\nr\ne\n \nr\ne\ns\ne\na\nr\nc\nh\n \nd\ni\nr\ne\nc\nt\ni\no\nn\ns\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\ns\n \no\nn\n \nf\nu\nt\nu\nr\ne\n \nr\ne\ns\ne\na\nr\nc\nh\n \np\nr\ni\no\nr\ni\nt\ni\ne\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \np\na\np\ne\nr\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nf\no\nr\nm\na\nt\n,\n \nf\no\nc\nu\ns\ni\nn\ng\n \no\nn\n \n'\nI\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nC\nh\na\nl\nl\ne\nn\ng\ne\n'\n,\n \n'\nP\nr\no\np\no\ns\ne\nd\n \nF\nu\nt\nu\nr\ne\n \nD\ni\nr\ne\nc\nt\ni\no\nn\n'\n,\n \n'\nP\no\nt\ne\nn\nt\ni\na\nl\n \nS\no\nl\nu\nt\ni\no\nn\ns\n'\n,\n \n'\nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \no\nf\n \nC\nu\nr\nr\ne\nn\nt\n \nA\np\np\nr\no\na\nc\nh\ne\ns\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \ne\nx\nt\ne\nn\nt\n \nt\no\n \nw\nh\ni\nc\nh\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ne\n \ne\na\nc\nh\n \no\nt\nh\ne\nr\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nf\nu\nt\nu\nr\ne\n \nd\ni\nr\ne\nc\nt\ni\no\nn\ns\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nk\ne\ny\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nn\nf\nl\ni\nc\nt\n \no\nr\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \ni\nn\n \nf\ni\nn\nd\ni\nn\ng\ns\n,\n \na\nn\nd\n \nc\nr\ni\nt\ni\nc\na\nl\nl\ny\n \na\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \nc\no\nn\nc\nl\nu\nd\ni\nn\ng\n \np\na\nr\na\ng\nr\na\np\nh\n \nt\nh\na\nt\n \ns\np\ne\nc\ni\nf\ni\nc\na\nl\nl\ny\n \nd\ni\ns\nc\nu\ns\ns\ne\ns\n \np\no\nt\ne\nn\nt\ni\na\nl\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \na\np\np\nr\no\na\nc\nh\ne\ns\n \nt\no\n \na\nd\nd\nr\ne\ns\ns\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n.\n \nS\nu\ng\ng\ne\ns\nt\n \ne\nx\np\nl\no\nr\ni\nn\ng\n \nm\ne\nt\nh\no\nd\ns\n \nf\nr\no\nm\n \nf\ni\ne\nl\nd\ns\n \ns\nu\nc\nh\n \na\ns\n \nc\no\ng\nn\ni\nt\ni\nv\ne\n \ns\nc\ni\ne\nn\nc\ne\n \n(\nt\no\n \nb\ne\nt\nt\ne\nr\n \nm\no\nd\ne\nl\n \nh\nu\nm\na\nn\n \ns\np\ne\ne\nc\nh\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n)\n,\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nt\nh\ne\no\nr\ny\n \n(\nt\no\n \nq\nu\na\nn\nt\ni\nf\ny\n \na\nn\nd\n \nm\ni\nn\ni\nm\ni\nz\ne\n \ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n \nl\no\ns\ns\n)\n,\n \na\nn\nd\n \nr\no\nb\nu\ns\nt\n \no\np\nt\ni\nm\ni\nz\na\nt\ni\no\nn\n \n(\nt\no\n \na\nd\nd\nr\ne\ns\ns\n \ne\nr\nr\no\nn\ne\no\nu\ns\n \ne\na\nr\nl\ny\n \nd\ne\nc\ni\ns\ni\no\nn\ns\n)\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \nf\nu\nt\nu\nr\ne\n \nd\ni\nr\ne\nc\nt\ni\no\nn\ns\n \na\nn\nd\n \no\np\ne\nn\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \nf\nu\nt\nu\nr\ne\n \nd\ni\nr\ne\nc\nt\ni\no\nn\ns\n \na\nn\nd\n \no\np\ne\nn\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nv\na\nr\ni\no\nu\ns\n \nf\ni\ne\nl\nd\ns\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\ns\ne\n \ni\ns\ns\nu\ne\ns\n \na\nn\nd\n \np\no\nt\ne\nn\nt\ni\na\nl\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \ns\no\nl\nu\nt\ni\no\nn\ns\n.\n\n\n\n\n#\n#\n \n9\n.\n \nC\no\nn\nc\nl\nu\ns\ni\no\nn\n\n\nI\nn\n \nc\no\nn\nc\nl\nu\ns\ni\no\nn\n \nt\no\n \nt\nh\ni\ns\n \ns\nu\nr\nv\ne\ny\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\nd\nv\na\nn\nc\ne\nm\ne\nn\nt\ns\n \na\nn\nd\n \nt\nr\ne\nn\nd\ns\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \ns\np\ne\ne\nc\nh\n-\nt\no\n-\nt\ne\nx\nt\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ne\nd\n \nt\nh\nr\no\nu\ng\nh\no\nu\nt\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\n.\n \nR\ne\ni\nt\ne\nr\na\nt\ne\n \nt\nh\ne\n \nm\na\ni\nn\n \na\nd\nv\na\nn\nt\na\ng\ne\ns\n \no\nf\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \na\np\np\nr\no\na\nc\nh\ne\ns\n \na\nn\nd\n \nt\nh\ne\n \np\nr\no\ng\nr\ne\ns\ns\n \nm\na\nd\ne\n \ni\nn\n \na\nd\nd\nr\ne\ns\ns\ni\nn\ng\n \nt\nh\ne\ni\nr\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \nt\nh\ne\n \nr\ne\nc\nu\nr\nr\ni\nn\ng\n \nt\nh\ne\nm\ne\ns\n \na\nn\nd\n \nm\no\ns\nt\n \ni\nm\np\na\nc\nt\nf\nu\nl\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \ni\nn\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \np\na\np\ne\nr\ns\n.\n\n\nI\nn\n \nc\no\nn\nc\nl\nu\ns\ni\no\nn\n,\n \nt\nh\ni\ns\n \ns\nu\nr\nv\ne\ny\n \ns\nu\nm\nm\na\nr\ni\nz\ne\ns\n \nt\nh\ne\n \nk\ne\ny\n \na\nd\nv\na\nn\nc\ne\nm\ne\nn\nt\ns\n \na\nn\nd\n \nt\nr\ne\nn\nd\ns\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \ns\np\ne\ne\nc\nh\n-\nt\no\n-\nt\ne\nx\nt\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n,\n \np\nr\no\nv\ni\nd\ni\nn\ng\n \na\n \nc\no\nn\nc\nl\nu\nd\ni\nn\ng\n \no\nv\ne\nr\nv\ni\ne\nw\n \no\nf\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\n,\n \ne\nm\np\nh\na\ns\ni\nz\ni\nn\ng\n \nt\nh\ne\n \ns\ni\ng\nn\ni\nf\ni\nc\na\nn\nt\n \np\nr\no\ng\nr\ne\ns\ns\n \ni\nn\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \ns\np\ne\ne\nc\nh\n-\nt\no\n-\nt\ne\nx\nt\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nR\ne\ni\nt\ne\nr\na\nt\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\nd\nv\na\nn\nt\na\ng\ne\ns\n \no\nf\n \nd\ni\nr\ne\nc\nt\n \nm\no\nd\ne\nl\ns\n \na\nn\nd\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nm\na\nj\no\nr\n \na\nd\nv\na\nn\nc\ne\nm\ne\nn\nt\ns\n \ni\nn\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n,\n \nt\nr\na\ni\nn\ni\nn\ng\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n,\n \nd\na\nt\na\ns\ne\nt\ns\n,\n \na\nn\nd\n \ne\nv\na\nl\nu\na\nt\ni\no\nn\n \nm\ne\nt\nh\no\nd\no\nl\no\ng\ni\ne\ns\n.\n \nO\nf\nf\ne\nr\n \na\n \nf\ni\nn\na\nl\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n \no\nn\n \nt\nh\ne\n \nc\nu\nr\nr\ne\nn\nt\n \ns\nt\na\nt\ne\n \no\nf\n \nt\nh\ne\n \nf\ni\ne\nl\nd\n,\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \nt\nh\ne\n \nk\ne\ny\n \na\nc\nh\ni\ne\nv\ne\nm\ne\nn\nt\ns\n \na\nn\nd\n \nr\ne\nm\na\ni\nn\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n.\n \nS\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \nc\no\nn\nt\nr\ni\nb\nu\nt\ni\no\nn\ns\n \na\nn\nd\n \nb\nr\ne\na\nk\nt\nh\nr\no\nu\ng\nh\ns\n \ni\nn\n \na\nr\ne\na\ns\n \nl\ni\nk\ne\n \ne\nn\nd\n-\nt\no\n-\ne\nn\nd\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\ns\n,\n \np\nr\ne\n-\nt\nr\na\ni\nn\ni\nn\ng\n \nt\ne\nc\nh\nn\ni\nq\nu\ne\ns\n,\n \na\nn\nd\n \nd\na\nt\na\ns\ne\nt\n \nc\nr\ne\na\nt\ni\no\nn\n.\n \nR\ne\ni\nt\ne\nr\na\nt\ne\n \nt\nh\ne\n \np\ne\nr\ns\ni\ns\nt\ne\nn\nt\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \no\np\ne\nn\n \nr\ne\ns\ne\na\nr\nc\nh\n \nq\nu\ne\ns\nt\ni\no\nn\ns\n \ni\nn\n \nt\nh\ne\n \nf\ni\ne\nl\nd\n \na\nn\nd\n \np\nr\no\np\no\ns\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nf\nu\nt\nu\nr\ne\n \nr\ne\ns\ne\na\nr\nc\nh\n \nd\ni\nr\ne\nc\nt\ni\no\nn\ns\n \nb\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \ng\na\np\ns\n \na\nn\nd\n \ne\nm\ne\nr\ng\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \ni\nn\n \nt\nh\ne\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n.\n \nE\nn\ns\nu\nr\ne\n \nt\nh\ne\n \np\nr\no\np\no\ns\ne\nd\n \nf\nu\nt\nu\nr\ne\n \nd\ni\nr\ne\nc\nt\ni\no\nn\ns\n \na\nr\ne\n \ns\np\ne\nc\ni\nf\ni\nc\n \na\nn\nd\n \na\nc\nt\ni\no\nn\na\nb\nl\ne\n,\n \nc\no\nn\nt\nr\ni\nb\nu\nt\ni\nn\ng\n \nt\no\n \nt\nh\ne\n \na\nd\nv\na\nn\nc\ne\nm\ne\nn\nt\n \no\nf\n \nt\nh\ne\n \nf\ni\ne\nl\nd\n.\n \nE\nn\ns\nu\nr\ne\n \nt\nh\ne\n \np\nr\no\np\no\ns\ne\nd\n \nf\nu\nt\nu\nr\ne\n \nd\ni\nr\ne\nc\nt\ni\no\nn\ns\n \na\nr\ne\n \nn\no\nt\n \nj\nu\ns\nt\n \ng\ne\nn\ne\nr\na\nl\n \ns\nt\na\nt\ne\nm\ne\nn\nt\ns\n \nb\nu\nt\n \na\nr\ne\n \ns\np\ne\nc\ni\nf\ni\nc\n \na\nn\nd\n \na\nc\nt\ni\no\nn\na\nb\nl\ne\n \nr\ne\ns\ne\na\nr\nc\nh\n \na\ng\ne\nn\nd\na\ns\n.\n \nE\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\n \nt\nh\ne\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \nn\na\nt\nu\nr\ne\n \no\nf\n \nt\nh\ne\ns\ne\n \nf\nu\nt\nu\nr\ne\n \nd\ni\nr\ne\nc\nt\ni\no\nn\ns\n,\n \ni\nn\nd\ni\nc\na\nt\ni\nn\ng\n \nh\no\nw\n \nc\no\nl\nl\na\nb\no\nr\na\nt\ni\no\nn\ns\n \no\nr\n \ni\nn\nt\ne\ng\nr\na\nt\ni\no\nn\ns\n \nw\ni\nt\nh\n \no\nt\nh\ne\nr\n \nf\ni\ne\nl\nd\ns\n \nc\no\nu\nl\nd\n \nb\ne\n \nc\nr\nu\nc\ni\na\nl\n \nf\no\nr\n \na\nd\nv\na\nn\nc\ni\nn\ng\n \nd\ni\nr\ne\nc\nt\n \ns\np\ne\ne\nc\nh\n-\nt\no\n-\nt\ne\nx\nt\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nI\nd\ne\nn\nt\ni\nf\ny\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nt\nr\ne\nn\nd\ns\n \na\nn\nd\n \np\na\nt\nt\ne\nr\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \na\nn\na\nl\ny\nz\ne\nd\n \ns\ne\nc\nt\ni\no\nn\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \nk\ne\ny\n \na\nd\nv\na\nn\nc\ne\nm\ne\nn\nt\ns\n,\n \nr\ne\nm\na\ni\nn\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n,\n \na\nn\nd\n \nf\nu\nt\nu\nr\ne\n \nd\ni\nr\ne\nc\nt\ni\no\nn\ns\n \ni\nn\n \nd\ni\nr\ne\nc\nt\n \ns\np\ne\ne\nc\nh\n-\nt\no\n-\nt\ne\nx\nt\n \nt\nr\na\nn\ns\nl\na\nt\ni\no\nn\n.\n \nS\ny\ns\nt\ne\nm\na\nt\ni\nc\na\nl\nl\ny\n \nc\no\nm\np\na\nr\ne\n \na\nn\nd\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\nh\ne\n \nk\ne\ny\n \nf\ni\nn\nd\ni\nn\ng\ns\n \na\nn\nd\n \nc\no\nn\nc\nl\nu\ns\ni\no\nn\ns\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \ns\nu\nr\nv\ne\ny\ne\nd\n \ns\ne\nc\nt\ni\no\nn\ns\n,\n \ne\nx\np\nl\ni\nc\ni\nt\nl\ny\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \na\nr\ne\na\ns\n \no\nf\n \nc\no\nr\nr\no\nb\no\nr\na\nt\ni\no\nn\n,\n \nc\no\nn\nf\nl\ni\nc\nt\n,\n \na\nn\nd\n \nk\ne\ny\n \nd\ni\ns\nc\nr\ne\np\na\nn\nc\ni\ne\ns\n \nr\ne\ng\na\nr\nd\ni\nn\ng\n \nt\nh\ne\n \no\nv\ne\nr\na\nl\nl\n \np\nr\no\ng\nr\ne\ns\ns\n \na\nn\nd\n \nf\nu\nt\nu\nr\ne\n \no\nu\nt\nl\no\no\nk\n \no\nf\n \nt\nh\ne\n \nf\ni\ne\nl\nd\n.\n \nA\nn\na\nl\ny\nz\ne\n \nt\nh\ne\n \np\no\nt\ne\nn\nt\ni\na\nl\n \nr\ne\na\ns\no\nn\ns\n \nf\no\nr\n \nt\nh\ne\ns\ne\n \nv\na\nr\ni\na\nt\ni\no\nn\ns\n,\n \nc\no\nn\ns\ni\nd\ne\nr\ni\nn\ng\n \nf\na\nc\nt\no\nr\ns\n \ns\nu\nc\nh\n \na\ns\n \nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\ns\n \ni\nn\n \nt\nh\ne\n \ns\nc\no\np\ne\n \na\nn\nd\n \nf\no\nc\nu\ns\n \no\nf\n \ne\na\nc\nh\n \ns\ne\nc\nt\ni\no\nn\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nk\ne\ny\n \na\ns\np\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \nd\ni\ns\nc\nu\ns\ns\ne\nd\n \ns\ne\nc\nt\ni\no\nn\ns\n \ni\nn\n \na\n \ns\nt\nr\nu\nc\nt\nu\nr\ne\nd\n \nf\no\nr\nm\na\nt\n,\n \nh\ni\ng\nh\nl\ni\ng\nh\nt\ni\nn\ng\n \n'\nK\ne\ny\n \nA\nd\nv\na\nn\nc\ne\nm\ne\nn\nt\n'\n,\n \n'\nR\ne\nm\na\ni\nn\ni\nn\ng\n \nC\nh\na\nl\nl\ne\nn\ng\ne\n'\n,\n \n'\nF\nu\nt\nu\nr\ne\n \nD\ni\nr\ne\nc\nt\ni\no\nn\n'\n,\n \n'\nI\nm\np\na\nc\nt\n \no\nn\n \nt\nh\ne\n \nF\ni\ne\nl\nd\n'\n,\n \na\nn\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\n \nA\nn\na\nl\ny\ns\ni\ns\n'\n.\n \nI\nn\n \nD\ni\ng\ne\ns\nt\n \nA\nn\na\nl\ny\ns\ni\ns\n,\n \ni\nn\nc\nl\nu\nd\ne\n \na\n \n\"\nC\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nL\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n\"\n \ns\nu\nb\ns\ne\nc\nt\ni\no\nn\n \nt\no\n \ns\nu\nm\nm\na\nr\ni\nz\ne\n \nt\nh\ne\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nn\nd\n \nu\nn\nr\ne\ns\no\nl\nv\ne\nd\n \ni\ns\ns\nu\ne\ns\n \ni\nn\n \nt\nh\ne\n \nc\no\nn\nc\nl\nu\ns\ni\no\nn\n.\n \nB\na\ns\ne\nd\n \no\nn\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \n'\nD\ne\nf\ne\nc\nt\n \nL\na\nb\ne\nl\ns\n'\n \na\nc\nr\no\ns\ns\n \nt\nh\ne\n \nr\ne\nv\ni\ne\nw\ne\nd\n \nl\ni\nt\ne\nr\na\nt\nu\nr\ne\n \ni\nn\n \nt\nh\ni\ns\n \ns\ne\nc\nt\ni\no\nn\n,\n \nt\nh\ne\n \no\nv\ne\nr\na\nr\nc\nh\ni\nn\ng\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \na\nr\ne\n.\n.\n.\n \nD\ne\nl\nv\ne\n \ni\nn\nt\no\n \nt\nh\ne\n \nu\nn\nd\ne\nr\nl\ny\ni\nn\ng\n \nc\na\nu\ns\ne\ns\n \no\nf\n \nt\nh\ne\n \ni\nd\ne\nn\nt\ni\nf\ni\ne\nd\n \nc\nh\na\nl\nl\ne\nn\ng\ne\ns\n \na\nn\nd\n \nl\ni\nm\ni\nt\na\nt\ni\no\nn\ns\n \ni\nn\n \nt\nh\ne\n \nc\no\nn\nc\nl\nu\ns\ni\no\nn\n \ns\ne\nc\nt\ni\no\nn\n.\n \nF\nr\no\nm\n \na\nn\n \ni\nn\nt\ne\nr\nd\ni\ns\nc\ni\np\nl\ni\nn\na\nr\ny\n \np\ne\nr\ns\np\ne\nc\nt\ni\nv\ne\n,\n \nc\no\nn\ns\ni\nd\ne\nr\n \nr\ne\nl\ne\nv\na\nn\nt\n \ni\nn\ns\ni\ng\nh\nt\ns\n \nf\nr\no\nm\n \nv\na\nr\ni\no\nu\ns\n \nf\ni\ne\nl\nd\ns\n \nt\no\n \no\nf\nf\ne\nr\n \na\n \nb\nr\no\na\nd\ne\nr\n \nu\nn\nd\ne\nr\ns\nt\na\nn\nd\ni\nn\ng\n \no\nf\n \nt\nh\ne\n \no\nv\ne\nr\na\nl\nl\n \ns\nu\nr\nv\ne\ny\n \na\nn\nd\n \ni\nt\ns\n \ni\nm\np\nl\ni\nc\na\nt\ni\no\nn\ns\n."}
